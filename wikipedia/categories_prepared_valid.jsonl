{"prompt":"Category 5 cable is used in structured cabling for computer networks such as Ethernet over twisted pair. The cable standard prescribes performance parameters for frequencies up to 100 MHz and is suitable for 10BASE-T, 100BASE-TX (Fast Ethernet), 1000BASE-T (Gigabit Ethernet), and 2.5GBASE-T. 10BASE-T and 100BASE-TX Ethernet connections require two wire pairs. 1000BASE-T and faster Ethernet connections require four wire pairs. Through the use of power over Ethernet (PoE), power can be carried over the cable in addition to Ethernet data. \nCat 5 is also used to carry other signals such as telephony and video. In some cases, multiple signals can be carried on a single cable; Cat 5 can carry two conventional telephone lines as well as 100BASE-TX in a single cable. The USOC\/RJ-61 wiring standard may be used in multi-line telephone connections. Various schemes exist for transporting both analog and digital video over the cable. HDBaseT (10.2 Gbit\/s) is one such scheme.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Within the IEEE 802.11 Working Group, the following IEEE Standards Association Standard and Amendments exist:\n\nIEEE 802.11-1997: The WLAN standard was originally 1 Mbit\/s and 2 Mbit\/s, 2.4 GHz RF and infrared (IR) standard (1997), all the others listed below are Amendments to this standard, except for Recommended Practices 802.11F and 802.11T.\nIEEE 802.11a: 54 Mbit\/s, 5 GHz standard (1999, shipping products in 2001)\nIEEE 802.11b: 5.5 Mbit\/s and 11 Mbit\/s, 2.4 GHz standard (1999)\nIEEE 802.11c: Bridge operation procedures; included in the IEEE 802.1D standard (2001)\nIEEE 802.11d: International (country-to-country) roaming extensions (2001)\nIEEE 802.11e: Enhancements: QoS, including packet bursting (2005)\nIEEE 802.11F: Inter-Access Point Protocol (2003) Withdrawn February 2006\nIEEE 802.11g: 54 Mbit\/s, 2.4 GHz standard (backwards compatible with b) (2003)\nIEEE 802.11h: Spectrum Managed 802.11a (5 GHz) for European compatibility (2004)\nIEEE 802.11i: Enhanced security (2004)\nIEEE 802.11j: Extensions for Japan (4.9-5.0 GHz) (2004)\nIEEE 802.11-2007: A new release of the standard that includes amendments a, b, d, e, g, h, i, and j. (July 2007)\nIEEE 802.11k: Radio resource measurement enhancements (2008)\nIEEE 802.11n: Higher Throughput WLAN at 2.4 and 5 GHz; 20 and 40 MHz channels; introduces MIMO to Wi-Fi (September 2009)\nIEEE 802.11p: WAVE—Wireless Access for the Vehicular Environment (such as ambulances and passenger cars) (July 2010)\nIEEE 802.11r: Fast BSS transition (FT) (2008)\nIEEE 802.11s: Mesh Networking, Extended Service Set (ESS) (July 2011)\nIEEE 802.11T: Wireless Performance Prediction (WPP)—test methods and metrics Recommendation cancelled\nIEEE 802.11u: Improvements related to HotSpots and 3rd-party authorization of clients, e.g., cellular network offload (February 2011)\nIEEE 802.11v: Wireless network management (February 2011)\nIEEE 802.11w: Protected Management Frames (September 2009)\nIEEE 802.11y: 3650–3700 MHz Operation in the U.S. (2008)\nIEEE 802.11z: Extensions to Direct Link Setup (DLS) (September 2010)\nIEEE 802.11-2012: A new release of the standard that includes amendments k, n, p, r, s, u, v, w, y, and z (March 2012)\nIEEE 802.11aa: Robust streaming of Audio Video Transport Streams (June 2012) - see Stream Reservation Protocol\nIEEE 802.11ac: Very High Throughput WLAN at 5 GHz; wider channels (80 and 160 MHz); Multi-user MIMO (down-link only) (December 2013)\nIEEE 802.11ad: Very High Throughput 60 GHz (December 2012) — see also WiGig\nIEEE 802.11ae: Prioritization of Management Frames (March 2012)\nIEEE 802.11af: TV Whitespace (February 2014)\nIEEE 802.11-2016: A new release of the standard that includes amendments aa, ac, ad, ae, and af (December 2016)\nIEEE 802.11ah: Sub-1 GHz license exempt operation (e.g., sensor network, smart metering) (December 2016)\nIEEE 802.11ai: Fast Initial Link Setup (December 2016)\nIEEE 802.11aj: China Millimeter Wave (February 2018)\nIEEE 802.11ak: Transit Links within Bridged Networks (June 2018)\nIEEE 802.11aq: Pre-association Discovery (July 2018)\nIEEE 802.11-2020: A new release of the standard that includes amendments ah, ai, aj, ak, and aq (December 2020)\nIEEE 802.11ax: High Efficiency WLAN at 2.4, 5 and 6 GHz; introduces OFDMA to Wi-Fi (February 2021)\nIEEE 802.11ay: Enhancements for Ultra High Throughput in and around the 60 GHz Band (March 2021)\nIEEE 802.11az: Next Generation Positioning (March 2023)\nIEEE 802.11ba: Wake Up Radio (March 2021)\nIEEE 802.11bd: Enhancements for Next Generation V2X (see also IEEE 802.11p) (March 2023)\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Computer operating systems provide various diagnostic tools to examine network interfaces and address configuration. Microsoft Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems may use ifconfig, netstat, route, lanstat, fstat, and iproute2 utilities to accomplish the task.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"A certificate may be revoked before it expires, for example because the secrecy of the private key has been compromised. Newer versions of popular browsers such as Firefox, Opera, and Internet Explorer on Windows Vista implement the Online Certificate Status Protocol (OCSP) to verify that this is not the case. The browser sends the certificate's serial number to the certificate authority or its delegate via OCSP (Online Certificate Status Protocol) and the authority responds, telling the browser whether the certificate is still valid or not. The CA may also issue a  CRL to tell people that these certificates are revoked. CRLs are no longer required by the CA\/Browser forum, nevertheless, they are still widely used by the CAs. Most revocation statuses on the Internet disappear soon after the expiration of the certificates.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"An IPv4 address has a size of 32 bits, which limits the address space to 4294967296 (232) addresses. Of this number, some addresses are reserved for special purposes such as private networks (~18 million addresses) and multicast addressing (~270 million addresses).\nIPv4 addresses are usually represented in dot-decimal notation, consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., 192.0.2.1.  Each part represents a group of 8 bits (an octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable Internet access or digital subscriber line (DSL) provider.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:\n\n\"The Arpanet period is somewhat well documented because the corporation in charge – BBN – left a physical record. Moving into the NSFNET era, it became an extraordinarily decentralized process. The record exists in people's basements, in closets. ... So much of what happened was done verbally and on the basis of individual trust.\"\nNotable works on the subject were published by Katie Hafner and Matthew Lyon, Where Wizards Stay Up Late: The Origins Of The Internet (1996), Roy Rosenzweig, \"Wizards, Bureaucrats, Warriors, and Hackers: Writing the History of the Internet\" (1998), and Janet Abbate, Inventing the Internet (2000).\nMost scholarship and literature on the Internet lists ARPANET as the prior network that was iterated on and studied to create it, although other early computer networks and experiments existed alongside or before ARPANET.These histories of the Internet have since been characterized as \"teleologies\" or \"Whig history\"; that is, they take the present to be the end point toward which history has been unfolding based on a single cause:\n\nIn the case of Internet history, the epoch-making event is usually said to be the demonstration of the 4-node ARPANET network in 1969. From that single happening the global Internet developed.\nIn addition to these characteristics, historians have cited methodological problems arising in their work:\n\n\"Internet history\" ... tends to be too close to its sources. Many Internet pioneers are alive, active, and eager to shape the histories that describe their accomplishments. Many museums and historians are equally eager to interview the pioneers and to publicize their stories.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The 2007 cyberattacks on Estonia were a series of cyberattacks that began on 27 April 2007 and targeted websites of Estonian organizations, including Estonian parliament, banks, ministries, newspapers, and broadcasters, amid the country's disagreement with Russia about the relocation of the Bronze Soldier of Tallinn, an elaborate Soviet-era grave marker, as well as war graves in Tallinn. The attacks triggered a number of military organizations around the world to reconsider the importance of network security to modern military doctrine. The direct result of the cyberattacks was the creation of the NATO Cooperative Cyber Defence Centre of Excellence in Tallinn.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"IEEE 802.11ah, published in 2017, defines a WLAN system operating at sub-1 GHz license-exempt bands. Due to the favorable propagation characteristics of the low frequency spectra, 802.11ah can provide improved transmission range compared with the conventional 802.11 WLANs operating in the 2.4 GHz and 5 GHz bands. 802.11ah can be used for various purposes including large scale sensor networks, extended range hotspot, and outdoor Wi-Fi for cellular traffic offloading, whereas the available bandwidth is relatively narrow. The protocol intends consumption to be competitive with low power Bluetooth, at a much wider range.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The process of change that generally coincided with \"Web 2.0\" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices.  This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information \"on the move\" – and used socially, as opposed to items on a desk at home or just used for work.Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as \"m.website.com\") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term \"App\" emerged (short for \"Application program\" or \"Program\") as did the \"App store\".\nThis \"mobile revolution\" has allowed for people to have a nearly unlimited amount of information at their fingertips. With the ability to access the internet from cell phones came a change in the way we consume media. In fact, looking at media consumption statistics, over half of media consumption between those aged 18 and 34 were using a smartphone.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The Internet allows greater flexibility in working hours and location, especially with the spread of unmetered high-speed connections. The Internet can be accessed almost anywhere by numerous means, including through mobile Internet devices. Mobile phones, datacards, handheld game consoles and cellular routers allow users to connect to the Internet wirelessly. Within the limitations imposed by small screens and other limited facilities of such pocket-sized devices, the services of the Internet, including email and the web, may be available. Service providers may restrict the services offered and mobile data charges may be significantly higher than other access methods.\nEducational material at all levels from pre-school to post-doctoral is available from websites. Examples range from CBeebies, through school and high-school revision guides and virtual universities, to access to top-end scholarly literature through the likes of Google Scholar. For distance education, help with homework and other assignments, self-guided learning, whiling away spare time or just looking up more detail on an interesting fact, it has never been easier for people to access educational information at any level from anywhere. The Internet in general and the World Wide Web in particular are important enablers of both formal and informal education. Further, the Internet allows researchers (especially those from the social and behavioral sciences) to conduct research remotely via virtual laboratories, with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results.The low cost and nearly instantaneous sharing of ideas, knowledge, and skills have made collaborative work dramatically easier, with the help of collaborative software. Not only can a group cheaply communicate and share ideas but the wide reach of the Internet allows such groups more easily to form. An example of this is the free software movement, which has produced, among other things, Linux, Mozilla Firefox, and OpenOffice.org (later forked into LibreOffice). Internet chat, whether using an IRC chat room, an instant messaging system, or a social networking service, allows colleagues to stay in touch in a very convenient way while working at their computers during the day. Messages can be exchanged even more quickly and conveniently than via email. These systems may allow files to be exchanged, drawings and images to be shared, or voice and video contact between team members.\nContent management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work. Business and project teams can share calendars as well as documents and other information. Such collaboration occurs in a wide variety of areas including scientific research, software development, conference planning, political activism and creative writing. Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread.\nThe Internet allows computer users to remotely access other computers and information stores easily from any access point. Access may be with computer security, i.e. authentication and encryption technologies, depending on the requirements. This is encouraging new ways of remote work, collaboration and information sharing in many industries. An accountant sitting at home can audit the books of a company based in another country, on a server situated in a third country that is remotely maintained by IT specialists in a fourth. These accounts could have been created by home-working bookkeepers, in other remote locations, based on information emailed to them from offices all over the world. Some of these things were possible before the widespread use of the Internet, but the cost of private leased lines would have made many of them infeasible in practice. An office worker away from their desk, perhaps on the other side of the world on a business trip or a holiday, can access their emails, access their data using cloud computing, or open a remote desktop session into their office PC using a secure virtual private network (VPN) connection on the Internet. This can give the worker complete access to all of their normal files and data, including email and other applications, while away from the office. It has been referred to among system administrators as the Virtual Private Nightmare, because it extends the secure perimeter of a corporate network into remote locations and its employees' homes.\nBy late 2010s Internet has been described as \"the main source of scientific information \"for the majority of the global North population\".: 111\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The technological advancements and practical applications achieved through the ARPANET were instrumental in shaping modern computer networking including the Internet. Development and implementation of the concepts of packet switching, decentralized communication, and the development of protocols like TCP\/IP laid the foundation for a global network that revolutionized communication, information sharing and collaborative research across the world.The ARPANET was related to many other research projects, which either influenced the ARPANET design, or which were ancillary projects or spun out of the ARPANET.\nSenator Al Gore authored the High Performance Computing and Communication Act of 1991, commonly referred to as \"The Gore Bill\", after hearing the 1988 concept for a National Research Network submitted to Congress by a group chaired by Leonard Kleinrock. The bill was passed on 9 December 1991 and led to the National Information Infrastructure (NII) which Gore called the information superhighway.\nThe ARPANET project was honored with two IEEE Milestones, both dedicated in 2009.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Core networks typically provided the following functionality:\n\nAggregation: The highest level of aggregation in a service provider network. The next level in the hierarchy under the core nodes is the distribution networks and then the edge networks. Customer-premises equipment (CPE) do not normally connect to the core networks of a large service provider.\nAuthentication: The function to decide whether the user requesting a service from the telecom network is authorized to do so within this network or not.\nCall Control\/Switching: call control or switching functionality decides the future course of call based on the call signalling processing. E.g. switching functionality may decide based on the \"called number\" that the call be routed towards a subscriber within this operator's network or with number portability more prevalent to another operator's network.\nCharging: This functionality of the collation and processing of charging data generated by various network nodes. Two common types of charging mechanisms found in present-day networks are prepaid charging and postpaid charging. See Automatic Message Accounting\nService Invocation: Core network performs the task of service invocation for its subscribers. Service invocation may happen based on some explicit action (e.g. call transfer) by user or implicitly (call waiting). It's important to note however that service \"execution\" may or may not be a core network functionality as third party network\/nodes may take part in actual service execution.\nGateways: Gateways shall be present in the core network to access other networks. Gateway functionality is dependent on the type of network it interfaces with.Physically, one or more of these logical functionalities may simultaneously exist in a given core network node.\nBesides above-mentioned functionalities, the following also formed part of a telecommunications core network:\n\nO&M: Operations & Maintenance centre or Operations Support Systems to configure and provision the core network nodes. Number of subscribers, peak hour call rate, nature of services, geographical preferences are some of the factors which impact the configuration. Network statistics collection (Performance Management), alarm monitoring (Fault Management) and logging of various network nodes actions (Event Management) also happens in the O&M centre. These stats, alarms and traces form important tools for a network operator to monitor the network health and performance and improvise on the same.\nSubscriber Database: Core network also hosts the subscribers database (e.g. HLR in GSM systems). Subscriber database is accessed by core network nodes for functions like authentication, profiling, service invocation etc.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The volume of Internet traffic is difficult to measure because no single point of measurement exists in the multi-tiered, non-hierarchical topology. Traffic data may be estimated from the aggregate volume through the peering points of the Tier 1 network providers, but traffic that stays local in large provider networks may not be accounted for.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"10.3125 Gbaud with NRZ (\"PAM2\") and 64b66b on 10 lanes per direction\nOne of the earliest coding used, this widens the coding scheme used in single lane 10GE and quad lane 40G to use 10 lanes. Due to the low symbol rate, relatively long ranges can be achieved at the cost of using a lot of cabling.\nThis also allows breakout to 10×10GE, provided that the hardware supports splitting the port.25.78125 Gbaud with NRZ (\"PAM2\") and 64b66b on 4 lanes per direction\nA sped-up variant of the above, this directly corresponds to 10GE\/40GE signalling at 2.5× speed. The higher symbol rate makes links more susceptible to errors.\nIf the device and transceiver support dual-speed operation, it is possible to reconfigure an 100G port to downspeed to 40G or 4×10G. There is no autonegotiation protocol for this, thus manual configuration is necessary.  Similarly, a port can be broken into 4×25G if implemented in the hardware. This is applicable even for CWDM4, if a CWDM demultiplexer and CWDM 25G optics are used appropriately.25.78125 Gbaud with NRZ (\"PAM2\") and RS-FEC(528,514) on 4 lanes per direction\nTo address the higher susceptibility to errors at these symbol rates, an application of Reed–Solomon error correction was defined in IEEE 802.3bj \/ Clause 91.  This replaces the 64b66b encoding with a 256b257b encoding followed by the RS-FEC application, which combines to the exact same overhead as 64b66b.  To the optical transceiver or cable, there is no distinction between this and 64b66b;  some interface types (e.g. CWDM4) are defined \"with or without FEC.\"26.5625 Gbaud with PAM4 and RS-FEC(544,514) on 2 lanes per direction\nThis achieves a further doubling in bandwidth per lane (used to halve the number of lanes) by employing pulse-amplitude modulation with 4 distinct analog levels, making each symbol carry 2 bits.  To keep up error margins, the FEC overhead is doubled from 2.7% to 5.8%, which explains the slight rise in symbol rate.53.125 Gbaud with PAM4 and RS-FEC(544,514) on 1 lane per direction\nFurther pushing silicon limits, this is a double rate variant of the previous, giving full 100GE operation over 1 medium lane.30.14475 Gbaud with DP-DQPSK and SD-FEC on 1 lane per direction\nMirroring OTN4 developments, DP-DQPSK (dual polarization differential quadrature phase shift keying) employs polarization to carry one axis of the DP-QPSK constellation.  Additionally, new soft decision FEC algorithms take additional information on analog signal levels as input to the error correction procedure.13.59375 Gbaud with PAM4, KP4 specific coding and RS-FEC(544,514) on 4 lanes per direction\nA half-speed variant of 26.5625 Gbaud with RS-FEC, with a 31320\/31280 step encoding the lane number into the signal, and further 92\/90 framing.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"In the early stages of development of the Internet Protocol, the network number was always the highest order octet (most significant eight bits). Because this method allowed for only 256 networks, it soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the addressing specification was revised with the introduction of classful network architecture.Classful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the class of the address. Three classes (A, B, and C) were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes (B and C). The following table gives an overview of this now-obsolete system.\n\nClassful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of networking in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes. Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"There have been various attempts at transporting data over exotic media:\n\nIP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.\nExtending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.Both cases have a large round-trip delay time, which gives slow two-way communication, but does not prevent sending large amounts of information.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Although the present-day, loose use of the term \"cyberspace\" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location.  It is for these reasons cyberspace has been described as the ultimate tax haven.In 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"DHCP is used to assign internal IP addresses to members of a home network. A DHCP server typically runs on the router with end devices as its clients. The router itself is a client of the external DHCP servers owned by the internet service provider. All DHCP clients request configuration settings using the DHCP protocol in order to acquire their IP address, a default route and one or more DNS server addresses. Once the client implements these settings, it will be able to communicate on that internet.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.\nIn packet-switched networks, routing protocols direct packet forwarding through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though because they lack specialized hardware, may offer limited performance. The routing process directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\nRouting can be contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices.  In large networks, the structured addressing used by routers outperforms unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"While at MIT in the 1950s, Licklider worked on Semi-Automatic Ground Environment (SAGE), a Cold War project to create a computer-aided air defense system. The SAGE system included computers that collected and presented data to a human operator, who then chose the appropriate response. He worked as a human factors expert, which helped convince him of the great potential for human\/computer interfaces.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.\nIn time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Security experts, such as Bruce Schneier, have demanded that voting machine source code should be publicly available for inspection. Others have also suggested publishing voting machine software under a free software license as is done in Australia.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The Ethernet physical layer evolved over a considerable time span and encompasses coaxial, twisted pair and fiber-optic physical media interfaces, with speeds from 1 Mbit\/s to 400 Gbit\/s. The first introduction of twisted-pair CSMA\/CD was StarLAN, standardized as 802.3 1BASE5. While 1BASE5 had little market penetration, it defined the physical apparatus (wire, plug\/jack, pin-out, and wiring plan) that would be carried over to 10BASE-T through 10GBASE-T.\nThe most common forms used are 10BASE-T, 100BASE-TX, and 1000BASE-T. All three use twisted-pair cables and 8P8C modular connectors. They run at 10 Mbit\/s, 100 Mbit\/s, and 1 Gbit\/s, respectively.Fiber optic variants of Ethernet (that commonly use SFP modules) are also very popular in larger networks, offering high performance, better electrical isolation and longer distance (tens of kilometers with some versions). In general, network protocol stack software will work similarly on all varieties.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Ethernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"J. C. R. Licklider, while working at BBN, proposed a computer network in his March 1960 paper Man-Computer Symbiosis:\nA network of such centers, connected to one another by wide-band communication lines [...] the functions of present-day libraries together with anticipated advances in information storage and retrieval and symbiotic functions suggested earlier in this paper\nIn August 1962, Licklider and Welden Clark published the paper \"On-Line Man-Computer Communication\" which was one of the first descriptions of a networked future.\nIn October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within ARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos in 1963 describing a distributed network to the IPTO staff, whom he called \"Members and Affiliates of the Intergalactic Computer Network\".Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for one of his successors, Robert Taylor, to initiate the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Certain devices on a home network are primarily concerned with enabling or supporting the communications of the kinds of end devices home-dwellers more directly interact with. Unlike their data center counterparts, these \"networking\" devices are compact and passively cooled,  aiming to be as hands-off and non-obtrusive as possible:\n\nA gateway establishes physical and data link layer connectivity to a WAN over a service provider's native telecommunications infrastructure. Such devices typically contain a cable, DSL, or optical modem bound to a network interface controller for Ethernet. Routers are often incorporated into these devices for additional convenience.\nA router establishes network layer connectivity between a WAN and the home network. It also performs the key function of network address translation that allows independently addressed devices within the same home network to establish transport layer connections across the WAN from a single, outward-facing WAN IP address. These devices often come with an integrated wireless access point and 4-port Ethernet switch.\nA network switch is used to allow devices on the home network to talk to one another via Ethernet. While the needs of most home networks are satisfied with the built-in wireless and\/or switching capabilities of their router, some situations require the addition of a separate switch with advanced capabilities. For example:\nA typical home router has 4 to 6 Ethernet LAN ports, so a router's switching capacity could be exceeded.\nA network device might require a non-standard port feature such as power over Ethernet (PoE). (IP cameras and IP phones)\nA wireless access point is required for connecting wireless devices to a network. When a router includes this device, it is referred to as a wireless router.\nA home automation or smart home controller acts as a gateway and router for low-power wireless networks of simple, non-data-intensive devices such as light bulbs and locks.\nA network bridge binds two different network interfaces to each other, often in order to grant a wired-only device access to a wireless network medium.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"There are two parts to a flow spec:\n\nWhat does the traffic look like? Done in the Traffic SPECification part, also known as TSPEC.\nWhat guarantees does it need? Done in the service Request SPECification part, also known as RSPEC.TSPECs include token bucket algorithm parameters. The idea is that there is a token bucket which slowly fills up with tokens, arriving at a constant rate. Every packet which is sent requires a token, and if there are no tokens, then it cannot be sent. Thus, the rate at which tokens arrive dictates the average rate of traffic flow, while the depth of the bucket dictates how 'bursty' the traffic is allowed to be.\nTSPECs typically just specify the token rate and the bucket depth. For example, a video with a refresh rate of 75 frames per second, with each frame taking 10 packets, might specify a token rate of 750 Hz, and a bucket depth of only 10. The bucket depth would be sufficient to accommodate the 'burst' associated with sending an entire frame all at once. On the other hand, a conversation would need a lower token rate, but a much higher bucket depth. This is because there are often pauses in conversations, so they can make do with less tokens by not sending the gaps between words and sentences. However, this means the bucket depth needs to be increased to compensate for the traffic being burstier.\nRSPECs specify what requirements there are for the flow: it can be normal internet 'best effort', in which case no reservation is needed. This setting is likely to be used for webpages, FTP, and similar applications. The 'Controlled Load' setting mirrors the performance of a lightly loaded network: there may be occasional glitches when two people access the same resource by chance, but generally both delay and drop rate are fairly constant at the desired rate. This setting is likely to be used by soft QoS applications. The 'Guaranteed' setting gives an absolutely bounded service, where the delay is promised to never go above a desired amount, and packets never dropped, provided the traffic stays within spec.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.\nThe following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:\n\nCircuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo.\nATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique, and modem enhancements.There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Network traffic entering a DiffServ domain is subjected to classification and conditioning.  A traffic classifier may inspect many different parameters in incoming packets, such as source address, destination address or traffic type and assign individual packets to a specific traffic class.  Traffic classifiers may honor any DiffServ markings in received packets or may elect to ignore or override those markings.  For tight control over volumes and type of traffic in a given class, a network operator may choose not to honor markings at the ingress to the DiffServ domain. Traffic in each class may be further conditioned by subjecting the traffic to rate limiters, traffic policers or shapers.: §3 The per-hop behavior is determined by the DS field in the IP header. The DS field contains the 6-bit DSCP value. Explicit Congestion Notification (ECN) occupies the least-significant 2 bits of the IPv4 TOS field and IPv6 traffic class (TC) field.In theory, a network could have up to 64 different traffic classes using the 64 available DSCP values. The DiffServ RFCs recommend, but do not require, certain encodings. This gives a network operator great flexibility in defining traffic classes. In practice, however, most networks use the following commonly defined per-hop behaviors:\n\nDefault Forwarding (DF) PHB — which is typically best-effort traffic\nExpedited Forwarding (EF) PHB — dedicated to low-loss, low-latency traffic\nAssured Forwarding (AF) PHB — gives assurance of delivery under prescribed conditions\nClass Selector PHBs — which maintain backward compatibility with the IP precedence field.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"MAC may refer to the sublayer that determines who is allowed to access the media at any one time (e.g. CSMA\/CD). Other times it refers to a frame structure delivered based on MAC addresses inside.\nThere are generally two forms of media access control: distributed and centralized. Both of these may be compared to communication between people. In a network made up of people speaking, i.e. a conversation, they will each pause a random amount of time and then attempt to speak again, effectively establishing a long and elaborate game of saying \"no, you first\".\nThe Media Access Control sublayer also performs frame synchronization, which determines the start and end of each frame of data in the transmission bitstream. It entails one of several methods: timing-based detection, character counting, byte stuffing, and bit stuffing.\n\nThe time-based approach expects a specified amount of time between frames.\nCharacter counting tracks the count of remaining characters in the frame header. This method, however, is easily disturbed if this field is corrupted.\nByte stuffing precedes the frame with a special byte sequence such as DLE STX and succeeds it with DLE ETX. Appearances of DLE (byte value 0x10) have to be escaped with another DLE. The start and stop marks are detected at the receiver and removed as well as the inserted DLE characters.\nSimilarly, bit stuffing replaces these start and end marks with flags consisting of a special bit pattern (e.g. a 0, six 1 bits and a 0). Occurrences of this bit pattern in the data to be transmitted are avoided by inserting a bit. To use the example where the flag is 01111110, a 0 is inserted after 5 consecutive 1's in the data stream. The flags and the inserted 0's are removed at the receiving end. This makes for arbitrary long frames and easy synchronization for the recipient. The stuffed bit is added even if the following data bit is 0, which could not be mistaken for a sync sequence, so that the receiver can unambiguously distinguish stuffed bits from normal bits.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Cyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Most social, biological, and technological networks display substantial non-trivial topological features, with patterns of connection between their elements that are neither purely regular nor purely random. Such features include a heavy tail in the degree distribution, a high clustering coefficient, assortativity or disassortativity among vertices, community structure, and hierarchical structure. In the case of directed networks these features also include reciprocity, triad significance profile and other features. In contrast, many of the mathematical models of networks that have been studied in the past, such as lattices and random graphs, do not show these features. The most complex structures can be realized by networks with a medium number of interactions. This corresponds to the fact that the maximum information content (entropy) is obtained for medium probabilities.\nTwo well-known and much studied classes of complex networks are scale-free networks and small-world networks, whose discovery and definition are canonical case-studies in the field. Both are characterized by specific structural features—power-law degree distributions for the former and short path lengths and high clustering for the latter. However, as the study of complex networks has continued to grow in importance and popularity, many other aspects of network structures have attracted attention as well.\nThe field continues to develop at a brisk pace, and has brought together researchers from many areas including mathematics, physics, electric power systems, biology, climate, computer science, sociology, epidemiology, and others. Ideas and tools from network science and engineering have been applied to the analysis of metabolic and genetic regulatory networks; the study of ecosystem stability and robustness; clinical science; the modeling and design of scalable communication networks such as the generation and visualization of complex wireless networks;  and a broad range of other practical issues.  Network science is the topic of many conferences in a variety of different fields, and has been the subject of numerous books both for the lay person and for the expert.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Europe is a major contributor to the growth of the international backbone as well as a contributor to the growth of Internet bandwidth. In 2003, Europe was credited with 82 percent of the world's international cross-border bandwidth. The company Level 3 Communications began to launch a line of dedicated Internet access and virtual private network services in 2011, giving large companies direct access to the tier 3 backbone. Connecting companies directly to the backbone will provide enterprises faster Internet service which meets a large market demand.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The most prominent component of the Internet model is the Internet Protocol (IP). IP enables internetworking and, in essence, establishes the Internet itself. Two versions of the Internet Protocol exist, IPv4 and IPv6.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting-edge research and development, not running a communications utility. In July 1975, the network was turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.\nThe networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, which began to form networks of fiber optic lines. A growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were. Data transmission speeds depended upon the type of connection, the slowest being analog telephone lines and the fastest using optical networking technology.\nSeveral other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP\/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.\n\nNASA developed the TCP\/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP\/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.\nIn 1981, NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP\/IP, and ran TCP\/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange. CSNET played a central role in popularizing the Internet outside the ARPANET.In 1986, the NSF created NSFNET, a 56 kbit\/s backbone to support the NSF-sponsored supercomputing centers. The NSFNET also provided support for the creation of regional research and education networks in the United States, and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit\/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit\/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990.\nNSFNET was expanded and upgraded to dedicated fiber, optical lasers and optical amplifier systems capable of delivering T3 start up speeds or 45 Mbit\/s in 1991. However, the T3 transition by MCI took longer than expected, allowing Sprint to establish a coast-to-coast long-distance commercial Internet service. When NSFNET was decommissioned in 1995, its optical networking backbones were handed off to several commercial Internet service providers, including MCI, PSI Net and Sprint. As a result, when the handoff was complete, Sprint and its Washington DC Network Access Points began to carry Internet traffic, and by 1996, Sprint was the world's largest carrier of Internet traffic.The research and academic community continues to develop and use advanced networks such as Internet2 in the United States and JANET in the United Kingdom.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The main reason terrestrial communications have been limited to non-commercial telecommunications functions is fog. Fog consistently keeps FSO laser links over 500 metres (1,600 ft) from achieving a year-round bit error rate of 1 per 100,000. Several entities are continually attempting to overcome these key disadvantages to FSO communications and field a system with a better quality of service. DARPA has sponsored over US$130 million in research toward this effort, with the ORCA and ORCLE programs.Other non-government groups are fielding tests to evaluate different technologies that some claim have the ability to address key FSO adoption challenges. As of October 2014, none have fielded a working system that addresses the most common atmospheric events.\nFSO research from 1998–2006 in the private sector totaled $407.1 million, divided primarily among four start-up companies. All four failed to deliver products that would meet telecommunications quality and distance standards:\nTerabeam received approximately $575 million in funding from investors such as Softbank, Mobius Venture Capital and Oakhill Venture Partners. AT&T and Lucent backed this attempt. The work ultimately failed, and the company was purchased in 2004 for $52 million (excluding warrants and options) by Falls Church, Va.-based YDI, effective June 22, 2004, and used the name Terabeam for the new entity. On September 4, 2007, Terabeam (then headquartered in San Jose, California) announced it would change its name to Proxim Wireless Corporation, and change its NASDAQ stock symbol from TRBM to PRXM.\nAirFiber received $96.1 million in funding, and never solved the weather issue. They sold out to MRV communications in 2003, and MRV sold their FSO units until 2012 when the end-of-life was abruptly announced for the Terescope series.\nLightPointe Communications received $76 million in start-up funds, and eventually reorganized to sell hybrid FSO-RF units to overcome the weather-based challenges.\nThe Maxima Corporation published its operating theory in Science, and received $9 million in funding before permanently shutting down. No known spin-off or purchase followed this effort.\nWireless Excellence developed and launched CableFree UNITY solutions that combine FSO with millimeter wave and radio technologies to extend distance, capacity and availability, with a goal of making FSO a more useful and practical technology.One private company published a paper on November 20, 2014, claiming they had achieved commercial reliability (99.999% availability) in extreme fog. There is no indication this product is currently commercially available.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"During the 2021 NSW Local Government Elections the online voting system \"iVote\" had technical issues that caused some access problems for some voters. Analysis done of these failures indicated a significant chance of the outages having impacted on the electoral results for the final positions. In the Kempsey ward, where the margin between the last elected and first non-elected candidates was only 69 votes, the electoral commission determined that the outage caused a 60% chance that the wrong final candidate was elected. Singleton had a 40% chance of having elected the wrong councillor, Shellharbour was a 7% chance and two other races were impacted by a sub-1% chance of having elected the wrong candidate. The NSW Supreme Court ordered the elections in Kempsey, Singleton and Shellharbour Ward A to be re-run. In the 2022 Kempsey re-vote the highest placed non-elected candidate from 2021, Dean Saul, was instead one the first councillors elected. This failure caused the NSW Government to suspend the iVote system from use in the 2023 New South Wales state election.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Computer networks extend interpersonal communications by electronic means with various technologies, such as email, instant messaging, online chat, voice and video telephone calls, and video conferencing. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer or use of a shared storage device. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. Distributed computing uses computing resources across a network to accomplish tasks.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Ethernet has evolved to include higher bandwidth, improved medium access control methods, and different physical media. The multidrop coaxial cable was replaced with physical point-to-point links connected by Ethernet repeaters or switches.Ethernet stations communicate by sending each other data packets: blocks of data individually sent and delivered. As with other IEEE 802 LANs, adapters come programmed with globally unique 48-bit MAC address so that each Ethernet station has a unique address. The MAC addresses are used to specify both the destination and the source of each data packet. Ethernet establishes link-level connections, which can be defined using both the destination and source addresses. On reception of a transmission, the receiver uses the destination address to determine whether the transmission is relevant to the station or should be ignored. A network interface normally does not accept packets addressed to other Ethernet stations.An EtherType field in each frame is used by the operating system on the receiving station to select the appropriate protocol module (e.g., an Internet Protocol version such as IPv4). Ethernet frames are said to be self-identifying, because of the EtherType field. Self-identifying frames make it possible to intermix multiple protocols on the same physical network and allow a single computer to use multiple protocols together. Despite the evolution of Ethernet technology, all generations of Ethernet (excluding early experimental versions) use the same frame formats. Mixed-speed networks can be built using Ethernet switches and repeaters supporting the desired Ethernet variants.Due to the ubiquity of Ethernet, and the ever-decreasing cost of the hardware needed to support it, by 2004 most manufacturers built Ethernet interfaces directly into PC motherboards, eliminating the need for a separate network card.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Various satellite constellations that are intended to provide global broadband coverage, such as SpaceX Starlink, employ laser communication for inter-satellite links. This effectively creates a space-based optical mesh network between the satellites.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Within cyber warfare, the individual must recognize the state actors involved in committing these cyberattacks against one another. The two predominant players that will be discussed is the age-old comparison of East versus West, China's cyber capabilities compared to United States' capabilities. There are many other state and non-state actors involved in cyber warfare, such as Russia, Iran, Iraq, and Al Qaeda; since China and the U.S. are leading the foreground in cyber warfare capabilities, they will be the only two states actors discussed.\nBut in Q2 2013, Akamai Technologies reported that Indonesia toppled China with a portion 38 percent of cyber attacks, a high increase from the 21 percent portion in the previous quarter. China set 33 percent and the US set at 6.9 percent. 79 percent of attacks came from the Asia Pacific region. Indonesia dominated the attacking to ports 80 and 443 by about 90 percent.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"In addition to framing, the data link layer may also detect and recover from transmission errors.  For a receiver to detect transmission errors, the sender must add redundant information as an error detection code to the frame sent.  When the receiver obtains a frame it verifies whether the received error detection code matches a recomputed error detection code.\nAn error detection code can be defined as a function that computes the r (amount of redundant bits) corresponding to each string of N total number of bits.  The simplest error detection code is the parity bit, which allows a receiver to detect transmission errors that have affected a single bit among the transmitted N + r bits.  If there are multiple flipped bits then the checking method might not be able to detect this on the receiver side. More advanced methods than parity error detection do exist providing higher grades of quality and features.\n\nA simple example of how this works using metadata is transmitting the word \"HELLO\", by encoding each letter as its position in the alphabet. Thus, the letter A is coded as 1, B as 2, and so on as shown in the table on the right.  Adding up the resulting numbers yields 8 + 5 + 12 + 12 + 15 = 52, and 5 + 2 = 7 calculates the metadata.  Finally, the \"8 5 12 12 15 7\" numbers sequence is transmitted, which the receiver will see on its end if there are no transmission errors.  The receiver knows that the last number received is the error-detecting metadata and that all data before is the message, so the receiver can recalculate the above math and if the metadata matches it can be concluded that the data has been received error-free.  Though, if the receiver sees something like a \"7 5 12 12 15 7\" sequence (first element altered by some error), it can run the check by calculating 7 + 5 + 12 + 12 + 15 = 51 and 5 + 1 = 6, and discard the received data as defective since 6 does not equal 7.\nMore sophisticated error detection and correction algorithms are designed to reduce the risk that multiple transmission errors in the data would cancel each other out and go undetected. An algorithm that can even detect if the correct bytes are received but out of order is the cyclic redundancy check or CRC. This algorithm is often used in the data link layer.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"It is possible for an exchange of emails to form a binding contract, so users must be careful about what they send through email correspondence. A signature block on an email may be interpreted as satisfying a signature requirement for a contract.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"In 2003, task group TGma was authorized to \"roll up\" many of the amendments to the 1999 version of the 802.11 standard. REVma or 802.11ma, as it was called, created a single document that merged 8 amendments (802.11a, b, d, e, g, h, i, j) with the base standard. Upon approval on 8 March 2007, 802.11REVma was renamed to the then-current base standard IEEE 802.11-2007.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"For home networks relying on powerline communication technology, how to deal with electrical noise injected into the system from standard household appliances remains the largest challenge. Whenever any appliance is turned on or turned off it creates noise that could possibly disrupt data transfer through the wiring. IEEE products that are certified to be HomePlug 1.0 compliant have been engineered to no longer interfere with, or receive interference from other devices plugged into the same home's electrical grid.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Email marketing via \"opt-in\" is often successfully used to send special sales offerings and new product information. Depending on the recipient's culture, email sent without permission—such as an \"opt-in\"—is likely to be viewed as unwelcome \"email spam\".\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The 'truncated' variant of the algorithm introduces a limit on c. This simply means that after a certain number of increases, the exponentiation stops. Without a limit on c, the delay between transmissions may become undesirably long if a sender repeatedly observes adverse events, e.g. due to a degradation in network service. In a randomized system this may occur by chance, leading to unpredictable latency; longer delays due to unbounded increases in c are exponentially less probable, but they are effectively inevitable on a busy network due to the law of large numbers. Limiting c helps to reduce the possibility of unexpectedly long transmission latencies and improve recovery times after a transient outage.\nFor example, if the ceiling is set at i = 10 in a truncated binary exponential backoff algorithm, (as it is in the IEEE 802.3 CSMA\/CD standard), then the maximum delay is 1023 slot times, i.e. 210 − 1.\nSelecting an appropriate backoff limit for a system involves striking a balance between collision probability and latency. By increasing the ceiling there is an exponential reduction in probability of collision on each transmission attempt. At the same time, increasing the limit also exponentially increases the range of possible latency times for a transmission, leading to less deterministic performance and an increase in the average latency. The optimal limit value for a system is specific to both the implementation and environment.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).\nThe IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the work of the IETF is organized into Working Groups. Standardization efforts of the Working Groups are often adopted by the Internet community, but the IETF does not control or patrol the Internet.The IETF grew out of quarterly meetings with U.S. government-funded researchers, starting in January 1986. Non-government representatives were invited by the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth meeting in February 1987. The seventh meeting in July 1987 was the first meeting with more than one hundred attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, the Netherlands, in July 1993. Today, the IETF meets three times per year and attendance has been as high as ca. 2,000 participants. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is typically ca. 50%, even at meetings held in the United States.The IETF is not a legal entity, has no governing board, no members, and no dues. The closest status resembling membership is being on an IETF or Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer-term research issues.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"During his time as director of ARPA's Information Processing Techniques Office (IPTO) from 1962 to 1964, he funded Project MAC at MIT. A large mainframe computer was designed to be shared by up to 30 simultaneous users, each sitting at a separate \"typewriter terminal\". He also funded similar projects at Stanford University, UCLA, UC Berkeley (called Project Genie), and the AN\/FSQ-32 at System Development Corporation.\nThis time-sharing technology later developed to become what today are known as servers.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Network connections can be established wirelessly using radio or other electromagnetic means of communication.\n\n Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 40 miles (64 km) apart.\nCommunications satellites – Satellites also communicate via microwave. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.\nCellular networks use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver.\nRadio and spread spectrum technologies – Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi.\nFree-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.\nExtending the Internet to interplanetary dimensions via radio waves and optical means, the Interplanetary Internet.\nIP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.The last two cases have a large round-trip delay time, which gives slow two-way communication but does not prevent sending large amounts of information (they can have high throughput).\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Internetworking, a combination of the components inter (between) and networking, started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network.\nThe first international heterogenous resource sharing network was the 1973 interconnection of the ARPANET with early British academic networks through the computer science department at University College London (UCL). In the ARPANET, the network elements used to connect individual networks were called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. By 1973-4, researchers in the United States, the United Kingdom and France had worked out an approach to internetworking where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible, as demonstrated in the CYCLADES network. Research at the National Physical Laboratory in the United Kingdom confirmed establishing a common host protocol would be more reliable and efficient. The ARPANET connection to UCL later evolved into SATNET. In 1977, ARPA demonstrated a three-way internetworking experiment, which linked a mobile vehicle in PRNET with nodes in the ARPANET, and via SATNET, to nodes at UCL. The X.25 protocol, on which public data networks were based in the 1970s and 1980s, was supplemented by the X.75 protocol which enabled internetworking. \nToday the interconnecting gateways are called routers. The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks.\nTo build an internetwork, the following are needed:: 103  A standardized scheme to address packets to any host on any participating network; a standardized protocol defining format and handling of transmitted packets; components interconnecting the participating networks by routing packets to their destinations based on standardized addresses.\nAnother type of interconnection of networks often occurs within enterprises at the link layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP\/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers and having an internetworking software layer that applications employ.\nThe Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate transport layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"A virus is a self-replicating program that can attach itself to another program or file in order to reproduce. The virus can hide in unlikely locations in the memory of a computer system and attach itself to whatever file it sees fit to execute its code. It can also change its digital footprint each time it replicates making it harder to track down in the computer.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"According to Charles Herzfeld, ARPA Director (1965–1967):The ARPANET was not started to create a Command and Control System that would survive a nuclear attack, as many now claim. To build such a system was, clearly, a major military need, but it was not ARPA's mission to do this; in fact, we would have been severely criticized had we tried. Rather, the ARPANET came out of our frustration that there were only a limited number of large, powerful research computers in the country, and that many research investigators, who should have access to them, were geographically separated from them.\nNonetheless, according to Stephen J. Lukasik, who as deputy director (1967–1970) and Director of DARPA (1970–1975) was \"the person who signed most of the checks for Arpanet's development\":\n\nThe goal was to exploit new computer technologies to meet the needs of military command and control against nuclear threats, achieve survivable control of US nuclear forces, and improve military tactical and management decision making.\nThe ARPANET incorporated distributed computation, and frequent re-computation, of routing tables. This increased the survivability of the network in the face of significant interruption. Automatic routing was technically challenging at the time. The ARPANET was designed to survive subordinate-network losses, since the principal reason was that the switching nodes and network links were unreliable, even without any nuclear attacks.The Internet Society agrees with Herzfeld in a footnote in their online article, A Brief History of the Internet:\n\nIt was from the RAND study that the false rumor started, claiming that the ARPANET was somehow related to building a network resistant to nuclear war. This was never true of the ARPANET, but was an aspect of the earlier RAND study of secure communication. The later work on internetworking did emphasize robustness and survivability, including the capability to withstand losses of large portions of the underlying networks.\nPaul Baran, the first to put forward a theoretical model for communication using packet switching, conducted the RAND study referenced above. Though the ARPANET did not exactly share Baran's project's goal, he said his work did contribute to the development of the ARPANET. Minutes taken by Elmer Shapiro of Stanford Research Institute at the ARPANET design meeting of 9–10 October 1967 indicate that a version of Baran's routing method (\"hot potato\") may be used, consistent with the NPL team's proposal at the Symposium on Operating System Principles in Gatlinburg.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Direct person-to-person communication includes non-verbal cues expressed in facial and other bodily articulation, that cannot be transmitted in traditional voice telephony. Video telephony restores such interactions to varying degrees. Social Context Cues Theory is a model to measure the success of different types of communication in maintaining the non-verbal cues present in face-to-face interactions. The research examines many different cues, such as the physical context, different facial expressions, body movements, tone of voice, touch and smell.\nVarious communication cues are lost with the usage of the telephone. The communicating parties are not able to identify the body movements, and lack touch and smell. Although this diminished ability to identify social cues is well known, Wiesenfeld, Raghuram, and Garud point out that there is a value and efficiency to the type of communication for different tasks. They examine work places in which different types of communication, such as the telephone, are more useful than face-to-face interaction.\nThe expansion of communication to mobile telephone service has created a different filter of the social cues than the land-line telephone. The use of instant messaging, such as texting, on mobile telephones has created a sense of community. In The Social Construction of Mobile Telephony it is suggested that each phone call and text message is more than an attempt to converse. Instead, it is a gesture which maintains the social network between family and friends. Although there is a loss of certain social cues through telephones, mobile phones bring new forms of expression of different cues that are understood by different audiences. New language additives attempt to compensate for the inherent lack of non-physical interaction.\nAnother social theory supported through telephony is the Media Dependency Theory. This theory concludes that people use media or a resource to attain certain goals. This theory states that there is a link between the media, audience, and the large social system. Telephones, depending on the person, help attain certain goals like accessing information, keeping in contact with others, sending quick communication, entertainment, etc.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Licklider played a similar role in conceiving of and funding early networking research. He formulated the earliest ideas of a global computer network in August 1962 at BBN, in a series of memos discussing the \"Intergalactic Computer Network\" concept. These ideas contained almost everything that the Internet is today, including cloud computing.While at IPTO he convinced Ivan Sutherland, Bob Taylor, and Lawrence G. Roberts that an all-encompassing computer network was a very important concept. He met with Donald Davies in 1965 and inspired his interest in data communications.In 1967 Licklider submitted the paper \"Televistas: Looking ahead through side windows\" to the Carnegie Commission on Educational Television. This paper describes a radical departure from the \"broadcast\" model of television. Instead Licklider advocates for a two-way communications network. The Carnegie Commission led to the creation of the Corporation for Public Broadcasting. Although the Commission's report explains that \"Dr. Licklider's paper was completed after the Commission had formulated its own conclusions,\" President Johnson said at the signing of the Public Broadcasting Act of 1967, \"So I think we must consider new ways to build a great network for knowledge—not just a broadcast system, but one that employs every means of sending and of storing information that the individual can use\".His 1968 paper The Computer as a Communication Device illustrates his vision of network applications and predicts the use of computer networks to support communities of common interest and collaboration without regard to location.In the same 1968 paper, J. C. R. Licklider and Robert W. Taylor wrote, \"Take any problem worthy of the name, and you find only a few people who can contribute effectively to its solution. Those people must be brought into close intellectual partnership so that their ideas can come into contact with one another. But bring these people together physically in one place to form a team, and you have trouble, for the most creative people are often not the best team players, and there are not enough top positions in a single organization to keep them all happy. Let them go their separate ways, and each creates his own empire, large or small, and devotes more time to the role of emperor than to the role of problem solver.  The principals still get together at meetings. They still visit one another. But the time scale of their communication stretches out, and the correlations among mental models degenerate between meetings so that it may take a year to do a week's communicating. There has to be some way of facilitating communication among people wit bout [sic] [without] bringing them together in one place.\" (Evan Herbert edited the article and acted as intermediary during its writing between Licklider in Boston and Taylor in Washington.)\nThe Licklider Transmission Protocol is named after him.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"An attack can be active or passive.\nAn \"active attack\" attempts to alter system resources or affect their operation.\nA \"passive attack\" attempts to learn or make use of information from the system but does not affect system resources (e.g., wiretapping).An attack can be perpetrated by an insider or from outside the organization;\nAn \"inside attack\" is an attack initiated by an entity inside the security perimeter (an \"insider\"), i.e., an entity that is authorized to access system resources but uses them in a way not approved by those who granted the authorization.\nAn \"outside attack\" is initiated from outside the perimeter, by an unauthorized or illegitimate user of the system (an \"outsider\"). In the Internet, potential outside attackers range from amateur pranksters to organized criminals, international terrorists, and hostile governments.\nA resource (both physical or logical), called an asset, can have one or more vulnerabilities that can be exploited by a threat agent in a threat action. As a result, the confidentiality, integrity or availability of resources may be compromised. Potentially, the damage may extend to resources in addition to the one initially identified as vulnerable, including further resources of the organization, and the resources of other involved parties (customers, suppliers).\nThe so-called CIA triad is the basis of information security.\nThe attack can be active when it attempts to alter system resources or affect their operation: so it compromises integrity or availability. A \"passive attack\" attempts to learn or make use of information from the system but does not affect system resources: so it compromises confidentiality.\nA threat is a potential for violation of security, which exists when there is a circumstance, capability, action or event that could breach security and cause harm. That is, a threat is a possible danger that might exploit a vulnerability. A threat can be either \"intentional\" (i.e., intelligent; e.g., an individual cracker or a criminal organization) or \"accidental\" (e.g., the possibility of a computer malfunctioning, or the possibility of an \"act of God\" such as an earthquake, a fire, or a tornado).A set of policies concerned with information security management, the information security management systems (ISMS), has been developed to manage, according to risk management principles, the countermeasures in order to accomplish to a security strategy set up following rules and regulations applicable in a country.An attack should lead to a security incident i.e. a security event that involves a security violation. In other words, a security-relevant system event in which the system's security policy is disobeyed or otherwise breached.\nThe overall picture represents the risk factors of the risk scenario.An organization should take steps to detect, classify and manage security incidents. The first logical step is to set up an incident response plan and eventually a computer emergency response team.\nIn order to detect attacks, a number of countermeasures can be set up at organizational, procedural, and technical levels. Computer emergency response team, information technology security audit and intrusion detection system are examples of these.An attack usually is perpetrated by someone with bad intentions: black hatted attacks falls in this category, while other perform penetration testing on an organization information system to find out if all foreseen controls are in place.\nThe attacks can be classified according to their origin: I.E. if it is conducted using one or more computers: in the last case is called a distributed attack. Botnets are used to conduct distributed attacks.\nOther classifications are according to the procedures used or the type of vulnerabilities exploited: attacks can be concentrated on network mechanisms or host features.\nSome attacks are physical: i.e. theft or damage of computers and other equipment. Others are attempts to force changes in the logic used by computers or network protocols in order to achieve unforeseen (by the original designer) result but useful for the attacker. Software used to for logical attacks on computers is called malware.\nThe following is a partial short list of attacks:\n\nPassive\nComputer and network surveillance\nNetwork\nWiretapping\nFiber tapping\nPort scan\nIdle scan\nHost\nKeystroke logging\nData scraping\nBackdoor\nActive\nDenial-of-service attack\nA DDos or Distributed Denial of service attack is an attempt made by a hacker to block access to a server or a website that is connected to the Internet. This is achieved using multiple computerized systems, which overloads the target system with requests, making it incapable of responding to any query.\nSpoofing\nMixed threat attack\nNetwork\nMan-in-the-middle\nMan-in-the-browser\nARP poisoning\nPing flood\nPing of death\nSmurf attack\nHost\nBuffer overflow\nHeap overflow\nStack overflow\nFormat string attack\nBy modality\nSupply chain attack\nSocial engineering\nExploitIn detail, there are a number of techniques to utilize in cyberattacks and a variety of ways to administer them to individuals or establishments on a broader scale. Attacks are broken down into two categories: syntactic attacks and semantic attacks. Syntactic attacks are straightforward; it is considered malicious software which includes viruses, worms, and Trojan horses.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n\nComputer keyboard\nDigital camera\nDigital video\nGraphics tablet\nImage scanner\nJoystick\nMicrophone\nMouse\nOverlay keyboard\nReal-time clock\nTrackball\nTouchscreen\nLight pen\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Today it can be important to distinguish between the Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.\nEmail privacy, without some security precautions, can be compromised because:\n\nemail messages are generally not encrypted.\nemail messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.\nmany Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.\nthe \"Received:\"-fields and other information in the email can often identify the sender, preventing anonymous communication.\nweb bugs invisibly embedded in HTML content can alert the sender of any email whenever an email is rendered as HTML (some e-mail clients do this when the user reads, or re-reads the e-mail) and from which IP address. It can also reveal whether an email was read on a smartphone or a PC, or Apple Mac device via the user agent string.There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail, or S\/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security\/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.\nAdditionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this. Finally, the attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"On July 18, 2006, a call for interest for a High Speed Study Group (HSSG) to investigate new standards for high speed Ethernet was held at the IEEE 802.3 plenary meeting in San Diego.The first 802.3 HSSG study group meeting was held in September 2006. In June 2007, a trade group called \"Road to 100G\" was formed after the NXTcomm trade show in Chicago.On December 5, 2007, the Project Authorization Request (PAR) for the P802.3ba 40 Gbit\/s and 100 Gbit\/s Ethernet Task Force was approved with the following project scope:\n\nThe purpose of this project is to extend the 802.3 protocol to operating speeds of 40 Gbit\/s and 100 Gbit\/s in order to provide a significant increase in bandwidth while maintaining maximum compatibility with the installed base of 802.3 interfaces, previous investment in research and development, and principles of network operation and management. The project is to provide for the interconnection of equipment satisfying the distance requirements of the intended applications.\n\nThe 802.3ba task force met for the first time in January 2008. This standard was approved at the June 2010 IEEE Standards Board meeting under the name IEEE Std 802.3ba-2010.The first 40 Gbit\/s Ethernet Single-mode Fibre PMD study group meeting was held in January 2010 and on March 25, 2010, the P802.3bg Single-mode Fibre PMD Task Force was approved for the 40 Gbit\/s serial SMF PMD.\n\nThe scope of this project is to add a single-mode fiber Physical Medium Dependent (PMD) option for serial 40 Gbit\/s operation by specifying additions to, and appropriate modifications of, IEEE Std 802.3-2008 as amended by the IEEE P802.3ba project (and any other approved amendment or corrigendum).\nOn June 17, 2010, the IEEE 802.3ba standard was approved. In March 2011, the IEEE 802.3bg standard was approved. On September 10, 2011, the P802.3bj 100 Gbit\/s Backplane and Copper Cable task force was approved.\nThe scope of this project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add 100 Gbit\/s 4-lane Physical Layer (PHY) specifications and management parameters for operation on backplanes and twinaxial copper cables, and specify optional Energy Efficient Ethernet (EEE) for 40 Gbit\/s and 100 Gbit\/s operation over backplanes and copper cables.\nOn May 10, 2013, the P802.3bm 40 Gbit\/s and 100 Gbit\/s Fiber Optic Task Force was approved.\nThis project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add 100 Gbit\/s Physical Layer (PHY) specifications and management parameters, using a four-lane electrical interface for operation on multimode and single-mode fiber optic cables, and to specify optional Energy Efficient Ethernet (EEE) for 40 Gbit\/s and 100 Gbit\/s operation over fiber optic cables. In addition, to add 40 Gbit\/s Physical Layer (PHY) specifications and management parameters for operation on extended reach (>10 km) single-mode fiber optic cables.\nAlso on May 10, 2013, the P802.3bq 40GBASE-T Task Force was approved.\nSpecify a Physical Layer (PHY) for operation at 40 Gbit\/s on balanced twisted-pair copper cabling, using existing Media Access Control, and with extensions to the appropriate physical layer management parameters.\nOn June 12, 2014, the IEEE 802.3bj standard was approved.On February 16, 2015, the IEEE 802.3bm standard was approved.On May 12, 2016, the IEEE P802.3cd Task Force started working to define next generation two-lane 100 Gbit\/s PHY.On May 14, 2018, the PAR for the IEEE P802.3ck Task Force was approved. The scope of this project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add Physical Layer specifications and Management Parameters for 100 Gbit\/s, 200 Gbit\/s, and 400 Gbit\/s electrical interfaces based on 100 Gbit\/s signaling.On December 5, 2018, the IEEE-SA Board approved the IEEE 802.3cd standard.\nOn November 12, 2018, the IEEE P802.3ct Task Force started working to define PHY supporting 100 Gbit\/s operation on a single wavelength capable of at least 80 km over a DWDM system (using a combination of phase and amplitude modulation with coherent detection).In May 2019, the IEEE P802.3cu Task Force started working to define single-wavelength 100 Gbit\/s PHYs for operation over SMF (Single-Mode Fiber) with lengths up to at least 2 km (100GBASE-FR1) and 10 km (100GBASE-LR1).In June 2020, the IEEE P802.3db Task Force started working to define a physical layer specification that supports 100 Gbit\/s operation over 1 pair of MMF with lengths up to at least 50 m.On February 11, 2021, the IEEE-SA Board approved the IEEE 802.3cu standard.On June 16, 2021, the IEEE-SA Board approved the IEEE 802.3ct standard.On September 21, 2022, the IEEE-SA Board approved the IEEE 802.3ck and 802.3db standards.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"A thin client is a minimal sort of client.  Thin clients use the resources of the host computer. A thin client generally only presents processed data provided by an application server, which performs the bulk of any required data processing. A device using web application (such as Office Web Apps) is a thin client.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Optical signal transmission over a nonlinear medium is principally an analog design problem. As such, it has evolved slower than digital circuit lithography (which generally progressed in step with Moore's law). This explains why 10 Gbit\/s transport systems existed since the mid-1990s, while the first forays into 100 Gbit\/s transmission happened about 15 years later – a 10x speed increase over 15 years is far slower than the 2x speed per 1.5 years typically cited for Moore's law.\nNevertheless, at least five firms (Ciena, Alcatel-Lucent, MRV, ADVA Optical and Huawei) made customer announcements for 100 Gbit\/s transport systems by August 2011, with varying degrees of capabilities. Although vendors claimed that 100 Gbit\/s light paths could use existing analog optical infrastructure, deployment of high-speed technology was tightly controlled and extensive interoperability tests were required before moving them into service.\nDesigning routers or switches which support 100 Gbit\/s interfaces is difficult. The need to process a 100 Gbit\/s stream of packets at line rate without reordering within IP\/MPLS microflows is one reason for this.\nAs of 2011, most components in the 100 Gbit\/s packet processing path (PHY chips, NPUs, memories) were not readily available off-the-shelf or require extensive qualification and co-design. Another problem is related to the low-output production of 100 Gbit\/s optical components, which were also not easily available – especially in pluggable, long-reach or tunable laser flavors.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"The field of technology available for telephony has broadened with the advent of new communication technologies. Telephony now includes the technologies of Internet services and mobile communication, including video conferencing.\nThe new technologies based on Internet Protocol (IP) concepts are often referred to separately as voice over IP (VoIP) telephony, also commonly referred to as IP telephony or Internet telephony. Unlike traditional phone service, IP telephony service is relatively unregulated by government. In the United States, the Federal Communications Commission (FCC) regulates phone-to-phone connections, but says they do not plan to regulate connections between a phone user and an IP telephony service provider.A specialization of digital telephony, Internet Protocol (IP) telephony involves the application of digital networking technology that was the foundation to the Internet to create, transmit, and receive telecommunications sessions over computer networks. Internet telephony is commonly known as voice over Internet Protocol (VoIP), reflecting the principle, but it has been referred with many other terms. VoIP has proven to be a disruptive technology that is rapidly replacing traditional telephone infrastructure technologies. As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 Newsweek article suggested that Internet telephony may be \"the next big thing\". As of 2006, many VoIP companies offer service to consumers and businesses.IP telephony uses an Internet connection and hardware IP phones, analog telephone adapters, or softphone computer applications to transmit conversations encoded as data packets. In addition to replacing plain old telephone service (POTS), IP telephony services compete with mobile phone services by offering free or lower cost connections via WiFi hotspots. VoIP is also used on private networks which may or may not have a connection to the global telephone network.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"An extranet is a network that is under the administrative control of a single organization but supports a limited connection to a specific external network.  For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers.  These other entities are not necessarily trusted from a security standpoint.  The network connection to an extranet is often, but not always, implemented via WAN technology.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"Across all variations of 802.11, maximum achievable throughputs are given either based on measurements under ideal conditions or in the layer-2 data rates. However, this does not apply to typical deployments in which data is being transferred between two endpoints, of which at least one is typically connected to a wired infrastructure and the other endpoint is connected to an infrastructure via a wireless link.\n\nThis means that, typically, data frames pass an 802.11 (WLAN) medium and are being converted to 802.3 (Ethernet) or vice versa. Due to the difference in the frame (header) lengths of these two media, the application's packet size determines the speed of the data transfer. This means applications that use small packets (e.g., VoIP) create dataflows with high-overhead traffic (i.e., a low goodput). Other factors that contribute to the overall application data rate are the speed with which the application transmits the packets (i.e., the data rate) and, of course, the energy with which the wireless signal is received. The latter is determined by distance and by the configured output power of the communicating devices.The same references apply to the attached graphs that show measurements of UDP throughput. Each represents an average (UDP) throughput (please note that the error bars are there but barely visible due to the small variation) of 25 measurements. Each is with a specific packet size (small or large) and with a specific data rate (10 kbit\/s – 100 Mbit\/s). Markers for traffic profiles of common applications are included as well. These figures assume there are no packet errors, which, if occurring, will lower the transmission rate further.\n\n###\n\n","completion":" Computer Network"}
{"prompt":"In 1838, Poe relocated to Philadelphia, where he lived at four different residences between 1838 and 1844, one of which at 532 N. 7th Street has been preserved as a National Historic Landmark.\nThat same year, Poe's novel The Narrative of Arthur Gordon Pym of Nantucket was published and widely reviewed. In the summer of 1839, he became assistant editor of Burton's Gentleman's Magazine. He published numerous articles, stories, and reviews, enhancing his reputation as a trenchant critic which he had established at the Messenger. Also in 1839, the collection Tales of the Grotesque and Arabesque was published in two volumes, though he made little money from it and it received mixed reviews.In June 1840, Poe published a prospectus announcing his intentions to start his own journal called The Stylus, although he originally intended to call it The Penn, since it would have been based in Philadelphia. He bought advertising space for his prospectus in the June 6, 1840, issue of Philadelphia's Saturday Evening Post: \"Prospectus of the Penn Magazine, a Monthly Literary journal to be edited and published in the city of Philadelphia by Edgar A. Poe.\" The journal was never produced before Poe's death.\nPoe left Burton's after about a year and found a position as writer and co-editor at Graham's Magazine, a successful monthly publication. In the last number of Graham's for 1841, Poe was among the co-signatories to an editorial note of celebration of the tremendous success the magazine had achieved in the past year: \"Perhaps the editors of no magazine, either in America or in Europe, ever sat down, at the close of a year, to contemplate the progress of their work with more satisfaction than we do now. Our success has been unexampled, almost incredible. We may assert without fear of contradiction that no periodical ever witnessed the same increase during so short a period.\"Around this time, Poe attempted to secure a position in the administration of John Tyler, claiming that he was a member of the Whig Party. He hoped to be appointed to the United States Custom House in Philadelphia with help from President Tyler's son Robert, an acquaintance of Poe's friend Frederick Thomas. Poe failed to show up for a meeting with Thomas to discuss the appointment in mid-September 1842, claiming to have been sick, though Thomas believed that he had been drunk. Poe was promised an appointment, but all positions were filled by others.One evening in January 1842, Virginia showed the first signs of consumption, or tuberculosis, while singing and playing the piano, which Poe described as breaking a blood vessel in her throat. She only partially recovered, and Poe began to drink more heavily under the stress of her illness. He left Graham's and attempted to find a new position, for a time angling for a government post. He returned to New York where he worked briefly at the Evening Mirror before becoming editor of the Broadway Journal, and later its owner. There Poe alienated himself from other writers by publicly accusing Henry Wadsworth Longfellow of plagiarism, though Longfellow never responded. On January 29, 1845, Poe's poem \"The Raven\" appeared in the Evening Mirror and became a popular sensation. It made Poe a household name almost instantly, though he was paid only $9 for its publication. It was concurrently published in The American Review: A Whig Journal under the pseudonym \"Quarles\".\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Feistel construction is also used in cryptographic algorithms other than block ciphers. For example, the optimal asymmetric encryption padding (OAEP) scheme uses a simple Feistel network to randomize ciphertexts in certain asymmetric-key encryption schemes.\nA generalized Feistel algorithm can be used to create strong permutations on small domains of size not a power of two (see format-preserving encryption).\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In 2021, 17 states passed laws and resolutions concerning cryptocurrency regulation. The U.S. Securities and Exchange Commission (SEC) is considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, SEC Chairman Gary Gensler responded to Senator Elizabeth Warren's letter regarding cryptocurrency regulation and called for legislation focused on \"crypto trading, lending and DeFi platforms,\" because of how vulnerable the investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.On 19 October 2021, The first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker \"BITO.\" ProShares CEO Michael L. Sapir said the ETF would expose Bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that the approval of the \"BITO\" ETF by the SEC was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto as well as the hesitance to accept crypto from retail investors. This event would eventually open more opportunities for new capital and new people in this space.The United States Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.On 17 February 2022, the Justice department named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to aid in identification of and dealing with misuse of cryptocurrencies and other digital assets.The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict the growing and profitable industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, President Biden issued an executive order. Followed by the executive order, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released  to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the U.S. cryptocurrency regulation history.In February 2023, the Securities and Exchange Commission (SEC) ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the U.S. The case would impact other major crypto exchanges operating staking programs.On 23 March 2023, the U.S. Securities and Exchange Commission (SEC) issued an alert to investors stating that firms offering crypto asset securities may not be complying with U.S. laws. The SEC stated that unregistered offerings of crypto asset securities may not include important information.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Despite their travails, Rejewski and Zygalski had fared better than some of their colleagues. Cadix's Polish military chiefs, Langer and Ciężki, had also been captured—by the Germans, as they tried to escape from France into Spain on the night of 10–11 March 1943—along with three other Poles: Antoni Palluth, Edward Fokczyński and Kazimierz Gaca. The first two became prisoners of war; the other three were sent as slave labourers to Germany, where Palluth and Fokczyński perished. Despite the varyingly dire circumstances in which they were held, none of them—Stefan Mayer emphasizes—betrayed the secret of Enigma's decryption, thus making it possible for the Allies to continue exploiting this vital intelligence resource.Before the war, Palluth, a lecturer in the 1929 secret Poznań University cryptology course, had been co-owner of AVA, which produced equipment for the Cipher Bureau, and knew many details of the decryption technology. In Warsaw, under German occupation, other Cipher Bureau workers were interrogated by German intelligence commissions, and some AVA workers were approached by German agents, but all kept silent about compromises to Enigma.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The historical roots of cryptoeconomics can be traced to the rise of altcoins, prominent among them the Ethereum project, which in 2015 pioneered the integration of smart contracts into its blockchain, thereby enabling a wide range of DeFi applications.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In contrast to smart cards, which are simple devices performing a single function, personal computers are doing many things at once. Thus, it is much more difficult to perform electromagnetic side-channel attacks against them, due to high levels of noise and fast clock rates. Despite these issues, researchers in 2015 and 2016 showed attacks against a laptop using a near-field magnetic probe. The resulting signal, observed for only a few seconds, was filtered, amplified, and digitized for offline key extraction. Most attacks require expensive, lab-grade equipment, and require the attacker to be extremely close to the victim computer. However, some researchers were able to show attacks using cheaper hardware and from distances of up to half a meter. These attacks, however, required the collection of more traces than the more expensive attacks.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The protocol is considered secure against eavesdroppers if G and g are chosen properly. In particular, the order of the group G must be large, particularly if the same group is used for large amounts of traffic. The eavesdropper has to solve the Diffie–Hellman problem to obtain gab. This is currently considered difficult for groups whose order is large enough. An efficient algorithm to solve the discrete logarithm problem would make it easy to compute a or b and solve the Diffie–Hellman problem, making this and many other public key cryptosystems insecure. Fields of small characteristic may be less secure.The order of G should have a large prime factor to prevent use of the Pohlig–Hellman algorithm to obtain a or b. For this reason, a Sophie Germain prime q is sometimes used to calculate p = 2q + 1, called a safe prime, since the order of G is then only divisible by 2 and q. g is then sometimes chosen to generate the order q subgroup of G, rather than G, so that the Legendre symbol of ga never reveals the low order bit of a. A protocol using such a choice is for example IKEv2.g is often a small integer such as 2. Because of the random self-reducibility of the discrete logarithm problem a small g is equally secure as any other generator of the same group.\nIf Alice and Bob use random number generators whose outputs are not completely random and can be predicted to some extent, then it is much easier to eavesdrop.\nIn the original description, the Diffie–Hellman exchange by itself does not provide authentication of the communicating parties and is thus vulnerable to a man-in-the-middle attack. Mallory (an active attacker executing the man-in-the-middle attack) may establish two distinct key exchanges, one with Alice and the other with Bob, effectively masquerading as Alice to Bob, and vice versa, allowing her to decrypt, then re-encrypt, the messages passed between them. Note that Mallory must continue to be in the middle, actively decrypting and re-encrypting messages every time Alice and Bob communicate. If she is ever absent, her previous presence is then revealed to Alice and Bob. They will know that all of their private conversations had been intercepted and decoded by someone in the channel. In most cases it will not help them get Mallory's private key, even if she used the same key for both exchanges.\nA method to authenticate the communicating parties to each other is generally needed to prevent this type of attack. Variants of Diffie–Hellman, such as STS protocol, may be used instead to avoid these types of attacks.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Unique Label Cover problem is a constraint satisfaction problem, where each constraint \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   involves two variables \n  \n    \n      \n        x\n        ,\n        y\n      \n    \n    {\\displaystyle x,y}\n  , and for each value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   there is a unique value of \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   that satisfies \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\nDetermining whether all the constraints can be satisfied is easy, but the Unique Game Conjecture (UGC) postulates that determining whether almost all the constraints (\n  \n    \n      \n        (\n        1\n        −\n        ε\n        )\n      \n    \n    {\\displaystyle (1-\\varepsilon )}\n  -fraction, for any constant \n  \n    \n      \n        ε\n        >\n        0\n      \n    \n    {\\displaystyle \\varepsilon >0}\n  ) can be satisfied or almost none of them (\n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  -fraction) can be satisfied is NP-hard.\nApproximation problems are often known to be NP-hard assuming UGC; such problems are referred to as UG-hard. \nIn particular, assuming UGC there is a semidefinite programming algorithm that achieves optimal approximation guarantees for many important problems.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Elizabethan spymaster Sir Francis Walsingham (1530–1590) is reported to have used a \"trellis\" to conceal the letters of a plaintext in communication with his agents. However, he generally preferred the combined code-cipher method known as a nomenclator, which was the practical state-of-the-art in his day. The trellis was described as a device with spaces that was reversible. It appears to have been a transposition tool that produced something much like the Rail fence cipher and resembled a chess board.\nCardano is not known to have proposed this variation, but he was a chess player who wrote a book on gaming, so the pattern would have been familiar to him.  Whereas the ordinary Cardan grille has arbitrary perforations, if his method of cutting holes is applied to the white squares of a chess board a regular pattern results.\nThe encipherer begins with the board in the wrong position for chess.  Each successive letter of the message is written in a single square.  If the message is written vertically, it is taken off horizontally and vice versa.\nAfter filling in 32 letters, the board is turned through 90 degrees and another 32 letters written (note that flipping the board horizontally or vertically is the equivalent).  Shorter messages are filled with null letters (i.e., padding).  Messages longer than 64 letters require another turn of the board and another sheet of paper.  If the plaintext is too short, each square must be filled up entirely with nulls.\nJ M T H H D L I S I Y P S L U I A O W A E T I E E N W A P D E N E N E L G O O N N A I T E E F N K E R L O O N D D N T T E N R X\nThis transposition method produces an invariant pattern and is not satisfactorily secure for anything other than cursory notes.\n33, 5, 41, 13, 49, 21, 57, 29, 1, 37, 9, 45, 17, 53, 25, 61, 34, 6, 42, 14, 50, 22, 58, 30, 2, 38, 10, 46, 18, 54, 26, 62, 35, 7, 43, 15, 51, 23, 59, 31, 3, 39, 11, 47, 19, 55, 27, 63, 36, 8, 44, 16, 52, 24, 60, 32, 4, 40, 12, 48, 20, 56, 28, 64\nA second transposition is needed to obscure the letters.  Following the chess analogy, the route taken might be the knight's move. Or some other path can be agreed upon, such as a reverse spiral, together with a specific number of nulls to pad the start and end of a message.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Non-repudiation, or more specifically non-repudiation of origin, is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.\nNote that these authentication, non-repudiation etc. properties rely on the secret key not having been revoked prior to its usage. Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an \"online\" check; e.g., checking a certificate revocation list or via the Online Certificate Status Protocol.  Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen. Of course, with stolen key pairs, the theft is often discovered only after the secret key's use, e.g., to sign a bogus certificate for espionage purpose.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Modern ciphers are more secure than classical ciphers and are designed  to withstand a wide range of attacks. An attacker should not be able to find the key used in a modern cipher, even if he knows any amount of plaintext and corresponding ciphertext. Modern encryption methods can be divided into the following categories:\n\nPrivate-key cryptography (symmetric key algorithm): the same key is used for encryption and decryption\nPublic-key cryptography (asymmetric key algorithm): two different keys are used for encryption and decryptionIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a public key is published and enables any sender to perform encryption, while a private key is kept secret by the receiver and enables only him to perform correct decryption.\nSymmetric key ciphers can be divided into block ciphers and stream ciphers. Block ciphers operate on fixed-length groups of bits, called blocks, with an unvarying transformation. Stream ciphers encrypt plaintext digits one at a time on a continuous stream of data and the transformation of successive digits varies during the encryption process.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person, but rather to one or more specific keys (or \"addresses\"). Thereby, Bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain. Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In early 2020, the CrypTool project decided to merge with a similar project of the same name, CrypTools, founded in 2017 in Australia by Luka Lafaye de Micheaux, Arthur Guiot, and Lucas Gruwez. CrypTool, much older and known, thus completely \"absorbs\" the project under its name.\n\nThe first impact of this merger is the rebranding of the project. A new logo, a new website, and the new CTO version are announced. Currently, it's still in development. Another change was the targeted audience. Previously, CrypTool focused on (university) students, and CrypTools on developers and young people. It was therefore necessary to broaden the audience.\nOn May 15, 2020, in the midst of the COVID-19 pandemic, CrypTool announces the creation of tools to test Decentralized contact tracing protocols. A new page is added to CTO with technical description of the algorithms involved in DP-3T and Exposure Notification. In addition to this, CrypTool also announced the implementation of a page dedicated to raising awareness of the cryptographic means related to privacy in these protocols, called the Corona Tracing Animation. The newer page stands out for its new design and its accessibility to ordinary users.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Every operation performed by a computer emits electromagnetic radiation and different operations emit radiation at different frequencies. In electromagnetic side-channel attacks, an attacker is only interested in a few frequencies at which encryption is occurring. Signal processing is responsible for isolating these frequencies from the vast multitude of extraneous radiation and noise. To isolate certain frequencies, a bandpass filter, which blocks frequencies outside of a given range, must be applied to the electromagnetic trace. Sometimes, the attacker does not know which frequencies encryption is performed at. In this case, the trace can be represented as a spectrogram, which can help determine which frequencies are most prevalent at different points of execution. Depending on the device being attacked and the level of noise, several filters may need to be applied.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate which is defined when the system is created and which is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency. In the case of cryptocurrency, companies or governments cannot produce new units, and have not so far provided backing for other firms, banks or corporate entities which hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.Within a proof-of-work system such as Bitcoin, the safety, integrity and balance of ledgers is maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme. In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.\nMost cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation. Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Hash flooding (also known as HashDoS) is a denial of service attack that uses hash collisions to exploit the worst-case (linear probe) runtime of hash table lookups. It was originally described in 2003. To execute such an attack, the attacker sends the server multiple pieces of data that hash to the same value and then tries to get the server to perform slow lookups. As the main focus of hash functions used in hash tables was speed instead of security, most major programming languages were affected, with new vulnerabilities of this class still showing up a decade after the original presentation.To prevent hash flooding without making the hash function overly complex, newer keyed hash functions are introduced, with the security objective that collisions are hard to find as long as the key is unknown. They may be slower than previous hashes, but are still much easier to compute than cryptographic hashes. As of 2021, Daniel J. Bernstein's SipHash (2012) is the most widely-used hash function in this class. (Non-keyed \"simple\" hashes remain safe to use as long as the application's hash table is not controllable from the outside.)\nIt is possible to perform an analogous attack to fill up Bloom filters using a (partial) preimage attack.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"A5\/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard.\nSecurity researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn't feel this way, and the algorithm as now fielded is a French design.\"According to professor Jan Arild Audestad, at the standardization process which started in 1982, A5\/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now estimated that 128 bits would in fact also still be secure as of 2014. Audestad, Peter van der Arend, and Thomas Haug say that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 56 bits. In general, a key of length 56 is \n  \n    \n      \n        \n          2\n          \n            128\n            −\n            56\n          \n        \n        =\n        \n          2\n          \n            72\n          \n        \n        =\n        4.7\n        ×\n        \n          10\n          \n            21\n          \n        \n      \n    \n    {\\displaystyle 2^{128-56}=2^{72}=4.7\\times 10^{21}}\n   times easier to break than a key of length 128.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"There exist groups for which computing discrete logarithms is apparently difficult.  In some cases (e.g. large prime order subgroups of  groups (Zp)×) there is not only no efficient algorithm known for the worst case, but the average-case complexity can be shown to be about as hard as the worst case using random self-reducibility.At the same time, the inverse problem of discrete exponentiation is not difficult (it can be computed efficiently using exponentiation by squaring, for example).  This asymmetry is analogous to the one between integer factorization and integer multiplication. Both asymmetries (and other possibly one-way functions) have been exploited in the construction of cryptographic systems.\nPopular choices for the group G in discrete logarithm cryptography (DLC) are the cyclic groups (Zp)× (e.g. ElGamal encryption, Diffie–Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (see Elliptic curve cryptography).\nWhile there is no publicly known algorithm for solving the discrete logarithm problem in general, the first three steps of the number field sieve algorithm only depend on the group G, not on the specific elements of G whose finite log is desired. By precomputing these three steps for a specific group, one need only carry out the last step, which is much less computationally expensive than the first three, to obtain a specific logarithm in that group.It turns out that much Internet traffic uses one of a handful of groups that are of order 1024 bits or less, e.g. cyclic groups with order of the Oakley primes specified in RFC 2409. The Logjam attack used this vulnerability to compromise a variety of Internet services that allowed the use of groups whose order was a 512-bit prime number, so called export grade.The authors of the Logjam attack estimate that the much more difficult precomputation needed to solve the discrete log problem for a 1024-bit prime would be within the budget of a large national intelligence agency such as the U.S. National Security Agency (NSA). The Logjam authors speculate that precomputation against widely reused 1024 DH primes is behind claims in leaked NSA documents that NSA is able to break much of current cryptography.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In 2007, IFSB was published. In 2010, S-FSB was published, which is 30% faster than the original.\nIn 2011, D. J. Bernstein and Tanja Lange published RFSB, which is 10x faster than the original FSB-256.\n RFSB was shown to run very fast on the Spartan 6 FPGA, reaching throughputs of around 5 Gbit\/s.>\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Hessian curve\nEdwards curve\nTwisted curve\nTwisted Hessian curve\nTwisted Edwards curve\nDoubling-oriented Doche–Icart–Kohel curve\nTripling-oriented Doche–Icart–Kohel curve\nJacobian curve\nMontgomery curve\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The output feedback (OFB) mode makes a block cipher into a synchronous stream cipher.  It generates keystream blocks, which are then XORed with the plaintext blocks to get the ciphertext.  Just as with other stream ciphers, flipping a bit in the ciphertext produces a flipped bit in the plaintext at the same location.  This property allows many error-correcting codes to function normally even when applied before encryption.\nBecause of the symmetry of the XOR operation, encryption and decryption are exactly the same:\n\n  \n    \n      \n        \n          C\n          \n            j\n          \n        \n        =\n        \n          P\n          \n            j\n          \n        \n        ⊕\n        \n          O\n          \n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle C_{j}=P_{j}\\oplus O_{j},}\n  \n\n  \n    \n      \n        \n          P\n          \n            j\n          \n        \n        =\n        \n          C\n          \n            j\n          \n        \n        ⊕\n        \n          O\n          \n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle P_{j}=C_{j}\\oplus O_{j},}\n  \n\n  \n    \n      \n        \n          O\n          \n            j\n          \n        \n        =\n        \n          E\n          \n            K\n          \n        \n        (\n        \n          I\n          \n            j\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle O_{j}=E_{K}(I_{j}),}\n  \n\n  \n    \n      \n        \n          I\n          \n            j\n          \n        \n        =\n        \n          O\n          \n            j\n            −\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle I_{j}=O_{j-1},}\n  \n\n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        =\n        \n          IV\n        \n        .\n      \n    \n    {\\displaystyle I_{0}={\\text{IV}}.}\n  \nEach output feedback block cipher operation depends on all previous ones, and so cannot be performed in parallel.  However, because the plaintext or ciphertext is only used for the final XOR, the block cipher operations may be performed in advance, allowing the final step to be performed in parallel once the plaintext or ciphertext is available.\nIt is possible to obtain an OFB mode keystream by using CBC mode with a constant string of zeroes as input. This can be useful, because it allows the usage of fast hardware implementations of CBC mode for OFB mode encryption.\nUsing OFB mode with a partial block as feedback like CFB mode reduces the average cycle length by a factor of 232 or more. A mathematical model proposed by Davies and Parkin and substantiated by experimental results showed that only with full feedback an average cycle length near to the obtainable maximum can be achieved. For this reason, support for truncated feedback was removed from the specification of OFB.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The historical Edgar Allan Poe has appeared as a fictionalized character, often in order to represent the \"mad genius\" or \"tormented artist\" and in order to exploit his personal struggles. Many such depictions also blend in with characters from his stories, suggesting that Poe and his characters share identities. Often, fictional depictions of Poe use his mystery-solving skills in such novels as The Poe Shadow by Matthew Pearl.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"To encrypt a message addressed to Alice in a hybrid cryptosystem, Bob does the following:\n\nObtains Alice's public key.\nGenerates a fresh symmetric key for the data encapsulation scheme.\nEncrypts the message under the data encapsulation scheme, using the symmetric key just generated.\nEncrypts the symmetric key under the key encapsulation scheme, using Alice's public key.\nSends both of these ciphertexts to Alice.To decrypt this hybrid ciphertext, Alice does the following:\n\nUses her private key to decrypt the symmetric key contained in the key encapsulation segment.\nUses this symmetric key to decrypt the message contained in the data encapsulation segment.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Technically speaking, a digital signature applies to a string of bits, whereas humans and applications \"believe\" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. WYSIWYS (What You See Is What You Sign) means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by Peter Landrock and Torben Pedersen to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The original Cardano Grille was a literary device for gentlemen's private correspondence.  Any suspicion of its use can lead to discoveries of hidden messages where no hidden messages exist at all, thus confusing the cryptanalyst. Letters and numbers in a random grid can take shape without substance. Obtaining the grille itself is a chief goal of the attacker.\nBut all is not lost if a grille copy can't be obtained. The later variants of the Cardano grille present problems which are common to all transposition ciphers. Frequency analysis will show a normal distribution of letters, and will suggest the language in which the plaintext was written. The problem, easily stated though less easily accomplished, is to identify the transposition pattern and so decrypt the ciphertext.  Possession of several messages written using the same grille is a considerable aid.\nGaines, in her standard work on hand ciphers and their cryptanalysis, gave a lengthy account of transposition ciphers, and devoted a chapter to the turning grille.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The mid-1970s saw two major public (i.e., non-secret) advances. First was the publication of the draft Data Encryption Standard in the U.S. Federal Register on 17 March 1975. The proposed DES cipher was submitted by a research group at IBM, at the invitation of the National Bureau of Standards (now NIST), in an effort to develop secure electronic communication facilities for businesses such as banks and other large financial organizations. After advice and modification by the NSA, acting behind the scenes, it was adopted and published as a Federal Information Processing Standard Publication in 1977 (currently at FIPS 46-3). DES was the first publicly accessible cipher to be 'blessed' by a national agency such as the NSA. The release of its specification by NBS stimulated an explosion of public and academic interest in cryptography.\nThe aging DES was officially replaced by the Advanced Encryption Standard (AES) in 2001 when NIST announced FIPS 197. After an open competition, NIST selected Rijndael, submitted by two Belgian cryptographers, to be the AES. DES, and more secure variants of it (such as Triple DES), are still used today, having been incorporated into many national and organizational standards. However, its 56-bit key-size has been shown to be insufficient to guard against brute force attacks (one such attack, undertaken by the cyber civil-rights group Electronic Frontier Foundation in 1997, succeeded in 56 hours.) As a result, use of straight DES encryption is now without doubt insecure for use in new cryptosystem designs, and messages protected by older cryptosystems using DES, and indeed all messages sent since 1976 using DES, are also at risk. Regardless of DES' inherent quality, the DES key size (56-bits) was thought to be too small by some even in 1976, perhaps most publicly by Whitfield Diffie. There was suspicion that government organizations even then had sufficient computing power to break DES messages; clearly others have achieved this capability.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The powers of 10 are\n\n  \n    \n      \n        …\n        ,\n        0.001\n        ,\n        0.01\n        ,\n        0.1\n        ,\n        1\n        ,\n        10\n        ,\n        100\n        ,\n        1000\n        ,\n        …\n        .\n      \n    \n    {\\displaystyle \\ldots ,0.001,0.01,0.1,1,10,100,1000,\\ldots .}\n  For any number a in this list, one can compute log10 a. For example, log10 10000 = 4, and log10 0.001 = −3. These are instances of the discrete logarithm problem.\nOther base-10 logarithms in the real numbers are not instances of the discrete logarithm problem, because they involve non-integer exponents. For example, the equation log10 53 = 1.724276… means that 101.724276… = 53. While integer exponents can be defined in any group using products and inverses, arbitrary real exponents, such as this 1.724276…, require other concepts such as the exponential function.\nIn group-theoretic terms, the powers of 10 form a cyclic group G under multiplication, and 10 is a generator for this group. The discrete logarithm log10 a is defined for any a in G.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Broadway Journal failed in 1846, and Poe moved to a cottage in Fordham, New York, in the Bronx. That home, now known as the Edgar Allan Poe Cottage, was relocated in later years to a park near the southeast corner of the Grand Concourse and Kingsbridge Road. Nearby, Poe befriended the Jesuits at St. John's College, now Fordham University. Virginia died at the cottage on January 30, 1847. Biographers and critics often suggest that Poe's frequent theme of the \"death of a beautiful woman\" stems from the repeated loss of women throughout his life, including his wife.Poe was increasingly unstable after his wife's death. He attempted to court poet Sarah Helen Whitman, who lived in Providence, Rhode Island. Their engagement failed, purportedly because of Poe's drinking and erratic behavior. There is also strong evidence that Whitman's mother intervened and did much to derail the relationship. Poe then returned to Richmond and resumed a relationship with his childhood sweetheart Sarah Elmira Royster.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Poe's writing reflects his literary theories, which he presented in his criticism and also in essays such as \"The Poetic Principle\". He disliked didacticism and allegory, though he believed that meaning in literature should be an undercurrent just beneath the surface. Works with obvious meanings, he wrote, cease to be art. He believed that work of quality should be brief and focus on a specific single effect. To that end, he believed that the writer should carefully calculate every sentiment and idea.Poe describes his method in writing \"The Raven\" in the essay \"The Philosophy of Composition\", and he claims to have strictly followed this method. It has been questioned whether he really followed this system, however. T. S. Eliot said: \"It is difficult for us to read that essay without reflecting that if Poe plotted out his poem with such calculation, he might have taken a little more pains over it: the result hardly does credit to the method.\" Biographer Joseph Wood Krutch described the essay as \"a rather highly ingenious exercise in the art of rationalization\".\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Schneier, Bruce. Applied Cryptography, John Wiley & Sons, 1994. ISBN 0-471-59756-2\nSchneier, Bruce. Protect Your Macintosh, Peachpit Press, 1994. ISBN 1-56609-101-2\nSchneier, Bruce. E-Mail Security, John Wiley & Sons, 1995. ISBN 0-471-05318-X\nSchneier, Bruce. Applied Cryptography, Second Edition, John Wiley & Sons, 1996. ISBN 0-471-11709-9\nSchneier, Bruce; Kelsey, John; Whiting, Doug; Wagner, David; Hall, Chris; Ferguson, Niels. The Twofish Encryption Algorithm, John Wiley & Sons, 1996. ISBN 0-471-35381-7\nSchneier, Bruce; Banisar, David. The Electronic Privacy Papers, John Wiley & Sons, 1997. ISBN 0-471-12297-1\nSchneier, Bruce. Secrets and Lies: Digital Security in a Networked World, John Wiley & Sons, 2000. ISBN 0-471-25311-1\nSchneier, Bruce. Beyond Fear: Thinking Sensibly About Security in an Uncertain World, Copernicus Books, 2003. ISBN 0-387-02620-7\nFerguson, Niels; Schneier, Bruce. Practical Cryptography, John Wiley & Sons, 2003. ISBN 0-471-22357-3\nSchneier, Bruce. Secrets and Lies: Digital Security in a Networked World, John Wiley & Sons, 2004. ISBN 978-0-471-45380-2\nSchneier, Bruce. Schneier on Security, John Wiley & Sons, 2008. ISBN 978-0-470-39535-6\nFerguson, Niels; Schneier, Bruce; Kohno, Tadayoshi. Cryptography Engineering, John Wiley & Sons, 2010. ISBN 978-0-470-47424-2\nSchneier, Bruce. Liars and Outliers: Enabling the Trust that Society Needs to Thrive, John Wiley & Sons, 2012. ISBN 978-1-118-14330-8\nSchneier, Bruce. Carry On: Sound Advice from Schneier on Security, John Wiley & Sons, 2013. ISBN 978-1118790816\nSchneier, Bruce. Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World, W. W. Norton & Company, 2015. ISBN 978-0-393-24481-6\nSchneier, Bruce. Click Here to Kill Everybody: Security and Survival in a Hyper-connected World, W. W. Norton & Company, 2018. ISBN 978-0-393-60888-5\nSchneier, Bruce. We Have Root: Even More Advice from Schneier on Security, John Wiley & Sons, 2019.  ISBN 978-1119643012\nSchneier, Bruce. A Hacker’s Mind: How the Powerful Bend Society’s Rules, and How to Bend them Back, W. W. Norton & Company, 2023.  ISBN 978-0-393-86666-7\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The UK and US employed large numbers of women in their code-breaking operation, with close to 7,000 reporting to Bletchley Park  \nand 11,000 to the separate US Army and Navy operations, around Washington, DC. By tradition in Japan and Nazi doctrine in Germany, women were excluded from war work, at least until late in the war. Even after encryption systems were broken, large amounts of work were needed to respond to changes made, recover daily key settings for multiple networks, and intercept, process, translate, prioritize and analyze the huge volume of enemy messages generated in a global conflict. A few women, including Elizabeth Friedman and Agnes Meyer Driscoll, had been major contributors to US code-breaking in the 1930s and the Navy and Army began actively recruiting top graduates of women's colleges shortly before the attack on Pearl Harbor. Liza Mundy argues that this disparity in utilizing the talents of women between the Allies and Axis made a strategic difference in the war.: p.29\n\n###\n\n","completion":" Cryptography"}
{"prompt":"For any cipher, the most basic method of attack is brute force—trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 256 bits to 56 bits to fit on a single chip.\nIn academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented—or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: \"There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES.\" The machine brute-forced a key in a little more than 2 days' worth of searching.\nThe next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well.  One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware—see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. SciEngines RIVYERA held the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has further lowered this time.\nIn 2012, David Hulton and Moxie Marlinspike announced a system with 48 Xilinx Virtex-6 LX240T FPGAs, each FPGA containing 40 fully pipelined DES cores running at 400 MHz, for a total capacity of  768 gigakeys\/sec. The system can exhaustively search the entire 56-bit DES key space in about 26 hours and this service is offered for a fee online.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Although Camellia is patented, it is available under a royalty-free license.  This has allowed the Camellia cipher to become part of the OpenSSL Project, under an open-source license, since November 2006. It has also allowed it to become part of the Mozilla's NSS (Network Security Services) module.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Entering a PIN code to activate the smart card commonly requires a numeric keypad. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a keystroke logger, potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often EAL3 certified.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"CrypTool has received several international awards as an educational program, such as the TeleTrusT Special Award 2004, EISA 2004, IT Security Award NRW 2004, and Selected Landmark in the Land of Ideas 2008 award.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Diffusion means that if we change a single bit of the plaintext, then about half of the bits in the ciphertext should change, and similarly, if we change one bit of the ciphertext, then about half of the plaintext bits should change. This is equivalent to the expectation that encryption schemes exhibit an avalanche effect.\nThe purpose of diffusion is to hide the statistical relationship between the ciphertext and the plain text. For example, diffusion ensures that any patterns in the plaintext, such as redundant bits, are not apparent in the ciphertext. Block ciphers achieve this by \"diffusing\" the information about the plaintext's structure across the rows and columns of the cipher.\nIn substitution–permutation networks, diffusion is provided by permutation boxes (a.k.a. permutation layer). In the beginning of the 21st century a consensus had appeared where the designers preferred the permutation layer to consist of linear Boolean functions, although nonlinear functions can be used, too.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In a treatise entitled as Risala fi l-Illa al-Failali l-Madd wa l-Fazr (Treatise on the Efficient Cause of the Flow and Ebb), al-Kindi presents a theory on tides which \"depends on the changes which take place in bodies owing to the rise and fall of temperature.\" In order to support his argument, he gave a description of a scientific experiment as follows:\n\nOne can also observe by the senses... how in consequence of extreme cold air changes into water. To do this, one takes a glass bottle, fills it completely with snow, and closes its end carefully. Then one determines its weight by weighing. One places it in a container... which has previously been weighed. On the surface of the bottle the air changes into water, and appears upon it like the drops on large porous pitchers, so that a considerable amount of water gradually collects inside the container. One then weighs the bottle, the water and the container, and finds their weight greater than previously, which proves the change. [...] Some foolish persons are of opinion that the snow exudes through the glass. This is impossible. There is no process by which water or snow can be made to pass through glass.\nIn explaining the natural cause of the wind, and the difference for its directions based on time and location, he wrote:\nWhen the sun is in its northern declination northerly places will heat up and it will be cold towards the south. Then the northern air will expand in a southerly direction because of the heat due to the contraction of the southern air. Therefore most of the summer winds are merits and most of the winter winds are not.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Synthetic initialization vector (SIV) is a nonce-misuse resistant block cipher mode.\nSIV synthesizes an internal IV using the pseudorandom function S2V. S2V is a keyed hash is based on CMAC, and the input to the function is:\n\nAdditional authenticated data (zero, one or many AAD fields are supported)\nPlaintext\nAuthentication key (K1).SIV encrypts the S2V output and the plaintext using AES-CTR, keyed with the encryption key (K2).\nSIV can support external nonce-based authenticated encryption, in which case one of the authenticated data fields is utilized for this purpose. RFC5297 specifies that for interoperability purposes the last authenticated data field should be used external nonce.\nOwing to the use of two keys, the authentication key K1 and encryption key K2, naming schemes for SIV AEAD-variants may lead to some confusion; for example AEAD_AES_SIV_CMAC_256 refers to AES-SIV with two AES-128 keys and not AES-256.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Kitab al-Ayn was the first dictionary written for the Arabic language. \"Ayn\" is the deepest letter in Arabic, and \"ayn\" may also mean a water source in the desert. Its title, \"the source\", reflects its author's goal to derive the etymological origins of Arabic vocabulary and lexicography.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The CrypTool project also includes the website CrypTool-Online, launched in 2009. This website allows users to try cryptographic methods directly within a browser on a PC or on a smartphone (using JavaScript), without the need to download and install software. \nThis site aims to present the topic in an easy and attractive way for new users and young people.  Advanced tasks still require the offline versions of CrypTool.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The above groups can be described algebraically as well as geometrically.. Given the curve y2 = x3 + ax + b over the field K (whose characteristic we assume to be neither 2 nor 3), and points P = (xP, yP) and Q = (xQ, yQ) on the curve, assume first that xP ≠ xQ (case 1)..\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Cryptocurrencies use various timestamping schemes to \"prove\" the validity of transactions added to the blockchain ledger without the need for a trusted third party.\nThe first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.\nAnother method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In his Kitab al-Fihrist (Catalogue), Ibn al-Nadim recounts the various names attached to the transmission of Kitab al-'Ayn, i.e. the isnad (chain of authorities).  He begins with Durustuyah's account that it was al-Kasrawi who said that al-Zaj al-Muhaddath had said that al-Khalil had explained the concept and structure of his dictionary to al-Layth b. al-Muzaffar b. Nasr b. Sayyar, had dictated edited portions to al-Layth and they had reviewed its preparation together. Ibn al-Nadim writes that a manuscript in the possession of Da'laj had probably belonged originally to Ibn al-'Ala al-Sijistani, who according to Durustuyah had been a member of a circle of scholars who critiqued the book. In this group was Abu Talib al-Mufaddal ibn Slamah, 'Abd Allah ibn Muhammad al-Karmani, Abu Bakr ibn Durayd and al-Huna'i al-Dawsi.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In February 2014, the world's largest Bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 Bitcoins belonging to their clients. This added up to approximately 7% of all Bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a Bitcoin fell from a high of about $1,160 in December to under $400 in February.On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70M using a hijacked company computer.On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year. Customers were still granted access to 75% of their assets.\nIn May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers. Exchanges lost an estimated $18m and Bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.\nOn 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release. Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The U.S. Securities and Exchange Commission separately brought a civil enforcement action against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold \"investment contracts representing shares in the profits they claimed would be generated\" from mining.In January 2018, Japanese exchange Coincheck reported that hackers had stolen $530M worth of cryptocurrencies.In June 2018, South Korean exchange Coinrail was hacked, losing over $37M worth of cryptos. The hack worsened an already ongoing cryptocurrency selloff by an additional $42 billion.On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in cryptocurrency stolen.A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets \"typically find their way to illegal markets and are used to fund further criminal activity\".According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, the following three categories make up the majority of illicit cryptocurrency uses: \"(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself.\" The report concludes that \"for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed\" and that \"the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses\".According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as Bitcoin for money laundering and terrorism financing is assessed as \"medium\" (from \"low\" in the previous 2017 report). Legal scholars suggested that the money laundering opportunities may be more perceived than real. Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.In December 2021, Monkey Kingdom - a NFT project based in Hong Kong lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Until 1937 the Cipher Bureau's German section, BS-4, had been housed in the Polish General Staff building – the stately 18th-century \"Saxon Palace\" – in Warsaw. That year BS-4 moved into specially constructed new facilities in the Kabaty Woods near Pyry, south of Warsaw.  There, working conditions were incomparably better than in the cramped quarters at the General Staff building.The move was dictated as well by requirements of security. Germany's Abwehr was always looking for potential traitors among the military and civilian workers at the General Staff building. Strolling agents, even if lacking access to the Staff building, could observe personnel entering and leaving, and photograph them with concealed miniature cameras. Annual Abwehr intelligence assignments for German agents in Warsaw placed a priority on securing informants at the Polish General Staff.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"If both the key encapsulation and data encapsulation schemes in a hybrid cryptosystem are secure against adaptive chosen ciphertext attacks, then the hybrid scheme inherits that property as well. However, it is possible to construct a hybrid scheme secure against adaptive chosen ciphertext attacks even if the key encapsulation has a slightly weakened security definition (though the security of the data encapsulation must be slightly stronger).\n\n###\n\n","completion":" Cryptography"}
{"prompt":"This property results in the cipher's security degrading quadratically, and needs to be taken into account when selecting a block size. There is a trade-off though as large block sizes can result in the algorithm becoming inefficient to operate. Earlier block ciphers such as the DES have typically selected a 64-bit block size, while newer designs such as the AES support block sizes of 128 bits or more, with some ciphers supporting a range of different block sizes.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.\nNode owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In 2013, The New York Times stated that Dual Elliptic Curve Deterministic Random Bit Generation (or Dual_EC_DRBG) had been included as a NIST national standard due to the influence of NSA, which had included a deliberate weakness in the algorithm and the recommended elliptic curve. RSA Security in September 2013 issued an advisory recommending that its customers discontinue using any software based on Dual_EC_DRBG. In the wake of the exposure of Dual_EC_DRBG as \"an NSA undercover operation\", cryptography experts have also expressed concern over the security of the NIST recommended elliptic curves, suggesting a return to encryption based on non-elliptic-curve groups.\n\nAdditionally, in August 2015, the NSA announced that it plans to replace Suite B with a new cipher suite due to concerns about quantum computing attacks on ECC.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Transaction fees for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction. The currency holder can choose a specific transaction fee, while network entities process transactions in order of highest offered fee to lowest. Cryptocurrency exchanges can simplify the process for currency holders by offering priority alternatives and thereby determine which fee will likely cause the transaction to be processed in the requested time.For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while Bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845, while for Bitcoin it corresponded to $0.659.Some cryptocurrencies have no transaction fees, and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers. A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades. A 2019 report by Bitwise Asset Management claimed that 95% of all Bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In 2005, Curve25519 was first released by Daniel J. Bernstein.In 2013, interest began to increase considerably when it was discovered that the NSA had potentially implemented a backdoor into the P-256 curve based Dual_EC_DRBG algorithm. While not directly related, suspicious aspects of the NIST's P curve constants led to concerns that the NSA had chosen values that gave them an advantage in breaking the encryption.\n\"I no longer trust the constants. I believe the NSA has manipulated them through their relationships with industry.\"\n\nSince 2013, Curve25519 has become the de facto alternative to P-256, being used in a wide variety of applications. Starting in 2014, OpenSSH defaults to Curve25519-based ECDH and GnuPG adds support for Ed25519 keys for signing and encryption. Behavior for general SSH protocol is still being standardized as of 2018.In 2017, NIST announced that Curve25519 and Curve448 would be added to Special Publication 800-186, which specifies approved elliptic curves for use by the US Federal Government. Both are described in RFC 7748. A 2019 draft of \"FIPS 186-5\" notes the intention to allow usage of Ed25519 for digital signatures. A 2019 draft of Special Publication 800-186 notes the intention to allow usage of Curve25519.In 2018, DKIM specification was amended so as to allow signatures with this algorithm.Also in 2018, RFC 8446 was published as the new Transport Layer Security v1.3 standard. It recommends support for X25519, Ed25519, X448, and Ed448 algorithms.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The Americans referred to the intelligence resulting from cryptanalysis, perhaps especially that from the Purple machine, as 'Magic'. The British eventually settled on 'Ultra' for intelligence resulting from cryptanalysis, particularly that from message traffic protected by the various Enigmas. An earlier British term for Ultra had been 'Boniface' in an attempt to suggest, if betrayed, that it might have an individual agent as a source.\n\nAllied cipher machines used in World War II included the British TypeX and the American SIGABA; both were electromechanical rotor designs similar in spirit to the Enigma, albeit with major improvements. Neither is known to have been broken by anyone during the War. The Poles used the Lacida machine, but its security was found to be less than intended (by Polish Army cryptographers in the UK), and its use was discontinued. US troops in the field used the M-209 and the still less secure M-94 family machines. British SOE agents initially used 'poem ciphers' (memorized poems were the encryption\/decryption keys), but later in the War, they began to switch to one-time pads.\nThe VIC cipher (used at least until 1957 in connection with Rudolf Abel's NY spy ring) was a very complex hand cipher, and is claimed to be the most complicated known to have been used by the Soviets, according to David Kahn in Kahn on Codes. For the decrypting of Soviet ciphers (particularly when one-time pads were reused), see Venona project.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Crypto-macroeconomics is concerned with the regional, national, and international regulation of cryptocurrencies and DeFi transactions. The Group of Seven governments' interest in cryptocurrencies became evident in August 2014, when the United Kingdom Treasury commissioned a study of cryptocurrencies and their potential role in the UK economy, and issued its final report in January 2021. In June 2021, El Salvador became the first country to accept Bitcoin as legal tender. In August 2021, Cuba followed with a legal resolution to recognize and regulate cryptocurrencies such as Bitcoin. However, in September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal, completing a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Schneier is a proponent of full disclosure, i.e. making security issues public.\n\nIf researchers don't go public, things don’t get fixed. Companies don't see it as a security problem; they see it as a PR problem.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"While viruses in the wild have used cryptography in the past, the only purpose of such usage of cryptography was to avoid detection by antivirus software. For example, the tremor virus used polymorphism as a defensive technique in an attempt to avoid detection by anti-virus software. Though cryptography does assist in such cases to enhance the longevity of a virus, the capabilities of cryptography are not used in the payload. The One-half virus was amongst the first viruses known to have encrypted affected files.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In 2006, Hellman suggested the algorithm be called Diffie–Hellman–Merkle key exchange in recognition of Ralph Merkle's contribution to the invention of public-key cryptography (Hellman, 2006), writing:\n\nThe system...has since become known as Diffie–Hellman key exchange. While that system was first described in a paper by Diffie and me, it is a public key distribution system, a concept developed by Merkle, and hence should be called 'Diffie–Hellman–Merkle key exchange' if names are to be associated with it. I hope this small pulpit might help in that endeavor to recognize Merkle's equal contribution to the invention of public key cryptography.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"\"Movie-plot threat\" is a term Schneier coined that refers to very specific and dramatic terrorist attack scenarios, reminiscent of the behavior of terrorists in movies, rather than what terrorists actually do in the real world. Security measures created to protect against movie plot threats do not provide a higher level of real security, because such preparation only pays off if terrorists choose that one particular avenue of attack, which may not even be feasible. Real-world terrorists would also be likely to notice the highly specific security measures, and simply attack in some other way. The specificity of movie plot threats gives them power in the public imagination, however, so even extremely unrealistic security theater countermeasures may receive strong support from the public and legislators. Among many other examples of movie plot threats, Schneier described banning baby carriers from subways, for fear that they may contain explosives. Starting in April 2006, Schneier has had an annual contest to create the most fantastic movie-plot threat. In 2015, during the 8th and as of 17 February 2022 the last one, he mentioned that the contest may have run its course.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Abu Yusuf Yaqub ibn Ishaq al-Sabbah Al-Kindi, (A Manuscript on Deciphering Cryptographic Messages), 9th century included first known explanation of frequency analysis cryptanalysis\nMichel de Nostredame, (16th century prophet famed since 1555 for prognostications), known widely for his \"Les Propheties\" sets of quatrains composed from four languages into a ciphertext, deciphered in a series called \"Rise to Consciousness\" (Deschausses, M., Outskirts Press, Denver, CO, Nov 2008).\nRoger Bacon (English friar and polymath), Epistle on the secret Works of Art and Nullity of Magic, 13th century, possibly the first  European work on cryptography since Classical times, written in Latin and not widely available then or now\nJohannes Trithemius, Steganographia (\"Hidden Writing\"), written ca. 1499; pub 1606, banned by the Catholic Church 1609 as alleged discussion of magic, see Polygraphiae (below).\nJohannes Trithemius, Polygraphiae Libri Sex (\"Six Books on Polygraphy\"), 1518, first printed book on cryptography (thought to really be about magic by some observers at the time)\nGiovan Battista Bellaso, La cifra del. Sig. Giovan Battista Bellaso, 1553, first pub of the cypher widely misattributed to Vigenère.\nGiambattista della Porta, De Furtivis Literarum Notis (\"On concealed characters in writing\"), 1563.\nBlaise de Vigenère, Traicte de Chiffres, 1585.\nGustavus Selenus, Cryptomenytics, 1624, (modern era English trans by J W H Walden)\nJohn Wilkins, Mercury, 1647, earliest printed book in English about cryptography\nJohann Ludwig Klüber, Kryptographik Lehrbuch der Geheimschreibekunst (\"Cryptology: Instruction Book on the Art of Secret Writing\"), 1809.\nFriedrich Kasiski, Die Geheimschriften und die Dechiffrierkunst (\"Secret writing and the Art of Deciphering\"), pub 1863, contained the first public description of a technique for cryptanalyzing polyalphabetic cyphers.\nEtienne Bazeries, Les Chiffres secrets dévoilés (\"Secret ciphers unveiled\") about 1900.\nÉmile Victor Théodore Myszkowski, Cryptographie indéchiffrable: basée sur de nouvelles combinaisons rationelles (\"Unbreakable cryptography\"), published 1902.\nWilliam F. Friedman and others, the Riverbank Publications, a series of pamphlets written during and after World War I that are considered seminal to modern cryptanalysis, including no. 22 on the Index of Coincidence.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the Automotive Network Exchange for the automobile industry and the SAFE-BioPharma Association for the healthcare industry.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"The boomerang attack is based on differential cryptanalysis.. In differential cryptanalysis, an attacker exploits how differences in the input to a cipher (the plaintext) can affect the resultant difference at the output (the ciphertext).. A high probability \"differential\" (that is, an input difference that will produce a likely output difference) is needed that covers all, or nearly all, of the cipher.. The boomerang attack allows differentials to be used which cover only part of the cipher.. The attack attempts to generate a so-called \"quartet\" structure at a point halfway through the cipher.. For this purpose, say that the encryption action, E, of the cipher can be split into two consecutive stages, E0 and E1, so that E(M) = E1(E0(M)), where M is some plaintext message..\n\n###\n\n","completion":" Cryptography"}
{"prompt":"On October 23, 2017, Shaanan Cohney, Matthew Green, and Nadia Heninger, cryptographers at The University of Pennsylvania and Johns Hopkins University released details of the DUHK (Don't Use Hard-coded Keys) attack on WPA2 where hardware vendors use a hardcoded seed key for the ANSI X9.31 RNG algorithm, stating \"an attacker can brute-force encrypted data to discover the rest of the encryption parameters and deduce the master encryption key used to encrypt web sessions or virtual private network (VPN) connections.\"\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Alice decrypts a ciphertext                         (                    c                        1                             ,                    c                        2                             )                 {\\displaystyle (c_{1},c_{2})}    with her private key                         x                 {\\displaystyle x}    as follows:  Compute                         s         :=                    c                        1                                   x                                     {\\displaystyle s:=c_{1}^{x}}   ..\n\n###\n\n","completion":" Cryptography"}
{"prompt":"Kahn was awarded a doctorate (D.Phil) from Oxford University in 1974, in modern German history under the supervision of the then Regius professor of modern history, Hugh Trevor-Roper.\nKahn continued his work as a reporter and op-ed editor for Newsday until 1998 and served as a journalism professor at New York University.\nDespite past differences between Kahn and the National Security Agency over the information in The Codebreakers, Kahn was selected in 1995 to become NSA's scholar-in-residence. On October 26, 2010, Kahn attended a ceremony at NSA's National Cryptologic Museum (NCM) to commemorate his donation of his lifetime collection of cryptologic books, memorabilia, and artifacts to the museum and its library. The collection is housed at the NCM library and is non-circulating (that is, items cannot be checked out or loaned out), but photocopying and photography of items in the collection are allowed.\nKahn lives (as of 2012) in New York City. He has lived in Washington, D.C.; Paris, France; Freiburg, Germany; Oxford, England; and Great Neck, New York.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"In the context of cryptography, encryption serves as a mechanism to ensure confidentiality. Since data may be visible on the Internet, sensitive information such as passwords and personal communication may be exposed to potential interceptors. The process of encrypting and decrypting messages involves keys. The two main types of keys in cryptographic systems are symmetric-key and public-key (also known as asymmetric-key).Many complex cryptographic algorithms often use simple modular arithmetic in their implementations.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"According to al-Kindi, the goal of metaphysics is knowledge of God. For this reason, he does not make a clear distinction between philosophy and theology, because he believes they are both concerned with the same subject. Later philosophers, particularly al-Farabi and Avicenna, would strongly disagree with him on this issue, by saying that metaphysics is actually concerned with being qua being, and as such, the nature of God is purely incidental.Central to al-Kindi's understanding of metaphysics is God's absolute oneness, which he considers an attribute uniquely associated with God (and therefore not shared with anything else). By this he means that while we may think of any existent thing as being \"one\", it is in fact both \"one\" and many\". For example, he says that while a body is one, it is also composed of many different parts. A person might say \"I see an elephant\", by which he means \"I see one elephant\", but the term 'elephant' refers to a species of animal that contains many. Therefore, only God is absolutely one, both in being and in concept, lacking any multiplicity whatsoever. Some feel this understanding entails a very rigorous negative theology because it implies that any description which can be predicated to anything else, cannot be said about God.In addition to absolute oneness, al-Kindi also described God as the Creator. This means that He acts as both a final and efficient cause. Unlike later Muslim Neo-Platonic philosophers (who asserted that the universe existed as a result of God's existence \"overflowing\", which is a passive act), al-Kindi conceived of God as an active agent. In fact, of God as the agent, because all other intermediary agencies are contingent upon Him. The key idea here is that God \"acts\" through created intermediaries, which in turn \"act\" on one another – through a chain of cause and effect – to produce the desired result. In reality, these intermediary agents do not \"act\" at all, they are merely a conduit for God's own action. This is especially significant in the development of Islamic philosophy, as it portrayed the \"first cause\" and \"unmoved mover\" of Aristotelian philosophy as compatible with the concept of God according to Islamic revelation.\n\n###\n\n","completion":" Cryptography"}
{"prompt":"For any strings \n  \n    \n      \n        u\n        ,\n        v\n        ∈\n        (\n        V\n        ∪\n        Σ\n        \n          )\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle u,v\\in (V\\cup \\Sigma )^{*}}\n  , we say u directly yields v, written as \n  \n    \n      \n        u\n        ⇒\n        v\n        \n      \n    \n    {\\displaystyle u\\Rightarrow v\\,}\n  , if \n  \n    \n      \n        ∃\n        (\n        α\n        ,\n        β\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle \\exists (\\alpha ,\\beta )\\in R}\n   with \n  \n    \n      \n        α\n        ∈\n        V\n      \n    \n    {\\displaystyle \\alpha \\in V}\n   and \n  \n    \n      \n        \n          u\n          \n            1\n          \n        \n        ,\n        \n          u\n          \n            2\n          \n        \n        ∈\n        (\n        V\n        ∪\n        Σ\n        \n          )\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle u_{1},u_{2}\\in (V\\cup \\Sigma )^{*}}\n   such that \n  \n    \n      \n        u\n        \n        =\n        \n          u\n          \n            1\n          \n        \n        α\n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle u\\,=u_{1}\\alpha u_{2}}\n   and \n  \n    \n      \n        v\n        \n        =\n        \n          u\n          \n            1\n          \n        \n        β\n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle v\\,=u_{1}\\beta u_{2}}\n  . Thus, v is a result of applying the rule \n  \n    \n      \n        (\n        α\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\alpha ,\\beta )}\n   to u.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Parenthesized S-expressions represent linked list structures. There are several ways to represent the same list as an S-expression. A cons can be written in dotted-pair notation as (a . b), where a is the car and b the cdr. A longer proper list might be written (a . (b . (c . (d . nil)))) in dotted-pair notation. This is conventionally abbreviated as (a b c d) in list notation. An improper list may be written in a combination of the two – as (a b c . d) for the list of three conses whose last cdr is d (i.e., the list (a . (b . (c . d))) in fully specified form).\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"QBasic, a version of Microsoft QuickBASIC without the linker to make EXE files, is present in the Windows NT and DOS-Windows 95 streams of operating systems and can be obtained for more recent releases like Windows 7 which do not have them. Prior to DOS 5, the Basic interpreter was GW-Basic. QuickBasic is part of a series of three languages issued by Microsoft for the home and office power user and small-scale professional development; QuickC and QuickPascal are the other two. For Windows 95 and 98, which do not have QBasic installed by default, they can be copied from the installation disc, which will have a set of directories for old and optional software; other missing commands like Exe2Bin and others are in these same directories.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"It is decidable whether a given grammar is a regular grammar, as well as whether it is an LL(k) grammar for a given k≥0.: 233  If k is not given, the latter problem is undecidable.: 252 Given a context-free language, it is neither decidable whether it is regular, nor whether it is an LL(k) language for a given k.: 254\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Micro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator typically exists in modern Prolog's implementations. It is typically written as not(Goal) or \\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\+ X\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"In most dialects of Lisp including Common Lisp, by convention the value NIL evaluates to the value false in a boolean expression. In Scheme, since the IEEE standard in 1991, all values except #f, including NIL's equivalent in Scheme which is written as '(), evaluate to the value true in a boolean expression. (R5RS sec. 6.3.1)Where the constant representing the boolean value of true is T in most Lisps, in Scheme it is #t.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"A FRACTRAN program is an ordered list of positive fractions together with an initial positive integer input \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . The program is run by multiplying the integer \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   by the first fraction \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   in the list for which \n  \n    \n      \n        n\n        f\n      \n    \n    {\\displaystyle nf}\n   is an integer. The integer \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is then replaced by \n  \n    \n      \n        n\n        f\n      \n    \n    {\\displaystyle nf}\n   and the rule is repeated. If no fraction in the list produces an integer when multiplied by \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , the program halts. FRACTRAN was invented by mathematician John Conway.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"An interpreter is a program that reads another program, typically as text,  as seen in languages like Python. Interpreters read code, and produces the result directly. Interpreters typically read code line by line, and parses it to convert and execute the code as operations and actions.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Clojure supports anonymous functions through the \"fn\" special form:\n\nThere is also a reader syntax to define a lambda:\n\nLike Scheme, Clojure's \"named functions\" are simply syntactic sugar for lambdas bound to names:\n\nexpands to:\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Here follows a longer example of mathematical-style pseudocode, for the Ford–Fulkerson algorithm:\n\nalgorithm ford-fulkerson is\n    input: Graph G with flow capacity c, \n           source node s, \n           sink node t\n    output: Flow f such that f is maximal from s to t\n\n    (Note that f(u,v) is the flow from node u to node v, and c(u,v) is the flow capacity from node u to node v)\n\n    for each edge (u, v) in GE do\n        f(u, v) ← 0\n        f(v, u) ← 0\n\n    while there exists a path p from s to t in the residual network Gf do\n        let cf be the flow capacity of the residual network Gf\n        cf(p) ← min{cf(u, v) | (u, v) in p}\n        for each edge (u, v) in p do\n            f(u, v) ←  f(u, v) + cf(p)\n            f(v, u) ← −f(u, v)\n\n    return f\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"PyPy is a fast, compliant interpreter of Python 2.7 and 3.8. Its just-in-time compiler often brings a significant speed improvement over CPython but some libraries written in C cannot be used with it.\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the call stack in the same way, thus allowing massively concurrent programs. PyPy also has a stackless version.\nMicroPython and CircuitPython are Python 3 variants optimized for microcontrollers, including Lego Mindstorms EV3.\nPyston is a variant of the Python runtime that uses just-in-time compilation to speed up the execution of Python programs.\nCinder is a performance-oriented fork of CPython 3.8 that contains a number of optimizations including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Up to the 1985 ANSI COBOL standard had the ALTER statement which could be used to change the destination of an existing GO TO, which had to be in a paragraph by itself. The feature, which allowed polymorphism, was frequently condemned and seldom used.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"In MATLAB's programming language, the '%' character indicates a single-line comment.  Multi line comments are also available via %{ and %} brackets and can be nested, e.g.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Lua programs are not interpreted directly from the textual Lua file, but are compiled into bytecode, which is then run on the Lua virtual machine. The compilation process is typically invisible to the user and is performed during run-time, especially when a JIT compiler is used, but it can be done offline in order to increase loading performance or reduce the memory footprint of the host environment by leaving out the compiler. Lua bytecode can also be produced and executed from within Lua, using the dump function from the string library and the load\/loadstring\/loadfile functions. Lua version 5.3.4 is implemented in approximately 24,000 lines of C code.Like most CPUs, and unlike most virtual machines (which are stack-based), the Lua VM is register-based, and therefore more closely resembles an actual hardware design. The register architecture both avoids excessive copying of values and reduces the total number of instructions per function. The virtual machine of Lua 5 is one of the first register-based pure VMs to have a wide use. Parrot and Android's Dalvik are two other well-known register-based VMs.  PCScheme's VM was also register-based.This example is the bytecode listing of the factorial function defined above (as shown by the luac 5.1 compiler):\nfunction <factorial.lua:1,7> (9 instructions, 36 bytes at 0x8063c60)\n1 param, 6 slots, 0 upvalues, 6 locals, 2 constants, 0 functions\n\t1\t[2]\tLOADK    \t1 -1\t; 1\n\t2\t[3]\tLOADK    \t2 -2\t; 2\n\t3\t[3]\tMOVE     \t3 0\n\t4\t[3]\tLOADK    \t4 -1\t; 1\n\t5\t[3]\tFORPREP  \t2 1\t; to 7\n\t6\t[4]\tMUL      \t1 1 5\n\t7\t[3]\tFORLOOP  \t2 -2\t; to 6\n\t8\t[6]\tRETURN   \t1 2\n\t9\t[7]\tRETURN   \t0 1\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Localization is the core feature of the Citrine Programming Language. Citrine is designed to be translatable to every written human language. For instance the West Frisian language version is called Citrine\/FY. Citrine features localized keywords, localized numbers and localized punctuation. Users can translate code files from one language into another using a string-based approach. At the time of writing, Citrine supports 111 human languages. Support is not limited to well-known languages; all natural human languages are being accepted for inclusion, up to EGIDS-6.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Context-free languages are closed under the various operations, that is, if the languages K and L are \ncontext-free, so is the result of the following operations:\n\nunion K ∪ L; concatenation K ∘ L; Kleene star L*\nsubstitution (in particular homomorphism)\ninverse homomorphism\nintersection with a regular languageThey are not closed under general intersection (hence neither under complementation) and set difference.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"For communication to occur, protocols have to be selected. The rules can be expressed by algorithms and data structures. Hardware and operating system independence is enhanced by expressing the algorithms in a portable programming language. Source independence of the specification provides wider interoperability.\nProtocol standards are commonly created by obtaining the approval or support of a standards organization, which initiates the standardization process. The members of the standards organization agree to adhere to the work result on a voluntary basis. Often the members are in control of large market shares relevant to the protocol and in many cases, standards are enforced by law or the government because they are thought to serve an important public interest, so getting approval can be very important for the protocol.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"\"High-level language\" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses, and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input\/output, may also be present. One thing to note about high-level programming languages is that these languages allow the programmer to be detached and separated from the machine. That is, unlike low-level languages like assembly or machine language, high-level programming can amplify the programmer's instructions and trigger a lot of data movements in the background without their knowledge. The responsibility and power of executing instructions have been handed over to the machine from the programmer.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Cambridge Modula-2 by Cambridge Microprocessor Systems is based on a subset of PIM4 with language extensions for embedded development. The compiler runs on DOS and it generates code for Motorola 68000 series (M68k) based embedded microcontrollers running a MINOS operating system.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"After moving to New York City he trained initially as a radio technician and became interested in mathematics. He graduated from Columbia University with a bachelor's degree in 1949 and a master's degree in 1950, both in mathematics, and joined IBM in 1950. During his first three years, he worked on the Selective Sequence Electronic Calculator (SSEC); his first major project was to write a program to calculate positions of the Moon. In 1953 Backus developed the language Speedcoding, the first high-level language created for an IBM computer, to aid in software development for the IBM 701 computer.Programming was very difficult at this time, and in 1954 Backus assembled a team to define and develop Fortran for the IBM 704 computer. Fortran was the first high-level programming language to be put to broad use. This widely used language made computers practical and accessible machines for scientists and others without requiring them to have deep knowledge of the machinery.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The Wolfram Language is the programming language of Mathematica. Anonymous functions are important in programming the latter. There are several ways to create them. Below are a few anonymous functions that increment a number. The first is the most common. #1 refers to the first argument and & marks the end of the anonymous function.\n\nSo, for instance:\n\nAlso, Mathematica has an added construct to make recursive anonymous functions. The symbol '#0' refers to the entire function. The following function calculates the factorial of its input:\n\nFor example, 6 factorial would be:\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Shakespeare is designed to make programs look like Shakespearean plays. For example, the following statement declares a point in the program which can be reached via a GOTO-type statement:\n Act I: Hamlet's insults and flattery.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The Java programming language requires the presence of a software platform in order for compiled programs to be executed.\nOracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"ALGOL was developed jointly by a committee of European and American computer scientists in a meeting in 1958 at the Swiss Federal Institute of Technology in Zurich (cf. ALGOL 58). It specified three different syntaxes: a reference syntax, a publication syntax, and an implementation syntax. The different syntaxes permitted it to use different keyword names and conventions for decimal points (commas vs periods) for different languages.\nALGOL was used mostly by research computer scientists in the United States and in Europe. Its use in commercial applications was hindered by the absence of standard input\/output facilities in its description and the lack of interest in the language by large computer vendors other than Burroughs Corporation. ALGOL 60 did however become the standard for the publication of algorithms and had a profound effect on future language development.\n\nJohn Backus developed the Backus normal form method of describing programming languages specifically for ALGOL 58. It was revised and expanded by Peter Naur for ALGOL 60, and at Donald Knuth's suggestion renamed Backus–Naur form.Peter Naur: \"As editor of the ALGOL Bulletin I was drawn into the international discussions of the language and was selected to be member of the European language design group in November 1959. In this capacity I was the editor of the ALGOL 60 report, produced as the result of the ALGOL 60 meeting in Paris in January 1960.\"The following people attended the meeting in Paris (from 1 to 16 January):\n\nFriedrich Ludwig Bauer, Peter Naur, Heinz Rutishauser, Klaus Samelson, Bernard Vauquois, Adriaan van Wijngaarden, and Michael Woodger (from Europe)\nJohn Warner Backus, Julien Green, Charles Katz, John McCarthy, Alan Jay Perlis, and Joseph Henry Wegstein (from the USA).Alan Perlis gave a vivid description of the meeting: \"The meetings were exhausting, interminable, and exhilarating. One became aggravated when one's good ideas were discarded along with the bad ones of others. Nevertheless, diligence persisted during the entire period. The chemistry of the 13 was excellent.\"\nALGOL 60 inspired many languages that followed it. Tony Hoare remarked: \"Here is a language so far ahead of its time that it was not only an improvement on its predecessors but also on nearly all its successors.\" The Scheme programming language, a variant of Lisp that adopted the block structure and lexical scope of ALGOL, also adopted the wording \"Revised Report on the Algorithmic Language Scheme\" for its standards documents in homage to ALGOL.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language that places no constraints on the order in which operations are performed.Logic programming, with its current syntax of facts and rules, can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA4, Popler, Conniver, QLISP, and the concurrent language Ether.Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\nProlog also contributed to the development of the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog. \nAlthough Prolog was developed to combine declarative and procedural representations of knowledge, the purely declarative interpretation of logic programs returned to prominence around 1977 when Hervé Gallaire and Jack Minker organized a workshop on logic and databases, which later developed into the field of deductive databases and Datalog.\nThis focus on the logical, declarative reading of logic programs was given further impetus by the development of constraint logic programming in the 1980s and Answer Set Programming in the 1990s. It is also receiving renewed emphasis in recent applications of PrologThe Association for Logic Programming was founded in 1986 to promote Logic Programming.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.\nDepending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are \"escaped\", i.e. preceded by an escape sequence, in this case, the backslash \\. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid \"backslash-osis\" or leaning toothpick syndrome it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters ( ) and { } be primarily literal, and \"escape\" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are  {}[]()^$.|*+? and \\. The usual characters that become metacharacters when escaped are dswDSW and N.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Compilers are programs that read programs, also usually as some form of text, and converts the code into lower level machine code or operations. Compiled formats generated by compilers store the lower level actions as a file.  Compiled languages converted to machine code, tend to be a lot faster, as lower level operations are easier to run, and outcomes can be predicted and compiled ahead of time.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Chomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.Such rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations does not meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\nChomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved. Gerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"This Fortran IV code fragment demonstrates how comments are used in that language, which is very column-oriented. A letter \"C\" in column 1 causes the entire line to be treated as a comment.\n\nNote that the columns of a line are otherwise treated as four fields: 1 to 5 is the label field, 6 causes the line to be taken as a continuation of the previous statement; and declarations and statements go in 7 to 72.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The satellites of the Russian radionavigation-satellite service framework GLONASS, similar to the United States Global Positioning System (GPS), are programmed in Modula-2.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester.\nMercury Autocode had a limited repertoire of variables a-z and a'-z' and, in some ways resembled early versions of the later Dartmouth BASIC language.  It pre-dated ALGOL, having no concept of stacks and hence no recursion or dynamically-allocated arrays.  In order to overcome the relatively small store size available on Mercury, large programs were written as distinct \"chapters\", each of which constituted an overlay.  Some skill was required to minimise time-consuming transfers of control between chapters.  This concept of overlays from drum under user control became common until virtual memory became available in later machines.  Slightly different dialects of Mercury Autocode were implemented for the Ferranti Atlas (distinct from the later Atlas Autocode) and the ICT 1300 and 1900 range.\nThe version for the EDSAC 2 was devised by David Hartley of  University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A version was developed for the successor Titan (the prototype Atlas 2 computer) as a temporary stop-gap while a more substantially advanced language known as CPL was being developed. CPL was never completed but did give rise to BCPL (developed by M. Richards), which in turn led to B and ultimately C. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.Python provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses round to even: round(1.5) and round(2.5) both produce 2. Versions before 3 used round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.Python allows Boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.Python uses arbitrary-precision arithmetic for all integer operations. The Decimal type\/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes. The Fraction class in the fractions module provides arbitrary precision for rational numbers.Due to Python's extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The Lua programming language uses double-hyphens, --, for single line comments in a similar way to Ada, Eiffel, Haskell, SQL and VHDL languages. Lua also has block comments, which start with --[[ and run until a closing ]]\nFor example:\n\nA common technique to comment out a piece of code, is to enclose the code between --[[ and\n--]], as below:\n\nIn this case, it's possible to reactivate the code by adding a single hyphen to the first line:\n\nIn the first example, the --[[ in the first line starts a long comment, and the two hyphens in the last line\nare still inside that comment. In the second example, the sequence ---[[ starts an ordinary, single-line\ncomment, so that the first and the last lines become independent comments. In this case, the print is\noutside comments. In this case, the last line becomes an independent comment, as it starts with --.\nLong comments in Lua can be more complex than these, as you can read in the section called \"Long strings\" c.f. Programming in Lua.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Although Lua does not have a built-in concept of classes, object-oriented programming can be emulated using functions and tables. An object is formed by putting methods and fields in a table. Inheritance (both single and multiple) can be implemented with metatables, delegating nonexistent methods and fields to a parent object.\nThere is no such concept as \"class\" with these techniques; rather, prototypes are used, similar to Self or JavaScript. New objects are created either with a factory method (that constructs new objects from scratch) or by cloning an existing object.\nCreating a basic vector object:\n\nHere, setmetatable tells Lua to look for an element in the Vector table if it is not present in the vec table. vec.magnitude, which is equivalent to vec[\"magnitude\"], first looks in the vec table for the magnitude element. The vec table does not have a magnitude element, but its metatable delegates to the Vector table for the magnitude element when it's not found in the vec table.\nLua provides some syntactic sugar to facilitate object orientation. To declare member functions inside a prototype table, one can use function table:func(args), which is equivalent to function table.func(self, args). Calling class methods also makes use of the colon: object:func(args) is equivalent to object.func(object, args).\nThat in mind, here is a corresponding class with : syntactic sugar:\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"There are a large number of Smalltalk variants. The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language and compatible VM, the first version to be made publicly available and created in 1980. The first hardware-environments which ran the Smalltalk VMs were Xerox Alto computers.\nSmalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, Adele Goldberg wrote most of the documentation, and Dan Ingalls implemented most of the early versions. The first version, termed Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in \"a page of code\". A later variant used for research work is now termed Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.\nAfter significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created.  This system had a development environment featuring most of the now familiar tools, including a class library code browser\/editor. Smalltalk-80 added metaclasses, to help maintain the \"everything is an object\" (except variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways to create instances).\nSmalltalk-80 was the first language variant made available outside of PARC. In 1981 it was shared with Tektronix, Hewlett-Packard, Apple Computer, and DEC for review and debugging on their platforms. The August 1981 issue of Byte Magazine was devoted to Smalltalk-80 and brought its ideas to a large audience. Several books on Smalltalk-80 were also published. Smalltalk-80 became the basis for all future commercial versions of Smalltalk. The final release of Smalltalk-80 Version 1 was in November 1981. Xerox only distributed Version 1 to Apple, DEC, HP, and Tektronix, but these companies were allowed unrestricted\nredistribution via any system they built. This encouraged the wide spread of Smalltalk. Later, in 1983, Xerox released Smalltalk-80 Version 2. This version was generally available to the public, although under a restrictive license. Versions 1 and 2 were fairly similar, although Version 2 did have some added features such as a spelling corrector. Each release consisted of a virtual image (platform-independent file with object definitions) and a virtual machine specification.ANSI Smalltalk has been the standard language reference since 1998. Two currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks. (Dan Ingalls later ported Hobbes to Squeak.)\nDuring the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix\/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS\/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers.  While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge\/Smalltalk.  Easel introduced Enfin at this time on Windows and OS\/2.  Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge).  Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.\nIn 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk\/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom. VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.\nCincom, GemTalk, and Instantiations, continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java instead and it is, as of 2005, supported by Instantiations, Inc. who renamed the product VA Smalltalk (VAST Platform) and continue to release new versions yearly. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented toward research and use in commercial environments.\nA significant development, that has spread across all Smalltalk environments as of 2016, is the increasing usage of two web frameworks, Seaside and AIDA\/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone, and Instantiations incorporating and extending it.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition. Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.\n\nFactor\nForth\njq (function application is also supported)\nJoy\nPostScript\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"JavaScript\/ECMAScript supports anonymous functions.\n\nES6 supports \"arrow function\" syntax, where a => symbol separates the anonymous function's parameter list from the body:\n\nThis construct is often used in Bookmarklets. For example, to change the title of the current document (visible in its window's title bar) to its URL, the following bookmarklet may seem to work.\n\nHowever, as the assignment statement returns a value (the URL itself), many browsers actually create a new page to display this value.\nInstead, an anonymous function, that does not return a value, can be used:\n\nThe function statement in the first (outer) pair of parentheses declares an anonymous function, which is then executed when used with the last pair of parentheses. This is almost equivalent to the following, which populates the environment with f unlike an anonymous function.\n\nUse void() to avoid new pages for arbitrary anonymous functions:\n\nor just:\n\nJavaScript has syntactic subtleties for the semantics of defining, invoking and evaluating anonymous functions. These subliminal nuances are a direct consequence of the evaluation of parenthetical expressions. The following constructs which are called immediately-invoked function expression illustrate this:\n\n and \n\nRepresenting \"function(){ ... }\" by f, the form of the constructs are \na parenthetical within a parenthetical (f()) and a parenthetical applied to a parenthetical (f)().\nNote the general syntactic ambiguity of a parenthetical expression, parenthesized arguments to a function and the parentheses around the formal parameters in a function definition. In particular, JavaScript defines a , (comma) operator in the context of a parenthetical expression. It is no mere coincidence that the syntactic forms coincide for an expression and a function's arguments (ignoring the function formal parameter syntax)! If f is not identified in the constructs above, they become (()) and ()(). The first provides no syntactic hint of any resident function but the second MUST evaluate the first parenthetical as a function to be legal JavaScript. (Aside: for instance, the ()'s could be ([],{},42,\"abc\",function(){}) as long as the expression evaluates to a function.)\nAlso, a function is an Object instance (likewise objects are Function instances) and the object literal notation brackets, {} for braced code, are used when defining a function this way (as opposed to using new Function(...)). In a very broad non-rigorous sense (especially since global bindings are compromised), an arbitrary sequence of braced JavaScript statements, {stuff}, can be considered to be a fixed point of\n\nMore correctly but with caveats, \n\nNote the implications of the anonymous function in the JavaScript fragments that follow:\n\nfunction(){ ... }() without surrounding ()'s is generally not legal\n(f=function(){ ... }) does not \"forget\" f globally unlike (function f(){ ... })Performance metrics to analyze the space and time complexities of function calls, call stack, etc. in a JavaScript interpreter engine implement easily with these last anonymous function constructs. From the implications of the results, it is possible to deduce some of an engine's recursive versus iterative implementation details, especially tail-recursion.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"^a  \"step n\" is used to change the loop interval. If \"step\" is omitted, then the loop interval is 1.\n^b  This implements the universal quantifier (\"for all\" or \"\n  \n    \n      \n        ∀\n      \n    \n    {\\displaystyle \\forall }\n  \") as well as the existential quantifier (\"there exists\" or \"\n  \n    \n      \n        ∃\n      \n    \n    {\\displaystyle \\exists }\n  \").\n^c  THRU may be used instead of THROUGH.\n^d  «IS» GREATER «THAN» may be used instead of >.\n^e  Type of set expression must implement trait std::iter::IntoIterator.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Perhaps the most significant development in the early history of FORTRAN was the decision by the American Standards Association (now American National Standards Institute (ANSI)) to form a committee sponsored by the Business Equipment Manufacturers Association (BEMA) to develop an American Standard Fortran.  The resulting two standards, approved in March 1966, defined two languages, FORTRAN (based on FORTRAN IV, which had served as a de facto standard), and Basic FORTRAN (based on FORTRAN II, but stripped of its machine-dependent features).  The FORTRAN defined by the first standard, officially denoted X3.9-1966, became known as FORTRAN 66 (although many continued to term it FORTRAN IV, the language on which the standard was largely based).  FORTRAN 66 effectively became the first industry-standard version of FORTRAN. FORTRAN 66 included:\n\nMain program, SUBROUTINE, FUNCTION, and BLOCK DATA program units\nINTEGER, REAL, DOUBLE PRECISION, COMPLEX, and LOGICAL data types\nCOMMON, DIMENSION, and EQUIVALENCE statements\nDATA statement for specifying initial values\nIntrinsic and EXTERNAL (e.g., library) functions\nAssignment statement\nGO TO, computed GO TO, assigned GO TO, and ASSIGN statements\nLogical IF and arithmetic (three-way) IF statements\nDO loop statement\nREAD, WRITE, BACKSPACE, REWIND, and ENDFILE statements for sequential I\/O\nFORMAT statement and assigned format\nCALL, RETURN, PAUSE, and STOP statements\nHollerith constants in DATA and FORMAT statements, and as arguments to procedures\nIdentifiers of up to six characters in length\nComment lines\nEND line\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Principles of Programming Languages (POPL)\nProgramming Language Design and Implementation (PLDI)\nInternational Symposium on Memory Management (ISMM)\nLanguages, Compilers, and Tools for Embedded Systems (LCTES)\nSymposium on Principles and Practice of Parallel Programming (PPoPP)\nInternational Conference on Functional Programming (ICFP)\nSystems, Programming, Languages, and Applications: Software for Humanity (SPLASH)\nObject-Oriented Programming, Systems, Languages, and Applications (OOPSLA)\nHistory of Programming Languages (HOPL)\nDynamic Languages Symposium (DLS)\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"In modern protocol design, protocols are layered to form a protocol stack. Layering is a design principle that divides the protocol design task into smaller steps, each of which accomplishes a specific part, interacting with the other parts of the protocol only in a small number of well-defined ways. Layering allows the parts of a protocol to be designed and tested without a combinatorial explosion of cases, keeping each design relatively simple.\nThe communication protocols in use on the Internet are designed to function in diverse and complex settings. Internet protocols are designed for simplicity and modularity and fit into a coarse hierarchy of functional layers defined in the Internet Protocol Suite. The first two cooperating protocols, the Transmission Control Protocol (TCP) and the Internet Protocol (IP) resulted from the decomposition of the original Transmission Control Program, a monolithic communication protocol, into this layered communication suite.\nThe OSI model was developed internationally based on experience with networks that predated the internet as a reference model for general communication with much stricter rules of protocol interaction and rigorous layering.\nTypically, application software is built upon a robust data transport layer. Underlying this transport layer is a datagram delivery and routing mechanism that is typically connectionless in the Internet. Packet relaying across networks happens over another layer that involves only network link technologies, which are often specific to certain physical layer technologies, such as Ethernet. Layering provides opportunities to exchange technologies when needed, for example, protocols are often stacked in a tunneling arrangement to accommodate the connection of dissimilar networks. For example, IP may be tunneled across an Asynchronous Transfer Mode (ATM) network.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Many languages evaluate expressions inside switch blocks at runtime, allowing a number of less obvious uses for the construction. This prohibits certain compiler optimizations, so is more common in dynamic and scripting languages where the enhanced flexibility is more important than the performance overhead.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Gherkin is a language designed to define test cases to check the behavior of software, without specifying how that behavior is implemented. It is meant to be read and used by non-technical users using a natural language syntax and a line-oriented design. The tests defined with Gherkin must then be implemented in a general programming language. Then, the steps in a Gherkin program acts as a syntax for method invocation accessible to non-developers.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"During the late 1960s, high-level languages such as PL\/S, BLISS, BCPL, extended ALGOL (for Burroughs large systems) and C included some degree of access to low-level programming functions. One method for this is inline assembly, in which assembly code is embedded in a high-level language that supports this feature. Some of these languages also allow architecture-dependent compiler optimization directives to adjust the way a compiler uses the target processor architecture.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"R was started by professors Ross Ihaka and Robert Gentleman as a programming language to teach introductory statistics at the University of Auckland. The language took heavy inspiration from the S programming language with most S programs able to run unaltered in R as well as from Scheme's lexical scoping allowing for local variables. The name of the language comes from being an S language successor and the shared first letter of the authors, Ross and Robert. Ihaka and Gentleman first shared binaries of R on the data archive StatLib and the s-news mailing list in August 1993. In June 1995, statistician Martin Mächler convinced Ihaka and Gentleman to make R free and open-source under the GNU General Public License. Mailing lists for the R project began on 1 April 1997 preceding the release of version 0.50. R officially became a GNU project on 5 December 1997 when version 0.60 released. The first official 1.0 version was released on 29 February 2000.The Comprehensive R Archive Network (CRAN) was founded in 1997 by Kurt Hornik and Fritz Leisch to host R's source code, executable files, documentation, and user-created packages. Its name and scope mimics the Comprehensive TeX Archive Network and the Comprehensive Perl Archive Network. CRAN originally had three mirrors and 12 contributed packages. As of December 2022, it has 103 mirrors and 18,976 contributed packages.The R Core Team was formed in 1997 to further develop the language. As of January 2022, it consists of Chambers, Gentleman, Ihaka, and Mächler, plus statisticians Douglas Bates, Peter Dalgaard, Kurt Hornik, Michael Lawrence, Friedrich Leisch, Uwe Ligges, Thomas Lumley, Sebastian Meyer, Paul Murrell, Martyn Plummer, Brian Ripley, Deepayan Sarkar, Duncan Temple Lang, Luke Tierney, and Simon Urbanek, as well as computer scientist Tomas Kalibera. Stefano Iacus, Guido Masarotto, Heiner Schwarte, Seth Falcon, Martin Morgan, and Duncan Murdoch were members. In April 2003, the R Foundation was founded as a non-profit organization to provide further support for the R project.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Scratch is a block-based educational language. The text of the blocks is translated into many languages, and users can select different translations. Unicode characters are supported in variable and list names. (Scratch lists are not stored inside variables the way arrays or lists are handled in most languages. Variables only store strings, numbers, and, with workarounds, boolean values, while lists are a separate data type that store sequences of these values.) Projects can be \"translated\" by simply changing the language of the editor, although this does not translate the variable names.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Standards for some Lisp-derived programming languages include a specification for their S-expression syntax.  These include Common Lisp (ANSI standard document ANSI INCITS 226-1994 (R2004)), Scheme (R5RS and R6RS), and ISLISP.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"To date there have been at least 70 augmentations, extensions, derivations and sublanguages of ALGOL 60.\nThe Burroughs dialects included special system programming dialects such as ESPOL and NEWP.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"LOLCODE is designed to resemble the speech of lolcats. The following is the \"Hello World\" example:\n\nHAI\nCAN HAS STDIO?\nVISIBLE \"HAI WORLD!\"\nKTHXBYE\n\nWhile the semantics of LOLCODE is not unusual, its syntax has been described as a linguistic phenomenon, representing an unusual example of informal speech and internet slang in programming.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"It is certainly possible to represent hardware semantics using traditional programming languages such as C++, which operate on control flow semantics as opposed to data flow, although to function as such, programs must be augmented with extensive and unwieldy class libraries. Generally, however, software programming languages do not include any capability for explicitly expressing time, and thus cannot function as hardware description languages. Before the introduction of System Verilog in 2002, C++ integration with a logic simulator was one of the few ways to use object-oriented programming in hardware verification. System Verilog is the first major HDL to offer object orientation and garbage collection.\nUsing the proper subset of hardware description language, a program called a synthesizer, or logic synthesis tool, can infer hardware logic operations from the language statements and produce an equivalent netlist of generic hardware primitives to implement the specified behaviour. Synthesizers generally ignore the expression of any timing constructs in the text. Digital logic synthesizers, for example, generally use clock edges as the way to time the circuit, ignoring any timing constructs. The ability to have a synthesizable subset of the language does not itself make a hardware description language.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity, and for overemphasizing one aspect of software design and modeling (data\/objects) at the expense of other important aspects (computation\/algorithms).Luca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex. The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.Christopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP; however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.In an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.Alexander Stepanov compares object orientation unfavourably to generic programming:\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".Leo Brodie has suggested a connection between the standalone nature of objects and a tendency to duplicate code in violation of the don't repeat yourself principle of software development.\nSteve Yegge noted that, as opposed to functional programming:\nObject Oriented Programming puts the nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. It's a strangely skewed perspective.\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.Eric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency. Raymond compares this unfavourably to the approach taken with Unix and the C programming language.Rob Pike, a programmer involved in the creation of UTF-8 and Go, has called object-oriented programming \"the Roman numerals of computing\" and has said that OOP languages frequently shift the focus from data structures and algorithms to types. Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.Regarding inheritance, Bob Martin states that because they are software, related classes do not necessarily share the relationships of the things they represent.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The first published literate programming environment was WEB, introduced by Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and  C++, runs on most operating systems and can produce TeX and PDF documentation.\nThere are various other implementations of the literate programming concept as given below. Many of the newer ones among these don't have macros and hence violate the order of human logic principle, which makes them more of semi-literate tools. These however, allow cellular execution of code which makes them more on the likes of exploratory programming tools. \n\nOther useful tools include:\n\nThe Leo text editor is an outlining editor which supports optional noweb and CWEB markup. The author of Leo mixes two different approaches: first, Leo is an outlining editor, which helps with management of large texts; second, Leo incorporates some of the ideas of literate programming, which in its pure form (i.e., the way it is used by Knuth Web tool or tools like \"noweb\") is possible only with some degree of inventiveness and the use of the editor in a way not exactly envisioned by its author (in modified @root nodes). However, this and other extensions (@file nodes) make outline programming and text management successful and easy and in some ways similar to literate programming.\nThe Haskell programming language has native support for semi-literate programming. The compiler\/interpreter supports two file name extensions: .hs and .lhs; the latter stands for literate Haskell.The literate scripts can be full LaTeX source text, at the same time it can be compiled, with no changes, because the interpreter only compiles the text in a code environment, for example:\n\nThe code can be also marked in the Richard Bird style, starting each line with a greater than symbol and a space, preceding and ending the piece of code with blank lines.\nThe LaTeX listings package provides a lstlisting environment which can be used to embellish the source code. It can be used to define a code environment to use within Haskell to print the symbols in the following manner:\n\nwhich can be configured to yield:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                c\n                o\n                m\n                p\n                ::\n                (\n                β\n                →\n                γ\n                )\n                →\n                (\n                α\n                →\n                β\n                )\n                →\n                (\n                α\n                →\n                γ\n                )\n              \n            \n            \n              \n              \n                \n                (\n                g\n                comp\n                ⁡\n                f\n                )\n                x\n                =\n                g\n                (\n                f\n                x\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&comp::(\\beta \\to \\gamma )\\to (\\alpha \\to \\beta )\\to (\\alpha \\to \\gamma )\\\\&(g\\operatorname {comp} f)x=g(fx)\\end{aligned}}}\n  \nAlthough the package does not provide means to organize chunks of code, one can split the LaTeX source code in different files. See listings manual for an overview.The Web 68 Literate Programming system used Algol 68 as the underlying programming language, although there was nothing in the pre-processor 'tang' to force the use of that language.\nThe customization mechanism of the Text Encoding Initiative which enables the constraining, modification, or extension of the TEI scheme enables users to mix prose documentation with fragments of schema specification in their One Document Does-it-all format. From this prose documentation, schemas, and processing model pipelines can be generated and Knuth's Literate Programming paradigm is cited as the inspiration for this way of working.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Interrupt Driven Programming (1971)\nReversible Execution (1973)\nOptimization of Structured Programs (1974)\nPerspectives on software engineering (1978)\nA case study in rapid prototyping (1980)\nImplementation of language enhancements (1981)\nSoftware engineering practices in the United States and Japan (1984)\nA functional correctness model of program verification (1990)\nThe role for executable specifications in system maintenance (1991)\nSEL's software process-improvement program (1995)\nSoftware Engineering technology infusion within NASA (1996)\nExperimental models for validating computer technology (1998)\nA web-based tool for data analysis and presentation (1998)\nSoftware process improvement in small organizations: A case study (2005)\nUnderstanding the high-performance computing community: A software engineer’s perspective (2008)\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Inline comments in Python use the hash (#) character, as in the two examples in this code:\n\nBlock comments, as defined in this article, do not technically exist in Python. A bare string literal represented by a triple-quoted string can be used, but is not ignored by the interpreter in the same way that \"#\" comment is. In the examples below, the triple double-quoted strings act in this way as comments, but are also treated as docstrings:\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"He is the author of one book on type systems, Types and Programming Languages ISBN 0-262-16209-1.  He has also edited a collection of articles to create a second volume Advanced Topics in Types and Programming Languages ISBN 0-262-16228-8. Based on the notes he collected while learning category theory during his PhD, he also published an introductory book on this topic—Basic Category Theory for Computer Scientists, ISBN 0-262-66071-7. He is one of the authors of the freely available book Software Foundations.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Higher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator \n  \n    \n      \n        d\n        \n          \/\n        \n        d\n        x\n      \n    \n    {\\displaystyle d\/dx}\n  , which returns the derivative of a function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  .\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\nHigher-order functions enable partial application or currying, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"In mathematical logic, a formal theory is a set of sentences expressed in a formal language.\nA formal system (also called a logical calculus, or a logical system) consists of a formal language together with a deductive apparatus (also called a deductive system). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems \n  \n    \n      \n        \n          \n            F\n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {FS}}}\n   and \n  \n    \n      \n        \n          \n            F\n            \n              S\n              ′\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {FS'}}}\n   may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).\nA formal proof or derivation is a finite sequence of well-formed formulas (which may be interpreted as sentences, or propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Assembly directives, also called pseudo-opcodes, pseudo-operations or pseudo-ops, are commands given to an assembler \"directing it to perform operations other than assembling instructions\". Directives affect how the assembler operates and \"may affect the object code, the symbol table, the listing file, and the values of internal assembler parameters\". Sometimes the term pseudo-opcode is reserved for directives that generate object code, such as those that generate data.The names of pseudo-ops often start with a dot to distinguish them from machine instructions.  Pseudo-ops can make the assembly of the program dependent on parameters input by a programmer, so that one program can be assembled in different ways, perhaps for different applications. Or, a pseudo-op can be used to manipulate presentation of a program to make it easier to read and maintain. Another common use of pseudo-ops is to reserve storage areas for run-time data and optionally initialize their contents to known values.\nSymbolic assemblers let programmers associate arbitrary names (labels or symbols) with memory locations and various constants. Usually, every constant and variable is given a name so instructions can reference those locations by name, thus promoting self-documenting code. In executable code, the name of each subroutine is associated with its entry point, so any calls to a subroutine can use its name. Inside subroutines, GOTO destinations are given labels. Some assemblers support local symbols which are often lexically distinct from normal symbols (e.g., the use of \"10$\" as a GOTO destination).\nSome assemblers, such as NASM, provide flexible symbol management, letting programmers manage different namespaces, automatically calculate offsets within data structures, and assign labels that refer to literal values or the result of simple computations performed by the assembler. Labels can also be used to initialize constants and variables with relocatable addresses.\nAssembly languages, like most other computer languages, allow comments to be added to program source code that will be ignored during assembly. Judicious commenting is essential in assembly language programs, as the meaning and purpose of a sequence of binary machine instructions can be difficult to determine. The \"raw\" (uncommented) assembly language generated by compilers or disassemblers is quite difficult to read when changes must be made.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Just as software engineering (as a process) is defined by differing methodologies, so the programming languages (as models of computation) are defined by differing paradigms. Some languages are designed to support one paradigm (Smalltalk supports object-oriented programming, Haskell supports functional programming), while other programming languages support multiple paradigms (such as Object Pascal, C++, Java, JavaScript, C#, Scala, Visual Basic, Common Lisp, Scheme, Perl, PHP, Python, Ruby, Oz, and F#). For example, programs written in C++, Object Pascal or PHP can be purely procedural, purely object-oriented, or can contain elements of both or other paradigms. Software designers and programmers decide how to use those paradigm elements.\nIn object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes that act on a logical shared data structures.\nMany programming paradigms are as well known for the techniques they forbid as for those they enable. For instance, pure functional programming disallows use of side-effects, while structured programming disallows use of the goto statement. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles. Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.\nProgramming paradigms can also be compared with programming models, which allows invoking an execution model by using only an API. Programming models can also be classified into paradigms based on features of the execution model.\nFor parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  Thus, it is more convenient to use a base sequential language and insert API calls to parallel execution models via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"There are at least three different algorithms that decide whether and how a given regex matches a string.\nThe oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton (NFA) to be transformed into a deterministic finite automaton (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size m has the time and memory cost of O(2m), but it can be run on a string of size n in time O(n).  Note that the size of the expression is the size after abbreviations, such as numeric quantifiers, have been expanded.\nAn alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to O(mn). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the \"lazy DFA\" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky. Modern implementations include the re1-re2-sregex family based on Cox's code.\nThe third algorithm is to match the pattern against the input string by backtracking. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like (a|aa)*b that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called Regular expression Denial of Service (ReDoS).\nAlthough backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match. GNU grep (and the underlying gnulib DFA) uses such a strategy.Sublinear runtime algorithms have been achieved using Boyer-Moore (BM) based algorithms and related DFA optimization techniques such as the reverse scan. GNU grep, which supports a wide variety of POSIX syntaxes and extensions, uses BM for a first-pass prefiltering, and then uses an implicit DFA. Wu agrep, which implements approximate matching, combines the prefiltering into the DFA in BDM (backward DAWG matching). NR-grep's BNDM extends the BDM technique with Shift-Or bit-level parallelism.A few theoretical alternatives to backtracking for backreferences exist, and their \"exponents\" are tamer in that they are only related to the number of backreferences, a fixed property of some regexp languages such as POSIX. One naive method that duplicates a non-backtracking NFA for each backreference note has a complexity of \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n            k\n            +\n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathrm {O} }(n^{2k+2})}\n   time and \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathrm {O} }(n^{2k+1})}\n   space for a haystack of length n and k backreferences in the RegExp. A very recent theoretical work based on memory automata gives a tighter bound based on \"active\" variable nodes used, and a polynomial possibility for some backreferenced regexps.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The debugging process normally begins with identifying the steps to reproduce the problem. This can be a non-trivial task, particularly with parallel processes and some Heisenbugs for example. The specific user environment and usage history can also make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when parsing a large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Simplification may be done manually using a divide-and-conquer approach, in which the programmer attempts to remove some parts of original test case then checks if the problem still occurs. When debugging in a GUI, the programmer can try skipping some user interaction from the original problem description to check if the remaining actions are sufficient for causing the bug to occur.\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, tracing can be used. In simple cases, tracing is just a few print statements which output the values of variables at particular points during the execution of the program.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"As an example, consider this possible BNF for a U.S. postal address:\n\n \nThis translates into English as: \n\nA postal address consists of a name-part, followed by a street-address part, followed by a zip-code part.\nA name-part consists of either: a personal-part followed by a last name followed by an optional suffix (Jr., Sr., or dynastic number) and end-of-line, or a personal part followed by a name part (this rule illustrates the use of recursion in BNFs, covering the case of people who use multiple first and middle names and initials).\nA personal-part consists of either a first name or an initial followed by a dot.\nA street address consists of a house number, followed by a street name, followed by an optional apartment specifier, followed by an end-of-line.\nA zip-part consists of a town-name, followed by a comma, followed by a state code, followed by a ZIP-code followed by an end-of-line.\nAn opt-suffix-part consists of a suffix, such as \"Sr.\", \"Jr.\" or a roman-numeral, or an empty string (i.e. nothing).\nAn opt-apt-num consists of an apartment number or an empty string (i.e. nothing).Note that many things (such as the format of a first-name, apartment number, ZIP-code, and Roman numeral) are left unspecified here. If necessary, they may be described using additional BNF rules.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Debugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates. The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as debuggers. Debuggers are software tools which enable the programmer to monitor the execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term debugger can also refer to the person who is doing the debugging.\nGenerally, high-level programming languages, such as Java, make debugging easier, because they have features such as exception handling and type checking that make real sources of erratic behaviour easier to spot. In programming languages such as C or assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugger tools may be needed.\nIn certain situations, general purpose software tools that are language specific in nature can be very useful. These take the form of static code analysis tools. These tools look for a very specific set of known problems, some common and some rare, within the source code, concentrating more on the semantics (e.g. data flow) rather than the syntax, as compilers and interpreters do.\nBoth commercial and free tools exist for various languages; some claim to be able to detect hundreds of different problems. These tools can be extremely useful when checking very large source trees, where it is impractical to do code walk-throughs. A typical example of a problem detected would be a variable dereference that occurs before the variable is assigned a value. As another example, some such tools perform strong type checking when the language does not require it. Thus, they are better at locating likely errors in code that is syntactically correct. But these tools have a reputation of false positives, where correct code is flagged as dubious. The old Unix lint program is an early example.\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers, or in-circuit emulators (ICEs) are often used, alone or in combination. An ICE may perform many of the typical software debugger's tasks on low-level software and firmware.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages. The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.Large Perl programs start more slowly than similar programs in compiled languages because Perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article \"A Timely Start\", Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path. Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling. Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.A number of tools have been introduced to improve this situation. The first such tool was Apache's mod_perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.Once Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.The most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex, but flexible, XS mechanism.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"The dependency-based parse trees of dependency grammars see all nodes as terminal, which means they do not acknowledge the distinction between terminal and non-terminal categories. They are simpler on average than constituency-based parse trees because they contain fewer nodes. The dependency-based parse tree for the example sentence above is as follows:\n\nThis parse tree lacks the phrasal categories (S, VP, and NP) seen in the constituency-based counterpart above. Like the constituency-based tree, constituent structure is acknowledged. Any complete sub-tree of the tree is a constituent. Thus this dependency-based parse tree acknowledges the subject noun John and the object noun phrase the ball as constituents just like the constituency-based parse tree does.\nThe constituency vs. dependency distinction is far-reaching. Whether the additional syntactic structure associated with constituency-based parse trees is necessary or beneficial is a matter of debate.\n\n###\n\n","completion":" Programming Languages"}
{"prompt":"Social computing is an area that is concerned with the intersection of social behavior and computational systems. Human–computer interaction research develops theories, principles, and guidelines for user interface designers.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"There are several prizes in the field of software engineering:\nThe Codie awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.\nJolt Awards are awards in the software industry.\nStevens Award is a software engineering award given in memory of Wayne Stevens.\nHarlan Mills Award for \"contributions to the theory and practice of the information sciences, focused on software engineering\".\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Testing is central to extreme programming. Extreme programming's approach is that if a little testing can eliminate a few flaws, a lot of testing can eliminate many more flaws.\n\nUnit tests determine whether a given feature works as intended. Programmers write as many automated tests as they can think of that might \"break\" the code; if all tests run successfully, then the coding is complete. Every piece of code that is written is tested before moving on to the next feature.\nAcceptance tests verify that the requirements as understood by the programmers satisfy the customer's actual requirements.System-wide integration testing was encouraged, initially, as a daily end-of-day activity, for early detection of incompatible interfaces, to reconnect before the separate sections diverged widely from coherent functionality. However, system-wide integration testing has been reduced, to weekly, or less often, depending on the stability of the overall interfaces in the system.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"To satisfy the data modeling enhancement requirements that were identified in the IISS-6202 project, a sub-contractor, DACOM, obtained a license to the logical database design technique (LDDT) and its supporting software (ADAM). LDDT had been developed in 1982 by Robert G. Brown of The Database Design Group entirely outside the IDEF program and with no knowledge of IDEF1. LDDT combined elements of the relational data model, the E–R model, and generalization in a way specifically intended to support data modeling and the transformation of the data models into database designs. The graphic syntax of LDDT differed from that of IDEF1 and, more importantly, LDDT contained interrelated modeling concepts not present in IDEF1. Mary E. Loomis wrote a concise summary of the syntax and semantics of a substantial subset of LDDT, using terminology compatible with IDEF1 wherever possible. DACOM labeled the result IDEF1X and supplied it to the ICAM program.Because the IDEF program was funded by the government, the techniques are in the public domain. In addition to the ADAM software, sold by DACOM under the name Leverage, a number of CASE tools use IDEF1X as their representation technique for data modeling.\nThe IISS projects actually produced working prototypes of an information processing environment that would run in heterogeneous computing environments. Current advancements in such techniques as Java and JDBC are now achieving the goals of ubiquity and versatility across computing environments which was first demonstrated by IISS.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Health Informatics projects in Canada are implemented provincially, with different provinces creating different systems. A national, federally funded, not-for-profit organisation called Canada Health Infoway was created in 2001 to foster the development and adoption of electronic health records across Canada. As of December 31, 2008, there were 276 EHR projects under way in Canadian hospitals, other health-care facilities, pharmacies and laboratories, with an investment value of $1.5-billion from Canada Health Infoway.Provincial and territorial programmes include the following:\n\neHealth Ontario was created as an Ontario provincial government agency in September 2008. It has been plagued by delays and its CEO was fired over a multimillion-dollar contracts scandal in 2009.\nAlberta Netcare was created in 2003 by the Government of Alberta. Today the netCARE portal is used daily by thousands of clinicians. It provides access to demographic data, prescribed\/dispensed drugs, known allergies\/intolerances, immunizations, laboratory test results, diagnostic imaging reports, the diabetes registry and other medical reports. netCARE interface capabilities are being included in electronic medical record products that are being funded by the provincial government.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In Asia and Australia-New Zealand, the regional group called the Asia Pacific Association for Medical Informatics (APAMI) was established in 1994 and now consists of more than 15 member regions in the Asia Pacific Region.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"A build server compiles the code periodically. The build server may automatically run tests and\/or implement other continuous quality control processes. Such processes aim to improve software quality and delivery time by periodically running additional static analyses, measuring performance, extracting documentation from the source code, and facilitating manual QA processes.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Some other key events in the history of programming language theory since then:\n\n1950sNoam Chomsky developed the Chomsky hierarchy in the field of linguistics, a discovery which has directly impacted programming language theory and other branches of computer science.1960sThe Simula language was developed by Ole-Johan Dahl and Kristen Nygaard; it is widely considered to be the first example of an object-oriented programming language; Simula also introduced the concept of coroutines.\nIn 1964, Peter Landin is the first to realize Church's lambda calculus can be used to model programming languages. He introduces the SECD machine which \"interprets\" lambda expressions.\nIn 1965, Landin introduces the J operator, essentially a form of continuation.\nIn 1966, Landin introduces ISWIM, an abstract computer programming language in his article The Next 700 Programming Languages. It is influential in the design of languages leading to the Haskell programming language.\nIn 1966, Corrado Böhm introduced the programming language CUCH (Curry-Church).\nIn 1967, Christopher Strachey publishes his influential set of lecture notes Fundamental Concepts in Programming Languages, introducing the terminology R-values, L-values, parametric polymorphism, and ad hoc polymorphism.\nIn 1969, J. Roger Hindley publishes The Principal Type-Scheme of an Object in Combinatory Logic, later generalized into the Hindley–Milner type inference algorithm.\nIn 1969, Tony Hoare introduces the Hoare logic, a form of axiomatic semantics.\nIn 1969, William Alvin Howard observed that a \"high-level\" proof system, referred to as natural deduction, can be directly interpreted in its intuitionistic version as a typed variant of the model of computation known as lambda calculus. This became known as the Curry–Howard correspondence.1970sIn 1970, Dana Scott first publishes his work on denotational semantics.\nIn 1972, logic programming and Prolog were developed thus allowing computer programs to be expressed as mathematical logic.\nA team of scientists at Xerox PARC led by Alan Kay develop Smalltalk, an object-oriented language widely known for its innovative development environment.\nIn 1974, John C. Reynolds discovers System F. It had already been discovered in 1971 by the mathematical logician Jean-Yves Girard.\nFrom 1975, Gerald Jay Sussman and Guy Steele develop the Scheme programming language, a Lisp dialect incorporating lexical scoping, a unified namespace, and elements from the actor model including first-class continuations.\nBackus, at the 1977 Turing Award lecture, assailed the current state of industrial languages and proposed a new class of programming languages now known as function-level programming languages.\nIn 1977, Gordon Plotkin introduces Programming Computable Functions, an abstract typed functional language.\nIn 1978, Robin Milner introduces the Hindley–Milner type inference algorithm for ML. Type theory became applied as a discipline to programming languages, this application has led to tremendous advances in type theory over the years.1980sIn 1981, Gordon Plotkin publishes his paper on structured operational semantics.\nIn 1988, Gilles Kahn published his paper on natural semantics.\nThere emerged process calculi, such as the Calculus of Communicating Systems of Robin Milner, and the Communicating sequential processes model of C. A. R. Hoare, as well as similar models of concurrency such as the actor model of Carl Hewitt.\nIn 1985, the release of Miranda sparks an academic interest in lazy-evaluated pure functional programming languages. A committee was formed to define an open standard resulting in the release of the Haskell 1.0 standard in 1990.\nBertrand Meyer created the methodology Design by contract and incorporated it into the Eiffel programming language.1990sGregor Kiczales, Jim Des Rivieres and Daniel G. Bobrow published the book The Art of the Metaobject Protocol.\nEugenio Moggi and Philip Wadler introduced the use of monads for structuring programs written in functional programming languages.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the  Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC\/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"A medical robot is a robot used in the medical sciences. They include surgical robots. These are in most telemanipulators, which use the surgeon's activators on one side to control the \"effector\" on the other side. There are the following types of medical robots:\n\nSurgical robots: either allow surgical operations to be carried out with better precision than an unaided human surgeon or allow remote surgery where a human surgeon is not physically present with the patient.\nRehabilitation robots: facilitate and support the lives of infirm, elderly people, or those with dysfunction of body parts affecting movement. These robots are also used for rehabilitation and related procedures, such as training and therapy.\nBiorobots: a group of robots designed to imitate the cognition of humans and animals.\nTelepresence robots: allow off-site medical professionals to move, look around, communicate, and participate from remote locations.\nPharmacy automation: robotic systems to dispense oral solids in a retail pharmacy setting or preparing sterile IV admixtures in a hospital pharmacy setting.\nCompanion robot: has the capability to engage emotionally with users keeping them company and alerting if there is a problem with their health.\nDisinfection robot: has the capability to disinfect a whole room in mere minutes, generally using pulsed ultraviolet light. They are being used to fight Ebola virus disease.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Software maintenance refers to the activities required to provide cost-effective support after shipping the software product. Software maintenance is modifying and updating software applications after distribution to correct faults and to improve its performance. Software has a lot to do with the real world and when the real world changes, software maintenance is required. Software maintenance includes: error correction, optimization, deletion of unused and discarded features, and enhancement of features that already exist. Usually, maintenance takes up about 40% to 80% of the project cost therefore, focusing on maintenance keeps the costs down.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer Science\nInformation engineering\nInformation technology\nTraditional engineering\nComputer engineering\nElectrical engineering\nSoftware engineering\nDomain engineering\nInformation technology engineering\nKnowledge engineering\nUser interface engineering\nWeb engineering\nArts and Sciences\nMathematics\nComputer science\nInformation science\nApplication software\nInformation systems\nProgramming\nSystems Engineering\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Logging time, defect, and size data is an essential part of planning and tracking PSP projects, as historical data is used to improve estimating accuracy.\nThe PSP uses the PROxy-Based Estimation (PROBE) method to improve a developer's estimating skills for more accurate project planning. For project tracking, the PSP uses the earned value method.\nThe PSP also uses statistical techniques, such as correlation, linear regression, and standard deviation, to translate data into useful information for improving estimating, planning and quality. These statistical formulas are calculated by the PSP tool.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Extreme Programming Explained describes extreme programming as a software-development discipline that organizes people to produce higher-quality software more productively.\nXP attempts to reduce the cost of changes in requirements by having multiple short development cycles, rather than a long one.\nIn this doctrine, changes are a natural, inescapable and desirable aspect of software-development projects, and should be planned for, instead of attempting to define a stable set of requirements.\nExtreme programming also introduces a number of basic values, principles and practices on top of the agile methodology.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"A Business reference model is a reference model, concentrating on the functional and organizational aspects of the core business of an enterprise, service organization or government agency. In enterprise engineering a business reference model is part of an Enterprise Architecture Framework or Architecture Framework, which defines how to organize the structure and views associated with an Enterprise Architecture.\nA reference model in general is a model of something that embodies the basic goal or idea of something and can then be looked at as a reference for various purposes. A business reference model is a means to describe the business operations of an organization, independent of the organizational structure that perform them. Other types of business reference model can also depict the relationship between the business processes, business functions, and the business area's business reference model. These reference model can be constructed in layers, and offer a foundation for the analysis of service components, technology, data, and performance.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Parasoft Jtest uses runtime error detection to expose defects such as race conditions, exceptions, resource and memory leaks, and security attack vulnerabilities.\nIntel Inspector performs run-time threading and memory error analysis in Windows.\nParasoft Insure++ is a runtime memory analysis and error detection tool. Its Inuse component provides a graphical view of memory allocations over time, with specific visibility of overall heap usage, block allocations, possible outstanding leaks, etc.\nGoogle's Thread Sanitizer is a data race detection tool. It instruments LLVM IR to capture racy memory accesses.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"This technique effectively adds instructions to the target program to collect the required information. Note that instrumenting a program can cause performance changes, and may in some cases lead to inaccurate results and\/or heisenbugs.  The effect will depend on what information is being collected, on the level of timing details reported, and on whether basic block profiling is used in conjunction with instrumentation.  For example, adding code to count every procedure\/routine call will probably have less effect than counting how many times each statement is obeyed.  A few computers have special hardware to collect information; in this case the impact on the program is minimal.\nInstrumentation is key to determining the level of control and amount of time resolution available to the profilers. \n\nManual: Performed by the programmer, e.g. by adding instructions to explicitly calculate runtimes, simply count events or calls to measurement APIs such as the Application Response Measurement standard.\nAutomatic source level: instrumentation added to the source code by an automatic tool according to an instrumentation policy.\nIntermediate language: instrumentation added to assembly or decompiled bytecodes giving support for multiple higher-level source languages and avoiding (non-symbolic) binary offset re-writing issues.\nCompiler assisted\nBinary translation: The tool adds instrumentation to a compiled executable.\nRuntime instrumentation: Directly before execution the code is instrumented. The program run is fully supervised and controlled by the tool.\nRuntime injection: More lightweight than runtime instrumentation. Code is modified at runtime to have jumps to helper functions.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"e-CitizenThe e-Citizen qualification allows beginners to get online and start using the Internet. The qualification has been designed to provide a basic understanding of the Internet and to start using the web safely, from reading email to shopping online.\nMoR (Management of Risk)M_o_R Foundation is suitable for any organisation or individual seeing the need for guidance on a controlled approach to identification, assessment and control risk at strategic, programme, project and operational perspectives.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"CI should be used in combination with automated unit tests written through the practices of test-driven development. All unit tests in the developer's local environment should be run and passed before committing to the mainline. This helps prevent one developer's work-in-progress from breaking another developer's copy. Where necessary, incomplete features can be disabled before committing, using feature toggles, for instance.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Parnas earned his PhD at Carnegie Mellon University in electrical engineering. Parnas also earned a professional engineering license in Canada and was one of the first to apply traditional engineering principles to software design.\nHe worked there as a professor for many years.  He also taught at the University of North Carolina at Chapel Hill (U.S.), at the Department of Computer Science of the Technische Universität Darmstadt (Germany), the University of Victoria (British Columbia, Canada), Queen's University in Kingston, Ontario, McMaster University in Hamilton, Ontario, and University of Limerick (Republic of Ireland).\nDavid Parnas received a number of awards and honors:\n\nACM \"Best Paper\" Award, 1979\nNorbert Wiener Award for Social and Professional Responsibility, 1987\nTwo \"Most Influential Paper\" awards International Conference on Software Engineering, 1991 and 1995\nDoctor honoris causa of the Computer Science Department, ETH Zurich, Switzerland, 1986\nFellow of the Royal Society of Canada, 1992\nFellow of the Association for Computing Machinery, 1994\nDoctor honoris causa of the Louvain School of Engineering, University of Louvain (UCLouvain), Belgium, 1996\nACM SIGSOFT's \"Outstanding Research\" award, 1998\nIEEE Computer Society's 60th Anniversary Award, 2007\nDoctor honoris causa of the Faculty of Informatics, University of Lugano, Switzerland, 2008\nFellow of the Gesellschaft für Informatik, 2008\nFellow of the Institute of Electrical and Electronics Engineers (IEEE), 2009\nDoctor honoris causa of the Vienna University of Technology (Dr. Tech.H.C.), Vienna Austria, 2011\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Notable definitions of software engineering include:\n\n\"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software\"—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary\n\"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software\"—IEEE Standard Glossary of Software Engineering Terminology\n\"an engineering discipline that is concerned with all aspects of software production\"—Ian Sommerville\n\"the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines\"—Fritz Bauer\n\"a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs\"—Merriam-Webster\n\"'software engineering' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as 'programming integrated over time.'\"—Software Engineering at GoogleThe term has also been used less formally:\n\nas the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;\nas the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;\nas the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Type theory is the study of type systems; which are \"a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute\". Many programming languages are distinguished by the characteristics of their type systems.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and\/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.\nIn the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. NCEES ended the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial.The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a \"Software Engineering Code of Ethics\".\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The IDEF0 functional modeling method is designed to model the decisions, actions, and activities of an organization or system. It was derived from the established graphic modeling language structured analysis and design technique (SADT) developed by Douglas T. Ross and SofTech, Inc. In its original form, IDEF0 includes both a definition of a graphical modeling language (syntax and semantics) and a description of a comprehensive methodology for developing models. The US Air Force commissioned the SADT developers to develop a function model method for analyzing and communicating the functional perspective of a system. IDEF0 should assist in organizing system analysis and promote effective communication between the analyst and the customer through simplified graphical devices.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Goal-oriented design (or Goal-Directed design) \"is concerned with satisfying the needs and desires of the users of a product or service.\": xxviii, 31 Alan Cooper argues in The Inmates Are Running the Asylum that we need a new approach to solving interactive software-based problems.: 1  The problems with designing computer interfaces are fundamentally different from those that do not include software (e.g., hammers).  Cooper introduces the concept of cognitive friction, which is when the interface of a design is complex and difficult to use, and behaves inconsistently and unexpectedly, possessing different modes.: 22 Alternatively, interfaces can be designed to serve the needs of the service\/product provider. User needs may be poorly served by this approach.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In 1976, an entity-relationship (ER) graphic notation was introduced by Peter Chen. He stressed that it was a \"semantic\" modelling technique and independent of any database modelling techniques such as Hierarchical, CODASYL, Relational etc. Since then, languages for information models have continued to evolve. Some examples are the Integrated Definition Language 1 Extended (IDEF1X), the EXPRESS language and the Unified Modeling Language (UML).Research by contemporaries of Peter Chen such as J.R.Abrial (1974) and G.M Nijssen (1976) led to today's Fact Oriented Modeling (FOM) languages which are based on linguistic propositions rather than on \"entities\". FOM tools can be used to generate an ER model which means that the modeler can avoid the time-consuming and error prone practice of manual normalization. Object-Role Modeling language (ORM) and Fully Communication Oriented Information Modeling (FCO-IM) are both research results developed in the early 1990s, based upon earlier research.\nIn the 1980s there were several approaches to extend Chen’s Entity Relationship Model. Also important in this decade is REMORA by Colette Rolland.The ICAM Definition (IDEF) Language was developed from the U.S. Air Force ICAM Program during the 1976 to 1982 timeframe. The objective of the ICAM Program, according to Lee (1999), was to increase manufacturing productivity through the systematic application of computer technology. IDEF includes three different modeling methods: IDEF0, IDEF1, and IDEF2 for producing a functional model, an information model, and a dynamic model respectively. IDEF1X is an extended version of IDEF1. The language is in the public domain. It is a graphical representation and is designed using the ER approach and the relational theory. It is used to represent the “real world” in terms of entities, attributes, and relationships between entities. Normalization is enforced by KEY Structures and KEY Migration. The language identifies property groupings (Aggregation) to form complete entity definitions.EXPRESS was created as ISO 10303-11 for formally specifying information requirements of product data model. It is part of a suite of standards informally known as the STandard for the Exchange of Product model data (STEP). It was first introduced in the early 1990s. The language, according to Lee (1999), is a textual representation. In addition, a graphical subset of EXPRESS called EXPRESS-G is available. EXPRESS is based on programming languages and the O-O paradigm. A number of languages have contributed to EXPRESS. In particular, Ada, Algol, C, C++, Euler, Modula-2, Pascal, PL\/1, and SQL. EXPRESS consists of language elements that allow an unambiguous object definition and specification of constraints on the objects defined. It uses SCHEMA declaration to provide partitioning and it supports specification of data properties, constraints, and operations.UML is a modeling language for specifying, visualizing, constructing, and documenting the artifacts, rather than processes, of software systems. It was conceived originally by Grady Booch, James Rumbaugh, and Ivar Jacobson. UML was approved by the Object Management Group (OMG) as a standard in 1997. The language, according to Lee (1999), is non-proprietary and is available to the public. It is a graphical representation. The language is based on the objected-oriented paradigm. UML contains notations and rules and is designed to represent data requirements in terms of O-O diagrams. UML organizes a model in a number of views that present different aspects of a system. The contents of a view are described in diagrams that are graphs with model elements. A diagram contains model elements that represent common O-O concepts such as classes, objects, messages, and relationships among these concepts.IDEF1X, EXPRESS, and UML all can be used to create a conceptual model and, according to Lee (1999), each has its own characteristics. Although some may lead to a natural usage (e.g., implementation), one is not necessarily better than another. In practice, it may require more than one language to develop all information models when an application is complex. In fact, the modeling practice is often more important than the language chosen.Information models can also be expressed in formalized natural languages, such as Gellish. Gellish, which has natural language variants Gellish Formal English, Gellish Formal Dutch (Gellish Formeel Nederlands), etc. is an information representation language or modeling language that is defined in the Gellish smart Dictionary-Taxonomy, which has the form of a Taxonomy\/Ontology. A Gellish Database is not only suitable to store information models, but also knowledge models, requirements models and dictionaries, taxonomies and ontologies. Information models in Gellish English use Gellish Formal English expressions. For example, a geographic information model might consist of a number of Gellish Formal English expressions, such as: \n- the Eiffel tower <is located in> Paris\n- Paris <is classified as a> city\n\nwhereas information requirements and knowledge can be expressed for example as follows:\n\n- tower <shall be located in a> geographical area\n- city <is a kind of> geographical area\n\nSuch Gellish expressions use names of concepts (such as 'city') and relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish Formal English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models, because the dictionary contains definitions of more than 40000 concepts, including more than 600 standard relation types. Thus, an information model in Gellish consists of a collection of Gellish expressions that use those phrases and dictionary concepts to express facts or make statements, queries and answers.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"This section lists best practices suggested by various authors on how to achieve continuous integration, and how to automate this practice. Build automation is a best practice itself.Continuous integration—the practice of frequently integrating one's new or changed code with the existing code repository —should occur frequently enough that no intervening window remains between commit and build, and such that no errors can arise without developers noticing them and correcting them immediately. Normal practice is to trigger these builds by every commit to a repository, rather than a periodically scheduled build. The practicalities of doing this in a multi-developer environment of rapid commits are such that it is usual to trigger a short time after each commit, then to start a build when either this timer expires or after a rather longer interval since the last build. Note that since each new commit resets the timer used for the short time trigger, this is the same technique used in many button debouncing algorithms. In this way, the commit events are \"debounced\" to prevent unnecessary builds between a series of rapid-fire commits. Many automated tools offer this scheduling automatically.\nAnother factor is the need for a version control system that supports atomic commits; i.e., all of a developer's changes may be seen as a single commit operation. There is no point in trying to build from only half of the changed files.\nTo achieve these objectives, continuous integration relies on the following principles.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Kent Beck developed extreme programming during his work on the Chrysler Comprehensive Compensation System (C3) payroll project. Beck became the C3 project leader in March 1996. He began to refine the development methodology used in the project and wrote a book on the methodology (Extreme Programming Explained, published in October 1999). Chrysler cancelled the C3 project in February 2000, after seven years, when Daimler-Benz acquired the company. Ward Cunningham was another major influence on XP.\nMany extreme-programming practices have been around for some time; the methodology takes \"best practices\" to extreme levels. For example, the \"practice of test-first development, planning and writing tests before each micro-increment\" was used as early as NASA's Project Mercury, in the early 1960s. To shorten the total development time, some formal test documents (such as for acceptance testing) have been developed in parallel with (or shortly before) the software being ready for testing. A NASA independent test group can write the test procedures, based on formal requirements and logical limits, before programmers write the software and integrate it with the hardware. XP takes this concept to the extreme level, writing automated tests (sometimes inside software modules) which validate the operation of even small sections of software coding, rather than only testing the larger features.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Functional decomposition refers broadly to the process of resolving a functional relationship into its constituent parts in such a way that the original function can be reconstructed from those parts by function composition. In general, this process of decomposition is undertaken either for the purpose of gaining insight into the identity of the constituent components, or for the purpose of obtaining a compressed representation of the global function, a task which is feasible only when the constituent processes possess a certain level of modularity.\nFunctional decomposition has a prominent role in computer programming, where a major goal is to modularize processes to the greatest extent possible. For example, a library management system may be broken up into an inventory module, a patron information module, and a fee assessment module. In the early decades of computer programming, this was manifested as the \"art of subroutining,\" as it was called by some prominent practitioners.\nFunctional decomposition of engineering systems is a method for analyzing engineered systems. The basic idea is to try to divide a system in such a way that each block of the block diagram can be described without an \"and\" or \"or\" in the description.\nThis exercise forces each part of the system to have a pure function. When a system is composed of pure functions, they can be reused, or replaced. A usual side effect is that the interfaces between blocks become simple and generic. Since the interfaces usually become simple, it is easier to replace a pure function with a related, similar function.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"To ensure that there is proper feedback validating that the system meets the NFR specified performance metrics, any major system needs a monitoring subsystem. The planning, design, installation, configuration, and control of the monitoring subsystem are specified by an appropriately defined monitoring process.\nThe benefits are as follows:\n\nIt is possible to establish service level agreements at the use case level.\nIt is possible to turn on and turn off monitoring at periodic points or to support problem resolution.\nIt enables the generation of regular reports.\nIt enables the ability to track trends over time, such as the impact of increasing user loads and growing data sets on use case level performance.The trend analysis component of this cannot be undervalued. This functionality, properly implemented, will enable predicting when a given application undergoing gradually increasing user loads and growing data sets will exceed the specified non functional performance requirements for a given use case. This permits proper management budgeting, acquisition of, and deployment of the required resources to keep the system running within the parameters of the non functional performance requirements.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Although first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Mills served on the faculties of Iowa State University, Princeton, New York and Johns Hopkins Universities, the Universities of Maryland and Florida,\nand Florida Institute of Technology (FIT). At Johns Hopkins and Maryland, he initiated one of the first American university courses in structured programming. At Maryland, he developed a new two-semester freshman introduction to computer science and textbook \"Principles of Computer Programming: A Mathematical Approach\" with co-authors Basili, Gannon, and Hamlet. At FIT, he developed a new freshman and sophomore curriculum for software engineering using Ada as the underlying language with colleagues Engle and Newman.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems. The earliest known tool for use in computation is the abacus, and it is thought to have been invented in Babylon circa between 2700–2300 BC. Abaci, of a more modern design, are still used as calculation tools today.\nThe first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, the Manchester Baby. However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. The metal–oxide–silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.  The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer revolution.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The N2 Chart is a diagram in the shape of a matrix, representing functional or physical interfaces between system elements. It is used to systematically identify, define, tabulate, design, and analyze functional and physical interfaces. It applies to system interfaces and hardware and\/or software interfaces.The N2 diagram has been used extensively to develop data interfaces, primarily in the software areas. However, it can also be used to develop hardware interfaces. The basic N2 chart is shown in Figure 2. The system functions are placed on the diagonal; the remainder of the squares in the N × N matrix represent the interface inputs and outputs.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"This phase involves the determination of costs, benefits, and schedule impact. It has four components:\n\nSort by Value: Business sorts the user stories by Business Value.\nSort by Risk: Development sorts the stories by risk.\nSet Velocity: Development determines at what speed they can perform the project.\nChoose scope: The user stories that will be finished in the next release will be picked. Based on the user stories the release date is determined.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In 1976, Hamilton co-founded with Saydean Zeldin a company called Higher Order Software (HOS) to further develop ideas about error prevention and fault tolerance emerging from their experience at MIT working on the Apollo program. They created a product called USE.IT, based on the HOS methodology they developed at MIT. It was successfully used in numerous government programs including a project to formalize and implement C-IDEF, an automated version of IDEF, a modeling language developed by the U.S. Air Force in the Integrated Computer-Aided Manufacturing (ICAM) project. In 1980, British-Israeli computer scientist David Harel published a proposal for a structured programming language derived from HOS from the viewpoint of and\/or subgoals. Others have used HOS to formalize the semantics of linguistic quantifiers, and to formalize the design of reliable real-time embedded systems.Hamilton was the CEO of HOS through 1984 and left the company in 1985. In March 1986, she founded Hamilton Technologies, Inc. in Cambridge, Massachusetts. The company was developed around the Universal Systems Language (USL) and its associated automated environment, the 001 Tool Suite, based on her paradigm of development before the fact for systems design and software development.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Jan van Bemmel has described medical informatics as the theoretical and practical aspects of information processing and communication based on knowledge and experience derived from processes in medicine and health care.The Faculty of Clinical Informatics has identified six high level domains of core competency for clinical informaticians:\nHealth and Wellbeing in Practice\nInformation Technologies and Systems\nWorking with Data and Analytical Methods\nEnabling Human and Organizational Change\nDecision Making\nLeading Informatics Teams and projects.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The Internet is a global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP\/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web and the infrastructure to support email.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Following this fundamental choice, a second choice made by BDD relates to how the desired behavior should be specified. In this area BDD chooses to use a semi-formal format for behavioral specification which is borrowed from user story specifications from the field of object-oriented analysis and design. The scenario aspect of this format may be regarded as an application of Hoare logic to behavioral specification of software units using the domain-specific language of the situation.\nBDD specifies that business analysts and developers should collaborate in this area and should specify behavior in terms of user stories, which are each explicitly written down in a dedicated document. Each user story should, in some way, follow the following structure:\nTitle\nAn explicit title.Narrative\nA short introductory section with the following structure:\nAs a: the person or role who will benefit from the feature;\nI want: the feature;\nso that: the benefit or value of the feature.Acceptance criteria\nA description of each specific scenario of the narrative with the following structure:\nGiven: the initial context at the beginning of the scenario, in one or more clauses;\nWhen: the event that triggers the scenario;\nThen: the expected outcome, in one or more clauses.BDD does not have any formal requirements for exactly how these user stories must be written down, but it does insist that each team using BDD come up with a simple, standardized format for writing down the user stories which includes the elements listed above. However, in 2007 Dan North suggested a template for a textual format which has found wide following in different BDD software tools. A very brief example of this format might look like this:\n\nTitle: Returns and exchanges go to inventory.\n\nAs a store owner,\nI want to add items back to inventory when they are returned or exchanged,\nso that I can sell them again.\n\nScenario 1: Items returned for refund should be added to inventory.\nGiven that a customer previously bought a black sweater from me\nand I have three black sweaters in inventory,\nwhen they return the black sweater for a refund,\nthen I should have four black sweaters in inventory.\n\nScenario 2: Exchanged items should be returned to inventory.\nGiven that a customer previously bought a blue garment from me\nand I have two blue garments in inventory\nand three black garments in inventory,\nwhen they exchange the blue garment for a black garment,\nthen I should have three blue garments in inventory\nand two black garments in inventory.\n\nThe scenarios are ideally phrased declaratively rather than imperatively — in the business language, with no reference to elements of the UI through which the interactions take place.This format is referred to as the Gherkin language, which has a syntax similar to the above example. The term Gherkin, however, is specific to the Cucumber, JBehave, Lettuce, behave and Behat  software tools.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Application software, also known as an application or an app, is computer software designed to help the user perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user. \nApplication software applies the power of a particular computing platform or system software to a particular purpose. Some apps, such as Microsoft Office, are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a geography application for Windows or an Android application for education or Linux gaming. Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as killer applications.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Generally, software and information technology companies look for people who have strong programming skills, system analysis, and software testing skills.Many colleges teach practical skills that are crucial to becoming a software developer. As logical reasoning and critical thinking are important in becoming a software professional, this degree encompasses the complete process of software development from software design and development to final testing.Students who complete their undergraduate education in software engineering at a satisfactory level often pursue graduate studies such as a Master of Science in Information Technology (M.Sc IT) and sometimes continuing onto a doctoral program and earning a doctorate such as a Doctor of Information Technology (DIT).\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Making builds readily available to stakeholders and testers can reduce the amount of rework necessary when rebuilding a feature that doesn't meet requirements. Additionally, early testing reduces the chances that defects survive until deployment. Finding errors earlier can reduce the amount of work necessary to resolve them.\nAll programmers should start the day by updating the project from the repository. That way, they will all stay up to date.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Structured Analysis and Design Technique (SADT) is a software engineering methodology for describing systems as a hierarchy of functions, a diagrammatic notation for constructing a sketch for a software application. It offers building blocks to represent entities and activities, and a variety of arrows to relate boxes. These boxes and arrows have an associated informal semantics. SADT can be used as a functional analysis tool of a given process, using successive levels of details. The SADT method allows to define user needs for IT developments, which is used in industrial Information Systems, but also to explain and to present an activity's manufacturing processes, procedures.The SADT supplies a specific functional view of any enterprise by describing the functions and their relationships in a company. These functions fulfill the objectives of a company, such as sales, order planning, product design, part manufacturing, and human resource management. The SADT can depict simple functional relationships and can reflect data and control flow relationships between different functions. The IDEF0 formalism is based on SADT, developed by Douglas T. Ross in 1985.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In 1986, Hamilton received the Augusta Ada Lovelace Award by the Association for Women in Computing.\nIn 2003, she was given the NASA Exceptional Space Act Award for scientific and technical contributions. The award included $37,200, the largest amount awarded to any individual in NASA's history.\nIn 2009, she received the Outstanding Alumni Award by Earlham College.\nIn 2016, she received the Presidential Medal of Freedom from Barack Obama, the highest civilian honor in the United States.\nOn April 28, 2017, she received the Computer History Museum Fellow Award, which honors exceptional men and women whose computing ideas have changed the world.\nIn 2017, a \"Women of NASA\" LEGO set went on sale featuring minifigures of Hamilton, Mae Jemison, Sally Ride, and Nancy Grace Roman.\nIn 2018, she was awarded an honorary doctorate degree by the Polytechnic University of Catalonia.\nIn 2019, she was awarded The Washington Award.\nIn 2019, she was awarded an honorary doctorate degree by Bard College.\nIn 2019, she was awarded the Intrepid Lifetime Achievement Award.\nIn 2022, she was inducted into the National Aviation Hall of Fame in Dayton, Ohio.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"An extensional model is one that maps to the elements of a particular methodology or technology, and is thus a \"platform specific model\". The UML specification explicitly states that associations in class models are extensional and this is in fact self-evident by considering the extensive array of additional \"adornments\" provided by the specification over and above those provided by any of the prior candidate \"semantic modelling languages\".\"UML as a Data Modeling Notation, Part 2\"\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\n\nDifferent indent styles (whitespace)\nComments\nDecomposition\nNaming conventions for objects (such as variables, classes, functions, procedures, etc.)The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The programming languages listed here have event-based profilers:\n\nJava: the JVMTI (JVM Tools Interface) API, formerly JVMPI (JVM Profiling Interface), provides hooks to profilers, for trapping events like calls, class-load, unload, thread enter leave.\n.NET: Can attach a profiling agent as a COM server to the CLR using Profiling API. Like Java, the runtime then provides various callbacks into the agent, for trapping events like method JIT \/ enter \/ leave, object creation, etc. Particularly powerful in that the profiling agent can rewrite the target application's bytecode in arbitrary ways.\nPython: Python profiling includes the profile module, hotshot (which is call-graph based), and using the 'sys.setprofile' function to trap events like c_{call,return,exception}, python_{call,return,exception}.\nRuby: Ruby also uses a similar interface to Python for profiling. Flat-profiler in profile.rb, module, and ruby-prof a C-extension are present.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The focus of the cleanroom process is on defect prevention, rather than defect removal. The name \"cleanroom\" was chosen to evoke the cleanrooms used in the electronics industry to prevent the introduction of defects during the fabrication of semiconductors.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Bachman notation\nBarker's notation\nEXPRESS\nIDEF1X\n§ Crow's foot notation (also Martin notation)\n(min, max)-notation of Jean-Raymond Abrial in 1974\nUML class diagrams\nMerise\nObject-role modeling\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Ian Sommerville was born in Glasgow, Scotland in 1951.\nHe studied Physics at Strathclyde University and Computer Science at the University of St Andrews. He is married and has two daughters. As an amateur gourmet, he has written a number of restaurant reviews.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"There are several fields of study that either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Hamilton then joined the MIT Instrumentation Laboratory, which developed the Apollo Guidance Computer for the Apollo lunar exploration program. Hamilton was the first programmer hired for the Apollo project and in 1965 became Director of the Software Engineering Division. She was responsible for the team writing and testing all on board in flight software for the Apollo spacecraft's Command and Lunar Module and for the subsequent Skylab space station. Another part of her team designed and developed the systems software. This included error detection and recovery software such as restarts and the Display Interface Routines (also known as the Priority Displays), which Hamilton designed and developed. She worked to gain hands-on experience during a time when computer science courses were uncommon and software engineering courses did not exist.Her areas of expertise include: systems design and software development, enterprise and process modeling, development paradigm, formal systems modeling languages, system-oriented objects for systems modeling and development, automated life-cycle environments, methods for maximizing software reliability and reuse, domain analysis, correctness by built-in language properties, open-architecture techniques for robust systems, full life-cycle automation, quality assurance, seamless integration, error detection and recovery techniques, human-machine interface systems, operating systems, end-to-end testing techniques, and life-cycle management techniques. These techniques are intended to make code more reliable because they help programmers identify and fix errors sooner in the development process.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The concept of dimensions of interaction design were introduced in Moggridge's book Designing Interactions. Crampton Smith wrote that interaction design draws on four existing design languages, 1D, 2D, 3D, 4D. Kevin Silver later proposed a fifth dimension, behaviour.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Dynamic symbolic execution (also known as DSE or concolic execution) involves executing a test program on a concrete input, collecting the path constraints associated with the execution, and using a constraint solver (generally, an SMT solver) to generate new inputs that would cause the program to take a different control-flow path, thus increasing code coverage of the test suite. DSE can considered a type of fuzzing (\"white-box\" fuzzing).\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Candidates had to undergo a peer review of their education and professional qualifications in order to receive authorization to take the CSDP examination. Candidates therefore had to submit an application to the IEEE Computer Society that provided verifiable information regarding their educational background and professional experience.\nThe Certified Software Development Associate (CSDA) certification was available to graduating students and early-career software professionals who did not meet the eligibility requirements for the CSDP.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The CSDP examination content was based on the Guide To The Software Engineering Body of Knowledge.  The examination covered content from all primary knowledge areas in the SWEBOK Guide Version 3. Below is a list of the topics tested in terms of their proportion of the total examination.\nSoftware requirements 11%\nSoftware design 11%\nSoftware construction 9%\nSoftware testing 11%\nSoftware maintenance 5%\nSoftware configuration management 5%\nSoftware engineering management 8%\nSoftware engineering process 5%\nSoftware engineering methods 4%\nSoftware quality 7%\nSoftware engineering professional practice 5%\nSoftware engineering economics  5%\nComputing foundations 5%\nMathematical foundations 3%\nEngineering foundations 4%\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The development team should always be working on the latest version of the software.  Since different team members may have versions  saved locally with various changes and improvements, they should try to upload their current version to the code repository every few hours, or when a significant break presents itself.  Continuous integration will avoid delays later on in the project cycle, caused by integration problems.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"During this defining phase, the critical business processes are decomposed to critical use cases. Probe cases will be decomposed further, as needed, to single page (screen) transitions. These are the use cases that will be subjected to script driven performance testing.\nThe type of requirements that relate to performance engineering are the non-functional requirements, or NFR. While a functional requirement relates to which business operations are to be performed, a performance related non-functional requirement will relate to how fast that business operation performs under defined circumstances.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world's first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took five years to complete.\nWhile the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947, planar process by Jean Hoerni, the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959, the metal–oxide–semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Continuous deployment is a software engineering approach which uses automated software deployments.\nIn it, software is produced in short cycles but through automated software deployments even to production rather than requiring a \"click of a button\" for that last step.: 52  Therefore, continuous deployment can be considered a more sophisticated form of automation.\nAcademic literature differentiates between continuous delivery and continuous deployment according to deployment method; manual vs. automated.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Bernd Bruegge is the author of the following books:\n\nBernd Bruegge, Allen Dutoit:  Object-Oriented Software Engineering: Using UML, Patterns and Java (Third Edition). Prentice Hall, 2009. ISBN 978-0136061250.\nEva-Maria Kern, Heinz-Gerd Hegering, Bernd Brügge: Managing Development and Application of Digital Technologies. Sringer. 2006. ISBN 3-540-34128-5.He is also the author of many academic papers, for example:\n\nStephan Krusche, Dora Dzvonyar, Han Xu and Bernd Bruegge. Software Theater — Teaching Demo Oriented Prototyping. Transactions on Computing Education. ACM Journal. 2018\nStephan Krusche, Bernd Bruegge, Irina Camilleri, Kirill Krinkin, Andreas Seitz and Cecil Wöbker. Chaordic Learning: A Case Study. 39th International Conference on Software Engineering (ICSE'17), Software Engineering Education and Training, pages 87–96. ACM. Buenos Aires - Argentina, May 2017\nStephan Krusche, Andreas Seitz, Jürgen Börstler and Bernd Bruegge. Interactive Learning – Increasing Student Participation through Shorter Exercise Cycles. 19th Australasian Computing Education Conference (ACE'17), pages 17–26. ACM. Geelong - Australia, January 2017\nBernd Bruegge, Stephan Krusche and Lukas Alperowitz. Software Engineering Project Courses with Industrial Clients. Transactions on Computing Education 15(4), pages 17:1-17:31. ACM Journal. 2015\nBernd Bruegge, Allen Dutoit, Timo Wolf. Sysiphus: Enabling informal collaboration in global software development. In the proceedings of the International Conference on Global Software Engineering (ICGSE) 2006.\nMartin Bauer, Bernd Bruegge, et al. Design of a component-based augmented reality framework. In the Proceedings of IEEE and ACM International Symposium on Augmented Reality. 2001.\nBernd Bruegge, Allen Dutoit, et al. Transatlantic project courses in a university environment. In the Proceedings of the 7th Asia-Pacific Software Engineering Conference (APSEC), 2000.\nAllen Dutoit, Bernd Bruegge. Communication metrics for software development. IEEE Transactions on Software Engineering, 24(8), pp. 615–628. 1998.\nBernd Bruegge, Allen Dutoit. Communication metrics for software development. In the proceedings of the 19th ACM International Conference on Software Engineering (ICSE), 1997.\nBernd Bruegge, Ben Bennington. Applications of mobile computing and communication. IEEE Personal Communications, 3(1), pp. 64–71. 1996.\nBernd Bruegge, Robert Coyne. Teaching iterative and collaborative design: lessons and directions. Software Engineering Education, Springer, pp. 411–427, 1994.\nBernd Bruegge, Tim Gottschalk, Bin Luo. A framework for dynamic program analyzers. In ACM SIGPLAN Notices, 28(10), 1993.\nBernd Bruegge, Jim Blythe, et al. Object-oriented system modeling with OMT. In ACM SIGPLAN Notices, 27(10), pp. 359–376. 1992.\nBernd Bruegge, Peter Hibbard. Generalized path expressions: A high-level debugging mechanism. Journal of Systems and Software, 3(4), pp. 265–276. 1983.\nBernd Bruegge. Teaching an industry-oriented software engineering course. Software Engineering Education, Springer, pp. 63–87, 1992.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Requirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software requirements can be of three different types. There are functional requirements, non-functional requirements, and domain requirements. The operation of the software should be performed and the proper output should be expected for the user to use. Non-functional requirements deal with issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computer software, or just software, is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of programs, procedures, algorithms, as well as its documentation concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible.Software is also sometimes used in a more narrow sense, meaning application software only.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Software analysis is the process of analyzing the behavior of computer programs regarding a property such as performance, robustness, and security It can be performed without executing the program (static program analysis), during runtime (dynamic program analysis) or in a combination of both.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Computational science and engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, \"computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more\".\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"About Face: The Essentials of User Interface Design by Alan Cooper, about user interface design. ISBN 0-7645-2641-3\nThe Capability Maturity Model by Watts Humphrey. Written for the Software Engineering Institute, emphasizing management and process. (See Managing the Software Process ISBN 0-201-18095-2)\nThe Cathedral and the Bazaar by Eric Raymond about open source development.\nThe Decline and Fall of the American Programmer by Ed Yourdon predicts the end of software development in the U.S. ISBN 0-13-191958-X\nDesign Patterns by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. ISBN 0-201-63361-2\nExtreme Programming Explained by Kent Beck ISBN 0-321-27865-8\n\"Go To Statement Considered Harmful\" by Edsger Dijkstra.\n\"Internet, Innovation and Open Source:Actors in the Network\" — First Monday article by Ilkka Tuomi (2000) source\nThe Mythical Man-Month by Fred Brooks, about project management. ISBN 0-201-83595-9\nObject-oriented Analysis and Design by Grady Booch. ISBN 0-8053-5340-2\nPeopleware by Tom DeMarco and Tim Lister.  ISBN 0-932633-43-9\nThe pragmatic engineer versus the scientific designer by E. W. Dijkstra [1]\nPrinciples of Software Engineering Management by Tom Gilb about evolutionary processes. ISBN 0-201-19246-2\nThe Psychology of Computer Programming by Gerald Weinberg. Written as an independent consultant, partly about his years at IBM. ISBN 0-932633-42-0\nRefactoring: Improving the Design of Existing Code by Martin Fowler, Kent Beck, John Brant, William Opdyke, and Don Roberts. ISBN 0-201-48567-2\nThe Pragmatic Programmer: from journeyman to master by Andrew Hunt, and David Thomas. ISBN 0-201-61622-X\nSoftware Engineering Body of Knowledge (SWEBOK) ISO\/IEC TR 19759See also:\n\nImportant publications in software engineering in CS.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In one of the critical moments of the Apollo 11 mission, the Apollo Guidance Computer, together with the on-board flight software, averted an abort of the landing on the Moon. Three minutes before the lunar lander reached the Moon's surface, several computer alarms were triggered. According to software engineer Robert Wills, Buzz Aldrin entered the codes to request that the computer display altitude and other data on the computer’s screen. The system was designed to support seven simultaneous programs running, but Aldrin’s request was the eighth. This action was something he requested many times whilst working in the simulator. The result was a series of unexpected error codes during the live descent. The on-board flight software captured these alarms with the \"never supposed to happen displays\" interrupting the astronauts with priority alarm displays.\nHamilton had prepared for just this situation years before:\n\nThere was one other failsafe that Hamilton likes to remember. Her \"priority display\" innovation had created a knock-on risk that astronaut and computer would slip out of synch just when it mattered most. As the alarms went off and priority displays replaced normal ones, the actual switchover to new programmes behind the screens was happening \"a step slower\" than it would today.\nHamilton had thought long and hard about this. It meant that if Aldrin, say, hit a button on the priority display too quickly, he might still get a \"normal\" response. Her solution: when you see a priority display, first count to\nfive.\n\nBy some accounts, the astronauts had inadvertently left the rendezvous radar switch on, causing these alarms to be triggered (the claim that the radar was left on inadvertently by the astronauts is disputed by Robert Wills with the National Museum of Computing). The computer was overloaded with interrupts caused by incorrectly phased power supplied to the lander's rendezvous radar. The program alarms indicated \"executive overflows\", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them. The asynchronous executive designed by J. Halcombe Laning was used by Hamilton's team to develop asynchronous flight software:\n\nBecause of the flight software's system-software's error detection and recovery techniques that included its system-wide \"kill and recompute\" from a \"safe place\" restart approach to its snapshot and rollback techniques, the Display Interface Routines (AKA the priority displays) together with its man-in-the-loop capabilities were able to be created in order to have the capability to interrupt the astronauts' normal mission displays with priority displays of critical alarms in case of an emergency. This depended on our assigning a unique priority to every process in the software in order to ensure that all of its events would take place in the correct order and at the right time relative to everything else that was going on.\nHamilton's priority alarm displays interrupted the astronauts' normal displays to warn them that there was an emergency \"giving the astronauts a go\/no go decision (to land or not to land)\". Jack Garman, a NASA computer engineer in mission control, recognized the meaning of the errors that were presented to the astronauts by the priority displays and shouted, \"Go, go!\" and they continued. Paul Curto, senior technologist who nominated Hamilton for a NASA Space Act Award, called Hamilton's work \"the foundation for ultra-reliable software design\".Hamilton later wrote of the incident:\n\nThe computer (or rather the software in it) was smart enough to recognize that it was being asked to perform more tasks than it should be performing. It then sent out an alarm, which meant to the astronaut, 'I'm overloaded with more tasks than I should be doing at this time and I'm going to keep only the more important tasks'; i.e., the ones needed for landing ... Actually, the computer was programmed to do more than recognize error conditions. A complete set of recovery programs was incorporated into the software. The software's action, in this case, was to eliminate lower priority tasks and re-establish the more important ones ... If the computer hadn't recognized this problem and taken recovery action, I doubt if Apollo 11 would have been the successful moon landing it was.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"IDEF14, or integrated definition for network design method, is a method that targets the modeling and design of computer and communication networks. It can be used to model existing (\"as is\") or envisioned (\"to be\") networks. It helps the network designer to investigate potential network designs and to document design rationale. The fundamental goals of the IDEF14 research project developed from a perceived need for good network designs that can be implemented quickly and accurately.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"In  Pakistan and Nepal, Bachelor of Engineering in Software Engineering (BE Software) is an 8-semester course of study. This degree is provided by University of Engineering and Technology, Taxila, \nVirtual University of Pakistan,Superior university and many others and  Pokhara University Nepal.\nIn Bangladesh, this degree is named Bachelor of Science in Software Engineering (BS SE) which is also an 8-semester course of study.  University of Dhaka is the pioneer of Software Engineering education in Bangladesh offering Bachelor of Science in Software Engineering (BSSE) degree since 2009 with 6 months industry internship program.The Bachelor of Software Engineering degree is awarded to those who successfully complete an eight-semester program.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Employers generally seek applicants with strong programming, systems analysis and business skills.\n\"A large difference exists between the software engineering skills taught at a typical\nuniversity or college and the skills that are desired of a software engineer by a typical\nsoftware development organization. At the heart of this difference seems to be the way\nsoftware engineering is typically introduced to students: general theory is presented in a\nseries of lectures and put into (limited) practice in an associated class project.\"\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Time line of computing 2400 BC - 1949 - 1950-1979 - 1980-1989 - 1990-1999 - 2000-2009\nHistory of computing hardware up to third generation (1960s)\nHistory of computing hardware from 1960s to current\nHistory of computer hardware in Eastern Bloc countries\nHistory of personal computers\nHistory of laptops\nHistory of software engineering\nHistory of compiler writing\nHistory of the Internet\nHistory of the World Wide Web\nHistory of video games\nHistory of the graphical user interface\nTimeline of computing\nTimeline of operating systems\nTimeline of programming languages\nTimeline of artificial intelligence\nTimeline of cryptography\nTimeline of algorithms\nTimeline of quantum computing\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"The BCS is the only professional body in the United Kingdom with the ability to grant chartered status to IT professionals under its Royal Charter, granted to them by the Privy Council. Thus having the ability to grant Chartered (Professional) status to both its Fellows and Professional members. Known as Chartered IT Professional, they are entitled to use the suffix CITP. The BCS keeps a register of current Chartered Members and Fellows.Other Professional membership bodies apply to the BCS for a licence that enables them to award CITP to their eligible members.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Usability answers the question \"can someone use this interface?\". Jakob Nielsen describes usability as the quality attribute that describes how usable the interface is. Shneiderman proposes principles for designing more usable interfaces called \"Eight Golden Rules of Interface Design\"—which are well-known heuristics for creating usable systems.\n\n###\n\n","completion":" Software Engineering"}
{"prompt":"Hamilton has a sister Kathryn.She met her first husband, James Cox Hamilton, in the mid-1950s while attending college. They were married on June 15, 1958, the summer after she graduated from Earlham. She briefly taught high school mathematics and French at a public school in Boston, Indiana. The couple then moved to Boston, Massachusetts, where they had a daughter, Lauren, born on November 10, 1959. They divorced in 1967 and Margaret married Dan Lickly two years later.\n\n###\n\n","completion":" Software Engineering"}
