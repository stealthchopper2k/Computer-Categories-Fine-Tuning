{"prompt":"Cyber warfare utilizes techniques of defending and attacking information and computer networks that inhabit cyberspace, often through a prolonged cyber campaign or series of related campaigns. It denies an opponent's ability to do the same while employing technological instruments of war to attack an opponent's critical computer systems. Cyberterrorism, on the other hand, is \"the use of computer network tools to shut down critical national infrastructures (such as energy, transportation, government operations) or to coerce or intimidate a government or civilian population\". That means the result of both cyber warfare and cyberterrorism is the same, to damage critical infrastructures and computer systems linked together within the confines of cyberspace.\nThe financial crime expert Veit Buetterlin explained that organizations, including state actors, which cannot finance themselves through trade because of imposed sanctions, conduct cyber attacks on banks to generate funds.","completion":"Computer Network"}
{"prompt":"Category 5 cable is used in structured cabling for computer networks such as Ethernet over twisted pair. The cable standard prescribes performance parameters for frequencies up to 100 MHz and is suitable for 10BASE-T, 100BASE-TX (Fast Ethernet), 1000BASE-T (Gigabit Ethernet), and 2.5GBASE-T. 10BASE-T and 100BASE-TX Ethernet connections require two wire pairs. 1000BASE-T and faster Ethernet connections require four wire pairs. Through the use of power over Ethernet (PoE), power can be carried over the cable in addition to Ethernet data. \nCat 5 is also used to carry other signals such as telephony and video. In some cases, multiple signals can be carried on a single cable; Cat 5 can carry two conventional telephone lines as well as 100BASE-TX in a single cable. The USOC\/RJ-61 wiring standard may be used in multi-line telephone connections. Various schemes exist for transporting both analog and digital video over the cable. HDBaseT (10.2 Gbit\/s) is one such scheme.","completion":"Computer Network"}
{"prompt":"RFC 5322 specifies the syntax of the email header. Each email message has a header (the \"header section\" of the message, according to the specification), comprising a number of fields (\"header fields\"). Each field has a name (\"field name\" or \"header field name\"), followed by the separator character \":\", and a value (\"field body\" or \"header field body\").\nEach field name begins in the first character of a new line in the header section, and begins with a non-whitespace printable character. It ends with the separator character \":\". The separator is followed by the field value (the \"field body\"). The value can continue onto subsequent lines if those lines have space or tab as their first character. Field names and, without SMTPUTF8, field bodies are restricted to 7-bit ASCII characters. Some non-ASCII values may be represented using MIME encoded words.","completion":"Computer Network"}
{"prompt":"Traditionally, data-centric equipment such as computers and media players have been the primary tenants of a home network. However, due to the lowering cost of computing and the ubiquity of smartphone usage, many traditionally non-networked home equipment categories now include new variants capable of control or remote monitoring through an app on a smartphone. Newer startups and established home equipment manufacturers alike have begun to offer these products as part of a \"Smart\" or \"Intelligent\" or \"Connected Home\" portfolio. The control and\/or monitoring interfaces for these products can be accessed through proprietary smartphone applications specific to that product line.","completion":"Computer Network"}
{"prompt":"Energy management is a new and upcoming topic in particular at the home. Older systems tended to be cable however all new systems use one of a variety of wireless solutions.  This enables them to be effectively retrofitted into existing homes with the minimum of disruption.\nIf a cabled system is selected cabling needs to be deployed to the major appliances in the home.  The cabling is installed as part of the data cabling as per detailed in this article in the section titled \"Data Network Cabling\". In addition to a cable being installed to every major appliance you also need to install a data cable near the electricity meter.\nThe major appliances being considered at this stage are:\n\nElectric hot water system\nAir Conditioning\nPool pump\nFridge \/ freezer\nElectric vehicle charger\nBattery energy storage systems (BESS)Should a wireless system be selected the need for such disruption is removed. Smart plugs or switches can be used to connected the major appliances to the electricity supply and the home energy management system will wirelessly control them.","completion":"Computer Network"}
{"prompt":"In modern businesses, the design of file servers is complicated by competing demands for storage space, access speed, recoverability, ease of administration, security, and budget.  This is further complicated by a constantly changing environment, where new hardware and technology rapidly obsolesces old equipment, and yet must seamlessly come online in a fashion compatible with the older machinery.  To manage throughput, peak loads, and response time, vendors may utilize queuing theory to model how the combination of hardware and software will respond over various levels of demand.  Servers may also employ dynamic load balancing scheme to distribute requests across various pieces of hardware.\nThe primary piece of hardware equipment for servers over the last couple of decades has proven to be the hard disk drive.  Although other forms of storage are viable (such as magnetic tape and solid-state drives) disk drives have continued to offer the best fit for cost, performance, and capacity.","completion":"Computer Network"}
{"prompt":"Because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP IPv6, was developed in the mid-1990s, which provides vastly larger addressing capabilities and more efficient routing of Internet traffic. IPv6 uses 128 bits for the IP address and was standardized in 1998. IPv6 deployment has been ongoing since the mid-2000s and is currently in growing deployment around the world, since Internet address registries (RIRs) began to urge all resource managers to plan rapid adoption and conversion.IPv6 is not directly interoperable by design with IPv4. In essence, it establishes a parallel version of the Internet not directly accessible with IPv4 software. Thus, translation facilities must exist for internetworking or nodes must have duplicate networking software for both networks. Essentially all modern computer operating systems support both versions of the Internet Protocol. Network infrastructure, however, has been lagging in this development. Aside from the complex array of physical connections that make up its infrastructure, the Internet is facilitated by bi- or multi-lateral commercial contracts, e.g., peering agreements, and by technical specifications or protocols that describe the exchange of data over the network. Indeed, the Internet is defined by its interconnections and routing policies.","completion":"Computer Network"}
{"prompt":"Telesoft Technologies announced the dual 100G PCIe accelerator card, part of the MPAC-IP series. Telesoft also announced the STR 400G (Segmented Traffic Router) and the 100G MCE (Media Converter and Extension).","completion":"Computer Network"}
{"prompt":"Within the IEEE 802.11 Working Group, the following IEEE Standards Association Standard and Amendments exist:\n\nIEEE 802.11-1997: The WLAN standard was originally 1 Mbit\/s and 2 Mbit\/s, 2.4 GHz RF and infrared (IR) standard (1997), all the others listed below are Amendments to this standard, except for Recommended Practices 802.11F and 802.11T.\nIEEE 802.11a: 54 Mbit\/s, 5 GHz standard (1999, shipping products in 2001)\nIEEE 802.11b: 5.5 Mbit\/s and 11 Mbit\/s, 2.4 GHz standard (1999)\nIEEE 802.11c: Bridge operation procedures; included in the IEEE 802.1D standard (2001)\nIEEE 802.11d: International (country-to-country) roaming extensions (2001)\nIEEE 802.11e: Enhancements: QoS, including packet bursting (2005)\nIEEE 802.11F: Inter-Access Point Protocol (2003) Withdrawn February 2006\nIEEE 802.11g: 54 Mbit\/s, 2.4 GHz standard (backwards compatible with b) (2003)\nIEEE 802.11h: Spectrum Managed 802.11a (5 GHz) for European compatibility (2004)\nIEEE 802.11i: Enhanced security (2004)\nIEEE 802.11j: Extensions for Japan (4.9-5.0 GHz) (2004)\nIEEE 802.11-2007: A new release of the standard that includes amendments a, b, d, e, g, h, i, and j. (July 2007)\nIEEE 802.11k: Radio resource measurement enhancements (2008)\nIEEE 802.11n: Higher Throughput WLAN at 2.4 and 5 GHz; 20 and 40 MHz channels; introduces MIMO to Wi-Fi (September 2009)\nIEEE 802.11p: WAVE\u2014Wireless Access for the Vehicular Environment (such as ambulances and passenger cars) (July 2010)\nIEEE 802.11r: Fast BSS transition (FT) (2008)\nIEEE 802.11s: Mesh Networking, Extended Service Set (ESS) (July 2011)\nIEEE 802.11T: Wireless Performance Prediction (WPP)\u2014test methods and metrics Recommendation cancelled\nIEEE 802.11u: Improvements related to HotSpots and 3rd-party authorization of clients, e.g., cellular network offload (February 2011)\nIEEE 802.11v: Wireless network management (February 2011)\nIEEE 802.11w: Protected Management Frames (September 2009)\nIEEE 802.11y: 3650\u20133700 MHz Operation in the U.S. (2008)\nIEEE 802.11z: Extensions to Direct Link Setup (DLS) (September 2010)\nIEEE 802.11-2012: A new release of the standard that includes amendments k, n, p, r, s, u, v, w, y, and z (March 2012)\nIEEE 802.11aa: Robust streaming of Audio Video Transport Streams (June 2012) - see Stream Reservation Protocol\nIEEE 802.11ac: Very High Throughput WLAN at 5 GHz; wider channels (80 and 160 MHz); Multi-user MIMO (down-link only) (December 2013)\nIEEE 802.11ad: Very High Throughput 60 GHz (December 2012) \u2014 see also WiGig\nIEEE 802.11ae: Prioritization of Management Frames (March 2012)\nIEEE 802.11af: TV Whitespace (February 2014)\nIEEE 802.11-2016: A new release of the standard that includes amendments aa, ac, ad, ae, and af (December 2016)\nIEEE 802.11ah: Sub-1 GHz license exempt operation (e.g., sensor network, smart metering) (December 2016)\nIEEE 802.11ai: Fast Initial Link Setup (December 2016)\nIEEE 802.11aj: China Millimeter Wave (February 2018)\nIEEE 802.11ak: Transit Links within Bridged Networks (June 2018)\nIEEE 802.11aq: Pre-association Discovery (July 2018)\nIEEE 802.11-2020: A new release of the standard that includes amendments ah, ai, aj, ak, and aq (December 2020)\nIEEE 802.11ax: High Efficiency WLAN at 2.4, 5 and 6 GHz; introduces OFDMA to Wi-Fi (February 2021)\nIEEE 802.11ay: Enhancements for Ultra High Throughput in and around the 60 GHz Band (March 2021)\nIEEE 802.11az: Next Generation Positioning (March 2023)\nIEEE 802.11ba: Wake Up Radio (March 2021)\nIEEE 802.11bd: Enhancements for Next Generation V2X (see also IEEE 802.11p) (March 2023)","completion":"Computer Network"}
{"prompt":"The header features destination and source MAC addresses (each six octets in length), the EtherType field and, optionally, an IEEE 802.1Q tag or IEEE 802.1ad tag.\nThe EtherType field is two octets long and it can be used for two different purposes. Values of 1500 and below mean that it is used to indicate the size of the payload in octets, while values of 1536 and above indicate that it is used as an EtherType, to indicate which protocol is encapsulated in the payload of the frame. When used as EtherType, the length of the frame is determined by the location of the interpacket gap and valid frame check sequence (FCS).\nThe IEEE 802.1Q tag or IEEE 802.1ad tag, if present, is a four-octet field that indicates virtual LAN (VLAN) membership and IEEE 802.1p priority. The first two octets of the tag are called the Tag Protocol IDentifier (TPID) and double as the EtherType field indicating that the frame is either 802.1Q or 802.1ad tagged. 802.1Q uses a TPID of 0x8100. 802.1ad uses a TPID of 0x88a8.","completion":"Computer Network"}
{"prompt":"From 1947, he worked at the National Physical Laboratory (NPL) where Alan Turing was designing the Automatic Computing Engine (ACE) computer. It is said that Davies spotted mistakes in Turing's seminal 1936 paper On Computable Numbers, much to Turing's annoyance. These were perhaps some of the first \"programming\" bugs in existence, even if they were for a theoretical computer, the universal Turing machine. The ACE project was overambitious and floundered, leading to Turing's departure. Davies took over the project and concentrated on delivering the less ambitious Pilot ACE computer, which first worked in May 1950. A commercial spin-off, DEUCE was manufactured by English Electric Computers and became one of the best-selling machines of the 1950s.Davies also worked on applications of traffic simulation and machine translation. In the early 1960s, he worked on government technology initiatives designed to stimulate the British computer industry.","completion":"Computer Network"}
{"prompt":"File sharing is an example of transferring large amounts of data across the Internet. A computer file can be emailed to customers, colleagues and friends as an attachment. It can be uploaded to a website or File Transfer Protocol (FTP) server for easy download by others. It can be put into a \"shared location\" or onto a file server for instant use by colleagues. The load of bulk downloads to many users can be eased by the use of \"mirror\" servers or peer-to-peer networks. In any of these cases, access to the file may be controlled by user authentication, the transit of the file over the Internet may be obscured by encryption, and money may change hands for access to the file. The price can be paid by the remote charging of funds from, for example, a credit card whose details are also passed\u2014usually fully encrypted\u2014across the Internet. The origin and authenticity of the file received may be checked by digital signatures or by MD5 or other message digests. These simple features of the Internet, over a worldwide basis, are changing the production, sale, and distribution of anything that can be reduced to a computer file for transmission. This includes all manner of print publications, software products, news, music, film, video, photography, graphics and the other arts. This in turn has caused seismic shifts in each of the existing industries that previously controlled the production and distribution of these products.\nStreaming media is the real-time delivery of digital media for immediate consumption or enjoyment by end users. Many radio and television broadcasters provide Internet feeds of their live audio and video productions. They may also allow time-shift viewing or listening such as Preview, Classic Clips and Listen Again features. These providers have been joined by a range of pure Internet \"broadcasters\" who never had on-air licenses. This means that an Internet-connected device, such as a computer or something more specific, can be used to access online media in much the same way as was previously possible only with a television or radio receiver. The range of available types of content is much wider, from specialized technical webcasts to on-demand popular multimedia services. Podcasting is a variation on this theme, where\u2014usually audio\u2014material is downloaded and played back on a computer or shifted to a portable media player to be listened to on the move. These techniques using simple equipment allow anybody, with little censorship or licensing control, to broadcast audio-visual material worldwide.\nDigital media streaming increases the demand for network bandwidth. For example, standard image quality needs 1 Mbit\/s link speed for SD 480p, HD 720p quality requires 2.5 Mbit\/s, and the top-of-the-line HDX quality needs 4.5 Mbit\/s for 1080p.Webcams are a low-cost extension of this phenomenon. While some webcams can give full-frame-rate video, the picture either is usually small or updates slowly. Internet users can watch animals around an African waterhole, ships in the Panama Canal, traffic at a local roundabout or monitor their own premises, live and in real time. Video chat rooms and video conferencing are also popular with many uses being found for personal webcams, with and without two-way sound. YouTube was founded on 15 February 2005 and is now the leading website for free streaming video with more than two billion users. It uses an HTML5 based web player by default to stream and show video files. Registered users may upload an unlimited amount of video and build their own personal profile. YouTube claims that its users watch hundreds of millions, and upload hundreds of thousands of videos daily.","completion":"Computer Network"}
{"prompt":"A range of malicious email types exist. These range from various types of email scams, including \"social engineering\" scams such as advance-fee scam \"Nigerian letters\", to phishing, email bombardment and email worms.","completion":"Computer Network"}
{"prompt":"Computer operating systems provide various diagnostic tools to examine network interfaces and address configuration. Microsoft Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems may use ifconfig, netstat, route, lanstat, fstat, and iproute2 utilities to accomplish the task.","completion":"Computer Network"}
{"prompt":"A certificate may be revoked before it expires, for example because the secrecy of the private key has been compromised. Newer versions of popular browsers such as Firefox, Opera, and Internet Explorer on Windows Vista implement the Online Certificate Status Protocol (OCSP) to verify that this is not the case. The browser sends the certificate's serial number to the certificate authority or its delegate via OCSP (Online Certificate Status Protocol) and the authority responds, telling the browser whether the certificate is still valid or not. The CA may also issue a  CRL to tell people that these certificates are revoked. CRLs are no longer required by the CA\/Browser forum, nevertheless, they are still widely used by the CAs. Most revocation statuses on the Internet disappear soon after the expiration of the certificates.","completion":"Computer Network"}
{"prompt":"Most modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network.\nPackets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\nWith packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link is not overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free.\nThe physical link technologies of packet networks typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message.","completion":"Computer Network"}
{"prompt":"An IPv4 address has a size of 32 bits, which limits the address space to 4294967296 (232) addresses. Of this number, some addresses are reserved for special purposes such as private networks (~18 million addresses) and multicast addressing (~270 million addresses).\nIPv4 addresses are usually represented in dot-decimal notation, consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., 192.0.2.1.  Each part represents a group of 8 bits (an octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.","completion":"Computer Network"}
{"prompt":"File hosting allowed for people to expand their computer's hard drives and \"host\" their files on a server. Most file hosting services offer free storage, as well as larger storage amount for a fee. These services have greatly expanded the internet for business and personal use.\nGoogle Drive, launched on April 24, 2012, has become the most popular file hosting service. Google Drive allows users to store, edit, and share files with themselves and other users. Not only does this application allow for file editing, hosting, and sharing. It also acts as Google's own free-to-access office programs, such as Google Docs, Google Slides, and Google Sheets. This application served as a useful tool for University professors and students, as well as those who are in need of Cloud storage.Dropbox, released in June 2007 is a similar file hosting service that allows users to keep all of their files in a folder on their computer, which is synced with Dropbox's servers. This differs from Google Drive as it is not web-browser based. Now, Dropbox works to keep workers and files in sync and efficient.Mega, having over 200 million users, is an encrypted storage and communication system that offers users free and paid storage, with an emphasis on privacy. Being three of the largest file hosting services, Google Drive, Dropbox, and Mega all represent the core ideas and values of these services.","completion":"Computer Network"}
{"prompt":"In traditional libraries, the ability to find works of interest is directly related to how well they were cataloged. While cataloging electronic works digitized from a library's existing holding may be as simple as copying or moving a record from the print to the electronic form, complex and born-digital works require substantially more effort. To handle the growing volume of electronic publications, new tools and technologies have to be designed to allow effective automated semantic classification and searching. While full-text search can be used for some items, there are many common catalog searches which cannot be performed using full text, including: \n\nfinding texts which are translations of other texts\ndifferentiating between editions\/volumes of a text\/periodical\ninconsistent descriptors (especially subject headings)\nmissing, deficient or poor quality taxonomy practices\nlinking texts published under pseudonyms to the real authors (Samuel Clemens and Mark Twain, for example)\ndifferentiating non-fiction from parody (The Onion from The New York Times)","completion":"Computer Network"}
{"prompt":"A general-purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I\/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.","completion":"Computer Network"}
{"prompt":"A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable Internet access or digital subscriber line (DSL) provider.","completion":"Computer Network"}
{"prompt":"There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:\n\n\"The Arpanet period is somewhat well documented because the corporation in charge \u2013 BBN \u2013 left a physical record. Moving into the NSFNET era, it became an extraordinarily decentralized process. The record exists in people's basements, in closets. ... So much of what happened was done verbally and on the basis of individual trust.\"\nNotable works on the subject were published by Katie Hafner and Matthew Lyon, Where Wizards Stay Up Late: The Origins Of The Internet (1996), Roy Rosenzweig, \"Wizards, Bureaucrats, Warriors, and Hackers: Writing the History of the Internet\" (1998), and Janet Abbate, Inventing the Internet (2000).\nMost scholarship and literature on the Internet lists ARPANET as the prior network that was iterated on and studied to create it, although other early computer networks and experiments existed alongside or before ARPANET.These histories of the Internet have since been characterized as \"teleologies\" or \"Whig history\"; that is, they take the present to be the end point toward which history has been unfolding based on a single cause:\n\nIn the case of Internet history, the epoch-making event is usually said to be the demonstration of the 4-node ARPANET network in 1969. From that single happening the global Internet developed.\nIn addition to these characteristics, historians have cited methodological problems arising in their work:\n\n\"Internet history\" ... tends to be too close to its sources. Many Internet pioneers are alive, active, and eager to shape the histories that describe their accomplishments. Many museums and historians are equally eager to interview the pioneers and to publicize their stories.","completion":"Computer Network"}
{"prompt":"A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, Cat5 cabling, etc.) are almost entirely owned by the campus tenant or owner (an enterprise, university, government, etc.).\nFor example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.","completion":"Computer Network"}
{"prompt":"The Consultative Committee for Space Data Systems (CCSDS) packet telemetry standard defines the protocol used for the transmission of spacecraft instrument data over the deep-space channel. Under this standard, an image or other data sent from a spacecraft instrument is transmitted using one or more packets.","completion":"Computer Network"}
{"prompt":"Cabling for free to air TV requires the following:\n\nAn antenna\nCoaxial cable\nTV outletsAntenna types vary depending on location; an urban area with nearby transmitters will require a smaller antenna than a rural site with distant stations. The antenna is often mounted outdoors on the roof or a tower. A coaxial or twinlead cable is run from the antenna to the location where the television is located. One common type of cable is designated RG-6 Tri-shield or quad-shield cable. The cable is terminated on a television outlets, typically an F connector mounted on a face plate.  If there are multiple outlets, an RF splitter is used to divide the signal among them; outlets on the splitter are connected to television outlets at each location (living room, rec room, bedrooms, den, for example). RF splitters come with different types; some include amplifiers for multiple outlets. \nWhilst most TV outlets use the F connector the Television or digital set top box usually come with a connector known as Belling Lee so the cable used to connect from the TV outlet to the television will need to have an F connector in one end and a Belling Lee connector at the other end.\nThe distribution of pay TV through the home uses the same type of cabling used for Free to Air TV with some variations. The variations are:\n\nThere is no antenna as there is either a satellite dish or a cable from the street.\nThe cabling must be RG-6 quad shield.\nYou may be required to use the cable and cabling connectors approved by your pay TV provider\nA Pay TV Set Top Box needs to be installed at each television where you want to have access to Pay TV services.In most cases the pay TV company will supply and install the satellite dish or cable from the street and the cabling to the TV set. In many cases Pay TV services also require a telephone point to access movies on demand.\nIPTV is television delivered to the home over the Internet.  Any device for viewing IPTV must have an internet connection. This may be a wired connection, or wireless.","completion":"Computer Network"}
{"prompt":"In December 2005, a Gartner report on bird flu that concluded \"A pandemic wouldn't affect IT systems directly\" was humorously criticized for neglecting to consider RFC 1149 and RFC 2549 in its analysis.Known risks to the protocol include:\n\nCarriers being attacked by birds of prey. RFC2549: \"Unintentional encapsulation in hawks has been known to occur, with decapsulation being messy and the packets mangled.\"\nCarriers being blown off course. RFC1149: \"While broadcasting is not specified, storms can cause data loss.\"\nThe absence of viable local carriers. RFC6214: \"In some locations, such as New Zealand, a significant proportion of carriers are only able to execute short hops, and only at times when the background level of photon emission is extremely low.\" This describes the flightless and nocturnal nature of kiwis.\nLoss of availability of species, such as the extinction of the passenger pigeon.\nDisease affecting the carriers. RFC6214: \"There is a known risk of infection by the so-called H5N1 virus.\"\nThe network topologies supported for multicast communication are limited by the homing abilities of carriers. RFC6214: \"... [carriers] prove to have no talent for multihoming, and in fact enter a routing loop whenever multihoming is attempted.\"","completion":"Computer Network"}
{"prompt":"The 2007 cyberattacks on Estonia were a series of cyberattacks that began on 27 April 2007 and targeted websites of Estonian organizations, including Estonian parliament, banks, ministries, newspapers, and broadcasters, amid the country's disagreement with Russia about the relocation of the Bronze Soldier of Tallinn, an elaborate Soviet-era grave marker, as well as war graves in Tallinn. The attacks triggered a number of military organizations around the world to reconsider the importance of network security to modern military doctrine. The direct result of the cyberattacks was the creation of the NATO Cooperative Cyber Defence Centre of Excellence in Tallinn.","completion":"Computer Network"}
{"prompt":"Historically, voice and data communications were based on methods of circuit switching, as exemplified in the traditional telephone network, wherein each telephone call is allocated a dedicated end-to-end electronic connection between the two communicating stations. The connection is established by switching systems that connected multiple intermediate call legs between these systems for the duration of the call.\nThe traditional model of the circuit-switched telecommunication network was challenged in the early 1960s by Paul Baran at the RAND Corporation, who had been researching systems that could sustain operation during partial destruction, such as by nuclear war. He developed the theoretical model of distributed adaptive message block switching. However, the telecommunication establishment rejected the development in favor of existing models. Donald Davies at the United Kingdom's National Physical Laboratory (NPL) independently arrived at a similar concept in 1965.The earliest ideas for a computer network intended to allow general communications among computer users were formulated by computer scientist J. C. R. Licklider of Bolt Beranek and Newman (BBN), in April 1963, in memoranda discussing the concept of the \"Intergalactic Computer Network\". Those ideas encompassed many of the features of the contemporary Internet. In October 1963, Licklider was appointed head of the Behavioral Sciences and Command and Control programs at the Defense Department's Advanced Research Projects Agency (ARPA). He convinced Ivan Sutherland and Bob Taylor that this network concept was very important and merited development, although Licklider left ARPA before any contracts were assigned for development.Sutherland and Taylor continued their interest in creating the network, in part, to allow ARPA-sponsored researchers at various corporate and academic locales to utilize computers provided by ARPA, and, in part, to quickly distribute new software and other computer science results. Taylor had three computer terminals in his office, each connected to separate computers, which ARPA was funding: one for the System Development Corporation (SDC) Q-32 in Santa Monica, one for Project Genie at the University of California, Berkeley, and another for Multics at the Massachusetts Institute of Technology. Taylor recalls the circumstance: \"For each of these three terminals, I had three different sets of user commands. So, if I was talking online with someone at S.D.C., and I wanted to talk to someone I knew at Berkeley, or M.I.T., about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them. I said, 'Oh Man!', it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go. That idea is the ARPANET\".Donald Davies' work caught the attention of ARPANET developers at Symposium on Operating Systems Principles in October 1967. He gave the first public presentation, having coined the term packet switching, in August 1968 and incorporated it into the NPL network in England. The NPL network and ARPANET were the first two networks in the world to use packet switching. Roberts said the ARPANET and other packet switching networks built in the 1970s were similar \"in nearly all respects\" to Davies' original 1965 design.","completion":"Computer Network"}
{"prompt":"Norman Abramson, a professor at the University of Hawaii, developed the world's first wireless computer communication network, ALOHAnet. The system became operational in 1971 and included seven computers deployed over four islands to communicate with the central computer on the Oahu island without using phone lines.\nWireless LAN hardware initially cost so much that it was only used as an alternative to cabled LAN in places where cabling was difficult or impossible. Early development included industry-specific solutions and proprietary protocols, but at the end of the 1990s these were replaced by technical standards, primarily the various versions of IEEE 802.11 (in products using the Wi-Fi brand name). \nBeginning in 1991, a European alternative known as HiperLAN\/1 was pursued by the European Telecommunications Standards Institute (ETSI) with a first version approved in 1996.  This was followed by a HiperLAN\/2 functional specification with ATM influences accomplished February 2000.  Neither European standard achieved the commercial success of 802.11, although much of the work on HiperLAN\/2 has survived in the physical specification (PHY) for IEEE 802.11a, which is nearly identical to the PHY of HiperLAN\/2.\nIn 2009 802.11n was added to 802.11. It operates in both the 2.4 GHz and 5 GHz bands at a maximum data transfer rate of 600 Mbit\/s. Most newer routers are dual-band and able to utilize both wireless bands. This allows data communications to avoid the crowded 2.4 GHz band, which is also shared with Bluetooth devices and microwave ovens. The 5 GHz band also has more channels than the 2.4 GHz band, permitting a greater number of devices to share the space. Not all WLAN channels are available in all regions.\nA HomeRF group formed in 1997 to promote a technology aimed at residential use, but it disbanded in January 2003.","completion":"Computer Network"}
{"prompt":"Home theater pre-wiring requires knowledge of the number of speakers to be installed. \n\nTwo front speakers; one on the left of the screen and one on the right of the screen,\nOne front speaker cable just above or below the screen which is the middle front\nTwo rear speakers; one on the left and one of the right in line with front left and right speaker locations\nThe sub-woofer which can be anywhere in the room acoustically but must be relatively close to the active equipment the amplifier or surround sound receiver.Speaker cable is figure eight multi-strand copper cable. Cabling for the sub-woofer is typically a single shielded cable terminated on an RCA connector. A 7.1 channel system also needs cable for speakers that are installed between the front and back speakers.\nThe simplest layout for a home theater system is a single piece of furniture containing all one's AV equipment, which simplifies wiring. If, on the other hand, a front projection unit is to be employed, more thought must be given to the layout of the system. Several different cabling systems are commonly used for this application, including HDMI, DVI, and VGA.","completion":"Computer Network"}
{"prompt":"A two-way distance record for communication was set by the Mercury laser altimeter instrument aboard the MESSENGER spacecraft, and was able to communicate across a distance of 24 million km (15 million miles), as the craft neared Earth on a fly-by in May, 2005. The previous record had been set with a one-way detection of laser light from Earth, by the Galileo probe, of 6 million km (3.7 million mi) in 1992. Quote from Laser Communication in Space Demonstrations (EDRS)\nNASA's OPALS announced a breakthrough in space-to-ground communication December 9, 2014, uploading 175 megabytes in 3.5 seconds. Their system is also able to re-acquire tracking after the signal was lost due to cloud cover.\nIn the early morning hours of Oct. 18, 2013, NASA's Lunar Laser Communication Demonstration (LLCD) made history, transmitting data from lunar orbit to Earth at a rate of 622 megabits per second (Mbit\/s). LLCD was flown aboard the Lunar Atmosphere and Dust Environment Explorer satellite (LADEE), whose primary science mission was to investigate the tenuous and exotic atmosphere that exists around the moon.\nIn January 2013, NASA used lasers to beam an image of the Mona Lisa to the Lunar Reconnaissance Orbiter roughly 390,000 km (240,000 mi) away. To compensate for atmospheric interference, an error correction code algorithm similar to that used in CDs was implemented.On Dec. 7, 2021, Laser Communications Relay Demonstration (LCRD), another NASA project aimed to relay data between spacecraft and ground stations, launched from the Cape Canaveral Space Force Station in Florida. LCRD is NASA's first two-way, end-to-end optical relay. One of LCRD's first operational users will be the Integrated LCRD Low-Earth Orbit User Modem and Amplifier Terminal (ILLUMA-T), a payload that will be hosted on the International Space Station. The terminal will receive high-resolution science data from experiments and instruments onboard the space station and then transfer this data to LCRD, which will then transmit it to a ground station. After the data arrives on Earth, it will be delivered to mission operation centers and mission scientists.\nOn April 28, 2023, NASA and its partners achieved another major milestone in the future of space communications \u2013 achieving 200 gigabit per second (Gbps) throughput on a space-to-ground optical link between a satellite in orbit and Earth, the highest data rate ever achieved by optical communications technology.","completion":"Computer Network"}
{"prompt":"The wireless signal strength of the standard residential wireless router may not be powerful enough to cover the entire house or may not be able to get through to all floors of multiple floor residences. In such situations, the installation of one or more wireless repeaters may be necessary.","completion":"Computer Network"}
{"prompt":"Originally Internet email was completely ASCII text-based. MIME now allows body content text and some header content text in international character sets, but other headers and email addresses using UTF-8, while standardized have yet to be widely adopted.","completion":"Computer Network"}
{"prompt":"The protocols of the link layer operate within the scope of the local network connection to which a host is attached. This regime is called the link in TCP\/IP parlance and is the lowest component layer of the suite. The link includes all hosts accessible without traversing a router. The size of the link is therefore determined by the networking hardware design. In principle, TCP\/IP is designed to be hardware independent and may be implemented on top of virtually any link-layer technology. This includes not only hardware implementations but also virtual link layers such as virtual private networks and networking tunnels.\nThe link layer is used to move packets between the internet layer interfaces of two different hosts on the same link. The processes of transmitting and receiving packets on the link can be controlled in the device driver for the network card, as well as in firmware or by specialized chipsets. These perform functions, such as framing, to prepare the internet layer packets for transmission, and finally transmit the frames to the physical layer and over a transmission medium. The TCP\/IP model includes specifications for translating the network addressing methods used in the Internet Protocol to link-layer addresses, such as media access control (MAC) addresses. All other aspects below that level, however, are implicitly assumed to exist and are not explicitly defined in the TCP\/IP model.\nThe link layer in the TCP\/IP model has corresponding functions in Layer 2 of the OSI model.","completion":"Computer Network"}
{"prompt":"IEEE 802.11ah, published in 2017, defines a WLAN system operating at sub-1 GHz license-exempt bands. Due to the favorable propagation characteristics of the low frequency spectra, 802.11ah can provide improved transmission range compared with the conventional 802.11 WLANs operating in the 2.4 GHz and 5 GHz bands. 802.11ah can be used for various purposes including large scale sensor networks, extended range hotspot, and outdoor Wi-Fi for cellular traffic offloading, whereas the available bandwidth is relatively narrow. The protocol intends consumption to be competitive with low power Bluetooth, at a much wider range.","completion":"Computer Network"}
{"prompt":"In the Internet Protocol Suite (TCP\/IP), OSI's data link layer functionality is contained within its lowest layer, the link layer. The TCP\/IP link layer has the operating scope of the link a host is connected to, and only concerns itself with hardware issues to the point of obtaining hardware (MAC) addresses for locating hosts on the link and transmitting data frames onto the link. The link-layer functionality was described in RFC 1122 and is defined differently than the data link layer of OSI, and encompasses all methods that affect the local link.\nThe TCP\/IP model is not a top-down comprehensive design reference for networks. It was formulated for the purpose of illustrating the logical groups and scopes of functions needed in the design of the suite of internetworking protocols of TCP\/IP, as needed for the operation of the Internet. In general, direct or strict comparisons of the OSI and TCP\/IP models should be avoided, because the layering in TCP\/IP is not a principal design criterion and in general, considered to be \"harmful\" (RFC 3439). In particular, TCP\/IP does not dictate a strict hierarchical sequence of encapsulation requirements, as is attributed to OSI protocols.","completion":"Computer Network"}
{"prompt":"Due to its prominence and immediacy as an effective means of mass communication, the Internet has also become more politicized as it has grown.  This has led in turn, to discourses and activities that would once have taken place in other ways, migrating to being mediated by internet.\nExamples include political activities such as public protest and canvassing of support and votes, but also:\n\nThe spreading of ideas and opinions;\nRecruitment of followers, and \"coming together\" of members of the public, for ideas, products, and causes;\nProviding and widely distributing and sharing information that might be deemed sensitive or relates to whistleblowing (and efforts by specific countries to prevent this by censorship);\nCriminal activity and terrorism (and resulting law enforcement use, together with its facilitation by mass surveillance);\nPolitically motivated fake news.","completion":"Computer Network"}
{"prompt":"The process of change that generally coincided with \"Web 2.0\" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices.  This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information \"on the move\" \u2013 and used socially, as opposed to items on a desk at home or just used for work.Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as \"m.website.com\") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term \"App\" emerged (short for \"Application program\" or \"Program\") as did the \"App store\".\nThis \"mobile revolution\" has allowed for people to have a nearly unlimited amount of information at their fingertips. With the ability to access the internet from cell phones came a change in the way we consume media. In fact, looking at media consumption statistics, over half of media consumption between those aged 18 and 34 were using a smartphone.","completion":"Computer Network"}
{"prompt":"In a partially connected network, certain nodes are connected to exactly one other node; but some nodes are connected to two or more other nodes with a point-to-point link. This makes it possible to make use of some of the redundancy of mesh topology that is physically fully connected, without the expense and complexity required for a connection between every node in the network.","completion":"Computer Network"}
{"prompt":"Internet voting systems have gained popularity and have been used for government and membership organization elections and referendums in Estonia, and Switzerland as well as municipal elections in Canada and party primary elections in the United States and France. Internet voting has also been widely used in sub-national participatory budgeting processes, including in Brazil, France, United States, Portugal and Spain.Security experts have found security problems in every attempt at online voting, including systems in Australia, Estonia, Switzerland, Russia, and the United States.It has been argued political parties that have more support from the less fortunate\u2014who are unfamiliar with the Internet\u2014may suffer in the elections due to e-voting, which tends to increase voting in the upper\/middle class. It is unsure as to whether narrowing the digital divide would promote equal voting opportunities for people across various social, economic, and ethnic backgrounds. In the long run, this is contingent not only on internet accessibility but also depends on people's level of familiarity with the Internet.The effects of internet voting on overall voter turnout are unclear. A 2017 study of online voting in two Swiss cantons found that it had no effect on turnout, and a 2009 study of Estonia's national election found similar results. To the contrary, however, the introduction of online voting in municipal elections in the Canadian province of Ontario resulted in an average increase in turnout of around 3.5 percentage points. Similarly, a further study of the Swiss case found that while online voting did not increase overall turnout, it did induce some occasional voters to participate who would have abstained were online voting not an option.A paper on \u201cremote electronic voting and turnout in the Estonian 2007 parliamentary elections\u201d showed that rather than eliminating inequalities, e-voting might have enhanced the digital divide between higher and lower socioeconomic classes. People who lived greater distances from polling areas voted at higher levels with this service now available. The 2007 Estonian elections yielded a higher voter turnout from those who lived in higher income regions and who received formal education. Still regarding the Estonian Internet voting system, it was proved to be more cost-efficient than the rest of the voting systems offered in 2017 local elections.\nElectronic voting is perceived to be favored moreover by a certain demographic, namely the younger generation such as Generation X and Y voters. However, in recent elections about a quarter of e-votes were cast by the older demographic, such as individuals over the age of 55. Including this, about 20% of e-votes came from voters between the ages of 45 and 54. This goes to show that e-voting is not supported exclusively by the younger generations, but finding some popularity amongst Gen X and Baby Boomers as well. In terms of electoral results as well, the expectation that online voting would favor younger candidates has not been borne out in the data, with mayors in Ontario, Canada who were elected in online elections actually being slightly older on average than those elected by pencil and paper.Online voting is widely used privately for shareholder votes, \nand other private organizations.\nThe election management companies do not promise accuracy or privacy.\nIn fact one company uses an individual's past votes for research, and to target ads.Corporations and organizations routinely use Internet voting to elect officers and board members and for other proxy elections. Internet voting systems have been used privately in many modern nations and publicly in the United States, the UK, Switzerland and Estonia. In Switzerland, where it is already an established part of local referendums, voters get their passwords to access the ballot through the postal service. Most voters in Estonia can cast their vote in local and parliamentary elections, if they want to, via the Internet, as most of those on the electoral roll have access to an e-voting system, the largest run by any European Union country. It has been made possible because most Estonians carry a national identity card equipped with a computer-readable microchip and it is these cards which they use to get access to the online ballot. All a voter needs is a computer, an electronic card reader, their ID card and its PIN, and they can vote from anywhere in the world. Estonian e-votes can only be cast during the days of advance voting. On election day itself people have to go to polling stations and fill in a paper ballot.","completion":"Computer Network"}
{"prompt":"The Internet allows greater flexibility in working hours and location, especially with the spread of unmetered high-speed connections. The Internet can be accessed almost anywhere by numerous means, including through mobile Internet devices. Mobile phones, datacards, handheld game consoles and cellular routers allow users to connect to the Internet wirelessly. Within the limitations imposed by small screens and other limited facilities of such pocket-sized devices, the services of the Internet, including email and the web, may be available. Service providers may restrict the services offered and mobile data charges may be significantly higher than other access methods.\nEducational material at all levels from pre-school to post-doctoral is available from websites. Examples range from CBeebies, through school and high-school revision guides and virtual universities, to access to top-end scholarly literature through the likes of Google Scholar. For distance education, help with homework and other assignments, self-guided learning, whiling away spare time or just looking up more detail on an interesting fact, it has never been easier for people to access educational information at any level from anywhere. The Internet in general and the World Wide Web in particular are important enablers of both formal and informal education. Further, the Internet allows researchers (especially those from the social and behavioral sciences) to conduct research remotely via virtual laboratories, with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results.The low cost and nearly instantaneous sharing of ideas, knowledge, and skills have made collaborative work dramatically easier, with the help of collaborative software. Not only can a group cheaply communicate and share ideas but the wide reach of the Internet allows such groups more easily to form. An example of this is the free software movement, which has produced, among other things, Linux, Mozilla Firefox, and OpenOffice.org (later forked into LibreOffice). Internet chat, whether using an IRC chat room, an instant messaging system, or a social networking service, allows colleagues to stay in touch in a very convenient way while working at their computers during the day. Messages can be exchanged even more quickly and conveniently than via email. These systems may allow files to be exchanged, drawings and images to be shared, or voice and video contact between team members.\nContent management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work. Business and project teams can share calendars as well as documents and other information. Such collaboration occurs in a wide variety of areas including scientific research, software development, conference planning, political activism and creative writing. Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread.\nThe Internet allows computer users to remotely access other computers and information stores easily from any access point. Access may be with computer security, i.e. authentication and encryption technologies, depending on the requirements. This is encouraging new ways of remote work, collaboration and information sharing in many industries. An accountant sitting at home can audit the books of a company based in another country, on a server situated in a third country that is remotely maintained by IT specialists in a fourth. These accounts could have been created by home-working bookkeepers, in other remote locations, based on information emailed to them from offices all over the world. Some of these things were possible before the widespread use of the Internet, but the cost of private leased lines would have made many of them infeasible in practice. An office worker away from their desk, perhaps on the other side of the world on a business trip or a holiday, can access their emails, access their data using cloud computing, or open a remote desktop session into their office PC using a secure virtual private network (VPN) connection on the Internet. This can give the worker complete access to all of their normal files and data, including email and other applications, while away from the office. It has been referred to among system administrators as the Virtual Private Nightmare, because it extends the secure perimeter of a corporate network into remote locations and its employees' homes.\nBy late 2010s Internet has been described as \"the main source of scientific information \"for the majority of the global North population\".:\u200a111","completion":"Computer Network"}
{"prompt":"IEEE uses the phrase regdomain to refer to a legal regulatory region. Different countries define different levels of allowable transmitter power, time that a channel can be occupied, and different available channels. Domain codes are specified for the United States, Canada, ETSI (Europe), Spain, France, Japan, and China.\nMost Wi-Fi certified devices default to regdomain 0, which means least common denominator settings, i.e., the device will not transmit at a power above the allowable power in any nation, nor will it use frequencies that are not permitted in any nation.The regdomain setting is often made difficult or impossible to change so that the end-users do not conflict with local regulatory agencies such as the United States' Federal Communications Commission.","completion":"Computer Network"}
{"prompt":"Typically, the best scenarios for using this technology are:\n\nLAN-to-LAN connections on campuses at Fast Ethernet or Gigabit Ethernet speeds\nLAN-to-LAN connections in a city, a metropolitan area network\nTo cross a public road or other barriers which the sender and receiver do not own\nSpeedy service delivery of high-bandwidth access to optical fiber networks\nConverged voice-data connection\nTemporary network installation (for events or other purposes)\nReestablish high-speed connection quickly (disaster recovery)\nAs an alternative or upgrade add-on to existing wireless technologies\nEspecially powerful in combination with auto aiming systems, to power moving cars or a laptop while moving. or to use auto-aiming nodes to create a network with other nodes.\nAs a safety add-on for important fiber connections (redundancy)\nFor communications between spacecraft, including elements of a satellite constellation\nFor inter- and intra-chip communicationThe light beam can be very narrow, which makes FSO hard to intercept, improving security. It is comparatively easy to encrypt any data traveling across the FSO connection for additional security. FSO provides vastly improved electromagnetic interference (EMI) behavior compared to using microwaves.","completion":"Computer Network"}
{"prompt":"Sometimes, when modeling a network with more than one source, a supersource is introduced to the graph. This consists of a vertex connected to each of the sources with edges of infinite capacity, so as to act as a global source. A similar construct for sinks is called a supersink.","completion":"Computer Network"}
{"prompt":"The Internet Message Access Protocol (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like smartphones are increasingly used to check email while traveling and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually, the mail is left in folders in the mail server.","completion":"Computer Network"}
{"prompt":"Instant messaging is a set of communication technologies used for text-based communication between two (private messaging) or more (chat room) participants over the Internet or other types of networks (see also LAN messenger). IM chat happens in real-time. Of importance is that online chat and instant messaging differ from other technologies such as email due to the perceived quasi-synchrony of the communications by the users. Some systems permit messages to be sent to users not then 'logged on' (offline messages), thus removing some differences between IM and email (often done by sending the message to the associated email account).IM allows effective and efficient communication, allowing immediate receipt of acknowledgment or reply. However IM is basically not necessarily supported by transaction control. In many cases, instant messaging includes added features which can make it even more popular. For example, users may see each other via webcams, or talk directly for free over the Internet using a microphone and headphones or loudspeakers. Many applications allow file transfers, although they are usually limited in the permissible file-size. It is usually possible to save a text conversation for later reference. Instant messages are often logged in a local message history, making it similar to the persistent nature of emails.\nMajor IM services are controlled by their corresponding companies. They usually follow the client\u2013server model when all clients have to first connect to the central server. This requires users to trust this server because messages can generally be accessed by the company. Companies can be compelled to reveal their user's communication. Companies can also suspend user accounts for any reason. \nNon-IM types of chat include multicast transmission, usually referred to as \"chat rooms\", where participants might be anonymous or might be previously known to each other (for example collaborators on a project that is using chat to facilitate communication). \nAn instant message service center (IMSC) is a network element in the mobile telephone network which delivers instant messages. When a user sends an IM message to another user, the phone sends the message to the IMSC.  The IMSC stores the message and delivers it to the destination user when they are available. The IMSC usually has a configurable time limit for how long it will store the message. Few companies who make many of the IMSCs in use in the GSM world are Miyowa, Followap and OZ. Other players include Acision, Colibria, Ericsson, Nokia, Comverse Technology, Now Wireless, Jinny Software, Miyowa, Feelingk and few others.\nThe term \"Instant Messenger\" is a service mark of Time Warner and may not be used in software not affiliated with AOL in the United States. For this reason, in April 2007, the instant messaging client formerly named Gaim (or gaim) announced that they would be renamed \"Pidgin\".","completion":"Computer Network"}
{"prompt":"A darknet is an overlay network, typically running on the Internet, that is only accessible through specialized software. It is an anonymizing network where connections are made only between trusted peers \u2014 sometimes called friends (F2F) \u2014 using non-standard protocols and ports.\nDarknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.","completion":"Computer Network"}
{"prompt":"The introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of CA root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client.","completion":"Computer Network"}
{"prompt":"A packet is a block of data with length that can vary between successive packets, ranging from 7\nto 65,542 bytes, including the packet header.\n\nPacketized data is transmitted via frames, which are fixed-length data blocks. The size of a frame, including frame header and control information, can range up to 2048 bytes.\nPacket sizes are fixed during the development phase.Because packet lengths are variable but frame lengths are fixed, packet boundaries usually do not coincide with frame boundaries.","completion":"Computer Network"}
{"prompt":"Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\", he conceptualized and invented the first mechanical computer in the early 19th century. \nAfter working on his difference engine he announced his invention in 1822, in a paper to the Royal Astronomical Society, titled \"Note on the application of machinery to the computation of astronomical and mathematical tables\", he also designed to aid in navigational calculations, in 1833 he realized that a much more general design, an analytical engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.The machine was about a century ahead of its time. All the parts for his machine had to be made by hand \u2013 this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.","completion":"Computer Network"}
{"prompt":"In the final stage of IPv4 address exhaustion, the last IPv4 address block was assigned in January 2011 at the level of the regional Internet registries. IPv4 uses 32-bit addresses which limits the address space to 232 addresses, i.e. 4294967296 addresses. IPv4 is in the process of replacement by IPv6, its successor, which uses 128-bit addresses, providing 2128 addresses, i.e. 340282366920938463463374607431768211456, a vastly increased address space. The shift to IPv6 is expected to take a long time to complete.","completion":"Computer Network"}
{"prompt":"Ethernet II framing (also known as DIX Ethernet, named after DEC, Intel and Xerox, the major participants in its design), defines the two-octet EtherType field in an Ethernet frame, preceded by destination and source MAC addresses, that identifies an upper layer protocol encapsulated by the frame data. Most notably, an EtherType value of 0x0800 indicates that the frame contains an IPv4 datagram, 0x0806 indicates an ARP datagram, and 0x86DD indicates an IPv6 datagram. See EtherType \u00a7 Values for more.\n\nAs this industry-developed standard went through a formal IEEE standardization process, the EtherType field was changed to a (data) length field in the new 802.3 standard. Since the recipient still needs to know how to interpret the frame, the standard required an IEEE 802.2 header to follow the length and specify the type. Many years later, the 802.3x-1997 standard, and later versions of the 802.3 standard, formally approved of both types of framing. Ethernet II framing is the most common in Ethernet local area networks, due to its simplicity and lower overhead.\nIn order to allow some frames using Ethernet II framing and some using the original version of 802.3 framing to be used on the same Ethernet segment, EtherType values must be greater than or equal to 1536 (0x0600). That value was chosen because the maximum length of the payload field of an Ethernet 802.3 frame is 1500 octets (0x05DC). Thus if the field's value is greater than or equal to 1536, the frame must be an Ethernet II frame, with that field being a type field. If it's less than or equal to 1500, it must be an IEEE 802.3 frame, with that field being a length field. Values between 1500 and 1536, exclusive, are undefined. This convention allows software to determine whether a frame is an Ethernet II frame or an IEEE 802.3 frame, allowing the coexistence of both standards on the same physical medium.","completion":"Computer Network"}
{"prompt":"As of April 2018, 33.2% of Alexa top 1,000,000 websites use HTTPS as default and 70% of page loads (measured by Firefox Telemetry) use HTTPS. As of December 2022, 58.4% of the Internet's 135,422 most popular websites have a secure implementation of HTTPS, However despite TLS 1.3\u2019s release in 2018, adoption has been slow, with many still remain on the older TLS 1.2 protocol.","completion":"Computer Network"}
{"prompt":"The technological advancements and practical applications achieved through the ARPANET were instrumental in shaping modern computer networking including the Internet. Development and implementation of the concepts of packet switching, decentralized communication, and the development of protocols like TCP\/IP laid the foundation for a global network that revolutionized communication, information sharing and collaborative research across the world.The ARPANET was related to many other research projects, which either influenced the ARPANET design, or which were ancillary projects or spun out of the ARPANET.\nSenator Al Gore authored the High Performance Computing and Communication Act of 1991, commonly referred to as \"The Gore Bill\", after hearing the 1988 concept for a National Research Network submitted to Congress by a group chaired by Leonard Kleinrock. The bill was passed on 9 December 1991 and led to the National Information Infrastructure (NII) which Gore called the information superhighway.\nThe ARPANET project was honored with two IEEE Milestones, both dedicated in 2009.","completion":"Computer Network"}
{"prompt":"The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.","completion":"Computer Network"}
{"prompt":"The vulnerability factor exploits how vulnerable an organization or government establishment is to cyberattacks. Organizations without maintenance systems might be running on old servers which are more vulnerable than updated systems. An organization can be vulnerable to a denial of service attack and a government establishment can be defaced on a web page. A computer network attack disrupts the integrity or authenticity of data, usually through malicious code that alters program logic that controls data, leading to errors in the output.","completion":"Computer Network"}
{"prompt":"An IP address serves two principal functions: it identifies the host, or more specifically its network interface, and it provides the location of the host in the network, and thus the capability of establishing a path to that host. Its role has been characterized as follows: \"A name indicates what we seek. An address indicates where it is. A route indicates how to get there.\" The header of each IP packet contains the IP address of the sending host and that of the destination host.","completion":"Computer Network"}
{"prompt":"He received a French engineering degree in 1970, a Master's and a PhD degree from the University of California at Los Angeles Computer Science Department.\nHis PhD work, undertaken under the supervision of Dr Leonard Kleinrock, a pioneer in the area of networking, was related to the design of large computer networks. They were among the first to introduce and evaluate cluster-based hierarchical routing. Results obtained then are still very relevant and have recently inspired considerable work in the area of ad hoc networking.\nHe returned to Tunisia in 1976 and was given the task to create and chair the first Computer Science School of the country (ENSI). From 1982 to 1993, he chaired the CNI, Centre National de l'Informatique, a Government Office in charge of IT policies and strategic development.\nHe promoted the field of networking in Tunisia, through the organization of several national conferences and workshops dealing with networks. As chairman of CNI, he represented Tunisia at the general assembly and executive board meetings of the Intergovernmental Bureau for Informatics (IBI) in Rome, in the '80s, as well as those of the UNESCO IT Intergovernmental Committee, with a focus on issues related to developing countries.\nFrom 1993 till 1999, he served as Dean of ENSI, Ecole Nationale des Sciences de l'Informatique, a School of Engineering dedicated to the training of computer engineers. Since 1999, he continue teaching and serve as director of the CRISTAL Research Laboratory of the ENSI. He is also an advisor in IT fields to the Tunisian minister of Higher Education, Scientific Research and Technology.","completion":"Computer Network"}
{"prompt":"The 802.11b standard has a maximum raw data rate of 11 Mbit\/s (Megabits per second) and uses the same media access method defined in the original standard. 802.11b products appeared on the market in early 2000, since 802.11b is a direct extension of the modulation technique defined in the original standard. The dramatic increase in throughput of 802.11b (compared to the original standard) along with simultaneous substantial price reductions led to the rapid acceptance of 802.11b as the definitive wireless LAN technology.\nDevices using 802.11b experience interference from other products operating in the 2.4 GHz band. Devices operating in the 2.4 GHz range include microwave ovens, Bluetooth devices, baby monitors, cordless telephones, and some amateur radio equipment. As unlicensed intentional radiators in this ISM band, they must not interfere with and must tolerate interference from primary or secondary allocations (users) of this band, such as amateur radio.","completion":"Computer Network"}
{"prompt":"These tools have capabilities in common with drawing tools and network monitoring tools. They are more specialized than general drawing tools and provide network engineers and IT systems administrators a higher level of automation and the ability to develop more detailed network topologies and diagrams. Typical capabilities include but not limited to:\n\nDisplaying port \/ interface information on connections between devices on the maps\nVisualizing VLANs \/ subnets\nVisualizing virtual servers and storage\nVisualizing flow of network traffic across devices and networks\nDisplaying WAN and LAN maps by location\nImporting network configuration files to generate topologies automatically","completion":"Computer Network"}
{"prompt":"As the use of computers has spread throughout society, there are an increasing number of careers involving computers.\n\nThe need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.","completion":"Computer Network"}
{"prompt":"The Resource Reservation Protocol (RSVP) is described in RFC 2205. All machines on the network capable of sending QoS data send a PATH message every 30 seconds, which spreads out through the networks. Those who want to listen to them send a corresponding RESV (short for \"Reserve\") message which then traces the path backwards to the sender. The RESV message contains the flow specs.\nThe routers between the sender and listener have to decide if they can support the reservation being requested, and, if they cannot, they send a reject message to let the listener know about it. Otherwise, once they accept the reservation they have to carry the traffic.\nThe routers then store the nature of the flow, and also police it. This is all done in soft state, so if nothing is heard for a certain length of time, then the reader will time out and the reservation will be cancelled. This solves the problem if either the sender or the receiver crash or are shut down incorrectly without first cancelling the reservation. The individual routers may, at their option, police the traffic to check that it conforms to the flow specs.","completion":"Computer Network"}
{"prompt":"A network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.\nBridges come in three basic types:\n\nLocal bridges: Directly connect LANs\nRemote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers.\nWireless bridges: Can be used to join LANs or connect remote devices to LANs.","completion":"Computer Network"}
{"prompt":"The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and \"mice\" input devices are all hardware.","completion":"Computer Network"}
{"prompt":"Core networks typically provided the following functionality:\n\nAggregation: The highest level of aggregation in a service provider network. The next level in the hierarchy under the core nodes is the distribution networks and then the edge networks. Customer-premises equipment (CPE) do not normally connect to the core networks of a large service provider.\nAuthentication: The function to decide whether the user requesting a service from the telecom network is authorized to do so within this network or not.\nCall Control\/Switching: call control or switching functionality decides the future course of call based on the call signalling processing. E.g. switching functionality may decide based on the \"called number\" that the call be routed towards a subscriber within this operator's network or with number portability more prevalent to another operator's network.\nCharging: This functionality of the collation and processing of charging data generated by various network nodes. Two common types of charging mechanisms found in present-day networks are prepaid charging and postpaid charging. See Automatic Message Accounting\nService Invocation: Core network performs the task of service invocation for its subscribers. Service invocation may happen based on some explicit action (e.g. call transfer) by user or implicitly (call waiting). It's important to note however that service \"execution\" may or may not be a core network functionality as third party network\/nodes may take part in actual service execution.\nGateways: Gateways shall be present in the core network to access other networks. Gateway functionality is dependent on the type of network it interfaces with.Physically, one or more of these logical functionalities may simultaneously exist in a given core network node.\nBesides above-mentioned functionalities, the following also formed part of a telecommunications core network:\n\nO&M: Operations & Maintenance centre or Operations Support Systems to configure and provision the core network nodes. Number of subscribers, peak hour call rate, nature of services, geographical preferences are some of the factors which impact the configuration. Network statistics collection (Performance Management), alarm monitoring (Fault Management) and logging of various network nodes actions (Event Management) also happens in the O&M centre. These stats, alarms and traces form important tools for a network operator to monitor the network health and performance and improvise on the same.\nSubscriber Database: Core network also hosts the subscribers database (e.g. HLR in GSM systems). Subscriber database is accessed by core network nodes for functions like authentication, profiling, service invocation etc.","completion":"Computer Network"}
{"prompt":"The datagrams are called frames. Current 802.11 standards specify frame types for use in the transmission of data as well as management and control of wireless links.\nFrames are divided into very specific and standardized sections. Each frame consists of a MAC header, payload, and frame check sequence (FCS). Some frames may not have a payload.\n\nThe first two bytes of the MAC header form a frame control field specifying the form and function of the frame. This frame control field is subdivided into the following sub-fields:\n\nProtocol Version: Two bits representing the protocol version. The currently used protocol version is zero. Other values are reserved for future use.\nType: Two bits identifying the type of WLAN frame. Control, Data, and Management are various frame types defined in IEEE 802.11.\nSubtype: Four bits providing additional discrimination between frames. Type and Subtype are used together to identify the exact frame.\nToDS and FromDS: Each is one bit in size. They indicate whether a data frame is headed for a distribution system or it is getting out of it. Control and management frames set these values to zero. All the data frames will have one of these bits set.\nToDS = 0 and FromDS = 0\nCommunication within a basic service set or an independent basic service set (IBSS) network.\nToDS = 0 and FromDS = 1\nA frame sent by a station and directed to an AP accessed via the distribution system.\nToDS = 1 and FromDS = 0\nA frame exiting the distribution system for a station.\nToDS = 1 and FromDS = 1\nOnly kind of frame frame that uses all four MAC addresses in a DATA frame.\nAddress 1: final station address.\nAddress 2: access point address exiting from the distribution system.\nAddress 3: access point entrance to the distribution system (AP to which the source station is connected).\nAddress 4: address of the source station.\nMore Fragments: The More Fragments bit is set when a packet is divided into multiple frames for transmission. Every frame except the last frame of a packet will have this bit set.\nRetry: Sometimes frames require retransmission, and for this, there is a Retry bit that is set to one when a frame is resent. This aids in the elimination of duplicate frames.\nPower Management: This bit indicates the power management state of the sender after the completion of a frame exchange. Access points are required to manage the connection and will never set the power-saver bit.\nMore Data: The More Data bit is used to buffer frames received in a distributed system. The access point uses this bit to facilitate stations in power-saver mode. It indicates that at least one frame is available and addresses all stations connected.\nProtected Frame: The Protected Frame bit is set to the value of one if the frame body is encrypted by a protection mechanism such as Wired Equivalent Privacy (WEP), Wi-Fi Protected Access (WPA), or Wi-Fi Protected Access II (WPA2).\nOrder: This bit is set only when the \"strict ordering\" delivery method is employed. Frames and fragments are not always sent in order as it causes a transmission performance penalty.The next two bytes are reserved for the Duration ID field, indicating how long the field's transmission will take so other devices know when the channel will be available again. This field can take one of three forms: Duration, Contention-Free Period (CFP), and Association ID (AID).\nAn 802.11 frame can have up to four address fields. Each field can carry a MAC address. Address 1 is the receiver, Address 2 is the transmitter, Address 3 is used for filtering purposes by the receiver.  Address 4 is only present in data frames transmitted between access points in an Extended Service Set or between intermediate nodes in a mesh network.\nThe remaining fields of the header are:\n\nThe Sequence Control field is a two-byte section used to identify message order and eliminate duplicate frames. The first 4 bits are used for the fragmentation number, and the last 12 bits are the sequence number.\nAn optional two-byte Quality of Service control field, present in QoS Data frames; it was added with 802.11e.The payload or frame body field is variable in size, from 0 to 2304 bytes plus any overhead from security encapsulation, and contains information from higher layers.\nThe Frame Check Sequence (FCS) is the last four bytes in the standard 802.11 frame. Often referred to as the Cyclic Redundancy Check (CRC), it allows for integrity checks of retrieved frames. As frames are about to be sent, the FCS is calculated and appended. When a station receives a frame, it can calculate the FCS of the frame and compare it to the one received. If they match, it is assumed that the frame was not distorted during transmission.","completion":"Computer Network"}
{"prompt":"Email has often been called the killer application of the Internet. It predates the Internet, and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.\nIn addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the sflovers mailing list).\nDuring the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.","completion":"Computer Network"}
{"prompt":"The technological convergence of the mass media is the result of a long adaptation process of their communicative resources to the evolutionary changes of each historical moment. Thus, the new media became (plurally) an extension of the traditional media in cyberspace, allowing to the public access information in a wide range of digital devices. In other words, it is a cultural virtualization of human reality as a result of the migration from physical to virtual space (mediated by the ICTs), ruled by codes, signs and particular social relationships. Forwards, arise instant ways of communication, interaction and possible quick access to information, in which we are no longer mere senders, but also producers, reproducers, co-workers and providers. New technologies also help to \"connect\" people from different cultures outside the virtual space, which was unthinkable fifty years ago. In this giant relationships web, we mutually absorb each other's beliefs, customs, values, laws and habits, cultural legacies perpetuated by a physical-virtual dynamics in constant metamorphosis (ibidem). In this sense, Professor Doctor Marcelo Mendon\u00e7a Teixeira created, in 2013, a new model of communication to the virtual universe, based in Claude Elwood Shannon (1948) article \"A Mathematical Theory of Communication\".","completion":"Computer Network"}
{"prompt":"The first gigabit laser-based communication was achieved by the European Space Agency and called the European Data Relay System (EDRS) on November 28, 2014. The system is operational and is being used on a daily basis.","completion":"Computer Network"}
{"prompt":"Many major IM services and applications offer the call feature for user-to-user calls, conference calls, and voice messages. The call functionality is useful for professionals who utilize the application for work purposes and as a hands-free method. Videotelephony using a webcam is also possible by some.","completion":"Computer Network"}
{"prompt":"Electronic voting machines can be made fully accessible for persons with disabilities. Punched card and optical scan machines are not fully accessible for the blind or visually impaired, and lever machines can be difficult for voters with limited mobility and strength. Electronic machines can use headphones, sip and puff, foot pedals, joy sticks and other adaptive technology to provide the necessary accessibility.\nOrganizations such as the Verified Voting Foundation have criticized the accessibility of electronic voting machines and advocate alternatives. Some disabled voters (including the visually impaired) could use a tactile ballot, a ballot system using physical markers to indicate where a mark should be made, to vote a secret paper ballot. These ballots can be designed identically to those used by other voters. However, other disabled voters (including voters with dexterity disabilities) could be unable to use these ballots.","completion":"Computer Network"}
{"prompt":"In 2018, the Wi-Fi Alliance began using a consumer-friendly generation numbering scheme for the publicly used 802.11 protocols. Wi-Fi generations 1\u20136 refer to the 802.11b, 802.11a, 802.11g, 802.11n, 802.11ac, and 802.11ax protocols, in that order.","completion":"Computer Network"}
{"prompt":"A network is a directed graph G = (V, E) with a non-negative capacity function c for each edge, and without multiple arcs (i.e. edges with the same source and target nodes). Without loss of generality, we may assume that if (u, v) \u2208 E, then (v, u) is also a member of E. Additionally, if (v, u) \u2209 E then we may add (v, u) to E and then set the c(v, u) = 0. \nIf two nodes in G are distinguished \u2013 one as the source s and the other as the sink t \u2013 then (G, c, s, t) is called a flow network.","completion":"Computer Network"}
{"prompt":"Email has been widely accepted by businesses, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed Internet). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.It has some key benefits to business and other organizations, including:\n\nFacilitating logistics\nMuch of the business world relies on communications between people who are not physically in the same building, area, or even country; setting up and attending an in-person meeting, telephone call, or conference call can be inconvenient, time-consuming, and costly. Email provides a method of exchanging information between two or more people with no set-up costs and that is generally far less expensive than a physical meeting or phone call.\nHelping with synchronization\nWith real time communication by meetings or phone calls, participants must work on the same schedule, and each participant must spend the same amount of time in the meeting or call. Email allows asynchrony: each participant may control their schedule independently. Batch processing of incoming emails can improve workflow compared to interrupting calls.\nReducing cost\nSending an email is much less expensive than sending postal mail, or long distance telephone calls, telex or telegrams.\nIncreasing speed\nMuch faster than most of the alternatives.\nCreating a \"written\" record\nUnlike a telephone or in-person conversation, email by its nature creates a detailed written record of the communication, the identity of the sender(s) and recipient(s) and the date and time the message was sent. In the event of a contract or legal dispute, saved emails can be used to prove that an individual was advised of certain issues, as each email has the date and time recorded on it.\nPossibility of auto-processing and improved distribution\nAs well pre-processing of customer's orders or addressing the person in charge can be realized by automated procedures.","completion":"Computer Network"}
{"prompt":"The theory, design principles, and first instantiation of the backbone network came from the telephone core network when traffic was purely voice. The core network was the central part of a telecommunications network that provided various services to customers who were connected by the access network.  One of the main functions was to route telephone calls across the PSTN.\nTypically the term referred to the high capacity communication facilities that connect primary nodes. A core network provided paths for the exchange of information between different sub-networks.\nIn the United States, local exchange core networks were linked by several competing interexchange networks; in the rest of the world, the core network has been extended to national boundaries.\nCore networks usually had a mesh topology that provided any-to-any connections among devices on the network. Many main service providers would have their own core\/backbone networks that are interconnected. Some large enterprises have their own core\/backbone network, which are typically connected to the public networks.\nBackbone networks create links that allow long-distance transmission, usually 10 to 100 miles, and in certain cases - up to 150 miles. This makes backbone network essential to providing long-haul wireless solutions to provide internet service, especially to remote areas.","completion":"Computer Network"}
{"prompt":"In the first six months of 2017, two billion data records were stolen or impacted by cyber attacks, and ransomware payments reached US$2 billion, double that in 2016. In 2020, with the increase of remote work as an effect of the COVID-19 global pandemic, cybersecurity statistics reveal a huge increase in hacked and breached data. The worldwide information security market is forecast to reach $170.4 billion in 2022.","completion":"Computer Network"}
{"prompt":"Apart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers, repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions.","completion":"Computer Network"}
{"prompt":"Telegraphy\nThe practice of transmitting messages between two different places through an electromagnetic medium dates back to the electrical telegraph in the late 19th century, which was the first fully digital communication system. Radiotelegraphy began to be used commercially in the early 20th century. Telex became an operational teleprinter service in the 1930s. Such systems were limited to point-to-point communication between two end devices.\nInformation theory\nFundamental theoretical work in telecommunications technology was developed by Harry Nyquist and Ralph Hartley in the 1920s. Information theory, as enunciated by Claude Shannon, in the 1948, provided a firm theoretical underpinning to understand the trade-offs between signal-to-noise ratio, bandwidth, and error-free transmission in the presence of noise.\nComputers and modems\nEarly fixed-program computers in the 1940s were operated manually by entering small programs via switches in order to load and run a series of programs. As transistor technology evolved in the 1950s, central processing units and user terminals came into use by 1955. The mainframe computer model was devised and  modems, such as the Bell 101, allowed digital data to be transmitted over regular unconditioned telephone lines at low speeds by the late 1950s. These technologies made it possible to exchange data between remote computers. However, a fixed-line link was still necessary; the point-to-point communication model did not allow for direct communication between any two arbitrary systems. In addition, the applications were specific and not general purpose. Examples included SAGE (1958) and SABRE (1960).\nTime-sharing\nChristopher Strachey, who became Oxford University's first Professor of Computation, filed a patent application for time-sharing in February 1959. In June that year, he gave a paper \"Time Sharing in Large Fast Computers\" at the UNESCO Information Processing Conference in Paris where he passed the concept on to J. C. R. Licklider. Licklider, a vice president at Bolt Beranek and Newman, Inc. (BBN), promoted the idea of time-sharing as an alternative to batch processing. John McCarthy, at MIT, wrote a memo in 1959 that broadened the concept of time sharing to encompass multiple interactive user sessions, which resulted in the Compatible Time-Sharing System (CTSS) implemented at MIT. Other multi-user mainframe systems developed, such as PLATO at the University of Illinois Chicago. In the early 1960, the Advanced Research Projects Agency (ARPA) of the United States Department of Defense funded further research into time-sharing at MIT through Project MAC.","completion":"Computer Network"}
{"prompt":"A direct-recording electronic (DRE) voting machine records votes by means of a ballot display provided with mechanical or electro-optical components that can be activated by the voter (typically buttons or a touchscreen); that processes data with computer software; and that records voting data and ballot images in memory components. After the election it produces a tabulation of the voting data stored in a removable memory component and as a printed copy. The system may also provide a means for transmitting individual ballots or vote totals to a central location for consolidating and reporting results from precincts at the central location. These systems use a precinct count method that tabulates ballots at the polling place. They typically tabulate ballots as they are cast and print the results after the close of polling.In 2002, in the United States, the Help America Vote Act mandated that one handicapped accessible voting system be provided per polling place, which most jurisdictions have chosen to satisfy with the use of DRE voting machines, some switching entirely over to DRE. In 2004, 28.9% of the registered voters in the United States used some type of direct recording electronic voting system, up from 7.7% in 1996.\nIn 2004, India adopted Electronic Voting Machines (EVM) for its elections to its parliament with 380 million voters casting their ballots using more than one million voting machines. The Indian EVMs are designed and developed by two government-owned defence equipment manufacturing units, Bharat Electronics Limited (BEL) and Electronics Corporation of India Limited (ECIL). Both systems are identical, and are developed to the specifications of Election Commission of India. The system is a set of two devices running on 7.5 volt batteries. One device, the voting Unit is used by the voter, and another device called the control unit is operated by the electoral officer. Both units are connected by a five-metre cable. The voting unit has a blue button for each candidate. The unit can hold 16 candidates, but up to four units can be chained, to accommodate 64 candidates. The control unit has three buttons on the surface \u2013 one button to release a single vote, one button to see the total number of votes cast till now, and one button to close the election process. The result button is hidden and sealed. It cannot be pressed unless the close button has already been pressed. A controversy was raised when the voting machine malfunctioned which was shown in Delhi assembly. On 9 April 2019, the Supreme Court ordered the ECI to increase voter-verified paper audit trail (VVPAT) slips vote count to five randomly selected EVMs per assembly constituency, which means ECI has to count VVPAT slips of 20,625 EVMs before it certifies the final election results.","completion":"Computer Network"}
{"prompt":"Novell's \"raw\" 802.3 frame format was based on early IEEE 802.3 work. Novell used this as a starting point to create the first implementation of its own IPX Network Protocol over Ethernet. They did not use any LLC header but started the IPX packet directly after the length field. This does not conform to the IEEE 802.3 standard, but since IPX always has FF as the first two octets (while in IEEE 802.2 LLC that pattern is theoretically possible but extremely unlikely), in practice this usually coexists on the wire with other Ethernet implementations, with the notable exception of some early forms of DECnet which got confused by this.\nNovell NetWare used this frame type by default until the mid-nineties, and since NetWare was then very widespread, while IP was not, at some point in time most of the world's Ethernet traffic ran over \"raw\" 802.3 carrying IPX. Since NetWare 4.10, NetWare defaults to IEEE 802.2 with LLC (NetWare Frame Type Ethernet_802.2) when using IPX.","completion":"Computer Network"}
{"prompt":"A file server may be dedicated or non-dedicated. A dedicated server is designed specifically for use as a file server, with workstations attached for reading and writing files and databases.\nFile servers may also be categorized by the method of access: Internet file servers are frequently accessed by File Transfer Protocol or by HTTP (but are different from web servers, that often provide dynamic web content in addition to static files).  Servers on a LAN are usually accessed by SMB\/CIFS protocol  (Windows and Unix-like) or NFS protocol (Unix-like systems).\nDatabase servers, that provide access to a shared database via a database device driver, are not regarded as file servers even when the database is stored in files, as they are not designed to provide those files to users and tend to have differing technical requirements.","completion":"Computer Network"}
{"prompt":"Once a bridge learns the addresses of its connected nodes, it forwards data link layer frames using a layer-2 forwarding method. There are four forwarding methods a bridge can use, of which the second through fourth methods were performance-increasing methods when used on switch products with the same input and output port bandwidths:\n\nStore and forward: the switch buffers and verifies each frame before forwarding it; a frame is received in its entirety before it is forwarded.\nCut through: the switch starts forwarding after the frame's destination address is received. There is no error checking with this method. When the outgoing port is busy at the time, the switch falls back to store-and-forward operation. Also, when the egress port is running at a faster data rate than the ingress port, store-and-forward is usually used.\nFragment free: a method that attempts to retain the benefits of both store and forward and cut through.  Fragment free checks the first 64 bytes of the frame, where addressing information is stored. According to Ethernet specifications, collisions should be detected during the first 64 bytes of the frame, so frame transmissions that are aborted because of a collision will not be forwarded. Error checking of the actual data in the packet is left for the end device.\nAdaptive switching: a method of automatically selecting between the other three modes.","completion":"Computer Network"}
{"prompt":"A public network DRE voting system is an election system that uses electronic ballots and transmits vote data from the polling place to another location over a public network. Vote data may be transmitted as individual ballots as they are cast, periodically as batches of ballots throughout the election day, or as one batch at the close of voting. \nPublic network DRE voting system can utilize either precinct count or central count method. The central count method tabulates ballots from multiple precincts at a central location.","completion":"Computer Network"}
{"prompt":"The rapid technical advances that would propel the Internet into its place as a social system which have completely transformed the way humans interact with each other, took place during a relatively short period of no more than five years, from around 2005 to 2010, coinciding with the point in time in which IoT devices surpassed the number of humans alive at some point in the late 2000s. They included:\n\nThe call to \"Web 2.0\" in 2004 (first suggested in 1999),\nAccelerating adoption and commoditization among households of, and familiarity with, the necessary hardware (such as computers).\nAccelerating storage technology and data access speeds \u2013 hard drives emerged, took over from far smaller, slower floppy discs, and grew from megabytes to gigabytes (and by around 2010, terabytes), RAM from hundreds of kilobytes to gigabytes as typical amounts on a system, and Ethernet, the enabling technology for TCP\/IP, moved from common speeds of kilobits to tens of megabits per second, to gigabits per second.\nHigh speed Internet and wider coverage of data connections, at lower prices, allowing larger traffic rates, more reliable simpler traffic, and traffic from more locations,\nThe public's accelerating perception of the potential of computers to create new means and approaches to communication, the emergence of social media and websites such as Twitter and Facebook to their later prominence, and global collaborations such as Wikipedia (which existed before but gained prominence as a result),\nThe mobile device revolution, particularly with smartphones and tablet computers becoming widespread, which began to provide easy access to the Internet to much of human society of all ages, in their daily lives, and allowed them to share, discuss, and continually update, inquire, and respond.\nNon-volatile RAM rapidly grew in size and reliability, and decreased in price, becoming a commodity capable of enabling high levels of computing activity on these small handheld devices as well as solid-state drives (SSD).\nAn emphasis on power efficient processor and device design, rather than purely high processing power; one of the beneficiaries of this was Arm, a British company which had focused since the 1980s on powerful but low cost simple microprocessors. ARM architecture family rapidly gained dominance in the market for mobile and embedded devices.The term \"Web 2.0\" describes websites that emphasize user-generated content (including user-to-user interaction), usability, and interoperability. It first appeared in a January 1999 article called \"Fragmented Future\" written by Darcy DiNucci, a consultant on electronic information design, where she wrote:\n\"The Web we know now, which loads into a browser window in essentially static screenfuls, is only an embryo of the Web to come. The first glimmerings of Web 2.0 are beginning to appear, and we are just starting to see how that embryo might develop. The Web will be understood not as screenfuls of text and graphics but as a transport mechanism, the ether through which interactivity happens. It will [...] appear on your computer screen, [...] on your TV set [...] your car dashboard [...] your cell phone [...] hand-held game machines [...] maybe even your microwave oven.\"The term resurfaced during 2002\u20132004, and gained prominence in late 2004 following presentations by Tim O'Reilly and Dale Dougherty at the first Web 2.0 Conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the \"Web as Platform\", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that \"customers are building your business for you\". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be \"harnessed\" to create value.\nWeb 2.0 does not refer to an update to any technical specification, but rather to cumulative changes in the way Web pages are made and used. Web 2.0 describes an approach, in which sites focus substantially upon allowing users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking services, blogs, wikis, folksonomies, video sharing sites, hosted services, Web applications, and mashups. Terry Flew, in his 3rd Edition of New Media described what he believed to characterize the differences between Web 1.0 and Web 2.0:\n\n\"[The] move from personal websites to blogs and blog site aggregation, from publishing to participation, from web content as the outcome of large up-front investment to an ongoing and interactive process, and from content management systems to links based on tagging (folksonomy)\".This era saw several household names gain prominence through their community-oriented operation \u2013 YouTube, Twitter, Facebook, Reddit and Wikipedia being some examples.","completion":"Computer Network"}
{"prompt":"The value of fully meshed networks is proportional to the exponent of the number of subscribers, assuming that communicating groups of any two endpoints, up to and including all the endpoints, is approximated by Reed's Law.","completion":"Computer Network"}
{"prompt":"This example is from the Ethernet protocol, where a sending host is able to know when a collision has occurred (that is, another host has tried to transmit), when it is sending a frame. If both hosts attempted to re-transmit as soon as a collision occurred, there would be yet another collision \u2014 and the pattern would continue forever. The hosts must choose a random value within an acceptable range to ensure that this situation doesn't happen. An exponential backoff algorithm is therefore used. The value 51.2 \u03bcs is used as an example here because it is the slot time for a 10 Mbit\/s Ethernet line. However, 51.2 \u03bcs could be replaced by any positive value, in practice.\n\nWhen a collision first occurs, send a \"jamming signal\" to prevent further data from being sent.\nResend a frame after either 0 seconds or 51.2 \u03bcs, chosen at random.\nIf that fails, resend the frame after either 0 s, 51.2 \u03bcs, 102.4 \u03bcs, or 153.6 \u03bcs.\nIf that still fails, resend the frame after k \u00b7 51.2 \u03bcs, where k is a random integer between 0 and 23 \u2212 1.\nFor further failures, after the cth failed attempt, resend the frame after k \u00b7 51.2 \u03bcs, where k is a random integer between 0 and 2c \u2212 1.","completion":"Computer Network"}
{"prompt":"IPv4 address exhaustion has forced most Internet service providers to grant only a single WAN-facing IP address for each residential account. Multiple devices within a residence or small office are provisioned with internet access by establishing a local area network (LAN) for the local devices with IP addresses reservied for private networks.  A network router is configured with the provider's IP address on the WAN interface, which is shared among all devices in the LAN by network address translation.","completion":"Computer Network"}
{"prompt":"The volume of Internet traffic is difficult to measure because no single point of measurement exists in the multi-tiered, non-hierarchical topology. Traffic data may be estimated from the aggregate volume through the peering points of the Tier 1 network providers, but traffic that stays local in large provider networks may not be accounted for.","completion":"Computer Network"}
{"prompt":"Computer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack.","completion":"Computer Network"}
{"prompt":"As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. Each constituent network chooses the technologies and protocols it deploys from the technical standards that are developed by the Internet Engineering Task Force (IETF). However, successful interoperation of many networks requires certain parameters that must be common throughout the network. For managing such parameters, the Internet Assigned Numbers Authority (IANA) oversees the allocation and assignment of various technical identifiers.  In addition, the Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for the two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.","completion":"Computer Network"}
{"prompt":"IP addresses are assigned to a host either dynamically as they join the network, or persistently by configuration of the host hardware or software. Persistent configuration is also known as using a static IP address. In contrast, when a computer's IP address is assigned each time it restarts, this is known as using a dynamic IP address.\nDynamic IP addresses are assigned by network using Dynamic Host Configuration Protocol (DHCP). DHCP is the most frequently used technology for assigning addresses. It avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows devices to share the limited address space on a network if only some of them are online at a particular time. Typically, dynamic IP configuration is enabled by default in modern desktop operating systems.\nThe address assigned with DHCP is associated with a lease and usually has an expiration period. If the lease is not renewed by the host before expiry, the address may be assigned to another device. Some DHCP implementations attempt to reassign the same IP address to a host, based on its MAC address, each time it joins the network. A network administrator may configure DHCP by allocating specific IP addresses based on MAC address.\nDHCP is not the only technology used to assign IP addresses dynamically. Bootstrap Protocol is a similar protocol and predecessor to DHCP. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol.\nComputers and equipment used for the network infrastructure, such as routers and mail servers, are typically configured with static addressing.\nIn the absence or failure of static or dynamic address configurations, an operating system may assign a link-local address to a host using stateless address autoconfiguration.","completion":"Computer Network"}
{"prompt":"10.3125 Gbaud with NRZ (\"PAM2\") and 64b66b on 10 lanes per direction\nOne of the earliest coding used, this widens the coding scheme used in single lane 10GE and quad lane 40G to use 10 lanes. Due to the low symbol rate, relatively long ranges can be achieved at the cost of using a lot of cabling.\nThis also allows breakout to 10\u00d710GE, provided that the hardware supports splitting the port.25.78125 Gbaud with NRZ (\"PAM2\") and 64b66b on 4 lanes per direction\nA sped-up variant of the above, this directly corresponds to 10GE\/40GE signalling at 2.5\u00d7 speed. The higher symbol rate makes links more susceptible to errors.\nIf the device and transceiver support dual-speed operation, it is possible to reconfigure an 100G port to downspeed to 40G or 4\u00d710G. There is no autonegotiation protocol for this, thus manual configuration is necessary.  Similarly, a port can be broken into 4\u00d725G if implemented in the hardware. This is applicable even for CWDM4, if a CWDM demultiplexer and CWDM 25G optics are used appropriately.25.78125 Gbaud with NRZ (\"PAM2\") and RS-FEC(528,514) on 4 lanes per direction\nTo address the higher susceptibility to errors at these symbol rates, an application of Reed\u2013Solomon error correction was defined in IEEE 802.3bj \/ Clause 91.  This replaces the 64b66b encoding with a 256b257b encoding followed by the RS-FEC application, which combines to the exact same overhead as 64b66b.  To the optical transceiver or cable, there is no distinction between this and 64b66b;  some interface types (e.g. CWDM4) are defined \"with or without FEC.\"26.5625 Gbaud with PAM4 and RS-FEC(544,514) on 2 lanes per direction\nThis achieves a further doubling in bandwidth per lane (used to halve the number of lanes) by employing pulse-amplitude modulation with 4 distinct analog levels, making each symbol carry 2 bits.  To keep up error margins, the FEC overhead is doubled from 2.7% to 5.8%, which explains the slight rise in symbol rate.53.125 Gbaud with PAM4 and RS-FEC(544,514) on 1 lane per direction\nFurther pushing silicon limits, this is a double rate variant of the previous, giving full 100GE operation over 1 medium lane.30.14475 Gbaud with DP-DQPSK and SD-FEC on 1 lane per direction\nMirroring OTN4 developments, DP-DQPSK (dual polarization differential quadrature phase shift keying) employs polarization to carry one axis of the DP-QPSK constellation.  Additionally, new soft decision FEC algorithms take additional information on analog signal levels as input to the error correction procedure.13.59375 Gbaud with PAM4, KP4 specific coding and RS-FEC(544,514) on 4 lanes per direction\nA half-speed variant of 26.5625 Gbaud with RS-FEC, with a 31320\/31280 step encoding the lane number into the signal, and further 92\/90 framing.","completion":"Computer Network"}
{"prompt":"To test the experimental system of the \u201cspace Internet\u201d, Danuri (Korea Pathfinder Lunar Orbiter) successfully forwarded a number of photos taken, as well as several video files, including, BTS\u2019 \u201cDynamite\u201d from outer space to Earth at Korea's Ministry of Science and ICT, Korea Aerospace Research Institute (KARI), and the Electronics and Telecommunications Research Institute (ETRI) on 7 November 2022.","completion":"Computer Network"}
{"prompt":"In March 1982, the US Department of Defense declared TCP\/IP as the standard for all military computer networking. In the same year, NORSAR and Peter Kirstein's research group at University College London adopted the protocol. The migration of the ARPANET from NCP to TCP\/IP was officially completed on flag day January 1, 1983, when the new protocols were permanently activated.In 1985, the Internet Advisory Board (later Internet Architecture Board) held a three-day TCP\/IP workshop for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use. In 1985, the first Interop conference focused on network interoperability by broader adoption of TCP\/IP. The conference was founded by Dan Lynch, an early Internet activist. From the beginning, large corporations, such as IBM and DEC, attended the meeting.IBM, AT&T and DEC were the first major corporations to adopt TCP\/IP, this despite having competing proprietary protocols. In IBM, from 1984, Barry Appelman's group did TCP\/IP development. They navigated the corporate politics to get a stream of TCP\/IP products for various IBM systems, including MVS, VM, and OS\/2. At the same time, several smaller companies, such as FTP Software and the Wollongong Group, began offering TCP\/IP stacks for DOS and Microsoft Windows. The first VM\/CMS TCP\/IP stack came from the University of Wisconsin.Some of the early TCP\/IP stacks were written single-handedly by a few programmers. Jay Elinsky and Oleg Vishnepolsky of IBM Research wrote TCP\/IP stacks for VM\/CMS and OS\/2, respectively. In 1984 Donald Gillies at MIT wrote a ntcp multi-connection TCP which runs atop the IP\/PacketDriver layer maintained by John Romkey at MIT in 1983\u201384.  Romkey leveraged this TCP in 1986 when FTP Software was founded. Starting in 1985, Phil Karn created a multi-connection TCP application for ham radio systems (KA9Q TCP).The spread of TCP\/IP was fueled further in June 1989, when the University of California, Berkeley agreed to place the TCP\/IP code developed for BSD UNIX into the public domain. Various corporate vendors, including IBM, included this code in commercial TCP\/IP software releases. Microsoft released a native TCP\/IP stack in Windows 95. This event helped cement TCP\/IP's dominance over other protocols on Microsoft-based networks, which included IBM's Systems Network Architecture (SNA), and on other platforms such as Digital Equipment Corporation's DECnet, Open Systems Interconnection (OSI), and Xerox Network Systems (XNS).\nNonetheless, for a period in the late 1980s and early 1990s, engineers, organizations and nations were polarized over the issue of which standard, the OSI model or the Internet protocol suite, would result in the best and most robust computer networks.","completion":"Computer Network"}
{"prompt":"Network-attached storage (NAS) is file-level computer data storage connected to a computer network providing data access to a heterogeneous group of clients. NAS devices specifically are distinguished from file servers generally in a NAS being a computer appliance \u2013 a specialized computer built from the ground up for serving files \u2013 rather than a general purpose computer being used for serving files (possibly with other functions). In discussions of NASs, the term \"file server\" generally stands for a contrasting term, referring to general purpose computers only.\nAs of 2010 NAS devices are gaining popularity, offering a convenient method for sharing files between multiple computers. Potential benefits of network-attached storage, compared to non-dedicated file servers, include faster data access, easier administration, and simple configuration.NAS systems are networked appliances containing one or more hard drives, often arranged into logical, redundant storage containers or RAID arrays. Network Attached Storage removes the responsibility of file serving from other servers on the network. They typically provide access to files using network file sharing protocols such as NFS, SMB\/CIFS (Server Message Block\/Common Internet File System), or AFP.","completion":"Computer Network"}
{"prompt":"In the early stages of development of the Internet Protocol, the network number was always the highest order octet (most significant eight bits). Because this method allowed for only 256 networks, it soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the addressing specification was revised with the introduction of classful network architecture.Classful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the class of the address. Three classes (A, B, and C) were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes (B and C). The following table gives an overview of this now-obsolete system.\n\nClassful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of networking in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes. Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.","completion":"Computer Network"}
{"prompt":"Any code designed to do more than spread the worm is typically referred to as the \"payload\". Typical malicious payloads might delete files on a host system (e.g., the ExploreZip worm), encrypt files in a ransomware attack, or exfiltrate data such as confidential documents or passwords.Some worms may install a backdoor. This allows the computer to be remotely controlled by the worm author as a \"zombie\". Networks of such machines are often referred to as botnets and are very commonly used for a range of malicious purposes, including sending spam or performing DoS attacks.Some special worms attack industrial systems in a targeted manner. Stuxnet was primarily transmitted through LANs and infected thumb-drives, as its targets were never connected to untrusted networks, like the internet.  This virus can destroy the core production control computer software used by chemical, power generation and power transmission companies in various countries around the world - in Stuxnet's case, Iran, Indonesia and India were hardest hit - it was used to \"issue orders\" to other equipment in the factory, and to hide those commands from being detected.  Stuxnet used multiple vulnerabilities and four different zero-day exploits (eg: [1]) in Windows systems and Siemens SIMATICWinCC systems to attack the embedded programmable logic controllers of industrial machines.  Although these systems operate independently from the network, if the operator inserts a virus-infected drive into the system's USB interface, the virus will be able to gain control of the system without any other operational requirements or prompts.","completion":"Computer Network"}
{"prompt":"Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support circuit-switched digital telephony. However, due to its protocol neutrality and transport-oriented features, SONET\/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.","completion":"Computer Network"}
{"prompt":"There have been various attempts at transporting data over exotic media:\n\nIP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.\nExtending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.Both cases have a large round-trip delay time, which gives slow two-way communication, but does not prevent sending large amounts of information.","completion":"Computer Network"}
{"prompt":"The Internet has enabled new forms of social interaction, activities, and social associations. This phenomenon has given rise to the scholarly study of the sociology of the Internet.\nThe early Internet left an impact on some writers who used symbolism to write about it, such as describing the Internet as a \"means to connect individuals in a vast invisible net over all the earth.\"","completion":"Computer Network"}
{"prompt":"Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, \"Host Software\", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that \"obsoletes\" the original.","completion":"Computer Network"}
{"prompt":"A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Wired LANs are most commonly based on Ethernet technology.  Other networking technologies such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.A LAN can be connected to a wide area network (WAN) using a router. The defining characteristics of a LAN, in contrast to a WAN, include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to and in excess of 100 Gbit\/s, standardized by IEEE in 2010.","completion":"Computer Network"}
{"prompt":"A runt frame is an Ethernet frame that is less than the IEEE 802.3's minimum length of 64 octets. Runt frames are most commonly caused by collisions; other possible causes are a malfunctioning network card, buffer underrun, duplex mismatch or software issues.","completion":"Computer Network"}
{"prompt":"Although the present-day, loose use of the term \"cyberspace\" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location.  It is for these reasons cyberspace has been described as the ultimate tax haven.In 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.","completion":"Computer Network"}
{"prompt":"Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.","completion":"Computer Network"}
{"prompt":"802.11 technology has its origins in a 1985 ruling by the U.S. Federal Communications Commission that released the ISM band for unlicensed use.In 1991 NCR Corporation\/AT&T (now Nokia Labs and LSI Corporation) invented a precursor to 802.11 in Nieuwegein, the Netherlands. The inventors initially intended to use the technology for cashier systems. The first wireless products were brought to the market under the name WaveLAN with raw data rates of 1 Mbit\/s and 2 Mbit\/s.\nVic Hayes, who held the chair of IEEE 802.11 for 10 years, and has been called the \"father of Wi-Fi\", was involved in designing the initial 802.11b and 802.11a standards within the IEEE. He, along with Bell Labs Engineer Bruce Tuch, approached IEEE to create a standard.In 1999, the Wi-Fi Alliance was formed as a trade association to hold the Wi-Fi trademark under which most products are sold.The major commercial breakthrough came with Apple's adoption of Wi-Fi for their iBook series of laptops in 1999. It was the first mass consumer product to offer Wi-Fi network connectivity, which was then branded by Apple as AirPort. One year later IBM followed with its ThinkPad 1300 series in 2000.","completion":"Computer Network"}
{"prompt":"During the 2011 Egyptian revolution, the government of Egypt shut down the four major ISPs on January 27, 2011 at approximately 5:20 p.m. EST. Evidently the networks had not been physically interrupted, as the Internet transit traffic through Egypt was unaffected. Instead, the government shut down the Border Gateway Protocol (BGP) sessions announcing local routes. BGP is responsible for routing traffic between ISPs.Only one of Egypt's ISPs was allowed to continue operations. The ISP Noor Group provided connectivity only to Egypt's stock exchange as well as some government ministries. Other ISPs started to offer free dial-up Internet access in other countries.","completion":"Computer Network"}
{"prompt":"Reservation ALOHA, or R-ALOHA, is an effort to improve the efficiency of Slotted ALOHA. The improvements with Reservation ALOHA are markedly shorter delays and ability to efficiently support higher levels of utilization. As a contrast of efficiency, simulations have shown that Reservation ALOHA exhibits less delay at 80% utilization than Slotted ALOHA at 20\u201336% utilization.The chief difference between Slotted and Reservation ALOHA is that with Slotted ALOHA, any slot is available for utilization without regards to prior usage. Under Reservation ALOHA's contention-based reservation schema, the slot is temporarily considered \"owned\" by the station that successfully used it. Additionally, Reservation ALOHA simply stops sending data once the station has completed its transmission. As a rule, idle slots are considered available to all stations that may then implicitly reserve (utilize) the slot on a contention basis.","completion":"Computer Network"}
{"prompt":"In the west, the United States provides a different \"tone of voice\" when cyber warfare is on the tip of everyone's tongue. The United States provides security plans strictly in the response to cyber warfare, going on the defensive when they are being attacked by devious cyber methods. In the U.S., the responsibility of cybersecurity is divided between the Department of Homeland Security, the Federal Bureau of Investigation, and the Department of Defense. In recent years, a new department was created to specifically tend to cyber threats, this department is known as Cyber Command. Cyber Command is a military subcommand under US Strategic Command and is responsible for dealing with threats to the military cyber infrastructure. Cyber Command's service elements include Army Forces Cyber Command, the Twenty-Fourth Air Force, Fleet Cyber Command and Marine Forces Cyber Command. It ensures that the President can navigate and control information systems and that he also has military options available when defense of the nation needs to be enacted in cyberspace. Individuals at Cyber Command must pay attention to state and non-state actors who are developing cyber warfare capabilities in conducting cyber espionage and other cyberattacks against the nation and its allies. Cyber Command seeks to be a deterrence factor to dissuade potential adversaries from attacking the U.S., while being a multi-faceted department in conducting cyber operations of its own.\nThree prominent events took place which may have been catalysts in the creation of the idea of Cyber Command. There was a failure of critical infrastructure reported by the CIA where malicious activities against information technology systems disrupted electrical power capabilities overseas. This resulted in multi-city power outages across multiple regions. The second event was the exploitation of global financial services. In November 2008, an international bank had a compromised payment processor that allowed fraudulent transactions to be made at more than 130 automated teller machines in 49 cities within a 30-minute period. The last event was the systemic loss of U.S. economic value when an industry in 2008 estimated $1 trillion in losses of intellectual property to data theft. Even though all these events were internal catastrophes, they were very real in nature, meaning nothing can stop state or non-state actors to do the same thing on an even grander scale. Other initiatives like the Cyber Training Advisory Council were created to improve the quality, efficiency, and sufficiency of training for computer network defense, attack, and exploitation of enemy cyber operations.\nOn both ends of the spectrum, East and West nations show a \"sword and shield\" contrast in ideals. The Chinese have a more offensive minded idea for cyber warfare, trying to get the pre-emptive strike in the early stages of conflict to gain the upper-hand. In the U.S. there are more reactionary measures being taken at creating systems with impenetrable barriers to protect the nation and its civilians from cyberattacks.\nAccording to Homeland Preparedness News, many mid-sized U.S. companies have a difficult time defending their systems against cyber-attacks. Around 80 percent of assets vulnerable to a cyber-attack are owned by private companies and organizations. Former New York State Deputy Secretary for Public Safety Michael Balboni said that private entities \"do not have the type of capability, bandwidth, interest or experience to develop a proactive cyber analysis.\"In response to cyberattacks on 1 April 2015, President Obama issued an Executive Order establishing the first-ever economic sanctions. The Executive Order will impact individuals and entities (\"designees\") responsible for cyber-attacks that threaten the national security, foreign policy, economic health, or financial stability of the US. Specifically, the Executive Order authorizes the Treasury Department to freeze designees' assets.According to Ted Koppel's book, in 2008, the United States in collaboration with Israel, ran a cyber-attack on Iran's nuclear program, becoming \"the first to use a digital weapon as an instrument of policy\".","completion":"Computer Network"}
{"prompt":"IEEE 802.11ay is a standard that is being developed, also called EDMG: Enhanced Directional MultiGigabit PHY.  It is an amendment that defines a new physical layer for 802.11 networks to operate in the 60 GHz millimeter wave spectrum. It will be an extension of the existing 11ad, aimed to extend the throughput, range, and use-cases. The main use-cases include indoor operation and short-range communications due to atmospheric oxygen absorption and inability to penetrate walls. The peak transmission rate of 802.11ay is 40 Gbit\/s. The main extensions include: channel bonding (2, 3 and 4), MIMO (up to 4 streams) and higher modulation schemes.  The expected range is 300-500 m.","completion":"Computer Network"}
{"prompt":"A number of commercial certificate authorities exist, offering paid-for SSL\/TLS certificates of a number of types, including Extended Validation Certificates.\nLet's Encrypt, launched in April 2016, provides free and automated service that delivers basic SSL\/TLS certificates to websites. According to the Electronic Frontier Foundation, Let's Encrypt will make switching from HTTP to HTTPS \"as easy as issuing one command, or clicking one button.\" The majority of web hosts and cloud providers now leverage Let's Encrypt, providing free certificates to their customers.","completion":"Computer Network"}
{"prompt":"The IETF defines Voice Admit behavior in RFC 5865. The Voice Admit PHB has identical characteristics to the Expedited Forwarding PHB. However, Voice Admit traffic is also admitted by the network using a Call Admission Control (CAC) procedure. The recommended DSCP for voice admit is 101100B (44 or 2CH).","completion":"Computer Network"}
{"prompt":"DHCP is used to assign internal IP addresses to members of a home network. A DHCP server typically runs on the router with end devices as its clients. The router itself is a client of the external DHCP servers owned by the internet service provider. All DHCP clients request configuration settings using the DHCP protocol in order to acquire their IP address, a default route and one or more DNS server addresses. Once the client implements these settings, it will be able to communicate on that internet.","completion":"Computer Network"}
{"prompt":"Telephone wiring is required between the telephone company's service entrance and locations throughout the home. Often a home will have telephone outlets in the kitchen, study, living room or bedrooms for convenience. Telephone company regulations may limit the total number of telephones that can be in use at one time. The telephone cabling typically uses two pair twisted cable terminated onto a telephone plug. The cabling is typically installed as a daisy chain starting from the point where the telephone company connects to the home or outlets may each be wired back to the entrance.","completion":"Computer Network"}
{"prompt":"The application layer includes the protocols used by most applications for providing user services or exchanging application data over the network connections established by the lower-level protocols. This may include some basic network support services such as routing protocols and host configuration. Examples of application layer protocols include the Hypertext Transfer Protocol (HTTP), the File Transfer Protocol (FTP), the Simple Mail Transfer Protocol (SMTP), and the Dynamic Host Configuration Protocol (DHCP). Data coded according to application layer protocols are  encapsulated into transport layer protocol units (such as TCP streams or UDP datagrams), which in turn use lower layer protocols to effect actual data transfer.\nThe TCP\/IP model does not consider the specifics of formatting and presenting data and does not define additional layers between the application and transport layers as in the OSI model (presentation and session layers). According to the TCP\/IP model, such functions are the realm of libraries and application programming interfaces. The application layer in the TCP\/IP model is often compared to a combination of the fifth (session), sixth (presentation), and seventh (application) layers of the OSI model.\nApplication layer protocols are often associated with particular client\u2013server applications, and common services have well-known port numbers reserved by the Internet Assigned Numbers Authority (IANA). For example, the HyperText Transfer Protocol uses server port 80 and Telnet uses server port 23. Clients connecting to a service usually use ephemeral ports, i.e., port numbers assigned only for the duration of the transaction at random or from a specific range configured in the application. \nAt the application layer, the TCP\/IP model distinguishes between user protocols and support protocols. Support protocols provide services to a system of network infrastructure. User protocols are used for actual user applications. For example, FTP is a user protocol and DNS is a support protocol.\nAlthough the applications are usually aware of key qualities of the transport layer connection such as the endpoint IP addresses and port numbers, application layer protocols generally treat the transport layer (and lower) protocols as black boxes which provide a stable network connection across which to communicate. The transport layer and lower-level layers are unconcerned with the specifics of application layer protocols. Routers and switches do not typically examine the encapsulated traffic, rather they just provide a conduit for it. However, some firewall and bandwidth throttling applications use deep packet inspection to interpret application data. An example is the Resource Reservation Protocol (RSVP). It is also sometimes necessary for Applications affected by NAT to consider the application payload.","completion":"Computer Network"}
{"prompt":"The Internet protocol suite evolved through research and development funded over a period of time. In this process, the specifics of protocol components and their layering changed. In addition, parallel research and commercial interests from industry associations competed with design features. In particular, efforts in the International Organization for Standardization led to a similar goal, but with a wider scope of networking in general. Efforts to consolidate the two principal schools of layering, which were superficially similar, but diverged sharply in detail, led independent textbook authors to formulate abridging teaching tools.\nThe following table shows various such networking models. The number of layers varies between three and seven.\n\nSome of the networking models are from textbooks, which are secondary sources that may conflict with the intent of RFC 1122 and other IETF primary sources.","completion":"Computer Network"}
{"prompt":"A storage area network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the storage appears as locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.","completion":"Computer Network"}
{"prompt":"A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks.  A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area. When designing a network backbone, network performance and network congestion are critical factors to take into account.  Normally, the backbone network's capacity is greater than that of the individual networks connected to it.\nFor example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. Another example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks.","completion":"Computer Network"}
{"prompt":"The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper, On Computable Numbers. Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.","completion":"Computer Network"}
{"prompt":"Initially, as with its predecessor networks, the system that would evolve into the Internet was primarily for government and government body use. Although commercial use was forbidden, the exact definition of commercial use was unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. (Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.)\nAs a result, during the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. In 1989, MCI Mail became the first commercial email provider to get an experimental gateway to the Internet. The first commercial dialup ISP in the United States was The World, which opened in 1989.In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, 42 U.S.C. \u00a7 1862(g), which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.By 1990, ARPANET's goals had been fulfilled and new networking technologies exceeded the original scope and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point of the Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995, when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.An event held on 11 January 1994, The Superhighway Summit at UCLA's Royce Hall, was the \"first public conference bringing together all of the major industry, government and academic leaders in the field [and] also began the national dialogue about the Information Superhighway and its implications\".","completion":"Computer Network"}
{"prompt":"Automation refers to the ability to be able to control a range of devices in the home ranging from lights to curtains. The most common example of automation are referred to as Lighting control systems. Lighting control system need to be installed by a qualified professional as the cabling is only one element but without the equipment and programming you cannot even turn a light on. The cabling required when installing an automation system can be divided into two parts:\n\nElectrical\nData BusElectrical\nThis is cabling installed from the electrical switchboard to the light fitting or any other device that is to be controlled by the automation system. For example, if you have four down lights in a room and you wish to control each light individually, then each light will be wired back using electrical cabling back to the electrical switchboard. This means you will have four electrical cables installed from the electrical switchboard to the location where the light fittings will be installed. Each cable will be a three core active, neutral and earth cable.\nIf in that room you also have a free standing lamp plugged into a power point and you also want to control this from your automation system, you will need to have that power point individually wired back to the electrical switchboard.\nSo if you want to individually control every light fitting and every power point or power outlets then each one of these devices must be individually wired back to the electrical switchboard. As you can see this start to become quite a lot of electrical cabling so planning is essential.\nNote, when you are using an automation system, there is no need to install any electrical cabling to the light switches. In a traditional electrical installation without automation the lights in a room would be wired back to the light switch which in turn would be wired back to the switchboard or some similar arrangement, so keep reading.\nData Bus\nOnce you have installed the electrical cabling you need to install the data bus cable from the electrical switchboard to every location you want to have a light switch or control panel installed (control panel is like the code pad on a security system or touch screen that gives you access to various control functions). The most common cable used for this is a Category 5 cable. The cable can be installed in either a daisy chain or star wired configuration. The importance is to minimize the cable length to avoid a communications problem on the bus.","completion":"Computer Network"}
{"prompt":"Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.\nIn packet-switched networks, routing protocols direct packet forwarding through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though because they lack specialized hardware, may offer limited performance. The routing process directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\nRouting can be contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices.  In large networks, the structured addressing used by routers outperforms unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks.","completion":"Computer Network"}
{"prompt":"While at MIT in the 1950s, Licklider worked on Semi-Automatic Ground Environment (SAGE), a Cold War project to create a computer-aided air defense system. The SAGE system included computers that collected and presented data to a human operator, who then chose the appropriate response. He worked as a human factors expert, which helped convince him of the great potential for human\/computer interfaces.","completion":"Computer Network"}
{"prompt":"The Internet protocol suite does not presume any specific hardware or software environment.  It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP\/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Internet Group Management Protocol (IGMP). In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and Multicast Listener Discovery (MLD) and is often accompanied by an integrated IPSec security layer.","completion":"Computer Network"}
{"prompt":"The number of light fitting does depend on the type of light fitting and the lighting requirements in each room. The incandescent bulb made household lighting practical, but modern homes use a wide variety of light sources to provide desired light levels with higher energy efficiency than incandescent lamps. A lighting designer can provide specific recommendations for lighting in a home. Layout of lighting in the home must consider control of lighting since this affects the wiring. For example, multiway switching is useful for corridors and stairwells so that a light can be turned on and off from two locations. Outdoor yard lighting, and lighting for outbuildings such as garages may use switches inside the home.","completion":"Computer Network"}
{"prompt":"IP networks may be divided into subnetworks in both IPv4 and IPv6. For this purpose, an IP address is recognized as consisting of two parts: the network prefix in the high-order bits and the remaining bits called the rest field, host identifier, or interface identifier (IPv6), used for host numbering within a network. The subnet mask or CIDR notation determines how the IP address is divided into network and host parts.\nThe term subnet mask is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the routing prefix. For example, an IPv4 address and its subnet mask may be 192.0.2.1 and 255.255.255.0, respectively. The CIDR notation for the same IP address and subnet is 192.0.2.1\/24, because the first 24 bits of the IP address indicate the network and subnet.","completion":"Computer Network"}
{"prompt":"In 2013, researchers from Europe proposed that the electronic voting systems should be coercion evident. There should be a public evidence of the amount of coercion that took place in a particular elections. An internet voting system called \"Caveat Coercitor\" shows how coercion evidence in voting systems can be achieved.","completion":"Computer Network"}
{"prompt":"Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software. Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called \"firmware\".","completion":"Computer Network"}
{"prompt":"The ubiquity of email for knowledge workers and \"white collar\" employees has led to concerns that recipients face an \"information overload\" in dealing with increasing volumes of email. With the growth in mobile devices, by default employees may also receive work-related emails outside of their working day. This can lead to increased stress and decreased satisfaction with work. Some observers even argue it could have a significant negative economic effect, as efforts to read the many emails could reduce productivity.","completion":"Computer Network"}
{"prompt":"The simplest and most common problem using flow networks is to find what is called the maximum flow, which provides the largest possible total flow from the source to the sink in a given graph. There are many other problems which can be solved using max flow algorithms, if they are appropriately modeled as flow networks, such as bipartite matching, the assignment problem and the transportation problem. Maximum flow problems can be solved in polynomial time with various algorithms (see table). The max-flow min-cut theorem states that finding a maximal network flow is equivalent to finding a  cut of minimum capacity that separates the source and the sink, where a cut is the division of vertices such that the source is in one division and the sink is in another.\n\nIn a multi-commodity flow problem, you have multiple sources and sinks, and various \"commodities\" which are to flow from a given source to a given sink. This could be for example various goods that are produced at various factories, and are to be delivered to various given customers through the same transportation network.\nIn a minimum cost flow problem, each edge \n  \n    \n      \n        u\n        ,\n        v\n      \n    \n    {\\displaystyle u,v}\n   has a given cost \n  \n    \n      \n        k\n        (\n        u\n        ,\n        v\n        )\n      \n    \n    {\\displaystyle k(u,v)}\n  , and the cost of sending the flow \n  \n    \n      \n        f\n        (\n        u\n        ,\n        v\n        )\n      \n    \n    {\\displaystyle f(u,v)}\n   across the edge is \n  \n    \n      \n        f\n        (\n        u\n        ,\n        v\n        )\n        \u22c5\n        k\n        (\n        u\n        ,\n        v\n        )\n      \n    \n    {\\displaystyle f(u,v)\\cdot k(u,v)}\n  . The objective is to send a given amount of flow from the source to the sink, at the lowest possible price.\nIn a circulation problem, you have a lower bound \n  \n    \n      \n        \u2113\n        (\n        u\n        ,\n        v\n        )\n      \n    \n    {\\displaystyle \\ell (u,v)}\n   on the edges, in addition to the upper bound \n  \n    \n      \n        c\n        (\n        u\n        ,\n        v\n        )\n      \n    \n    {\\displaystyle c(u,v)}\n  . Each edge also has a cost. Often, flow conservation holds for all nodes in a circulation problem, and there is a connection from the sink back to the source. In this way, you can dictate the total flow with \n  \n    \n      \n        \u2113\n        (\n        t\n        ,\n        s\n        )\n      \n    \n    {\\displaystyle \\ell (t,s)}\n   and \n  \n    \n      \n        c\n        (\n        t\n        ,\n        s\n        )\n      \n    \n    {\\displaystyle c(t,s)}\n  . The flow circulates through the network, hence the name of the problem.\nIn a network with gains or generalized network each edge has a gain, a real number (not zero) such that, if the edge has gain g, and an amount x flows into the edge at its tail, then an amount gx flows out at the head.\nIn a source localization problem, an algorithm tries to identify the most likely source node of information diffusion through a partially observed network. This can be done in linear time for trees and cubic time for arbitrary networks and has applications ranging from tracking mobile phone users to identifying the originating source of disease outbreaks.","completion":"Computer Network"}
{"prompt":"Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.\nIn time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.","completion":"Computer Network"}
{"prompt":"Messaging applications have affected the way people communicate on their devices. A survey conducted by MetrixLabs showed that 63% of Baby Boomers, 63% of Generation X, and 67% of Generation Y said that they used messaging applications in place of texting. A Facebook survey showed that 65% of people surveyed thought that messaging applications made group messaging easier.","completion":"Computer Network"}
{"prompt":"In a fully connected network, all nodes are interconnected. (In graph theory this is called a complete graph.) The simplest fully connected network is a two-node network. A fully connected network doesn't need to use packet switching or broadcasting. However, since the number of connections grows quadratically with the number of nodes:\n\n  \n    \n      \n        c\n        =\n        \n          \n            \n              n\n              (\n              n\n              \u2212\n              1\n              )\n            \n            2\n          \n        \n        .\n        \n      \n    \n    {\\displaystyle c={\\frac {n(n-1)}{2}}.\\,}\n  \nThis makes it impractical for large networks.  This kind of topology does not trip and affect other nodes in the network.","completion":"Computer Network"}
{"prompt":"Wikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries. In those settings, they have been found useful for collaboration on grant writing, strategic planning, departmental documentation, and committee work. The United States Patent and Trademark Office uses a wiki to allow the public to collaborate on finding prior art relevant to examination of pending patent applications. Queens, New York has used a wiki to allow citizens to collaborate on the design and planning of a local park. The English Wikipedia has the largest user base among wikis on the World Wide Web and ranks in the top 10 among all Web sites in terms of traffic.","completion":"Computer Network"}
{"prompt":"Security experts, such as Bruce Schneier, have demanded that voting machine source code should be publicly available for inspection. Others have also suggested publishing voting machine software under a free software license as is done in Australia.","completion":"Computer Network"}
{"prompt":"The extended star network topology extends a physical star topology by one or more repeaters between the central node and the peripheral (or 'spoke') nodes. The repeaters are used to extend the maximum transmission distance of the physical layer, the point-to-point distance between the central node and the peripheral nodes. Repeaters allow greater transmission distance, further than would be possible using just the transmitting power of the central node. The use of repeaters can also overcome limitations from the standard upon which the physical layer is based.\nA physical extended star topology in which repeaters are replaced with hubs or switches is a type of hybrid network topology and is referred to as a physical hierarchical star topology, although some texts make no distinction between the two topologies.\nA physical hierarchical star topology can also be referred as a tier-star topology. This topology differs from a tree topology in the way star networks are connected together. A tier-star topology uses a central node, while a tree topology uses a central bus and can also be referred as a star-bus network.","completion":"Computer Network"}
{"prompt":"Sticky is an informal term used to describe a dynamically assigned IP address that seldom changes. IPv4 addresses, for example, are usually assigned with DHCP, and a DHCP service can use rules that maximize the chance of assigning the same address each time a client asks for an assignment. In IPv6, a prefix delegation can be handled similarly, to make changes as rare as feasible. In a typical home or small-office setup, a single router is the only device visible to an Internet service provider (ISP), and the ISP may try to provide a configuration that is as stable as feasible, i.e. sticky. On the local network of the home or business, a local DHCP server may be designed to provide sticky IPv4 configurations, and the ISP may provide a sticky IPv6 prefix delegation, giving clients the option to use sticky IPv6 addresses. Sticky should not be confused with static; sticky configurations have no guarantee of stability, while static configurations are used indefinitely and only changed deliberately.","completion":"Computer Network"}
{"prompt":"Small standalone embedded home network devices typically require remote configuration from a PC on the same network. For example, broadband modems are often configured through a web browser running on a PC in the same network. These devices usually use a minimal Linux distribution with a lightweight HTTP server running in the background to allow the user to conveniently modify system variables from a GUI rendered in their browser. These pages use HTML forms extensively and make attempts to offer styled, visually appealing views that are also descriptive and easy to use.","completion":"Computer Network"}
{"prompt":"Packet switched communication may also be connection-oriented, which is called virtual circuit mode communication. Due to the packet switching, the communication may suffer from variable bit rate and delay, due to varying traffic load and packet queue lengths. Connection-oriented communication are not necessarily reliable protocols.","completion":"Computer Network"}
{"prompt":"Cyber attacking telecommunication infrastructures have straightforward results. Telecommunication integration is becoming common practice, systems such as voice and IP networks are merging. Everything is being run through the internet because the speeds and storage capabilities are endless. Denial-of-service attacks can be administered as previously mentioned, but more complex attacks can be made on BGP routing protocols or DNS infrastructures. It is less likely that an attack would target or compromise the traditional telephony network of SS7 switches, or an attempted attack on physical devices such as microwave stations or satellite facilities. The ability would still be there to shut down those physical facilities to disrupt telephony networks. The whole idea on these cyberattacks is to cut people off from one another, to disrupt communication, and by doing so, to impede critical information being sent and received. In cyber warfare, this is a critical way of gaining the upper hand in a conflict. By controlling the flow of information and communication, a nation can plan more accurate strikes and enact better counter-attack measures on their enemies.","completion":"Computer Network"}
{"prompt":"Planning for the project began in 1971. Design and staffing started in 1972, and November 1973 saw the first demonstration, using three hosts and one packet switch.Pouzin coined the term catenet in 1973, in a note circulated to the International Networking Working Group, to describe a system of packet-switched communication networks interconnected via gateways. He later published these ideas in a 1974 paper \"A Proposal for Interconnecting Packet Switching Networks\".Deployment of the network continued in 1974, with three packet switches installed by February, although at that point the network was only operational for three hours each day. By June the network was up to seven switches, and was available throughout the day for experimental use.\nA terminal concentrator was also developed that year, since time-sharing was still a prevalent mode of computer use. In 1975, the network shrank slightly due to budgetary constraints, but the setback was only temporary. At that point, the network provided remote login, remote batch and file transfer user application services.\nBy 1976 the network was in full deployment, eventually numbering 20 nodes with connections to NPL in London, ESA in Rome, and to the European Informatics Network (EIN).","completion":"Computer Network"}
{"prompt":"Though a relatively new feature, peer-to-peer payments are available on major messaging platforms. This functionality allows individuals to use one application for both communication and financial tasks. The lack of a service fee also makes messaging apps advantageous to financial applications. Major platforms such as Facebook messenger and WeChat already offer a payment feature, and this functionality is likely to become a standard amongst IM apps competing in the market.","completion":"Computer Network"}
{"prompt":"Financial infrastructures could be hit hard by cyberattacks as the financial system is linked by computer systems. Money is constantly being exchanged in these institutions and if cyberterrorists were to attack and if transactions were rerouted and large amounts of money stolen, financial industries would collapse and civilians would be without jobs and security. Operations would stall from region to region causing nationwide economic degradation. In the U.S. alone, the average daily volume of transactions hit $3 trillion and 99% of it is non-cash flow. To be able to disrupt that amount of money for one day or for a period of days can cause lasting damage making investors pull out of funding and erode public confidence.\nA cyberattack on a financial institution or transactions may be referred to as a cyber heist. These attacks may start with phishing that targets employees, using social engineering to coax information from them. They may allow attackers to hack into the network and put keyloggers on the accounting systems. In time, the cybercriminals are able to obtain password and keys information. An organization's bank accounts can then be accessed via the information they have stolen using the keyloggers. In May 2013, a gang carried out a US$40 million cyber heist from the Bank of Muscat.","completion":"Computer Network"}
{"prompt":"The Ethernet physical layer evolved over a considerable time span and encompasses coaxial, twisted pair and fiber-optic physical media interfaces, with speeds from 1 Mbit\/s to 400 Gbit\/s. The first introduction of twisted-pair CSMA\/CD was StarLAN, standardized as 802.3 1BASE5. While 1BASE5 had little market penetration, it defined the physical apparatus (wire, plug\/jack, pin-out, and wiring plan) that would be carried over to 10BASE-T through 10GBASE-T.\nThe most common forms used are 10BASE-T, 100BASE-TX, and 1000BASE-T. All three use twisted-pair cables and 8P8C modular connectors. They run at 10 Mbit\/s, 100 Mbit\/s, and 1 Gbit\/s, respectively.Fiber optic variants of Ethernet (that commonly use SFP modules) are also very popular in larger networks, offering high performance, better electrical isolation and longer distance (tens of kilometers with some versions). In general, network protocol stack software will work similarly on all varieties.","completion":"Computer Network"}
{"prompt":"The distance per twist is commonly referred to as pitch. Each of the four pairs in a Cat 5 cable has differing pitch to minimize crosstalk between the pairs. The pitch of the twisted pairs is not specified in the standard.","completion":"Computer Network"}
{"prompt":"Aitia International introduced the C-GEP FPGA-based switching platform in February 2013. Aitia also produce 100G\/40G Ethernet PCS\/PMA+MAC IP cores for FPGA developers and academic researchers.","completion":"Computer Network"}
{"prompt":"Ethernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.","completion":"Computer Network"}
{"prompt":"An improvement to the original ALOHA protocol was Slotted ALOHA, which introduced discrete timeslots and increased the maximum throughput. A station can start a transmission only at the beginning of a timeslot, and thus collisions are reduced. In this case, only transmission-attempts within 1 frame-time and not 2 consecutive frame-times need to be considered, since collisions can only occur during each timeslot. Thus, the probability of there being zero transmission attempts by other stations in a single timeslot is:\n\n  \n    \n      \n        P\n        r\n        o\n        \n          b\n          \n            s\n            l\n            o\n            t\n            t\n            e\n            d\n          \n        \n        =\n        \n          e\n          \n            \u2212\n            G\n          \n        \n      \n    \n    {\\displaystyle Prob_{slotted}=e^{-G}}\n  \nthe probability of a transmission requiring exactly k attempts is (k-1 collisions and 1 success):\n  \n    \n      \n        P\n        r\n        o\n        \n          b\n          \n            s\n            l\n            o\n            t\n            t\n            e\n            d\n          \n        \n        k\n        =\n        \n          e\n          \n            \u2212\n            G\n          \n        \n        (\n        1\n        \u2212\n        \n          e\n          \n            \u2212\n            G\n          \n        \n        \n          )\n          \n            k\n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle Prob_{slotted}k=e^{-G}(1-e^{-G})^{k-1}}\n  \nThe throughput is:\n\n  \n    \n      \n        \n          S\n          \n            s\n            l\n            o\n            t\n            t\n            e\n            d\n          \n        \n        =\n        G\n        \n          e\n          \n            \u2212\n            G\n          \n        \n      \n    \n    {\\displaystyle S_{slotted}=Ge^{-G}}\n  \nThe maximum throughput is 1\/e frames per frame-time (reached when G = 1), which is approximately 0.368 frames per frame-time, or 36.8%.\nSlotted ALOHA is used in low-data-rate tactical satellite communications networks by military forces, in subscriber-based satellite communications networks, mobile telephony call setup, set-top box communications and in the contactless RFID technologies.","completion":"Computer Network"}
{"prompt":"The first mobile computers were heavy and ran from mains power. The 50 lb (23 kg) IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries \u2013 and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s.\nThese smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market. These are powered by System on a Chip (SoCs), which are complete computers on a microchip the size of a coin.","completion":"Computer Network"}
{"prompt":"The Uniform Resource Identifier (URI) scheme HTTPS has identical usage syntax to the HTTP scheme. However, HTTPS signals the browser to use an added encryption layer of SSL\/TLS to protect the traffic. SSL\/TLS is especially suited for HTTP, since it can provide some protection even if only one side of the communication is authenticated. This is the case with HTTP transactions over the Internet, where typically only the server is authenticated (by the client examining the server's certificate).\nHTTPS creates a secure channel over an insecure network. This ensures reasonable protection from eavesdroppers and man-in-the-middle attacks, provided that adequate cipher suites are used and that the server certificate is verified and trusted.\nBecause HTTPS piggybacks HTTP entirely on top of TLS, the entirety of the underlying HTTP protocol can be encrypted. This includes the request's URL, query parameters, headers, and cookies (which often contain identifying information about the user). However, because website addresses and port numbers are necessarily part of the underlying TCP\/IP protocols, HTTPS cannot protect their disclosure. In practice this means that even on a correctly configured web server, eavesdroppers can infer the IP address and port number of the web server, and sometimes even the domain name (e.g. www.example.org, but not the rest of the URL) that a user is communicating with, along with the amount of data transferred and the duration of the communication, though not the content of the communication.Web browsers know how to trust HTTPS websites based on certificate authorities that come pre-installed in their software. Certificate authorities are in this way being trusted by web browser creators to provide valid certificates. Therefore, a user should trust an HTTPS connection to a website if and only if all of the following are true:\n\nThe user trusts that their device, hosting the browser and the method to get the browser itself, is not compromised (i.e. there is no supply chain attack).\nThe user trusts that the browser software correctly implements HTTPS with correctly pre-installed certificate authorities.\nThe user trusts the certificate authority to vouch only for legitimate websites (i.e. the certificate authority is not compromised and there is no mis-issuance of certificates).\nThe website provides a valid certificate, which means it was signed by a trusted authority.\nThe certificate correctly identifies the website (e.g., when the browser visits \"https:\/\/example.com\", the received certificate is properly for \"example.com\" and not some other entity).\nThe user trusts that the protocol's encryption layer (SSL\/TLS) is sufficiently secure against eavesdroppers.HTTPS is especially important over insecure networks and networks that may be subject to tampering.  Insecure networks, such as public Wi-Fi access points, allow anyone on the same local network to packet-sniff and discover sensitive information not protected by HTTPS. Additionally, some free-to-use and paid WLAN networks have been observed tampering with webpages by engaging in packet injection in order to serve their own ads on other websites. This practice can be exploited maliciously in many ways, such as by injecting malware onto webpages and stealing users' private information.HTTPS is also important for connections over the Tor network, as malicious Tor nodes could otherwise damage or alter the contents passing through them in an insecure fashion and inject malware into the connection. This is one reason why the Electronic Frontier Foundation and the Tor Project started the development of HTTPS Everywhere, which is included in Tor Browser.As more information is revealed about global mass surveillance and criminals stealing personal information, the use of HTTPS security on all websites is becoming increasingly important regardless of the type of Internet connection being used. Even though metadata about individual pages that a user visits might not be considered sensitive, when aggregated it can reveal a lot about the user and compromise the user's privacy.Deploying HTTPS also allows the use of HTTP\/2 and HTTP\/3 (and their predecessors SPDY and QUIC), which are new HTTP versions designed to reduce page load times, size, and latency.\nIt is recommended to use HTTP Strict Transport Security (HSTS) with HTTPS to protect users from man-in-the-middle attacks, especially SSL stripping.HTTPS should not be confused with the seldom-used Secure HTTP (S-HTTP) specified in RFC 2660.","completion":"Computer Network"}
{"prompt":"J. C. R. Licklider, while working at BBN, proposed a computer network in his March 1960 paper Man-Computer Symbiosis:\nA network of such centers, connected to one another by wide-band communication lines [...] the functions of present-day libraries together with anticipated advances in information storage and retrieval and symbiotic functions suggested earlier in this paper\nIn August 1962, Licklider and Welden Clark published the paper \"On-Line Man-Computer Communication\" which was one of the first descriptions of a networked future.\nIn October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within ARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos in 1963 describing a distributed network to the IPTO staff, whom he called \"Members and Affiliates of the Intergalactic Computer Network\".Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for one of his successors, Robert Taylor, to initiate the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.","completion":"Computer Network"}
{"prompt":"Certain devices on a home network are primarily concerned with enabling or supporting the communications of the kinds of end devices home-dwellers more directly interact with. Unlike their data center counterparts, these \"networking\" devices are compact and passively cooled,  aiming to be as hands-off and non-obtrusive as possible:\n\nA gateway establishes physical and data link layer connectivity to a WAN over a service provider's native telecommunications infrastructure. Such devices typically contain a cable, DSL, or optical modem bound to a network interface controller for Ethernet. Routers are often incorporated into these devices for additional convenience.\nA router establishes network layer connectivity between a WAN and the home network. It also performs the key function of network address translation that allows independently addressed devices within the same home network to establish transport layer connections across the WAN from a single, outward-facing WAN IP address. These devices often come with an integrated wireless access point and 4-port Ethernet switch.\nA network switch is used to allow devices on the home network to talk to one another via Ethernet. While the needs of most home networks are satisfied with the built-in wireless and\/or switching capabilities of their router, some situations require the addition of a separate switch with advanced capabilities. For example:\nA typical home router has 4 to 6 Ethernet LAN ports, so a router's switching capacity could be exceeded.\nA network device might require a non-standard port feature such as power over Ethernet (PoE). (IP cameras and IP phones)\nA wireless access point is required for connecting wireless devices to a network. When a router includes this device, it is referred to as a wireless router.\nA home automation or smart home controller acts as a gateway and router for low-power wireless networks of simple, non-data-intensive devices such as light bulbs and locks.\nA network bridge binds two different network interfaces to each other, often in order to grant a wired-only device access to a wireless network medium.","completion":"Computer Network"}
{"prompt":"There are two parts to a flow spec:\n\nWhat does the traffic look like? Done in the Traffic SPECification part, also known as TSPEC.\nWhat guarantees does it need? Done in the service Request SPECification part, also known as RSPEC.TSPECs include token bucket algorithm parameters. The idea is that there is a token bucket which slowly fills up with tokens, arriving at a constant rate. Every packet which is sent requires a token, and if there are no tokens, then it cannot be sent. Thus, the rate at which tokens arrive dictates the average rate of traffic flow, while the depth of the bucket dictates how 'bursty' the traffic is allowed to be.\nTSPECs typically just specify the token rate and the bucket depth. For example, a video with a refresh rate of 75 frames per second, with each frame taking 10 packets, might specify a token rate of 750 Hz, and a bucket depth of only 10. The bucket depth would be sufficient to accommodate the 'burst' associated with sending an entire frame all at once. On the other hand, a conversation would need a lower token rate, but a much higher bucket depth. This is because there are often pauses in conversations, so they can make do with less tokens by not sending the gaps between words and sentences. However, this means the bucket depth needs to be increased to compensate for the traffic being burstier.\nRSPECs specify what requirements there are for the flow: it can be normal internet 'best effort', in which case no reservation is needed. This setting is likely to be used for webpages, FTP, and similar applications. The 'Controlled Load' setting mirrors the performance of a lightly loaded network: there may be occasional glitches when two people access the same resource by chance, but generally both delay and drop rate are fairly constant at the desired rate. This setting is likely to be used by soft QoS applications. The 'Guaranteed' setting gives an absolutely bounded service, where the delay is promised to never go above a desired amount, and packets never dropped, provided the traffic stays within spec.","completion":"Computer Network"}
{"prompt":"The next great advance in computing power came with the advent of the integrated circuit (IC).\nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C., on 7 May 1952.The first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\". However, Kilby's invention was a hybrid integrated circuit (hybrid IC), rather than a monolithic integrated circuit (IC) chip. Kilby's IC had external wire connections, which made it difficult to mass-produce.Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. Noyce's invention was the first true monolithic IC chip. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. Noyce's monolithic IC was fabricated using the planar process, developed by his colleague Jean Hoerni in early 1959. In turn, the planar process was based on Mohamed M. Atalla's work on semiconductor surface passivation by silicon dioxide in the late 1950s.Modern monolithic ICs are predominantly MOS (metal\u2013oxide\u2013semiconductor) integrated circuits, built from MOSFETs (MOS transistors). The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962. General Microelectronics later introduced the first commercial MOS IC in 1964, developed by Robert Norman. Following the development of the self-aligned gate (silicon-gate) MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967, the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968. The MOSFET has since become the most critical device component in modern ICs.The development of the MOS integrated circuit led to the invention of the microprocessor, and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Federico Faggin with his silicon-gate MOS IC technology, along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel. In the early 1970s, MOS IC technology enabled the integration of more than 10,000 transistors on a single chip.System on a Chip (SoCs) are complete computers on a microchip (or chip) the size of a coin. They may or may not have integrated RAM and flash memory. If not integrated, the RAM is usually placed directly above (known as Package on package) or below (on the opposite side of the circuit board) the SoC, and the flash memory is usually placed right next to the SoC, this all done to improve data transfer speeds, as the data signals do not have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (Such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power.","completion":"Computer Network"}
{"prompt":"Data in a frame is typically protected from channel errors by error-correcting codes.\n\nEven when the channel errors exceed the correction capability of the error-correcting code, the presence of errors is nearly always detected by the error-correcting code or by a separate error-detecting code.\nFrames for which uncorrectable errors are detected are marked as undecodable and typically are deleted.","completion":"Computer Network"}
{"prompt":"Some neighborhoods support running fiber optic cables running directly into homes. This enables service providers to offer internet services with much higher bandwidth and\/or lower latency characteristics associated with end-to-end optical signaling.","completion":"Computer Network"}
{"prompt":"Modems (modulator-demodulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Early modems modulated audio signals sent over a standard voice telephone line. Modems are still commonly used for telephone lines, using a digital subscriber line technology and cable television systems using DOCSIS technology.","completion":"Computer Network"}
{"prompt":"Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.\nThe following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:\n\nCircuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo.\nATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique, and modem enhancements.There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.","completion":"Computer Network"}
{"prompt":"Early network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be globally unique. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.\nComputers not connected to the Internet, such as factory machines that communicate only with each other via TCP\/IP, need not have globally unique IP addresses. Today, such private networks are widely used and typically connect to the Internet with network address translation (NAT), when needed.\nThree non-overlapping ranges of IPv4 addresses for private networks are reserved. These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry. Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets; for example, many home routers automatically use a default address range of 192.168.0.0 through 192.168.0.255 (192.168.0.0\/24).","completion":"Computer Network"}
{"prompt":"A wireless distribution system (WDS) enables the wireless interconnection of access points in an IEEE 802.11 network. It allows a wireless network to be expanded using multiple access points without the need for a wired backbone to link them, as is traditionally required. The notable advantage of a WDS over some other solutions is that it preserves the MAC addresses of client packets across links between access points.An access point can be either a main, relay, or remote base station. A main base station is typically connected to the wired Ethernet. A relay base station relays data between remote base stations, wireless clients or other relay stations to either a main or another relay base station. A remote base station accepts connections from wireless clients and passes them to relay or main stations.\nBecause data is forwarded wirelessly, consuming wireless bandwidth, throughput in this method is halved for wireless clients not connected to a main base station. Connections between base stations are done at layer-2 and do not involve or require layer-3 IP addresses. WDS capability may also be referred to as repeater mode because it appears to bridge and accept wireless clients at the same time (unlike traditional bridging).\nAll base stations in a WDS must be configured to use the same radio channel and share WEP keys or WPA keys if they are used. They can be configured to different service set identifiers. WDS also requires that every base station be configured to forward to others in the system as mentioned above.","completion":"Computer Network"}
{"prompt":"Network traffic entering a DiffServ domain is subjected to classification and conditioning.  A traffic classifier may inspect many different parameters in incoming packets, such as source address, destination address or traffic type and assign individual packets to a specific traffic class.  Traffic classifiers may honor any DiffServ markings in received packets or may elect to ignore or override those markings.  For tight control over volumes and type of traffic in a given class, a network operator may choose not to honor markings at the ingress to the DiffServ domain. Traffic in each class may be further conditioned by subjecting the traffic to rate limiters, traffic policers or shapers.:\u200a\u00a73\u200aThe per-hop behavior is determined by the DS field in the IP header. The DS field contains the 6-bit DSCP value. Explicit Congestion Notification (ECN) occupies the least-significant 2 bits of the IPv4 TOS field and IPv6 traffic class (TC) field.In theory, a network could have up to 64 different traffic classes using the 64 available DSCP values. The DiffServ RFCs recommend, but do not require, certain encodings. This gives a network operator great flexibility in defining traffic classes. In practice, however, most networks use the following commonly defined per-hop behaviors:\n\nDefault Forwarding (DF) PHB \u2014 which is typically best-effort traffic\nExpedited Forwarding (EF) PHB \u2014 dedicated to low-loss, low-latency traffic\nAssured Forwarding (AF) PHB \u2014 gives assurance of delivery under prescribed conditions\nClass Selector PHBs \u2014 which maintain backward compatibility with the IP precedence field.","completion":"Computer Network"}
{"prompt":"MAC may refer to the sublayer that determines who is allowed to access the media at any one time (e.g. CSMA\/CD). Other times it refers to a frame structure delivered based on MAC addresses inside.\nThere are generally two forms of media access control: distributed and centralized. Both of these may be compared to communication between people. In a network made up of people speaking, i.e. a conversation, they will each pause a random amount of time and then attempt to speak again, effectively establishing a long and elaborate game of saying \"no, you first\".\nThe Media Access Control sublayer also performs frame synchronization, which determines the start and end of each frame of data in the transmission bitstream. It entails one of several methods: timing-based detection, character counting, byte stuffing, and bit stuffing.\n\nThe time-based approach expects a specified amount of time between frames.\nCharacter counting tracks the count of remaining characters in the frame header. This method, however, is easily disturbed if this field is corrupted.\nByte stuffing precedes the frame with a special byte sequence such as DLE STX and succeeds it with DLE ETX. Appearances of DLE (byte value 0x10) have to be escaped with another DLE. The start and stop marks are detected at the receiver and removed as well as the inserted DLE characters.\nSimilarly, bit stuffing replaces these start and end marks with flags consisting of a special bit pattern (e.g. a 0, six 1 bits and a 0). Occurrences of this bit pattern in the data to be transmitted are avoided by inserting a bit. To use the example where the flag is 01111110, a 0 is inserted after 5 consecutive 1's in the data stream. The flags and the inserted 0's are removed at the receiving end. This makes for arbitrary long frames and easy synchronization for the recipient. The stuffed bit is added even if the following data bit is 0, which could not be mistaken for a sync sequence, so that the receiver can unambiguously distinguish stuffed bits from normal bits.","completion":"Computer Network"}
{"prompt":"Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.","completion":"Computer Network"}
{"prompt":"Interpacket gap (IPG) is idle time between packets. After a packet has been sent, transmitters are required to transmit a minimum of 96 bits (12 octets) of idle line state before transmitting the next packet.","completion":"Computer Network"}
{"prompt":"During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems).","completion":"Computer Network"}
{"prompt":"Email header fields can be multi-line, with each line recommended to be no more than 78 characters, although the limit is 998 characters. Header fields defined by RFC 5322 contain only US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 may be used. In some examples, the IETF EAI working group defines some standards track extensions, replacing previous experimental extensions so UTF-8 encoded Unicode characters may be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some government agents.The message header must include at least the following fields:\nFrom: The email address, and, optionally, the name of the author(s). Some email clients are changeable through account settings.\nDate: The local time and date the message was written. Like the From: field, many email clients fill this in automatically before sending. The recipient's client may display the time in the format and time zone local to them.RFC 3864 describes registration procedures for message header fields at the IANA; it provides for permanent and provisional field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:\nTo: The email address(es), and optionally name(s) of the message's recipient(s). Indicates primary recipients (multiple allowed), for secondary recipients see Cc: and Bcc: below.\nSubject: A brief summary of the topic of the message. Certain abbreviations are commonly used in the subject, including \"RE:\" and \"FW:\".\nCc: Carbon copy; Many email clients mark email in one's inbox differently depending on whether they are in the To: or Cc: list.\nBcc: Blind carbon copy; addresses are usually only specified during SMTP delivery, and not usually listed in the message header.\nContent-Type: Information about how the message is to be displayed, usually a MIME type.\nPrecedence: commonly with values \"bulk\", \"junk\", or \"list\"; used to indicate automated \"vacation\" or \"out of office\" responses should not be returned for this mail, e.g. to prevent vacation notices from sent to all other subscribers of a mailing list. Sendmail uses this field to affect prioritization of queued email, with \"Precedence: special-delivery\" messages delivered sooner. With modern high-bandwidth networks, delivery priority is less of an issue than it was. Microsoft Exchange respects a fine-grained automatic response suppression mechanism, the X-Auto-Response-Suppress field.\nMessage-ID: Also an automatic-generated field to prevent multiple deliveries and for reference in In-Reply-To: (see below).\nIn-Reply-To: Message-ID of the message this is a reply to. Used to link related messages together. This field only applies to reply messages.\nReferences: Message-ID of the message this is a reply to, and the message-id of the message the previous reply was a reply to, etc.\nReply-To: Address should be used to reply to the message.\nSender: Address of the sender acting on behalf of the author listed in the From: field (secretary, list manager, etc.).\nArchived-At: A direct link to the archived form of an individual email message.The To: field may be unrelated to the addresses to which the message is delivered. The delivery list is supplied separately to the transport protocol, SMTP, which may be extracted from the header content. The \"To:\" field is similar to the addressing at the top of a conventional letter delivered according to the address on the outer envelope. In the same way, the \"From:\" field may not be the sender. Some mail servers apply email authentication systems to messages relayed. Data pertaining to the server's activity is also part of the header, as defined below.\nSMTP defines the trace information of a message saved in the header using the following two fields:\nReceived: after an SMTP server accepts a message, it inserts this trace record at the top of the header (last to first).\nReturn-Path: after the delivery SMTP server makes the final delivery of a message, it inserts this field at the top of the header.Other fields added on top of the header by the receiving server may be called trace fields.\nAuthentication-Results: after a server verifies authentication, it can save the results in this field for consumption by downstream agents.\nReceived-SPF: stores results of SPF checks in more detail than Authentication-Results.\nDKIM-Signature: stores results of DomainKeys Identified Mail (DKIM) decryption to verify the message was not changed after it was sent.\nAuto-Submitted: is used to mark automatic-generated messages.\nVBR-Info: claims VBR whitelisting","completion":"Computer Network"}
{"prompt":"Cyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.","completion":"Computer Network"}
{"prompt":"The Category 5e specification improves upon the Category 5 specification by further mitigating crosstalk. The bandwidth (100 MHz) and physical construction are the same between the two, and most Cat 5 cables actually happen to meet Cat 5e specifications even though they are not certified as such. Category 5 was deprecated in 2001 and superseded by the Category 5e specification.The Category 6 specification improves upon the Category 5e specification by extending frequency response and further reducing crosstalk. The improved performance of Cat 6 provides 250 MHz bandwidth. Category 6A cable provides 500 MHz bandwidth. Both variants are backward compatible with Category 5 and 5e cables.","completion":"Computer Network"}
{"prompt":"Cell relay systems break variable-length user packets into groups of fixed-length cells, that add addressing and verification information. Frame length is fixed in networking hardware, based on time delay and user packet-length considerations. One user data message may be segmented over many cells.\nCell relay systems may also carry bitstream-based data such as PDH traffic, by breaking it into streams of cells, with a lightweight synchronization and clock recovery shim. Thus cell relay systems may potentially carry any combination of stream-based and packet-based data. This is a form of statistical time division multiplexing.\nCell relay is an implementation of fast packet-switching technology that is used in connection-oriented broadband integrated services digital networks (B-ISDN, and its better-known supporting technology ATM) and connectionless IEEE 802.6 switched multi-megabit data service (SMDS).\nAt any time there is information to be transmitted; the switch basically sends the data units. Connections don't have to be negotiated like circuit switching. Channels don't have to be allocated because channels do not exist in ATM, and on condition that there is an adequate amount of bandwidth to maintain it, there can be indefinite transmissions over the same facility.\nCell relay utilizes data cells of a persistent size. Frames are comparable to data packets; however they contrast from cells in that they may fluctuate in size based on circumstances. This type of technology is not secure for the reason that its procedures do not support error handling or data recovery. Per se, all delicate and significant transmissions may perhaps be transported faster via fixed-sized cells, which are simpler to transmit compared to variable-sized frames or packets.","completion":"Computer Network"}
{"prompt":"A Trojan horse is designed to perform legitimate tasks but it also performs unknown and unwanted activity. It can be the basis of many viruses and worms installing onto the computer as keyboard loggers and backdoor software. In a commercial sense, Trojans can be imbedded in trial versions of software and can gather additional intelligence about the target without the person even knowing it happening. All three of these are likely to attack an individual and establishment through emails, web browsers, chat clients, remote software, and updates.","completion":"Computer Network"}
{"prompt":"The forwarding information base stored in content-addressable memory (CAM) is initially empty. For each received Ethernet frame the switch learns from the frame's source MAC address and adds this together with an interface identifier to the forwarding information base. The switch then forwards the frame to the interface found in the CAM based on the frame's destination MAC address. If the destination address is unknown the switch sends the frame out on all interfaces (except the ingress interface). This behavior is called unicast flooding.","completion":"Computer Network"}
{"prompt":"Internetworking requires sending data from the source network to the destination network. This process is called routing and is supported by host addressing and identification using the hierarchical IP addressing system. The internet layer provides an unreliable datagram transmission facility between hosts located on potentially different IP networks by forwarding datagrams to an appropriate next-hop router for further relaying to its destination. The internet layer has the responsibility of sending packets across potentially multiple networks. With this functionality, the internet layer makes possible internetworking, the interworking of different IP networks, and it essentially establishes the Internet.\nThe internet layer does not distinguish between the various transport layer protocols. IP carries data for a variety of different upper layer protocols. These protocols are each identified by a unique protocol number: for example, Internet Control Message Protocol (ICMP) and Internet Group Management Protocol (IGMP) are protocols 1 and 2, respectively.\nThe Internet Protocol is the principal component of the internet layer, and it defines two addressing systems to identify network hosts and to locate them on the network. The original address system of the ARPANET and its successor, the Internet, is Internet Protocol version 4 (IPv4). It uses a 32-bit IP address and is therefore capable of identifying approximately four billion hosts. This limitation was eliminated in 1998 by the standardization of Internet Protocol version 6 (IPv6) which uses 128-bit addresses. IPv6 production implementations emerged in approximately 2006.","completion":"Computer Network"}
{"prompt":"The central node communications processor was an HP 2100 minicomputer called the Menehune, which is the Hawaiian language word for dwarf people, and was named for its similar role to the original ARPANET Interface Message Processor (IMP) which was being deployed at about the same time. In the original system, the Menehune forwarded correctly received user data to the UH central computer, an IBM System 360\/65 time-sharing system. Outgoing messages from the 360 were converted into packets by the Menehune, which were queued and broadcast to the remote users at a data rate of 9600 bit\/s. Unlike the half-duplex radios at the user TCUs, the Menehune was interfaced to the radio channels with full-duplex radio equipment.The original user interface developed for the system was an all-hardware unit called an ALOHAnet Terminal Control Unit (TCU) and was the sole piece of equipment necessary to connect a terminal into the ALOHA channel. The TCU was composed of a UHF antenna, transceiver, modem, buffer and control unit. The buffer was designed for a full line length of 80 characters, which allowed handling of both the 40- and 80-character fixed-length packets defined for the system. The typical user terminal in the original system consisted of a Teletype Model 33 or a dumb CRT user terminal connected to the TCU using a standard RS-232 interface. Shortly after the original ALOHA network went into operation, the TCU was redesigned with one of the first Intel microprocessors, and the resulting upgrade was called a Programmable Control Unit (PCU).\nAdditional basic functions performed by the TCUs and PCUs were generation of a cyclic-parity-check code vector and decoding of received packets for packet error detection purposes, and generation of packet retransmissions using a simple random interval generator. If an acknowledgment was not received from the Menehune after the prescribed number of automatic retransmissions, a flashing light was used as an indicator to the human user. Also, since the TCUs and PCUs did not send acknowledgments to the Menehune, a steady warning light was displayed to the human user when an error was detected in a received packet. Considerable simplification was incorporated into the initial design of the TCU as well as the PCU for interfacing a human user into the network.\nIn later versions of the system, simple radio relays were placed in operation to connect the main network on the island of Oahu to other islands in Hawaii, and Menehune routing capabilities were expanded to allow user nodes to exchange packets with other user nodes, the ARPANET, and an experimental satellite network.","completion":"Computer Network"}
{"prompt":"A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.\nVPN may have best-effort performance or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider.","completion":"Computer Network"}
{"prompt":"A serial backbone is the simplest kind of backbone network.  Serial backbones consist of two or more internet working devices connected to each other by a single cable in a daisy-chain fashion. A daisy chain is a group of connectivity devices linked together in a serial fashion. Hubs are often connected in this way to extend a network. However, hubs are not the only device that can be connected in a serial backbone. Gateways, routers, switches and bridges more commonly form part of the backbone. The serial backbone topology could be used for enterprise-wide networks, though it is rarely implemented for that purpose.","completion":"Computer Network"}
{"prompt":"Licklider was born on March 11, 1915, in St. Louis, Missouri. He was the only child of Joseph Parron Licklider, a Baptist minister, and Margaret Robnett Licklider. Despite his father's religious background, he was not religious in later life.He studied at Washington University in St. Louis, where he received a B.A. with a triple major in physics, mathematics, and psychology in 1937 and an M.A. in psychology in 1938. He received a Ph.D. in psychoacoustics from the University of Rochester in 1942. Thereafter he worked at Harvard University as a research fellow and lecturer in the Psycho-Acoustic Laboratory from 1943 to 1950.\nHe became interested in information technology, and moved to MIT in 1950 as an associate professor, where he served on a committee that established MIT Lincoln Laboratory and a psychology program for engineering students. While at MIT, Licklider was involved in the SAGE project as head of the team concerned with human factors. In 1957, he received the Franklin V. Taylor Award from the Society of Engineering Psychologists. In 1958, he was elected President of the Acoustical Society of America, and in 1990 he received the Commonwealth Award for Distinguished Service.Licklider left MIT to become a vice president at Bolt Beranek and Newman in 1957. He learned about time-sharing from Christopher Strachey at a UNESCO-sponsored conference on Information Processing in Paris in 1959. At BBN he developed the BBN Time-Sharing System and conducted the first public demonstration of time-sharing.In October 1962, Licklider was appointed head of the Information Processing Techniques Office (IPTO) at ARPA, the United States Department of Defense Advanced Research Projects Agency, an appointment he kept through July 1964. In April 1963, he sent a memo to his colleagues in outlining the early challenges presented in establishing a time-sharing network of computers with the software of that time. Ultimately his vision led to ARPANet, the precursor of today's Internet.After serving as manager of information sciences, systems and applications at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York from 1964 to 1967, Licklider rejoined MIT as a professor of electrical engineering in 1968. During this period, he concurrently served as director of Project MAC until 1971. Project MAC had produced the first computer time-sharing system, CTSS, and one of the first online setups with the development of Multics (work on which commenced in 1964). Multics provided inspiration for some elements of the Unix operating system developed at Bell Labs by Ken Thompson and Dennis Ritchie in 1970.Following a second stint as IPTO director (1974\u20131975), his MIT faculty line was transferred to the Institute's Laboratory for Computer Science, where he was based for the remainder of his career. He was a founding member of Infocom in 1979, known for their interactive fiction computer games. He retired and became professor emeritus in 1985. He died in 1990 in Arlington, Massachusetts; his cremated remains are interred in Mount Auburn Cemetery.","completion":"Computer Network"}
{"prompt":"A public IP address is a globally routable unicast IP address, meaning that the address is not an address reserved for use in private networks, such as those reserved by RFC 1918, or the various IPv6 address formats of local scope or site-local scope, for example for link-local addressing. Public IP addresses may be used for communication between hosts on the global Internet.\nIn a home situation, a public IP address is the IP address assigned to the home's network by the ISP. In this case, it is also locally visible by logging into the router configuration.Most public IP addresses change, and relatively often. Any type of IP address that changes is called a dynamic IP address. In home networks, the ISP usually assigns a dynamic IP. If an ISP gave a home network an unchanging address, it's more likely to be abused by customers who host websites from home, or by hackers who can try the same IP address over and over until they breach a network.","completion":"Computer Network"}
{"prompt":"Autonegotiation is the procedure by which two connected devices choose common transmission parameters, e.g. speed and duplex mode. Autonegotiation was initially an optional feature, first introduced with 100BASE-TX, while it is also backward compatible with 10BASE-T. Autonegotiation is mandatory for 1000BASE-T and faster.","completion":"Computer Network"}
{"prompt":"A thick client, also known as a rich client or fat client, is a client that performs the bulk of any data processing operations itself, and does not necessarily rely on the server. The personal computer is a common example of a fat client, because of its relatively large set of features and capabilities and its light reliance upon a server. For example, a computer running an art program (such as Krita or Sketchup) that ultimately shares the result of its work on a network is a thick client. A computer that runs almost entirely as a standalone machine save to send or receive files via a network is by a standard called a workstation.","completion":"Computer Network"}
{"prompt":"An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.","completion":"Computer Network"}
{"prompt":"It has been alleged by groups such as the UK-based Open Rights Group that a lack of testing, inadequate audit procedures, and insufficient attention given to system or process design with electronic voting leaves \"elections open to error and fraud\".\nIn 2009, the Federal Constitutional Court of Germany found that when using voting machines the \"verification of the result must be possible by the citizen reliably and without any specialist knowledge of the subject.\" The DRE Nedap-computers used till then did not fulfill that requirement. The decision did not ban electronic voting as such, but requires all essential steps in elections to be subject to public examinability.In 2013, The California Association of Voting Officials was formed to maintain efforts toward publicly owned General Public License open source voting systems","completion":"Computer Network"}
{"prompt":"Remote work is facilitated by tools such as groupware, virtual private networks, conference calling, videotelephony, and VoIP so that work may be performed from any location, most conveniently the worker's home. It can be efficient and useful for companies as it allows workers to communicate over long distances, saving significant amounts of travel time and cost. More workers have adequate bandwidth at home to use these tools to link their home to their corporate intranet and internal communication networks.","completion":"Computer Network"}
{"prompt":"The IETF defines the Assured Forwarding (AF) behavior in RFC 2597 and RFC 3260.  Assured forwarding allows the operator to provide assurance of delivery as long as the traffic does not exceed some subscribed rate.  Traffic that exceeds the subscription rate faces a higher probability of being dropped if congestion occurs.\nThe AF behavior group defines four separate AF classes with all traffic within one class having the same priority.  Within each class, packets are given a drop precedence (high, medium or low, where higher precedence means more dropping). The combination of classes and drop precedence yields twelve separate DSCP encodings from AF11 through AF43 (see table).\n\nSome measure of priority and proportional fairness is defined between traffic in different classes. Should congestion occur between classes, the traffic in the higher class is given priority. Rather than using strict priority queuing, more balanced queue servicing algorithms such as fair queuing or weighted fair queuing are likely to be used. If congestion occurs within a class, the packets with the higher drop precedence are discarded first. To prevent issues associated with tail drop, more sophisticated drop selection algorithms such as random early detection are often used.","completion":"Computer Network"}
{"prompt":"Network congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. When networks are congested and queues become too full, packets have to be discarded, and so networks rely on re-transmission. Typical effects of congestion include queueing delay, packet loss or the blocking of new connections.  A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in the network throughput or to a reduction in network throughput.\nNetwork protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion\u2014even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.\nModern networks use congestion control, congestion avoidance and traffic control techniques to try to avoid congestion collapse (i.e. endpoints typically slow down or sometimes even stop transmission entirely when the network is congested). These techniques include: exponential backoff in protocols such as 802.11's CSMA\/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit\/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables).\nFor the Internet, RFC 2914 addresses the subject of congestion control in detail.","completion":"Computer Network"}
{"prompt":"Digital libraries offer a variety of software packages, including those tailored for kids' educational games (for notable ones see Category:Digital library software). Institutional repository software, which focuses primarily on ingest, preservation and access of locally produced documents, particularly locally produced academic outputs, can be found in Institutional repository software. This software may be proprietary, as is the case with the Library of Congress which uses Digiboard and CTS to manage digital content.The design and implementation in digital libraries are constructed so computer systems and software can make use of the information when it is exchanged. These are referred to as semantic digital libraries. Semantic libraries are also used to socialize with different communities from a mass of social networks. DjDL is a type of semantic digital library. Keywords-based and semantic search are the two main types of searches. A tool is provided in the semantic search that create a group for augmentation and refinement for keywords-based search. Conceptual knowledge used in DjDL is centered around two forms; the subject ontology and the set of concept search patterns based on the ontology. The three type of ontologies that are associated to this search are bibliographic ontologies, community-aware ontologies, and subject ontologies.","completion":"Computer Network"}
{"prompt":"Many people use the World Wide Web to access news, weather and sports reports, to plan and book vacations and to pursue their personal interests. People use chat, messaging and email to make and stay in touch with friends worldwide, sometimes in the same way as some previously had pen pals. Social networking services such as Facebook have created new ways to socialize and interact. Users of these sites are able to add a wide variety of information to pages, pursue common interests, and connect with others. It is also possible to find existing acquaintances, to allow communication among existing groups of people. Sites like LinkedIn foster commercial and business connections. YouTube and Flickr specialize in users' videos and photographs. Social networking services are also widely used by businesses and other organizations to promote their brands, to market to their customers and to encourage posts to \"go viral\". \"Black hat\" social media techniques are also employed by some organizations, such as spam accounts and astroturfing.\nA risk for both individuals and organizations writing posts (especially public posts) on social networking services, is that especially foolish or controversial posts occasionally lead to an unexpected and possibly large-scale backlash on social media from other Internet users. This is also a risk in relation to controversial offline behavior, if it is widely made known. The nature of this backlash can range widely from counter-arguments and public mockery, through insults and hate speech, to, in extreme cases, rape and death threats. The online disinhibition effect describes the tendency of many individuals to behave more stridently or offensively online than they would in person. A significant number of feminist women have been the target of various forms of harassment in response to posts they have made on social media, and Twitter in particular has been criticised in the past for not doing enough to aid victims of online abuse.For organizations, such a backlash can cause overall brand damage, especially if reported by the media. However, this is not always the case, as any brand damage in the eyes of people with an opposing opinion to that presented by the organization could sometimes be outweighed by strengthening the brand in the eyes of others. Furthermore, if an organization or individual gives in to demands that others perceive as wrong-headed, that can then provoke a counter-backlash.\nSome websites, such as Reddit, have rules forbidding the posting of personal information of individuals (also known as doxxing), due to concerns about such postings leading to mobs of large numbers of Internet users directing harassment at the specific individuals thereby identified. In particular, the Reddit rule forbidding the posting of personal information is widely understood to imply that all identifying photos and names must be censored in Facebook screenshots posted to Reddit. However, the interpretation of this rule in relation to public Twitter posts is less clear, and in any case, like-minded people online have many other ways they can use to direct each other's attention to public social media posts they disagree with.\nChildren also face dangers online such as cyberbullying and approaches by sexual predators, who sometimes pose as children themselves. Children may also encounter material that they may find upsetting, or material that their parents consider to be not age-appropriate. Due to naivety, they may also post personal information about themselves online, which could put them or their families at risk unless warned not to do so. Many parents choose to enable Internet filtering or supervise their children's online activities in an attempt to protect their children from inappropriate material on the Internet. The most popular social networking services, such as Facebook and Twitter, commonly forbid users under the age of 13. However, these policies are typically trivial to circumvent by registering an account with a false birth date, and a significant number of children aged under 13 join such sites anyway. Social networking services for younger children, which claim to provide better levels of protection for children, also exist.The Internet has been a major outlet for leisure activity since its inception, with entertaining social experiments such as MUDs and MOOs being conducted on university servers, and humor-related Usenet groups receiving much traffic. Many Internet forums have sections devoted to games and funny videos. The Internet pornography and online gambling industries have taken advantage of the World Wide Web. Although many governments have attempted to restrict both industries' use of the Internet, in general, this has failed to stop their widespread popularity.Another area of leisure activity on the Internet is multiplayer gaming. This form of recreation creates communities, where people of all ages and origins enjoy the fast-paced world of multiplayer games. These range from MMORPG to first-person shooters, from role-playing video games to online gambling. While online gaming has been around since the 1970s, modern modes of online gaming began with subscription services such as GameSpy and MPlayer. Non-subscribers were limited to certain types of game play or certain games. Many people use the Internet to access and download music, movies and other works for their enjoyment and relaxation. Free and fee-based services exist for all of these activities, using centralized servers and distributed peer-to-peer technologies. Some of these sources exercise more care with respect to the original artists' copyrights than others.\nInternet usage has been correlated to users' loneliness. Lonely people tend to use the Internet as an outlet for their feelings and to share their stories with others, such as in the \"I am lonely will anyone speak to me\" thread.\nA 2017 book claimed that the Internet consolidates most aspects of human endeavor into singular arenas of which all of humanity are potential members and competitors, with fundamentally negative impacts on mental health as a result. While successes in each field of activity are pervasively visible and trumpeted, they are reserved for an extremely thin sliver of the world's most exceptional, leaving everyone else behind. Whereas, before the Internet, expectations of success in any field were supported by reasonable probabilities of achievement at the village, suburb, city or even state level, the same expectations in the Internet world are virtually certain to bring disappointment today: there is always someone else, somewhere on the planet, who can do better and take the now one-and-only top spot.Cybersectarianism is a new organizational form that involves, \"highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy, while still linked remotely to a larger network of believers who share a set of practices and texts, and often a common devotion to a particular leader. Overseas supporters provide funding and support; domestic practitioners distribute tracts, participate in acts of resistance, and share information on the internal situation with outsiders. Collectively, members and practitioners of such sects construct viable virtual communities of faith, exchanging personal testimonies and engaging in the collective study via email, online chat rooms, and web-based message boards.\" In particular, the British government has raised concerns about the prospect of young British Muslims being indoctrinated into Islamic extremism by material on the Internet, being persuaded to join terrorist groups such as the so-called \"Islamic State\", and then potentially committing acts of terrorism on returning to Britain after fighting in Syria or Iraq.\nCyberslacking can become a drain on corporate resources; the average UK employee spent 57 minutes a day surfing the Web while at work, according to a 2003 study by Peninsula Business Services. Internet addiction disorder is excessive computer use that interferes with daily life. Nicholas G. Carr believes that Internet use has other effects on individuals, for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity.","completion":"Computer Network"}
{"prompt":"Most social, biological, and technological networks display substantial non-trivial topological features, with patterns of connection between their elements that are neither purely regular nor purely random. Such features include a heavy tail in the degree distribution, a high clustering coefficient, assortativity or disassortativity among vertices, community structure, and hierarchical structure. In the case of directed networks these features also include reciprocity, triad significance profile and other features. In contrast, many of the mathematical models of networks that have been studied in the past, such as lattices and random graphs, do not show these features. The most complex structures can be realized by networks with a medium number of interactions. This corresponds to the fact that the maximum information content (entropy) is obtained for medium probabilities.\nTwo well-known and much studied classes of complex networks are scale-free networks and small-world networks, whose discovery and definition are canonical case-studies in the field. Both are characterized by specific structural features\u2014power-law degree distributions for the former and short path lengths and high clustering for the latter. However, as the study of complex networks has continued to grow in importance and popularity, many other aspects of network structures have attracted attention as well.\nThe field continues to develop at a brisk pace, and has brought together researchers from many areas including mathematics, physics, electric power systems, biology, climate, computer science, sociology, epidemiology, and others. Ideas and tools from network science and engineering have been applied to the analysis of metabolic and genetic regulatory networks; the study of ecosystem stability and robustness; clinical science; the modeling and design of scalable communication networks such as the generation and visualization of complex wireless networks;  and a broad range of other practical issues.  Network science is the topic of many conferences in a variety of different fields, and has been the subject of numerous books both for the lay person and for the expert.","completion":"Computer Network"}
{"prompt":"Europe is a major contributor to the growth of the international backbone as well as a contributor to the growth of Internet bandwidth. In 2003, Europe was credited with 82 percent of the world's international cross-border bandwidth. The company Level 3 Communications began to launch a line of dedicated Internet access and virtual private network services in 2011, giving large companies direct access to the tier 3 backbone. Connecting companies directly to the backbone will provide enterprises faster Internet service which meets a large market demand.","completion":"Computer Network"}
{"prompt":"In 1965, Davies developed the idea of packet switching, dividing computer messages into packets that are routed independently across a network, possibly via differing routes, and are reassembled at the destination.\nDavies used the word \"packets\" after consulting with a linguist because it was capable of being translated into languages other than English without compromise. Davies' key insight came in the realisation that computer network traffic was inherently \"bursty\" with periods of silence, compared with relatively constant telephone traffic. He designed and proposed a commercial national data network based on packet switching in his 1966 Proposal for the Development of a National Communications Service for On-line Data Processing.In 1966 he returned to the NPL at Teddington just outside London, where he headed and transformed its computing activity. He became interested in data communications following a visit to the Massachusetts Institute of Technology, where he saw that a significant problem with the new time-sharing computer systems was the cost of keeping a phone connection open for each user. Davies was the first to describe the concept of an \"Interface computer\", in 1966, today known as a router. He and his team were one of the first to use the term 'protocol' in a data-commutation context in 1967. The NPL team also carried out simulation work on packet networks, including datagram networks.His work on packet switching, presented by his colleague Roger Scantlebury, initially caught the attention of the developers of ARPANET, a US defence network, at the Symposium on Operating Systems Principles in October 1967. In Scantlebury's report following the conference, he noted \"It would appear that the ideas in the NPL paper at the moment are more advanced than any proposed in the USA\". Larry Roberts of the Advanced Research Projects Agency in the United States applied Davies' concepts of packet switching in the late 1960s for the ARPANET, which went on to become a predecessor to the Internet. These early years of computer resource sharing were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.\nDavies first presented his own ideas on packet switching at a conference in Edinburgh on 5 August 1968. At NPL Davies directed the development of a local-area packet-switched network, the Mark I NPL network. It was replaced with the Mark II in 1973, and remained in operation until 1986, influencing other research in the UK and Europe, including Louis Pouzin's CYCLADES project in France.\nUnbeknown to him, Paul Baran of the RAND Corporation in the United States was also working on a similar concept; when Baran became aware of Davies's work he acknowledged that they both had equally discovered the concept. Baran was happy to acknowledge that Davies had come up with the same idea as him independently. In an e-mail to Davies, he wrote You and I share a common view of what packet switching is all about, since you and I independently came up with the same ingredients.\nLeonard Kleinrock, a contemporary working on analysing message flow using queueing theory, developed a theoretical basis for the operation of message switching networks in his PhD thesis during 1961-2, published as a book in 1964. However, Kleinrock's later claim to have developed the theoretical basis of packet switching networks is disputed by other Internet pioneers, including by Robert Taylor, Baran and Davies. Davies and Baran are recognized by historians and the U.S. National Inventors Hall of Fame for independently inventing the concept of digital packet switching used in modern computer networking including the Internet.","completion":"Computer Network"}
{"prompt":"The most prominent component of the Internet model is the Internet Protocol (IP). IP enables internetworking and, in essence, establishes the Internet itself. Two versions of the Internet Protocol exist, IPv4 and IPv6.","completion":"Computer Network"}
{"prompt":"Smart speakers\nTelevision: Some new TVs and DVRs include integrated WiFi connectivity which allows the user to access services such as Netflix and YouTube.\nHome audio: Digital audio players, and stereo systems with network connectivity can allow a user to easily access their music library, often using Bonjour to discover and interface with an instance of iTunes running on a remote PC.\nGaming: Video game consoles rely on connectivity to the home network to enable a significant portion of their overall features, such as the multiplayer in games, social network integration, ability to purchase or demo new games, and receive software updates. Recent consoles have begun more aggressively pursuing the role of the sole entertainment and media hub of the home.\nDLNA is a common protocol used for interoperability between networked media-centric devices in the home.Some older entertainment devices may not feature the appropriate network interfaces required for home network connectivity. In some situations, USB dongles and PCI Network Interface Cards are available as accessories that enable this functionality.","completion":"Computer Network"}
{"prompt":"IEEE 802.11af, also referred to as \"White-Fi\" and \"Super Wi-Fi\", is an amendment, approved in February 2014, that allows WLAN operation in TV white space spectrum in the VHF and UHF bands between 54 and 790 MHz. It uses cognitive radio technology to transmit on unused TV channels, with the standard taking measures to limit interference for primary users, such as analog TV, digital TV, and wireless microphones. Access points and stations determine their position using a satellite positioning system such as GPS, and use the Internet to query a geolocation database (GDB) provided by a regional regulatory agency to discover what frequency channels are available for use at a given time and position. The physical layer uses OFDM and is based on 802.11ac. The propagation path loss as well as the attenuation by materials such as brick and concrete is lower in the UHF and VHF bands than in the 2.4 GHz and 5 GHz bands, which increases the possible range. The frequency channels are 6 to 8 MHz wide, depending on the regulatory domain. Up to four channels may be bonded in either one or two contiguous blocks. MIMO operation is possible with up to four streams used for either space\u2013time block code (STBC) or multi-user (MU) operation. The achievable data rate per spatial stream is 26.7 Mbit\/s for 6 and 7 MHz channels, and 35.6 Mbit\/s for 8 MHz channels. With four spatial streams and four bonded channels, the maximum data rate is 426.7 Mbit\/s for 6 and 7 MHz channels and 568.9 Mbit\/s for 8 MHz channels.","completion":"Computer Network"}
{"prompt":"Electronic voting systems may use electronic ballot to store votes in computer memory. Systems which use them exclusively are called DRE voting systems. When electronic ballots are used there is no risk of exhausting the supply of ballots.  Additionally, these electronic ballots remove the need for printing of paper ballots, a significant cost. When administering elections in which ballots are offered in multiple languages (in some areas of the United States, public elections are required by the National Voting Rights Act of 1965), electronic ballots can be programmed to provide ballots in multiple languages for a single machine.  The advantage with respect to ballots in different languages appears to be unique to electronic voting. For example, King County, Washington's demographics require them under U.S. federal election law to provide ballot access in Chinese. With any type of paper ballot, the county has to decide how many Chinese-language ballots to print, how many to make available at each polling place, etc.  Any strategy that can assure that Chinese-language ballots will be available at all polling places is certain, at the very least, to result in a significant number of wasted ballots. (The situation with lever machines would be even worse than with paper: the only apparent way to reliably meet the need would be to set up a Chinese-language lever machine at each polling place, few of which would be used at all.)\nCritics argue the need for extra ballots in any language can be mitigated by providing a process to print ballots at voting locations.  They argue further, the cost of software validation, compiler trust validation, installation validation, delivery validation and validation of other steps related to electronic voting is complex and expensive, thus electronic ballots are not guaranteed to be less costly than printed ballots.","completion":"Computer Network"}
{"prompt":"The InterPlanetary Internet Special Interest Group of the Internet Society has worked on defining protocols and standards that would make the IPN possible. The Delay-Tolerant Networking Research Group (DTNRG) is the primary group researching Delay-tolerant networking (DTN). Additional research efforts focus on various uses of the new technology.The canceled Mars Telecommunications Orbiter had been planned to establish an Interplanetary Internet link between Earth and Mars, in order to support other Mars missions. Rather than using RF, it would have used optical communications using laser beams for their higher data rates. \"Lasercom sends information using beams of light and optical elements, such as telescopes and optical amplifiers, rather than RF signals, amplifiers,  and antennas\"NASA JPL tested the DTN protocol with their Deep Impact Networking (DINET) experiment on board the Deep Impact\/EPOXI spacecraft in October, 2008.In May 2009, DTN was deployed to a payload on board the ISS.  NASA and BioServe Space Technologies, a research group at the University of Colorado, have been continuously testing DTN on two Commercial Generic Bioprocessing Apparatus (CGBA) payloads. CGBA-4 and CGBA-5 serve as computational and communications platforms which are remotely controlled from BioServe's Payload Operations Control Center (POCC) in Boulder, CO. In October 2012 ISS Station commander Sunita Williams remotely operated Mocup (Meteron Operations and Communications Prototype), a \"cat-sized\" Lego Mindstorms robot fitted with a BeagleBoard computer and webcam, located in the European Space Operations Centre in Germany in an experiment using DTN. These initial experiments provide insight into future missions where DTN will enable the extension of networks into deep space to explore other planets and solar system points of interest. Seen as necessary for space exploration, DTN enables timeliness of data return from operating assets which results in reduced risk and cost, increased crew safety, and improved operational awareness and science return for NASA and additional space agencies.DTN has several major arenas of application, in addition to the Interplanetary Internet, which include sensor networks, military and tactical communications, disaster recovery, hostile environments, mobile devices and remote outposts. As an example of a remote outpost, imagine an isolated Arctic village, or a faraway island, with electricity, one or more computers, but no communication connectivity. With the addition of a simple wireless hotspot in the village, plus DTN-enabled devices on, say, dog sleds or fishing boats, a resident would be able to check their e-mail or click on a Wikipedia article, and have their requests forwarded to the nearest networked location on the sled's or boat's next visit, and get the replies on its return.","completion":"Computer Network"}
{"prompt":"CSMA\/CA can optionally be supplemented by the exchange of a Request to Send (RTS) packet sent by the sender S, and a Clear to Send (CTS) packet sent by the intended receiver R. Thus alerting all nodes within range of the sender, receiver or both, to not transmit for the duration of the main transmission. This is known as the IEEE 802.11 RTS\/CTS exchange. Implementation of RTS\/CTS helps to partially solve the hidden node problem that is often found in wireless networking.","completion":"Computer Network"}
{"prompt":"Most Wi-Fi networks are deployed in infrastructure mode. In infrastructure mode, wireless clients, such as laptops and smartphones, connect to the WAP to join the network. The WAP usually has a wired network connection and may have permanent wireless connections to other WAPs.\nWAPs are usually fixed and provide service to their client nodes within range. Some networks will have multiple WAPs using the same SSID and security arrangement. In that case, connecting to any WAP on that network joins the client to the network, and the client software will try to choose the WAP that gives the best service, such as the WAP with the strongest signal.","completion":"Computer Network"}
{"prompt":"The Purdy Polynomial hash algorithm was developed for the ARPANET to protect passwords in 1971 at the request of Larry Roberts, head of ARPA at that time. It computed a polynomial of degree 224 + 17 modulo the 64-bit prime p = 264 \u2212 59. The algorithm was later used by Digital Equipment Corporation (DEC) to hash passwords in the VMS operating system and is still being used for this purpose.","completion":"Computer Network"}
{"prompt":"After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting-edge research and development, not running a communications utility. In July 1975, the network was turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.\nThe networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, which began to form networks of fiber optic lines. A growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were. Data transmission speeds depended upon the type of connection, the slowest being analog telephone lines and the fastest using optical networking technology.\nSeveral other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP\/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.\n\nNASA developed the TCP\/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP\/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.\nIn 1981, NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP\/IP, and ran TCP\/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange. CSNET played a central role in popularizing the Internet outside the ARPANET.In 1986, the NSF created NSFNET, a 56 kbit\/s backbone to support the NSF-sponsored supercomputing centers. The NSFNET also provided support for the creation of regional research and education networks in the United States, and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit\/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit\/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990.\nNSFNET was expanded and upgraded to dedicated fiber, optical lasers and optical amplifier systems capable of delivering T3 start up speeds or 45 Mbit\/s in 1991. However, the T3 transition by MCI took longer than expected, allowing Sprint to establish a coast-to-coast long-distance commercial Internet service. When NSFNET was decommissioned in 1995, its optical networking backbones were handed off to several commercial Internet service providers, including MCI, PSI Net and Sprint. As a result, when the handoff was complete, Sprint and its Washington DC Network Access Points began to carry Internet traffic, and by 1996, Sprint was the world's largest carrier of Internet traffic.The research and academic community continues to develop and use advanced networks such as Internet2 in the United States and JANET in the United Kingdom.","completion":"Computer Network"}
{"prompt":"A network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.\nThe NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.\nIn Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address\u2014usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.","completion":"Computer Network"}
{"prompt":"Independence\nComputer viruses generally require a host program. The virus writes its own code into the host program. When the program runs, the written virus program is executed first, causing infection and damage. A worm does not need a host program, as it is an independent program or code chunk. Therefore, it is not restricted by the host program, but can run independently and actively carry out attacks.Exploit attacks\nBecause a worm is not limited by the host program, worms can take advantage of various operating system vulnerabilities to carry out active attacks. For example, the \"Nimda\" virus exploits vulnerabilities to attack.\nComplexity\nSome worms are combined with web page scripts, and are hidden in HTML pages using VBScript, ActiveX and other technologies. When a user accesses a webpage containing a virus, the virus automatically resides in memory and waits to be triggered. There are also some worms that are combined with backdoor programs or Trojan horses, such as \"Code Red\".Contagiousness\nWorms are more infectious than traditional viruses. They not only infect local computers, but also all servers and clients on the network based on the local computer. Worms can easily spread through shared folders, e-mails, malicious web pages, and servers with a large number of vulnerabilities in the network.","completion":"Computer Network"}
{"prompt":"The main reason terrestrial communications have been limited to non-commercial telecommunications functions is fog. Fog consistently keeps FSO laser links over 500 metres (1,600 ft) from achieving a year-round bit error rate of 1 per 100,000. Several entities are continually attempting to overcome these key disadvantages to FSO communications and field a system with a better quality of service. DARPA has sponsored over US$130 million in research toward this effort, with the ORCA and ORCLE programs.Other non-government groups are fielding tests to evaluate different technologies that some claim have the ability to address key FSO adoption challenges. As of October 2014, none have fielded a working system that addresses the most common atmospheric events.\nFSO research from 1998\u20132006 in the private sector totaled $407.1 million, divided primarily among four start-up companies. All four failed to deliver products that would meet telecommunications quality and distance standards:\nTerabeam received approximately $575 million in funding from investors such as Softbank, Mobius Venture Capital and Oakhill Venture Partners. AT&T and Lucent backed this attempt. The work ultimately failed, and the company was purchased in 2004 for $52 million (excluding warrants and options) by Falls Church, Va.-based YDI, effective June 22, 2004, and used the name Terabeam for the new entity. On September 4, 2007, Terabeam (then headquartered in San Jose, California) announced it would change its name to Proxim Wireless Corporation, and change its NASDAQ stock symbol from TRBM to PRXM.\nAirFiber received $96.1 million in funding, and never solved the weather issue. They sold out to MRV communications in 2003, and MRV sold their FSO units until 2012 when the end-of-life was abruptly announced for the Terescope series.\nLightPointe Communications received $76 million in start-up funds, and eventually reorganized to sell hybrid FSO-RF units to overcome the weather-based challenges.\nThe Maxima Corporation published its operating theory in Science, and received $9 million in funding before permanently shutting down. No known spin-off or purchase followed this effort.\nWireless Excellence developed and launched CableFree UNITY solutions that combine FSO with millimeter wave and radio technologies to extend distance, capacity and availability, with a goal of making FSO a more useful and practical technology.One private company published a paper on November 20, 2014, claiming they had achieved commercial reliability (99.999% availability) in extreme fog. There is no indication this product is currently commercially available.","completion":"Computer Network"}
{"prompt":"During the 2021 NSW Local Government Elections the online voting system \"iVote\" had technical issues that caused some access problems for some voters. Analysis done of these failures indicated a significant chance of the outages having impacted on the electoral results for the final positions. In the Kempsey ward, where the margin between the last elected and first non-elected candidates was only 69 votes, the electoral commission determined that the outage caused a 60% chance that the wrong final candidate was elected. Singleton had a 40% chance of having elected the wrong councillor, Shellharbour was a 7% chance and two other races were impacted by a sub-1% chance of having elected the wrong candidate. The NSW Supreme Court ordered the elections in Kempsey, Singleton and Shellharbour Ward A to be re-run. In the 2022 Kempsey re-vote the highest placed non-elected candidate from 2021, Dean Saul, was instead one the first councillors elected. This failure caused the NSW Government to suspend the iVote system from use in the 2023 New South Wales state election.","completion":"Computer Network"}
{"prompt":"Computer networks extend interpersonal communications by electronic means with various technologies, such as email, instant messaging, online chat, voice and video telephone calls, and video conferencing. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer or use of a shared storage device. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. Distributed computing uses computing resources across a network to accomplish tasks.","completion":"Computer Network"}
{"prompt":"Many academic libraries are actively involved in building repositories of their institution's books, papers, theses, and other works that can be digitized or were 'born digital'. Many of these repositories are made available to the general public with few restrictions, in accordance with the goals of open access, in contrast to the publication of research in commercial journals, where the publishers usually limit access rights. Irrespective of access rights, institutional, truly free, and corporate repositories can be referred to as digital libraries. Institutional repository software is designed for archiving, organizing, and searching a library's content. Popular open-source solutions include DSpace, Greenstone Digital Library (GSDL), EPrints, Digital Commons, and the Fedora Commons-based systems Islandora and Samvera.","completion":"Computer Network"}
{"prompt":"IEEE 802.11aj is a derivative of 802.11ad for use in the 45 GHz unlicensed spectrum available in some regions of the world (specifically China); it also provides additional capabilities for use in the 60 GHz band.Alternatively known as China Millimeter Wave (CMMW).","completion":"Computer Network"}
{"prompt":"Ethernet has evolved to include higher bandwidth, improved medium access control methods, and different physical media. The multidrop coaxial cable was replaced with physical point-to-point links connected by Ethernet repeaters or switches.Ethernet stations communicate by sending each other data packets: blocks of data individually sent and delivered. As with other IEEE 802 LANs, adapters come programmed with globally unique 48-bit MAC address so that each Ethernet station has a unique address. The MAC addresses are used to specify both the destination and the source of each data packet. Ethernet establishes link-level connections, which can be defined using both the destination and source addresses. On reception of a transmission, the receiver uses the destination address to determine whether the transmission is relevant to the station or should be ignored. A network interface normally does not accept packets addressed to other Ethernet stations.An EtherType field in each frame is used by the operating system on the receiving station to select the appropriate protocol module (e.g., an Internet Protocol version such as IPv4). Ethernet frames are said to be self-identifying, because of the EtherType field. Self-identifying frames make it possible to intermix multiple protocols on the same physical network and allow a single computer to use multiple protocols together. Despite the evolution of Ethernet technology, all generations of Ethernet (excluding early experimental versions) use the same frame formats. Mixed-speed networks can be built using Ethernet switches and repeaters supporting the desired Ethernet variants.Due to the ubiquity of Ethernet, and the ever-decreasing cost of the hardware needed to support it, by 2004 most manufacturers built Ethernet interfaces directly into PC motherboards, eliminating the need for a separate network card.","completion":"Computer Network"}
{"prompt":"Various satellite constellations that are intended to provide global broadband coverage, such as SpaceX Starlink, employ laser communication for inter-satellite links. This effectively creates a space-based optical mesh network between the satellites.","completion":"Computer Network"}
{"prompt":"Within cyber warfare, the individual must recognize the state actors involved in committing these cyberattacks against one another. The two predominant players that will be discussed is the age-old comparison of East versus West, China's cyber capabilities compared to United States' capabilities. There are many other state and non-state actors involved in cyber warfare, such as Russia, Iran, Iraq, and Al Qaeda; since China and the U.S. are leading the foreground in cyber warfare capabilities, they will be the only two states actors discussed.\nBut in Q2 2013, Akamai Technologies reported that Indonesia toppled China with a portion 38 percent of cyber attacks, a high increase from the 21 percent portion in the previous quarter. China set 33 percent and the US set at 6.9 percent. 79 percent of attacks came from the Asia Pacific region. Indonesia dominated the attacking to ports 80 and 443 by about 90 percent.","completion":"Computer Network"}
{"prompt":"Paper-based voting systems originated as a system where votes are cast and counted by hand, using paper ballots. With the advent of electronic tabulation came systems where paper cards or sheets could be marked by hand, but counted electronically. These systems included punched card voting, marksense and later digital pen voting systems.These systems can include a ballot marking device or electronic ballot marker that allows voters to make their selections using an electronic input device, usually a touch screen system similar to a DRE. Systems including a ballot marking device can incorporate different forms of assistive technology. In 2004, Open Voting Consortium demonstrated the 'Dechert Design', a General Public License open source paper ballot printing system with open source bar codes on each ballot.","completion":"Computer Network"}
{"prompt":"In a partially connected mesh topology, there are at least two nodes with two or more paths between them to provide redundant paths in case the link providing one of the paths fails. Decentralization is often used to compensate for the single-point-failure disadvantage that is present when using a single device as a central node (e.g., in star and tree networks). A special kind of mesh, limiting the number of hops between two nodes, is a hypercube. The number of arbitrary forks in mesh networks makes them more difficult to design and implement, but their decentralized nature makes them very useful.\nThis is similar in some ways to a grid network, where a linear or ring topology is used to connect systems in multiple directions.  A multidimensional ring has a toroidal topology, for instance.\nA fully connected network, complete topology, or full mesh topology is a network topology in which there is a direct link between all pairs of nodes. In a fully connected network with n nodes, there are \n  \n    \n      \n        \n          \n            \n              n\n              (\n              n\n              \u2212\n              1\n              )\n            \n            2\n          \n        \n        \n      \n    \n    {\\displaystyle {\\frac {n(n-1)}{2}}\\,}\n   direct links. Networks designed with this topology are usually very expensive to set up, but provide a high degree of reliability due to the multiple paths for data that are provided by the large number of redundant links between nodes. This topology is mostly seen in military applications.","completion":"Computer Network"}
{"prompt":"Exponential backoff is commonly utilised as part of rate limiting mechanisms in computer systems such as web services, to help enforce fair distribution of access to resources and prevent network congestion. Each time a service informs a client that it is sending requests too frequently, the client reduces its rate by some predetermined factor, until the client's request rate reaches an acceptable equilibrium. The service may enforce rate limiting by refusing to respond to requests when the client is sending them too frequently, so that misbehaving clients are not allowed to exceed their allotted resources.\nA benefit of utilising an exponential backoff algorithm, over of a fixed rate limit, is that rate limits can be achieved dynamically without providing any prior information to the client. In the event that resources are unexpectedly constrained, e.g. due to heavy load or a service disruption, backoff requests and error responses from the service can automatically decrease the request rate from clients. This can help maintain some level of availability rather than overloading the service. In addition, quality of service can be prioritised to certain clients based on their individual importance, e.g. by reducing the backoff for emergency calls on a telephone network during periods of high load.\nIn a simple version of the algorithm, messages are delayed by predetermined (non-random) time. For example, in SIP protocol over unreliable transport (such as UDP) the client retransmits requests at an interval that starts at T1 seconds (usually, 500 ms, which is the estimate of the round-trip time) and doubles after every retransmission until it reaches T2 seconds (which defaults to 4 s). This results in retransmission intervals of 500 ms, 1 s, 2 s, 4 s, 4 s, 4 s, etc.","completion":"Computer Network"}
{"prompt":"The Internet is a global network that comprises many voluntarily interconnected autonomous networks. It operates without a central governing body. The technical underpinning and standardization of the core protocols (IPv4 and IPv6) is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. To maintain interoperability, the principal name spaces of the Internet are administered by the Internet Corporation for Assigned Names and Numbers (ICANN). ICANN is governed by an international board of directors drawn from across the Internet technical, business, academic, and other non-commercial communities. ICANN coordinates the assignment of unique identifiers for use on the Internet, including domain names, IP addresses, application port numbers in the transport protocols, and many other parameters. Globally unified name spaces are essential for maintaining the global reach of the Internet. This role of ICANN distinguishes it as perhaps the only central coordinating body for the global Internet.Regional Internet registries (RIRs) were established for five regions of the world. The African Network Information Center (AfriNIC) for Africa, the American Registry for Internet Numbers (ARIN) for North America, the Asia-Pacific Network Information Centre (APNIC) for Asia and the Pacific region, the Latin American and Caribbean Internet Addresses Registry (LACNIC) for Latin America and the Caribbean region, and the R\u00e9seaux IP Europ\u00e9ens \u2013 Network Coordination Centre (RIPE NCC) for Europe, the Middle East, and Central Asia were delegated to assign IP address blocks and other Internet parameters to local registries, such as Internet service providers, from a designated pool of addresses set aside for each region.\nThe National Telecommunications and Information Administration, an agency of the United States Department of Commerce, had final approval over changes to the DNS root zone until the IANA stewardship transition on 1 October 2016. The Internet Society (ISOC) was founded in 1992 with a mission to \"assure the open development, evolution and use of the Internet for the benefit of all people throughout the world\". Its members include individuals (anyone may join) as well as corporations, organizations, governments, and universities. Among other activities ISOC provides an administrative home for a number of less formally organized groups that are involved in developing and managing the Internet, including: the IETF, Internet Architecture Board (IAB), Internet Engineering Steering Group (IESG), Internet Research Task Force (IRTF), and Internet Research Steering Group (IRSG). On 16 November 2005, the United Nations-sponsored World Summit on the Information Society in Tunis established the Internet Governance Forum (IGF) to discuss Internet-related issues.","completion":"Computer Network"}
{"prompt":"The infrastructure for telephone systems at the time was based on circuit switching, which requires pre-allocation of a dedicated communication line for the duration of the call. Telegram services had developed store and forward telecommunication techniques. Western Union's Automatic Telegraph Switching System Plan 55-A was based on message switching. The U.S. military's AUTODIN network became operational in 1962. These systems, like SAGE and SBRE, still required rigid routing structures that were prone to single point of failure.\nThe technology was considered vulnerable for strategic and military use because there were no alternative paths for the communication in case of a broken link. In the early 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information would be transmitted across a \"distributed\" network, divided into what he called \"message blocks\".In addition to being prone to a single point of failure, existing telegraphic techniques were inefficient and inflexible. Beginning in 1965 Donald Davies, at the National Physical Laboratory in the United Kingdom, developed a more detailed proposal of the concept, designed for computer networking, which he called packet switching, the term that would ultimately be adopted.Packet switching is a technique for transmitting computer data by splitting it into very short, standardized chunks, attaching routing information to each of these chunks, and transmitting them independently through a computer network. It provides better bandwidth utilization than traditional circuit-switching used for telephony, and enables the connection of computers with different transmission and receive rates. It is a distinct concept to message switching.","completion":"Computer Network"}
{"prompt":"While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they built organizations for Internet resource administration and to share operational experience, which enabled more transmission facilities to be put into place.","completion":"Computer Network"}
{"prompt":"Simple switched Ethernet networks, while a great improvement over repeater-based Ethernet, suffer from single points of failure, attacks that trick switches or hosts into sending data to a machine even if it is not intended for it, scalability and security issues with regard to switching loops, broadcast radiation, and multicast traffic.Advanced networking features in switches use Shortest Path Bridging (SPB) or the Spanning Tree Protocol (STP) to maintain a loop-free, meshed network, allowing physical loops for redundancy (STP) or load-balancing (SPB). Shortest Path Bridging includes the use of the link-state routing protocol IS-IS to allow larger networks with shortest path routes between devices.\nAdvanced networking features also ensure port security, provide protection features such as MAC lockdown and broadcast radiation filtering, use VLANs to keep different classes of users separate while using the same physical infrastructure, employ multilayer switching to route between different classes, and use link aggregation to add bandwidth to overloaded links and to provide some redundancy.In 2016, Ethernet replaced InfiniBand as the most popular system interconnect of TOP500 supercomputers.","completion":"Computer Network"}
{"prompt":"The IETF defines Expedited Forwarding (EF) behavior in RFC 3246. The EF PHB has the characteristics of low delay, low loss and low jitter.  These characteristics are suitable for voice, video and other realtime services.  EF traffic is often given strict priority queuing above all other traffic classes.  Because an overload of EF traffic will cause queuing delays and affect the jitter and delay tolerances within the class, admission control, traffic policing and other mechanisms may be applied to EF traffic. The recommended DSCP for EF is 101110B (46 or 2EH).","completion":"Computer Network"}
{"prompt":"In addition to framing, the data link layer may also detect and recover from transmission errors.  For a receiver to detect transmission errors, the sender must add redundant information as an error detection code to the frame sent.  When the receiver obtains a frame it verifies whether the received error detection code matches a recomputed error detection code.\nAn error detection code can be defined as a function that computes the r (amount of redundant bits) corresponding to each string of N total number of bits.  The simplest error detection code is the parity bit, which allows a receiver to detect transmission errors that have affected a single bit among the transmitted N + r bits.  If there are multiple flipped bits then the checking method might not be able to detect this on the receiver side. More advanced methods than parity error detection do exist providing higher grades of quality and features.\n\nA simple example of how this works using metadata is transmitting the word \"HELLO\", by encoding each letter as its position in the alphabet. Thus, the letter A is coded as 1, B as 2, and so on as shown in the table on the right.  Adding up the resulting numbers yields 8 + 5 + 12 + 12 + 15 = 52, and 5 + 2 = 7 calculates the metadata.  Finally, the \"8 5 12 12 15 7\" numbers sequence is transmitted, which the receiver will see on its end if there are no transmission errors.  The receiver knows that the last number received is the error-detecting metadata and that all data before is the message, so the receiver can recalculate the above math and if the metadata matches it can be concluded that the data has been received error-free.  Though, if the receiver sees something like a \"7 5 12 12 15 7\" sequence (first element altered by some error), it can run the check by calculating 7 + 5 + 12 + 12 + 15 = 51 and 5 + 1 = 6, and discard the received data as defective since 6 does not equal 7.\nMore sophisticated error detection and correction algorithms are designed to reduce the risk that multiple transmission errors in the data would cancel each other out and go undetected. An algorithm that can even detect if the correct bytes are received but out of order is the cyclic redundancy check or CRC. This algorithm is often used in the data link layer.","completion":"Computer Network"}
{"prompt":"Dell's Force10 switches support 40 Gbit\/s interfaces. These 40 Gbit\/s fiber-optical interfaces using QSFP+ transceivers can be found on the Z9000 distributed core switches, S4810 and S4820 as well as the blade-switches MXL and the IO-Aggregator. The Dell PowerConnect 8100 series switches also offer 40 Gbit\/s QSFP+ interfaces.","completion":"Computer Network"}
{"prompt":"Bandwidth in bit\/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by processes such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap and bandwidth allocation (using, for example, bandwidth allocation protocol and dynamic bandwidth allocation).","completion":"Computer Network"}
{"prompt":"It is possible for an exchange of emails to form a binding contract, so users must be careful about what they send through email correspondence. A signature block on an email may be interpreted as satisfying a signature requirement for a contract.","completion":"Computer Network"}
{"prompt":"Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.\nThe World Wide Web, E-mail, printing and network file sharing are examples of well-known network services. Network services such as Domain Name System (DNS) give names for IP and MAC addresses (people remember names like nm.lan better than numbers like 210.121.67.18), and Dynamic Host Configuration Protocol (DHCP) to ensure that the equipment on the network has a valid IP address.Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.","completion":"Computer Network"}
{"prompt":"A data packet on the wire and the frame as its payload consist of binary data. Ethernet transmits data with the most-significant octet (byte) first; within each octet, however, the least-significant bit is transmitted first.The internal structure of an Ethernet frame is specified in IEEE 802.3. The table below shows the complete Ethernet packet and the frame inside, as transmitted, for the payload size up to the MTU of 1500 octets. Some implementations of Gigabit Ethernet and other higher-speed variants of Ethernet support larger frames, known as jumbo frames.\n\nThe optional 802.1Q tag consumes additional space in the frame. Field sizes for this option are shown in brackets in the table above. IEEE 802.1ad (Q-in-Q) allows for multiple tags in each frame. This option is not illustrated here.","completion":"Computer Network"}
{"prompt":"The term digital library was first popularized by the NSF\/DARPA\/NASA Digital Libraries Initiative in 1994. With the availability of the computer networks the information resources are expected to stay distributed and accessed as needed, whereas in Vannevar Bush's essay As We May Think (1945) they were to be collected and kept within the researcher's Memex.\nThe term virtual library was initially used interchangeably with digital library, but is now primarily used for libraries that are virtual in other senses (such as libraries which aggregate distributed content). In the early days of digital libraries, there was discussion of the similarities and differences among the terms digital, virtual, and electronic.A distinction is often made between content that was created in a digital format, known as born-digital, and information that has been converted from a physical medium, e.g. paper, through digitization. Not all electronic content is in digital data format. The term hybrid library is sometimes used for libraries that have both physical collections and electronic collections. For example, American Memory is a digital library within the Library of Congress.\nSome important digital libraries also serve as long term archives, such as arXiv and the Internet Archive. Others, such as the Digital Public Library of America, seek to make digital information from various institutions widely accessible online.","completion":"Computer Network"}
{"prompt":"The European Union's General Data Protection Regulation went into effect May 2018. Since then, the protection of the privacy of employees, customers and other stakeholders (e.g. consultants) has become more and more a significant concern for most companies (at least, all those having an interest in markets and countries where regulations are in place to protect the privacy).","completion":"Computer Network"}
{"prompt":"One method to detect errors with voting machines is parallel testing, which are conducted on the Election Day with randomly picked machines. The ACM published a study showing that, to change the outcome of the 2000 U.S. Presidential election, only 2 votes in each precinct would have needed to be changed.","completion":"Computer Network"}
{"prompt":"In 2003, task group TGma was authorized to \"roll up\" many of the amendments to the 1999 version of the 802.11 standard. REVma or 802.11ma, as it was called, created a single document that merged 8 amendments (802.11a, b, d, e, g, h, i, j) with the base standard. Upon approval on 8 March 2007, 802.11REVma was renamed to the then-current base standard IEEE 802.11-2007.","completion":"Computer Network"}
{"prompt":"On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress HR discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying (\"with some caveats\") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to The New York Times.On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC chairman, Tom Wheeler, commented, \"This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept.\"On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new \"Net Neutrality\" regulations.On December 14, 2017, the FCC repealed their March 12, 2015 decision by a 3\u20132 vote regarding net neutrality rules.","completion":"Computer Network"}
{"prompt":"An IP address conflict occurs when two devices on the same local physical or wireless network claim to have the same IP address. A second assignment of an address generally stops the IP functionality of one or both of the devices.  Many modern operating systems notify the administrator of IP address conflicts. When IP addresses are assigned by multiple people and systems with differing methods, any of them may be at fault. If one of the devices involved in the conflict is the default gateway access beyond the LAN for all devices on the LAN, all devices may be impaired.","completion":"Computer Network"}
{"prompt":"By 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.\n\nEarly digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939 in Berlin, was one of the earliest examples of an electromechanical relay computer.\nIn 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5\u201310 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating-point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was not itself a universal computer but could be extended to be Turing complete.Zuse's next computer, the Z4, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the ETH Zurich. The computer was manufactured by Zuse's own company, Zuse KG, which was founded in 1941 as the first company with the sole purpose of developing computers in Berlin.","completion":"Computer Network"}
{"prompt":"For home networks relying on powerline communication technology, how to deal with electrical noise injected into the system from standard household appliances remains the largest challenge. Whenever any appliance is turned on or turned off it creates noise that could possibly disrupt data transfer through the wiring. IEEE products that are certified to be HomePlug 1.0 compliant have been engineered to no longer interfere with, or receive interference from other devices plugged into the same home's electrical grid.","completion":"Computer Network"}
{"prompt":"Email marketing via \"opt-in\" is often successfully used to send special sales offerings and new product information. Depending on the recipient's culture, email sent without permission\u2014such as an \"opt-in\"\u2014is likely to be viewed as unwelcome \"email spam\".","completion":"Computer Network"}
{"prompt":"Standard complementary instant messaging applications offer functions like file transfer, contact list(s), the ability to hold several simultaneous conversations, etc. These may be all the functions that a small business needs, but larger organizations will require more sophisticated applications that can work together. The solution to finding applications capable of this is to use enterprise versions of instant messaging applications. These include titles like XMPP, Lotus Sametime, Microsoft Office Communicator, etc., which are often integrated with other enterprise applications such as workflow systems. These enterprise applications, or enterprise application integration (EAI), are built to certain constraints, namely storing data in a common format.\nThere have been several attempts to create a unified standard for instant messaging: IETF's Session Initiation Protocol (SIP) and SIP for Instant Messaging and Presence Leveraging Extensions (SIMPLE), Application Exchange (APEX), Instant Messaging and Presence Protocol (IMPP), the open XML-based Extensible Messaging and Presence Protocol (XMPP), and Open Mobile Alliance's Instant Messaging and Presence Service developed specifically for mobile devices.\nMost attempts at producing a unified standard for the major IM providers (AOL, Yahoo! and Microsoft) have failed, and each continues to use its own proprietary protocol.\nHowever, while discussions at IETF were stalled, Reuters signed the first inter-service provider connectivity agreement in September 2003. This agreement enabled AIM, ICQ and MSN Messenger users to talk with Reuters Messaging counterparts and vice versa. Following this, Microsoft, Yahoo! and AOL agreed to a deal in which Microsoft's Live Communications Server 2005 users would also have the possibility to talk to public instant messaging users. This deal established SIP\/SIMPLE as a standard for protocol interoperability and established a connectivity fee for accessing public instant messaging groups or services. Separately, on October 13, 2005, Microsoft and Yahoo! announced that by the 3rd quarter of 2006 they would interoperate using SIP\/SIMPLE, which was followed, in December 2005, by the AOL and Google strategic partnership deal in which Google Talk users would be able to communicate with AIM and ICQ users provided they have an AIM account.\nThere are two ways to combine the many disparate protocols:\n\nCombine the many disparate protocols inside the IM client application.\nCombine the many disparate protocols inside the IM server application. This approach moves the task of communicating with the other services to the server. Clients need not know or care about other IM protocols. For example, LCS 2005 Public IM Connectivity. This approach is popular in XMPP servers; however, the so-called transport projects suffer the same reverse engineering difficulties as any other project involved with closed protocols or formats.Some approaches allow organizations to deploy their own, private instant messaging network by enabling them to restrict access to the server (often with the IM network entirely behind their firewall) and administer user permissions. Other corporate messaging systems allow registered users to also connect from outside the corporation LAN, by using an encrypted, firewall-friendly, HTTPS-based protocol. Usually, a dedicated corporate IM server has several advantages, such as pre-populated contact lists, integrated authentication, and better security and privacy.\nCertain networks have made changes to prevent them from being used by such multi-network IM clients. For example, Trillian had to release several revisions and patches to allow its users to access the MSN, AOL, and Yahoo! networks, after changes were made to these networks. The major IM providers usually cite the need for formal agreements, and security concerns as reasons for making these changes.\nThe use of proprietary protocols has meant that many instant messaging networks have been incompatible and users have been unable to reach users on other networks. This may have allowed social networking with IM-like features and text messaging an opportunity to gain market share at the expense of IM.","completion":"Computer Network"}
{"prompt":"Because of its government funding, certain forms of traffic were discouraged or prohibited.\nLeonard Kleinrock claims to have committed the first illegal act on the Internet, having sent a request for return of his electric razor after a meeting in England in 1973. At the time, use of the ARPANET for personal reasons was unlawful.In 1978, against the rules of the network, Gary Thuerk of Digital Equipment Corporation (DEC) sent out the first mass email to approximately 400 potential clients via the ARPANET. He claims that this resulted in $13 million worth of sales in DEC products, and highlighted the potential of email marketing.A 1982 handbook on computing at MIT's AI Lab stated regarding network etiquette:\nIt is considered illegal to use the ARPANet for anything which is not in direct support of Government business ... personal messages to other ARPANet subscribers (for example, to arrange a get-together or check and say a friendly hello) are generally not considered harmful ... Sending electronic mail over the ARPANet for commercial profit or political purposes is both anti-social and illegal. By sending such messages, you can offend many people, and it is possible to get MIT in serious trouble with the Government agencies which manage the ARPANet.","completion":"Computer Network"}
{"prompt":"In new home construction, wiring for all electrical services can be easily installed before the walls are finished. In existing buildings, installation of a new system, such as a security system or home theatre, may require additional effort to install concealed wiring. Multiple-unit dwellings such as condominiums and apartment houses may have additional installation complexity in distributing services within a house.\nServices commonly found include:\n\nPower points (wall outlets)\nLight fixtures and switches\nTelephone\nInternet\nTelevision, either broadcast, cable, or satelliteHigh-end features might include:\n\nHome theater\nDistributed audio\nSecurity monitoring\nSecurity CCTV\nAutomation\nEnergy managementPower and telecommunication services generally require entry points into the home and a location for connection equipment. For electric power supply, a cable is run either overhead or underground into a distribution board in the home.  A distribution board, or circuit breaker panel, is typically a metal box mounted on a wall of the home. In many new homes, the location of the electrical switchboard is on the outside of the external wall of the garage.\nHow services are connected will vary depending on the service provider and location of the home.\nThe following home services are supported by discrete wiring systems\nInformation and Communications\nEntertainment\nEnergy Management\nSecurity and Safety\nDigital Home Health\nAged and Assisted Living\nIntelligent Lighting and Power.","completion":"Computer Network"}
{"prompt":"A default forwarding (DF) PHB is the only required behavior.  Essentially, any traffic that does not meet the requirements of any of the other defined classes uses DF.  Typically, DF has best-effort forwarding characteristics.  The recommended DSCP for DF is 0.","completion":"Computer Network"}
{"prompt":"The range of CAN is 1 km to 5 km. If two buildings have the same domain and they are connected with a network, then it will be considered as CAN only. Though the CAN is mainly used for corporate campuses so the data link will be high speed.","completion":"Computer Network"}
{"prompt":"The star topology reduces the probability of a network failure by connecting all of the peripheral nodes (computers, etc.) to a central node.  When the physical star topology is applied to a logical bus network such as Ethernet, this central node (traditionally a hub) rebroadcasts all transmissions received from any peripheral node to all peripheral nodes on the network, sometimes including the originating node. All peripheral nodes may thus communicate with all others by transmitting to, and receiving from, the central node only.  The failure of a transmission line linking any peripheral node to the central node will result in the isolation of that peripheral node from all others, but the remaining peripheral nodes will be unaffected. However, the disadvantage is that the failure of the central node will cause the failure of all of the peripheral nodes.\nIf the central node is passive, the originating node must be able to tolerate the reception of an echo of its own transmission, delayed by the two-way round trip transmission time  (i.e. to and from the central node) plus any delay generated in the central node. An active star network has an active central node that usually has the means to prevent echo-related problems.\nA tree topology (a.k.a. hierarchical topology) can be viewed as a collection of star networks arranged in a hierarchy.  This tree structure has individual peripheral nodes (e.g. leaves) which are required to transmit to and receive from one other node only and are not required to act as repeaters or regenerators.  Unlike the star network, the functionality of the central node may be distributed.\nAs in the conventional star network, individual nodes may thus still be isolated from the network by a single-point failure of a transmission path to the node. If a link connecting a leaf fails, that leaf is isolated; if a connection to a non-leaf node fails, an entire section of the network becomes isolated from the rest.\nTo alleviate the amount of network traffic that comes from broadcasting all signals to all nodes, more advanced central nodes were developed that are able to keep track of the identities of the nodes that are connected to the network.  These network switches will \"learn\" the layout of the network by \"listening\" on each port during normal data transmission, examining the data packets and recording the address\/identifier of each connected node and which port it is connected to in a lookup table held in memory. This lookup table then allows future transmissions to be forwarded to the intended destination only.","completion":"Computer Network"}
{"prompt":"IEEE 802.11be Extremely High Throughput (EHT) is the potential next amendment to the 802.11 IEEE standard, and will likely be designated as Wi-Fi 7. It will build upon 802.11ax, focusing on WLAN indoor and outdoor operation with stationary and pedestrian speeds in the 2.4 GHz, 5 GHz, and 6 GHz frequency bands.","completion":"Computer Network"}
{"prompt":"In order for IntServ to work, all routers along the traffic path must support it. Furthermore, many states must be stored in each router. As a result, IntServ works on a small-scale, but as the system scales up to larger networks or the Internet, it becomes resource intensive to track of all of the reservations.One way to solve the scalability problem is by using a multi-level approach, where per-microflow resource reservation (such as resource reservation for individual users) is done in the edge network, while in the core network resources are reserved for aggregate flows only.  The routers that lie between these different levels must adjust the amount of aggregate bandwidth reserved from the core network so that the reservation requests for individual flows from the edge network can be better satisfied.","completion":"Computer Network"}
{"prompt":"In a distributed bus network, all of the nodes of the network are connected to a common transmission medium with more than two endpoints, created by adding branches to the main section of the transmission medium \u2013 the physical distributed bus topology functions in exactly the same fashion as the physical linear bus topology because all nodes share a common transmission medium.","completion":"Computer Network"}
{"prompt":"Electronic voting technology intends to speed the counting of ballots, reduce the cost of paying staff to count votes manually and can provide improved accessibility for disabled voters. Also in the long term, expenses are expected to decrease.\nResults can be reported and published faster.\nVoters save time and cost by being able to vote independently from their location. This may increase overall voter turnout. The citizen groups benefiting most from electronic elections are the ones living abroad, citizens living in rural areas far away from polling stations and the disabled with mobility impairments.","completion":"Computer Network"}
{"prompt":"The 'truncated' variant of the algorithm introduces a limit on c. This simply means that after a certain number of increases, the exponentiation stops. Without a limit on c, the delay between transmissions may become undesirably long if a sender repeatedly observes adverse events, e.g. due to a degradation in network service. In a randomized system this may occur by chance, leading to unpredictable latency; longer delays due to unbounded increases in c are exponentially less probable, but they are effectively inevitable on a busy network due to the law of large numbers. Limiting c helps to reduce the possibility of unexpectedly long transmission latencies and improve recovery times after a transient outage.\nFor example, if the ceiling is set at i = 10 in a truncated binary exponential backoff algorithm, (as it is in the IEEE 802.3 CSMA\/CD standard), then the maximum delay is 1023 slot times, i.e. 210 \u2212 1.\nSelecting an appropriate backoff limit for a system involves striking a balance between collision probability and latency. By increasing the ceiling there is an exponential reduction in probability of collision on each transmission attempt. At the same time, increasing the limit also exponentially increases the range of possible latency times for a transmission, leading to less deterministic performance and an increase in the average latency. The optimal limit value for a system is specific to both the implementation and environment.","completion":"Computer Network"}
{"prompt":"An ad hoc network is a network where stations communicate only peer-to-peer (P2P). There is no base and no one gives permission to talk. This is accomplished using the Independent Basic Service Set (IBSS). A Wi-Fi Direct network is a different type of wireless network where stations communicate peer-to-peer. In a peer-to-peer network wireless devices within range of each other can discover and communicate directly without involving central access points.\nIn a Wi-Fi P2P group, the group owner operates as an access point and all other devices are clients. There are two main methods to establish a group owner in the Wi-Fi Direct group. In one approach, the user sets up a P2P group owner manually. This method is also known as autonomous group owner (autonomous GO). In the second method, called negotiation-based group creation, two devices compete based on the group owner intent value. The device with higher intent value becomes a group owner and the second device becomes a client. Group owner intent value can depend on whether the wireless device performs a cross-connection between an infrastructure WLAN service and a P2P group, available power in the wireless device, whether the wireless device is already a group owner in another group or a received signal strength of the first wireless device.\n\nIEEE 802.11 defines the PHY and medium access control (MAC) layers based on carrier-sense multiple access with collision avoidance (CSMA\/CA). This is in contrast to Ethernet which uses carrier-sense multiple access with collision detection (CSMA\/CD). The 802.11 specification includes provisions designed to minimize collisions because mobile units have to contend with the hidden node problem where two mobile units may both be in range of a common access point, but out of range of each other.","completion":"Computer Network"}
{"prompt":"A distributed star is a network topology that is composed of individual networks that are based upon the physical star topology connected in a linear fashion \u2013 i.e., 'daisy-chained' \u2013 with no central or top level connection point (e.g., two or more 'stacked' hubs, along with their associated star connected nodes or 'spokes').","completion":"Computer Network"}
{"prompt":"The transmission media (often referred to in the literature as the physical media) used to link devices to form a computer network include electrical cables (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2 \u2014 the physical layer and the data link layer.\nA widely adopted family of transmission media used in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Ethernet transmits data over both copper and fiber cables.  Wireless LAN standards (e.g. those defined by IEEE 802.11) use radio waves, or others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.","completion":"Computer Network"}
{"prompt":"The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor.","completion":"Computer Network"}
{"prompt":"The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).\nThe IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the work of the IETF is organized into Working Groups. Standardization efforts of the Working Groups are often adopted by the Internet community, but the IETF does not control or patrol the Internet.The IETF grew out of quarterly meetings with U.S. government-funded researchers, starting in January 1986. Non-government representatives were invited by the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth meeting in February 1987. The seventh meeting in July 1987 was the first meeting with more than one hundred attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, the Netherlands, in July 1993. Today, the IETF meets three times per year and attendance has been as high as ca. 2,000 participants. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is typically ca. 50%, even at meetings held in the United States.The IETF is not a legal entity, has no governing board, no members, and no dues. The closest status resembling membership is being on an IETF or Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer-term research issues.","completion":"Computer Network"}
{"prompt":"The Internet backbone consists of many networks owned by numerous companies. Optical fiber trunk lines consist of many fiber cables bundled to increase capacity, or bandwidth. Fiber-optic communication remains the medium of choice for Internet backbone providers for several reasons. Fiber-optics allow for fast data speeds and large bandwidth, they suffer relatively little attenuation, allowing them to cover long distances with few repeaters, and they are also immune to crosstalk and other forms of electromagnetic interference which plague electrical transmission. The real-time routing protocols and redundancy built into the backbone is also able to reroute traffic in case of a failure. The data rates of backbone lines have increased over time. In 1998, all of the United States' backbone networks had utilized the slowest data rate of 45 Mbit\/s. However, technological improvements allowed for 41 percent of backbones to have data rates of 2,488 Mbit\/s or faster by the mid 2000s.","completion":"Computer Network"}
{"prompt":"The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 \"to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world\". With offices near Washington, DC, US, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form \"chapters\" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision-making.","completion":"Computer Network"}
{"prompt":"CAUI-10\nCAUI-10 is a 100 Gbit\/s 10-lane electrical interface defined in 802.3ba.CAUI-4\nCAUI-4 is a 100 Gbit\/s  4-lane electrical interface defined in 802.3bm Annex 83E with a nominal signaling rate for each lane of 25.78125 GBd using NRZ modulation.100GAUI-4\n100GAUI-4 is a 100 Gbit\/s  4-lane electrical interface defined in 802.3cd Annex 135D\/E with a nominal signaling rate for each lane of 26.5625 GBd using NRZ modulation and RS-FEC(544,514) so suitable for use with 100GBASE-CR2, 100GBASE-KR2, 100GBASE-SR2, 100GBASE-DR, 100GBASE-FR1, 100GBASE-LR1 PHYs.100GAUI-2\n100GAUI-2 is a 100 Gbit\/s  2-lane electrical interface defined in 802.3cd Annex 135F\/G with a nominal signaling rate for each lane of 26.5625 GBd using PAM4 modulation and RS-FEC(544,514) so suitable for use with 100GBASE-CR2, 100GBASE-KR2, 100GBASE-SR2, 100GBASE-DR, 100GBASE-FR1, 100GBASE-LR1 PHYs.100GAUI-1\n100GAUI-1 is a 100 Gbit\/s  1-lane electrical interface defined in 802.3ck Annex 120F\/G with a nominal signaling rate for each lane of 53.125 GBd using PAM4 modulation and RS-FEC(544,514) so suitable for use with 100GBASE-CR1, 100GBASE-KR1, 100GBASE-SR1, 100GBASE-DR, 100GBASE-FR1, 100GBASE-LR1 PHYs.","completion":"Computer Network"}
{"prompt":"Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff\u2013Berry Computer (ABC) in 1942, the first \"automatic electronic digital computer\". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.\nDuring World War II, the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women. To crack the more sophisticated German Lorenz SZ 40\/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.\nThe ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the \"ENIAC girls\".It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.","completion":"Computer Network"}
{"prompt":"Data wiring has two components, these are:\n\nData service delivery\nData network cableThe three most common ways data services are delivered to the home:\n\nADSL service on the back of the telephone cabling\nCable Modem\nFiberADSL service\nADSL services are typically delivered using the telephone cabling. An ADSL modem needs a filter to segregate voice handsets from the ADSL modem. \nCable Modem\ncable modems are typically installed in location where there is an existing Pay TV service outlet. The installation requires the installation of a Pay TV outlet (F connector). \nFiber\nFiber is the least common but it is growing in numbers. If the home has fiber to it then the fiber terminates on what is known as an Optical Network Termination unit (ONT) and it has a data port on it. Cabling from the street to the point where the ONT is installed is fiber and is typically installed by the service provider.\nIn all three cases the equipment supplied by the Internet provider will have a connection to the computers installed in the building.  This is the data network cabling or LAN cabling.\nIf more than one computer or device (PC, printers, TV etc.) is to be connected in the home,  LAN cabling will be required. The cabling used for data networking is similar to the phone cabling as it is twisted pair but of a much higher quality. The cable is known as Category (Cat) 5 or Cat 6. The cabling must be installed as a star wired configuration, that is the cabling runs from the point next to the modem, hub, or router  uninterrupted up to the outlet next to the device that needs to be connected. Computer network wiring cannot be chained from one outlet to the next; each outlet is wired individually back to the hub or router next to the modem.  If only one computer is required, it can be directly plugged into the modem.   An alternative to a wired LAN especially useful for mobile devices is a wireless LAN, which can reduce or eliminate all the fixed wiring.","completion":"Computer Network"}
{"prompt":"During his time as director of ARPA's Information Processing Techniques Office (IPTO) from 1962 to 1964, he funded Project MAC at MIT. A large mainframe computer was designed to be shared by up to 30 simultaneous users, each sitting at a separate \"typewriter terminal\". He also funded similar projects at Stanford University, UCLA, UC Berkeley (called Project Genie), and the AN\/FSQ-32 at System Development Corporation.\nThis time-sharing technology later developed to become what today are known as servers.","completion":"Computer Network"}
{"prompt":"Network connections can be established wirelessly using radio or other electromagnetic means of communication.\n\n Terrestrial microwave \u2013 Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 40 miles (64 km) apart.\nCommunications satellites \u2013 Satellites also communicate via microwave. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.\nCellular networks use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver.\nRadio and spread spectrum technologies \u2013 Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi.\nFree-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.\nExtending the Internet to interplanetary dimensions via radio waves and optical means, the Interplanetary Internet.\nIP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.The last two cases have a large round-trip delay time, which gives slow two-way communication but does not prevent sending large amounts of information (they can have high throughput).","completion":"Computer Network"}
{"prompt":"A firewall is a network device or software for controlling network security and access rules. Firewalls are inserted in connections between secure internal networks and potentially insecure external networks such as the Internet. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.","completion":"Computer Network"}
{"prompt":"Because it was never a goal for the ARPANET to support IMPs from vendors other than BBN, the IMP-to-IMP protocol and message format were not standardized. However, the IMPs did nonetheless communicate amongst themselves to perform link-state routing, to do reliable forwarding of messages, and to provide remote monitoring and management functions to ARPANET's Network Control Center. Initially, each IMP had a 6-bit identifier and supported up to 4 hosts, which were identified with a 2-bit index. An ARPANET host address, therefore, consisted of both the port index on its IMP and the identifier of the IMP, which was written with either port\/IMP notation or as a single byte; for example, the address of MIT-DMG (notable for hosting development of Zork) could be written as either 1\/6 or 70. An upgrade in early 1976 extended the host and IMP numbering to 8-bit and 16-bit, respectively.In addition to primary routing and forwarding responsibilities, the IMP ran several background programs, titled TTY, DEBUG, PARAMETER-CHANGE, DISCARD, TRACE, and STATISTICS. These were given host numbers in order to be addressed directly and provided functions independently of any connected host. For example, \"TTY\" allowed an on-site operator to send ARPANET packets manually via the teletype connected directly to the IMP.","completion":"Computer Network"}
{"prompt":"In addition to the malicious code threat, the use of instant messaging at work also creates a risk of non-compliance to laws and regulations governing use of electronic communications in businesses.\nIn the United States alone there are over 10,000 laws and regulations related to electronic messaging and records retention. The better-known of these include the Sarbanes\u2013Oxley Act, HIPAA, and SEC 17a-3.\nClarification from the Financial Industry Regulatory Authority (FINRA) was issued to member firms in the financial services industry in December, 2007, noting that \"electronic communications\", \"email\", and \"electronic correspondence\" may be used interchangeably and can include such forms of electronic messaging as instant messaging and text messaging. Changes to Federal Rules of Civil Procedure, effective December 1, 2006, created a new category for electronic records which may be requested during discovery in legal proceedings.\nMost nations also regulate use of electronic messaging and electronic records retention in similar fashion as the United States. The most common regulations related to IM at work involve the need to produce archived business communications to satisfy government or judicial requests under law. Many instant messaging communications fall into the category of business communications that must be archived and retrievable.","completion":"Computer Network"}
{"prompt":"The residual capacity of an arc e with respect to a pseudo-flow f is denoted cf, and it is the difference between the arc's capacity and its flow. That is, cf (e) = c(e) - f(e). From this we can construct a residual network, denoted Gf (V, Ef), with a capacity function cf which models the amount of available capacity on the set of arcs in G = (V, E). More specifically, capacity function cf of each arc (u, v) in the residual network represents the amount of flow which can be transferred from u to v given the current state of the flow within the network.\nThis concept is used in Ford\u2013Fulkerson algorithm which computes the maximum flow in a flow network.\nNote that there can be an unsaturated path (a path with available capacity) from u to v in the residual network, even though there is no such path from u to v in the original network. Since flows in opposite directions cancel out, decreasing the flow from v to u is the same as increasing the flow from u to v.","completion":"Computer Network"}
{"prompt":"In the Internet, a hostname is a domain name assigned to a host computer. This is usually a combination of the host's local name with its parent domain's name. For example, en.wikipedia.org consists of a local hostname (en) and the domain name wikipedia.org. This kind of hostname is translated into an IP address via the local hosts file, or the Domain Name System (DNS) resolver. It is possible for a single host computer to have several hostnames; but generally the operating system of the host prefers to have one hostname that the host uses for itself.\nAny domain name can also be a hostname, as long as the restrictions mentioned below are followed. So, for example, both en.wikipedia.org and wikipedia.org are hostnames because they both have IP addresses assigned to them. A hostname may be a domain name, if it is properly organized into the domain name system. A domain name may be a hostname if it has been assigned to an Internet host and associated with the host's IP address.","completion":"Computer Network"}
{"prompt":"Low power, close range communication based on IEEE 802.15 standards has a strong presence in homes. Bluetooth continues to be the technology of choice for most wireless accessories such as keyboards, mice, headsets, and game controllers. These connections are often established in a transient, ad-hoc manner and are not thought of as permanent residents of a home network.","completion":"Computer Network"}
{"prompt":"Common methods of Internet access by users include dial-up with a computer modem via telephone circuits, broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite, and cellular telephone technology (e.g. 3G, 4G). The Internet may often be accessed from computers in libraries and Internet caf\u00e9s. Internet access points exist in many public places such as airport halls and coffee shops. Various terms are used, such as public Internet kiosk, public access terminal, and Web payphone. Many hotels also have public terminals that are usually fee-based. These terminals are widely accessed for various usages, such as ticket booking, bank deposit, or online payment. Wi-Fi provides wireless access to the Internet via local computer networks. Hotspots providing such access include Wi-Fi caf\u00e9s, where users need to bring their own wireless devices, such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based.\nGrassroots efforts have led to wireless community networks. Commercial Wi-Fi services that cover large areas are available in many cities, such as New York, London, Vienna, Toronto, San Francisco, Philadelphia, Chicago and Pittsburgh, where the Internet can then be accessed from places such as a park bench. Experiments have also been conducted with proprietary mobile wireless networks like Ricochet, various high-speed data services over cellular networks, and fixed wireless services. Modern smartphones can also access the Internet through the cellular carrier network. For Web browsing, these devices provide applications such as Google Chrome, Safari, and Firefox and a wide variety of other Internet software may be installed from app stores. Internet usage by mobile and tablet devices exceeded desktop worldwide for the first time in October 2016.","completion":"Computer Network"}
{"prompt":"The first four nodes were designated as a testbed for developing and debugging the 1822 protocol, which was a major undertaking. While they were connected electronically in 1969, network applications were not possible until the Network Control Protocol was implemented in 1970 enabling the first two host-host protocols, remote login (Telnet) and file transfer (FTP) which were specified and implemented between 1969 and 1973. The network was declared operational in 1971. Network traffic began to grow once email was established at the majority of sites by around 1973.","completion":"Computer Network"}
{"prompt":"Internetworking, a combination of the components inter (between) and networking, started as a way to connect disparate types of networking technology, but it became widespread through the developing need to connect two or more local area networks via some sort of wide area network.\nThe first international heterogenous resource sharing network was the 1973 interconnection of the ARPANET with early British academic networks through the computer science department at University College London (UCL). In the ARPANET, the network elements used to connect individual networks were called gateways, but the term has been deprecated in this context, because of possible confusion with functionally different devices. By 1973-4, researchers in the United States, the United Kingdom and France had worked out an approach to internetworking where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible, as demonstrated in the CYCLADES network. Research at the National Physical Laboratory in the United Kingdom confirmed establishing a common host protocol would be more reliable and efficient. The ARPANET connection to UCL later evolved into SATNET. In 1977, ARPA demonstrated a three-way internetworking experiment, which linked a mobile vehicle in PRNET with nodes in the ARPANET, and via SATNET, to nodes at UCL. The X.25 protocol, on which public data networks were based in the 1970s and 1980s, was supplemented by the X.75 protocol which enabled internetworking. \nToday the interconnecting gateways are called routers. The definition of an internetwork today includes the connection of other types of computer networks such as personal area networks.\nTo build an internetwork, the following are needed::\u200a103\u200a A standardized scheme to address packets to any host on any participating network; a standardized protocol defining format and handling of transmitted packets; components interconnecting the participating networks by routing packets to their destinations based on standardized addresses.\nAnother type of interconnection of networks often occurs within enterprises at the link layer of the networking model, i.e. at the hardware-centric layer below the level of the TCP\/IP logical interfaces. Such interconnection is accomplished with network bridges and network switches. This is sometimes incorrectly termed internetworking, but the resulting system is simply a larger, single subnetwork, and no internetworking protocol, such as Internet Protocol, is required to traverse these devices. However, a single computer network may be converted into an internetwork by dividing the network into segments and logically dividing the segment traffic with routers and having an internetworking software layer that applications employ.\nThe Internet Protocol is designed to provide an unreliable (not guaranteed) packet service across the network. The architecture avoids intermediate network elements maintaining any state of the network. Instead, this function is assigned to the endpoints of each communication session. To transfer data reliably, applications must utilize an appropriate transport layer protocol, such as Transmission Control Protocol (TCP), which provides a reliable stream. Some applications use a simpler, connection-less transport protocol, User Datagram Protocol (UDP), for tasks which do not require reliable delivery of data or that require real-time service, such as video streaming or voice chat.","completion":"Computer Network"}
{"prompt":"The ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can operate only on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation\u2014although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return Boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing Boolean logic.\nSuperscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.","completion":"Computer Network"}
{"prompt":"Networks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of the physical extent or geographic scale.","completion":"Computer Network"}
{"prompt":"Distributed audio provides music throughout the house, where the music sources are all centralized. Rooms are provided with speakers and controls to adjust volume or music source. A system may have central controls or allow for off-site control.","completion":"Computer Network"}
{"prompt":"At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.\nIn August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP\/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit\/s, serving a Sun host computer and twelve US Robotics dial-up modems.\nIn 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Ivory Coast and Benin in 1998.\nAfrica is building an Internet infrastructure. AFRINIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.","completion":"Computer Network"}
{"prompt":"On 8 February 2020, the telecommunication network of Iran witnessed extensive disruptions at 11:44 a.m. local time, which lasted for about an hour. The Ministry of Information and Communications Technology of Iran confirmed it as a Distributed Denial of Service (DDoS) attack. The Iranian authorities activated the \"Digital Fortress\" cyber-defense mechanism to repel. Also known as DZHAFA, it led to a drop of 75 percent in the national internet connectivity.On the noon of 26 October 2021, A cyberattack caused all 4,300 fuel stations in Iran to disrupt and disable government-issued cards for buying subsidized fuel. This cyberattack also caused digital billboards to display messages against the Iranian government.","completion":"Computer Network"}
{"prompt":"Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include Microsoft Word documents, PDF documents, and scanned images of paper documents. In principle, there is no technical restriction on the size or number of attachments. However, in practice, email clients, servers, and Internet service providers implement various limitations on the size of files, or complete email \u2013 typically to 25MB or less. Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ from what the user sees, which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, various file hosting services are available and commonly used.","completion":"Computer Network"}
{"prompt":"A virus is a self-replicating program that can attach itself to another program or file in order to reproduce. The virus can hide in unlikely locations in the memory of a computer system and attach itself to whatever file it sees fit to execute its code. It can also change its digital footprint each time it replicates making it harder to track down in the computer.","completion":"Computer Network"}
{"prompt":"A distributed backbone is a backbone network that consists of a number of connectivity devices connected to a series of central connectivity devices, such as hubs, switches, or routers, in a hierarchy.   This kind of topology allows for simple expansion and limited capital outlay for growth, because more layers of devices can be added to existing layers.  In a distributed backbone network, all of the devices that access the backbone share the transmission media, as every device connected to this network is sent all transmissions placed on that network.Distributed backbones, in all practicality, are in use by all large-scale networks. Applications in enterprise-wide scenarios confined to a single building are also practical, as certain connectivity devices can be assigned to certain floors or departments.  Each floor or department possesses a LAN and a wiring closet with that workgroup's main hub or router connected to a bus-style network using backbone cabling.  Another advantage of using a distributed backbone is the ability for network administrator to segregate workgroups for ease of management.There is the possibility of single points of failure, referring to connectivity devices high in the series hierarchy.  The distributed backbone must be designed to separate network traffic circulating on each individual LAN from the backbone network traffic by using access devices such as routers and bridges.","completion":"Computer Network"}
{"prompt":"SMS is the acronym for \u201cshort message service\u201d and allows mobile phone users to send text messages without an Internet connection, while instant messaging provides similar services through an Internet connection. SMS was a much more dominant form of communication before smartphones became widely used globally. While SMS relied on traditional paid telephone services, instant messaging apps on mobiles were available for free or a minor data charge. In 2012 SMS volume peaked, and in 2013 chat apps surpassed SMS in global message volume.Easier group messaging was another advantage of smartphone messaging apps and also contributed to their adoption. Before the introduction of messaging apps, smartphone users could only participate in single-person interactions via mobile voice calls or SMS. With the introduction of messaging apps, the group chat functionality allows all the members to see an entire thread of everyone's responses. Members can also respond directly to each other, rather than having to go through the member who started the group message, to relay the information.However, SMS still remains popular in the United States because it is usually included free in monthly phone bundles. While SMS volumes in some countries like Denmark, Spain and Singapore dropped up to two-thirds from 2011 to 2013, in the United States SMS use only dropped by about one quarter.","completion":"Computer Network"}
{"prompt":"An intranet structure needs key personnel committed to maintaining the intranet and keeping content current. For feedback on the intranet, social networking can be done through a forum for users to indicate what they want and what they do not like.","completion":"Computer Network"}
{"prompt":"By 1976, the French PTT was developing Transpac, a packet network based on the emerging X.25 standard. The academic debates between datagram and virtual circuit networks continued for some time, but were eventually cut short by bureaucratic decisions.\nData transmission was a state monopoly in France at the time, and IRIA needed a special dispensation to run the CYCLADES network. The PTT did not agree to funding by the government of a competitor to their Transpac network, and insisted that the permission and funding be rescinded. By 1981, Cyclades was forced to shut down.","completion":"Computer Network"}
{"prompt":"The basic service set (BSS) is a set of all stations that can communicate with each other at PHY layer. Every BSS has an identification (ID) called the BSSID, which is the MAC address of the access point servicing the BSS.\nThere are two types of BSS: Independent BSS (also referred to as IBSS), and infrastructure BSS. An independent BSS (IBSS) is an ad hoc network that contains no access points, which means they cannot connect to any other basic service set. In an IBSS the STAs are configured in ad hoc (peer-to-peer) mode.\nAn extended service set (ESS) is a set of connected BSSs. Access points in an ESS are connected by a distribution system. Each ESS has an ID called the SSID which is a 32-byte (maximum) character string.\nA distribution system (DS) connects access points in an extended service set. The concept of a DS can be used to increase network coverage through roaming between cells. DS can be wired or wireless. Current wireless distribution systems are mostly based on WDS or Mesh protocols, though other systems are in use.","completion":"Computer Network"}
{"prompt":"Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.\nComputer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.\nSurveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent\/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.However, many civil rights and privacy groups\u2014such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union\u2014have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\".","completion":"Computer Network"}
{"prompt":"According to Charles Herzfeld, ARPA Director (1965\u20131967):The ARPANET was not started to create a Command and Control System that would survive a nuclear attack, as many now claim. To build such a system was, clearly, a major military need, but it was not ARPA's mission to do this; in fact, we would have been severely criticized had we tried. Rather, the ARPANET came out of our frustration that there were only a limited number of large, powerful research computers in the country, and that many research investigators, who should have access to them, were geographically separated from them.\nNonetheless, according to Stephen J. Lukasik, who as deputy director (1967\u20131970) and Director of DARPA (1970\u20131975) was \"the person who signed most of the checks for Arpanet's development\":\n\nThe goal was to exploit new computer technologies to meet the needs of military command and control against nuclear threats, achieve survivable control of US nuclear forces, and improve military tactical and management decision making.\nThe ARPANET incorporated distributed computation, and frequent re-computation, of routing tables. This increased the survivability of the network in the face of significant interruption. Automatic routing was technically challenging at the time. The ARPANET was designed to survive subordinate-network losses, since the principal reason was that the switching nodes and network links were unreliable, even without any nuclear attacks.The Internet Society agrees with Herzfeld in a footnote in their online article, A Brief History of the Internet:\n\nIt was from the RAND study that the false rumor started, claiming that the ARPANET was somehow related to building a network resistant to nuclear war. This was never true of the ARPANET, but was an aspect of the earlier RAND study of secure communication. The later work on internetworking did emphasize robustness and survivability, including the capability to withstand losses of large portions of the underlying networks.\nPaul Baran, the first to put forward a theoretical model for communication using packet switching, conducted the RAND study referenced above. Though the ARPANET did not exactly share Baran's project's goal, he said his work did contribute to the development of the ARPANET. Minutes taken by Elmer Shapiro of Stanford Research Institute at the ARPANET design meeting of 9\u201310 October 1967 indicate that a version of Baran's routing method (\"hot potato\") may be used, consistent with the NPL team's proposal at the Symposium on Operating System Principles in Gatlinburg.","completion":"Computer Network"}
{"prompt":"A diskless node is a mixture of the above two client models. Similar to a fat client, it processes locally, but relies on the server for storing persistent data. This approach offers features from both the fat client (multimedia support, high performance) and the thin client (high manageability, flexibility). A device running an online version of the video game Diablo III is an example of diskless node.","completion":"Computer Network"}
{"prompt":"Once a cyberattack has been initiated, there are certain targets that need to be attacked to cripple the opponent. Certain infrastructures as targets have been highlighted as critical infrastructures in times of conflict that can severely cripple a nation. Control systems, energy resources, finance, telecommunications, transportation, and water facilities are seen as critical infrastructure targets during conflict. A new report on the industrial cybersecurity problems, produced by the British Columbia Institute of Technology, and the PA Consulting Group, using data from as far back as 1981, reportedly has found a 10-fold increase in the number of successful cyberattacks on infrastructure Supervisory Control and Data Acquisition (SCADA) systems since 2000. Cyberattacks that have an adverse physical effect are known as cyber-physical attacks.","completion":"Computer Network"}
{"prompt":"Direct person-to-person communication includes non-verbal cues expressed in facial and other bodily articulation, that cannot be transmitted in traditional voice telephony. Video telephony restores such interactions to varying degrees. Social Context Cues Theory is a model to measure the success of different types of communication in maintaining the non-verbal cues present in face-to-face interactions. The research examines many different cues, such as the physical context, different facial expressions, body movements, tone of voice, touch and smell.\nVarious communication cues are lost with the usage of the telephone. The communicating parties are not able to identify the body movements, and lack touch and smell. Although this diminished ability to identify social cues is well known, Wiesenfeld, Raghuram, and Garud point out that there is a value and efficiency to the type of communication for different tasks. They examine work places in which different types of communication, such as the telephone, are more useful than face-to-face interaction.\nThe expansion of communication to mobile telephone service has created a different filter of the social cues than the land-line telephone. The use of instant messaging, such as texting, on mobile telephones has created a sense of community. In The Social Construction of Mobile Telephony it is suggested that each phone call and text message is more than an attempt to converse. Instead, it is a gesture which maintains the social network between family and friends. Although there is a loss of certain social cues through telephones, mobile phones bring new forms of expression of different cues that are understood by different audiences. New language additives attempt to compensate for the inherent lack of non-physical interaction.\nAnother social theory supported through telephony is the Media Dependency Theory. This theory concludes that people use media or a resource to attain certain goals. This theory states that there is a link between the media, audience, and the large social system. Telephones, depending on the person, help attain certain goals like accessing information, keeping in contact with others, sending quick communication, entertainment, etc.","completion":"Computer Network"}
{"prompt":"Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.\nIn 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazaa in 2006, and Limewire in 2010 to shut down or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.","completion":"Computer Network"}
{"prompt":"Wireless LAN based on the IEEE 802.11 standards, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. IEEE 802.11 shares many properties with wired Ethernet.","completion":"Computer Network"}
{"prompt":"A global area network (GAN) is a network used for supporting mobile users across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.","completion":"Computer Network"}
{"prompt":"Licklider played a similar role in conceiving of and funding early networking research. He formulated the earliest ideas of a global computer network in August 1962 at BBN, in a series of memos discussing the \"Intergalactic Computer Network\" concept. These ideas contained almost everything that the Internet is today, including cloud computing.While at IPTO he convinced Ivan Sutherland, Bob Taylor, and Lawrence G. Roberts that an all-encompassing computer network was a very important concept. He met with Donald Davies in 1965 and inspired his interest in data communications.In 1967 Licklider submitted the paper \"Televistas: Looking ahead through side windows\" to the Carnegie Commission on Educational Television. This paper describes a radical departure from the \"broadcast\" model of television. Instead Licklider advocates for a two-way communications network. The Carnegie Commission led to the creation of the Corporation for Public Broadcasting. Although the Commission's report explains that \"Dr. Licklider's paper was completed after the Commission had formulated its own conclusions,\" President Johnson said at the signing of the Public Broadcasting Act of 1967, \"So I think we must consider new ways to build a great network for knowledge\u2014not just a broadcast system, but one that employs every means of sending and of storing information that the individual can use\".His 1968 paper The Computer as a Communication Device illustrates his vision of network applications and predicts the use of computer networks to support communities of common interest and collaboration without regard to location.In the same 1968 paper, J. C. R. Licklider and Robert W. Taylor wrote, \"Take any problem worthy of the name, and you find only a few people who can contribute effectively to its solution. Those people must be brought into close intellectual partnership so that their ideas can come into contact with one another. But bring these people together physically in one place to form a team, and you have trouble, for the most creative people are often not the best team players, and there are not enough top positions in a single organization to keep them all happy. Let them go their separate ways, and each creates his own empire, large or small, and devotes more time to the role of emperor than to the role of problem solver.  The principals still get together at meetings. They still visit one another. But the time scale of their communication stretches out, and the correlations among mental models degenerate between meetings so that it may take a year to do a week's communicating. There has to be some way of facilitating communication among people wit bout [sic] [without] bringing them together in one place.\" (Evan Herbert edited the article and acted as intermediary during its writing between Licklider in Boston and Taylor in Washington.)\nThe Licklider Transmission Protocol is named after him.","completion":"Computer Network"}
{"prompt":"Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the mbox format. The specific format used is often indicated by special filename extensions:\n\neml\nUsed by many email clients including Novell GroupWise, Microsoft Outlook Express, Lotus notes, Windows Mail, Mozilla Thunderbird, and Postbox. The files contain the email contents as plain text in MIME format, containing the email header and body, including attachments in one or more of several formats.\nemlx\nUsed by Apple Mail.\nmsg\nUsed by Microsoft Office Outlook and OfficeLogic Groupware.\nmbx\nUsed by Opera Mail, KMail, and Apple Mail based on the mbox format.Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.","completion":"Computer Network"}
{"prompt":"Microsoft offers simple access control features built into their Windows operating system. Homegroup is a feature that allows shared disk access, shared printer access and shared scanner access among all computers and users (typically family members) in a home, in a similar fashion as in a small office workgroup, e.g., by means of distributed peer-to-peer networking (without a central server). Additionally, a home server may be added for increased functionality. The Windows HomeGroup feature was introduced with Microsoft Windows 7 in order to simplify file sharing in residences. All users (typically all family members), except guest accounts, may access any shared library on any computer that is connected to the home group. Passwords are not required from the family members during logon. Instead, secure file sharing is possible by means of a temporary password that is used when adding a computer to the HomeGroup.","completion":"Computer Network"}
{"prompt":"Network delay is a design and performance characteristic of a telecommunications network. It specifies the latency for a bit of data to travel across the network from one communication endpoint to another. Delay may differ slightly, depending on the location of the specific pair of communicating endpoints. Engineers usually report both the maximum and average delay, and they divide the delay into several components, the sum of which is the total delay:\n\nProcessing delay \u2013  time it takes a router to process the packet header\nQueuing delay \u2013  time the packet spends in routing queues\nTransmission delay \u2013  time it takes to push the packet's bits onto the link\nPropagation delay \u2013  time for a signal to propagate through the mediaA certain minimum level of delay is experienced by signals due to the time it takes to transmit a packet serially through a link. This delay is extended by more variable levels of delay due to network congestion. IP network delays can range from less than a microsecond to several hundred milliseconds.","completion":"Computer Network"}
{"prompt":"In 1985, the NSF funded the establishment of national supercomputing centers at several universities and provided network access and network interconnectivity with the NSFNET project in 1986. NSFNET became the Internet backbone for government agencies and universities.\nThe ARPANET project was formally decommissioned in 1990. The original IMPs and TIPs were phased out as the ARPANET was shut down after the introduction of the NSFNet, but some IMPs remained in service as late as July 1990.In the wake of the decommissioning of the ARPANET on 28 February 1990, Vinton Cerf wrote the following lamentation, entitled \"Requiem of the ARPANET\":\n\nIt was the first, and being first, was best,\n but now we lay it down to ever rest.\nNow pause with me a moment, shed some tears.\nFor auld lang syne, for love, for years and years\n of faithful service, duty done, I weep.\n Lay down thy packet, now, O friend, and sleep.\n-Vinton Cerf","completion":"Computer Network"}
{"prompt":"An attack can be active or passive.\nAn \"active attack\" attempts to alter system resources or affect their operation.\nA \"passive attack\" attempts to learn or make use of information from the system but does not affect system resources (e.g., wiretapping).An attack can be perpetrated by an insider or from outside the organization;\nAn \"inside attack\" is an attack initiated by an entity inside the security perimeter (an \"insider\"), i.e., an entity that is authorized to access system resources but uses them in a way not approved by those who granted the authorization.\nAn \"outside attack\" is initiated from outside the perimeter, by an unauthorized or illegitimate user of the system (an \"outsider\"). In the Internet, potential outside attackers range from amateur pranksters to organized criminals, international terrorists, and hostile governments.\nA resource (both physical or logical), called an asset, can have one or more vulnerabilities that can be exploited by a threat agent in a threat action. As a result, the confidentiality, integrity or availability of resources may be compromised. Potentially, the damage may extend to resources in addition to the one initially identified as vulnerable, including further resources of the organization, and the resources of other involved parties (customers, suppliers).\nThe so-called CIA triad is the basis of information security.\nThe attack can be active when it attempts to alter system resources or affect their operation: so it compromises integrity or availability. A \"passive attack\" attempts to learn or make use of information from the system but does not affect system resources: so it compromises confidentiality.\nA threat is a potential for violation of security, which exists when there is a circumstance, capability, action or event that could breach security and cause harm. That is, a threat is a possible danger that might exploit a vulnerability. A threat can be either \"intentional\" (i.e., intelligent; e.g., an individual cracker or a criminal organization) or \"accidental\" (e.g., the possibility of a computer malfunctioning, or the possibility of an \"act of God\" such as an earthquake, a fire, or a tornado).A set of policies concerned with information security management, the information security management systems (ISMS), has been developed to manage, according to risk management principles, the countermeasures in order to accomplish to a security strategy set up following rules and regulations applicable in a country.An attack should lead to a security incident i.e. a security event that involves a security violation. In other words, a security-relevant system event in which the system's security policy is disobeyed or otherwise breached.\nThe overall picture represents the risk factors of the risk scenario.An organization should take steps to detect, classify and manage security incidents. The first logical step is to set up an incident response plan and eventually a computer emergency response team.\nIn order to detect attacks, a number of countermeasures can be set up at organizational, procedural, and technical levels. Computer emergency response team, information technology security audit and intrusion detection system are examples of these.An attack usually is perpetrated by someone with bad intentions: black hatted attacks falls in this category, while other perform penetration testing on an organization information system to find out if all foreseen controls are in place.\nThe attacks can be classified according to their origin: I.E. if it is conducted using one or more computers: in the last case is called a distributed attack. Botnets are used to conduct distributed attacks.\nOther classifications are according to the procedures used or the type of vulnerabilities exploited: attacks can be concentrated on network mechanisms or host features.\nSome attacks are physical: i.e. theft or damage of computers and other equipment. Others are attempts to force changes in the logic used by computers or network protocols in order to achieve unforeseen (by the original designer) result but useful for the attacker. Software used to for logical attacks on computers is called malware.\nThe following is a partial short list of attacks:\n\nPassive\nComputer and network surveillance\nNetwork\nWiretapping\nFiber tapping\nPort scan\nIdle scan\nHost\nKeystroke logging\nData scraping\nBackdoor\nActive\nDenial-of-service attack\nA DDos or Distributed Denial of service attack is an attempt made by a hacker to block access to a server or a website that is connected to the Internet. This is achieved using multiple computerized systems, which overloads the target system with requests, making it incapable of responding to any query.\nSpoofing\nMixed threat attack\nNetwork\nMan-in-the-middle\nMan-in-the-browser\nARP poisoning\nPing flood\nPing of death\nSmurf attack\nHost\nBuffer overflow\nHeap overflow\nStack overflow\nFormat string attack\nBy modality\nSupply chain attack\nSocial engineering\nExploitIn detail, there are a number of techniques to utilize in cyberattacks and a variety of ways to administer them to individuals or establishments on a broader scale. Attacks are broken down into two categories: syntactic attacks and semantic attacks. Syntactic attacks are straightforward; it is considered malicious software which includes viruses, worms, and Trojan horses.","completion":"Computer Network"}
{"prompt":"In the core implementation of Interplanetary Internet,  satellites orbiting a planet communicate to other planet's satellites. Simultaneously, these planets revolve around the Sun with long distances, and thus many challenges face the communications. The reasons and the resultant challenges are:\nThe motion and long distances between planets:  The interplanetary communication is greatly delayed due to the interplanetary distances and the motion of the planets. The delay is variable and long, ranges from a couple of minutes (Earth-to-Mars), to a couple of hours (Pluto-to-Earth), depending on their relative positions. The interplanetary communication also suspends due to the solar conjunction, when the sun's radiation hinders the direct communication between the planets. As such, the communication characterizes lossy links and intermittent link connectivity.\nLow embeddable payload: Satellites can only carry a small payload, which poses challenges to the power, mass, size, and cost for communication hardware design. An asymmetric bandwidth would be the result of this limitation.  This asymmetry reaches ratios up to 1000:1 as downlink:uplink bandwidth portion.\nAbsence of fixed infrastructure: The graph of participating nodes in a specific planet to a specific planet communication keeps changing over time, due to the constant motion. The routes of the planet-to-planet communication are planned and scheduled rather than being opportunistic.The Interplanetary Internet design must address these challenges to operate successfully and achieve good communication with other planets. It also must use the few available resources efficiently in the system.","completion":"Computer Network"}
{"prompt":"Juniper Networks announced 100GbE for its T-series routers in June 2009. The 1x100GbE option followed in Nov 2010, when a joint press release with academic backbone network Internet2 marked the first production 100GbE interfaces going live in real network.In the same year, Juniper demonstrated 100GbE operation between core (T-series) and edge (MX 3D) routers. Juniper, in March 2011, announced first shipments of 100GbE interfaces to a major North American service provider (Verizon).\nIn April 2011, Juniper deployed a 100GbE system on the UK education network JANET. In July 2011, Juniper announced 100GbE with Australian ISP iiNet on their T1600 routing platform. Juniper started shipping the MPC3E line card for the MX router, a 100GbE CFP MIC, and a 100GbE LR4 CFP optics in March 2012. In Spring 2013, Juniper Networks announced the availability of the MPC4E line card for the MX router that includes 2 100GbE CFP slots and 8 10GbE SFP+ interfaces.\nIn June 2015, Juniper Networks announced the availability of its CFP-100GBASE-ZR module which is a plug & play solution that brings 80 km 100GbE to MX & PTX based networks. The CFP-100GBASE-ZR module uses DP-QPSK modulation and coherent receiver technology with an optimized DSP and FEC implementation. The low-power module can be directly retrofitted into existing CFP sockets on MX and PTX routers.","completion":"Computer Network"}
{"prompt":"Brocade Communications Systems introduced their first 100GbE products (based on the former Foundry Networks MLXe hardware) in September 2010. In June 2011, the new product went live at the AMS-IX traffic exchange point in Amsterdam.","completion":"Computer Network"}
{"prompt":"Estimates of the Internet's electricity usage have been the subject of controversy, according to a 2014 peer-reviewed research paper that found claims differing by a factor of 20,000 published in the literature during the preceding decade, ranging from 0.0064 kilowatt hours per gigabyte transferred (kWh\/GB) to 136 kWh\/GB. The researchers attributed these discrepancies mainly to the year of reference (i.e. whether efficiency gains over time had been taken into account) and to whether \"end devices such as personal computers and servers are included\" in the analysis.In 2011, academic researchers estimated the overall energy used by the Internet to be between 170 and 307 GW, less than two percent of the energy used by humanity. This estimate included the energy needed to build, operate, and periodically replace the estimated 750 million laptops, a billion smart phones and 100 million servers worldwide as well as the energy that routers, cell towers, optical switches, Wi-Fi transmitters and cloud storage devices use when transmitting Internet traffic. According to a non-peer reviewed study published in 2018 by The Shift Project (a French think tank funded by corporate sponsors), nearly 4% of global CO2 emissions could be attributed to global data transfer and the necessary infrastructure. The study also said that online video streaming alone accounted for 60% of this data transfer and therefore contributed to over 300 million tons of CO2 emission per year, and argued for new \"digital sobriety\" regulations restricting the use and size of video files.","completion":"Computer Network"}
{"prompt":"When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n\nComputer keyboard\nDigital camera\nDigital video\nGraphics tablet\nImage scanner\nJoystick\nMicrophone\nMouse\nOverlay keyboard\nReal-time clock\nTrackball\nTouchscreen\nLight pen","completion":"Computer Network"}
{"prompt":"In June 2003, a third modulation standard was ratified: 802.11g. This works in the 2.4 GHz band (like 802.11b), but uses the same OFDM based transmission scheme as 802.11a. It operates at a maximum physical layer bit rate of 54 Mbit\/s exclusive of forward error correction codes, or about 22 Mbit\/s average throughput. 802.11g hardware is fully backward compatible with 802.11b hardware, and therefore is encumbered with legacy issues that reduce throughput by ~21% when compared to 802.11a.The then-proposed 802.11g standard was rapidly adopted in the market starting in January 2003, well before ratification, due to the desire for higher data rates as well as reductions in manufacturing costs. By summer 2003, most dual-band 802.11a\/b products became dual-band\/tri-mode, supporting a and b\/g in a single mobile adapter card or access point. Details of making b and g work well together occupied much of the lingering technical process; in an 802.11g network, however, the activity of an 802.11b participant will reduce the data rate of the overall 802.11g network.\nLike 802.11b, 802.11g devices also suffer interference from other products operating in the 2.4 GHz band, for example, wireless keyboards.","completion":"Computer Network"}
{"prompt":"Today it can be important to distinguish between the Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.\nEmail privacy, without some security precautions, can be compromised because:\n\nemail messages are generally not encrypted.\nemail messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.\nmany Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.\nthe \"Received:\"-fields and other information in the email can often identify the sender, preventing anonymous communication.\nweb bugs invisibly embedded in HTML content can alert the sender of any email whenever an email is rendered as HTML (some e-mail clients do this when the user reads, or re-reads the e-mail) and from which IP address. It can also reveal whether an email was read on a smartphone or a PC, or Apple Mac device via the user agent string.There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail, or S\/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security\/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.\nAdditionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this. Finally, the attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.","completion":"Computer Network"}
{"prompt":"Control frames facilitate the exchange of data frames between stations. Some common 802.11 control frames include:\n\nAcknowledgement (ACK) frame: After receiving a data frame, the receiving station will send an ACK frame to the sending station if no errors are found. If the sending station does not receive an ACK frame within a predetermined period of time, the sending station will resend the frame.\nRequest to Send (RTS) frame: The RTS and CTS frames provide an optional collision reduction scheme for access points with hidden stations. A station sends an RTS frame as the first step in a two-way handshake required before sending data frames.\nClear to Send (CTS) frame: A station responds to an RTS frame with a CTS frame. It provides clearance for the requesting station to send a data frame. The CTS provides collision control management by including a time value for which all other stations are to hold off transmission while the requesting station transmits.","completion":"Computer Network"}
{"prompt":"802.11n is an amendment that improves upon the previous 802.11 standards; its first draft of certification was published in 2006. The 802.11n standard was retroactively labelled as Wi-Fi 4 by the Wi-Fi Alliance. The standard added support for multiple-input multiple-output antennas (MIMO). 802.11n operates on both the 2.4 GHz and the 5 GHz bands. Support for 5 GHz bands is optional. Its net data rate ranges from 54 Mbit\/s to 600 Mbit\/s. The IEEE has approved the amendment, and it was published in October 2009. Prior to the final ratification, enterprises were already migrating to 802.11n networks based on the Wi-Fi Alliance's certification of products conforming to a 2007 draft of the 802.11n proposal.","completion":"Computer Network"}
{"prompt":"There are two definitions for wireless LAN roaming:\n\nInternal roaming: The mobile station (MS) moves from one access point (AP) to another AP within a home network if the signal strength is too weak. An authentication server performs the re-authentication of MS via 802.1x (e.g. with PEAP). The billing of QoS is in the home network. A MS roaming from one access point to another often interrupts the flow of data between the MS and an application connected to the network. The MS, for instance, periodically monitors the presence of alternative APs (ones that will provide a better connection). At some point, based on proprietary mechanisms, the MS decides to re-associate with an AP having a stronger wireless signal. The MS, however, may lose a connection with an AP before associating with another access point. To provide reliable connections with applications, the MS must generally include software that provides session persistence.\nExternal roaming: The MS (client) moves into a WLAN of another wireless Internet service provider (WISP) and takes their services. The user can use a foreign network independently from their home network, provided that the foreign network allows visiting users on their network. There must be special authentication and billing systems for mobile services in a foreign network.","completion":"Computer Network"}
{"prompt":"Electronic voting systems may offer advantages compared to other voting techniques. An electronic voting system can be involved in any one of a number of steps in the setup, distributing, voting, collecting, and counting of ballots, and thus may or may not introduce advantages into any of these steps. Potential disadvantages exist as well including the potential for flaws or weakness in any electronic component.\nCharles Stewart of the Massachusetts Institute of Technology estimates that 1 million more ballots were counted in the 2004 US presidential election than in 2000 because electronic voting machines detected votes that paper-based machines would have missed.In May 2004 the U.S. Government Accountability Office released a report titled \"Electronic Voting Offers Opportunities and Presents Challenges\", analyzing both the benefits and concerns created by electronic voting. A second report was released in September 2005 detailing some of the concerns with electronic voting, and ongoing improvements, titled \"Federal Efforts to Improve Security and Reliability of Electronic Voting Systems Are Under Way, but Key Activities Need to Be Completed\".","completion":"Computer Network"}
{"prompt":"Hackers from Azerbaijan and Armenia have actively participated in cyber warfare as part of the Nagorno-Karabakh conflicyber warfare over the disputed region of Nagorno-Karabakh, with Azerbaijani hackers targeting Armenian websites and posting Ilham Aliyev's statements.","completion":"Computer Network"}
{"prompt":"The simplest topology with a dedicated link between two endpoints. Easiest to understand, of the variations of point-to-point topology, is a point-to-point communication channel that appears, to the user, to be permanently associated with the two endpoints. A child's tin can telephone is one example of a physical dedicated channel.\nUsing circuit-switching or packet-switching technologies, a point-to-point circuit can be set up dynamically and dropped when no longer needed. Switched point-to-point topologies are the basic model of conventional telephony.\nThe value of a permanent point-to-point network is unimpeded communications between the two endpoints. The value of an on-demand point-to-point connection is proportional to the number of potential pairs of subscribers and has been expressed as Metcalfe's Law.","completion":"Computer Network"}
{"prompt":"The Linux kernel's Netfilter framework, which implements NAT in Linux, has features and modules for several NAT ALGs:\n\nAmanda protocol\nFTP\nIRC\nSIP\nTFTP\nIPsec\nH.323\nPPTP\nL2TP","completion":"Computer Network"}
{"prompt":"Based on international research initiatives, particularly the contributions of R\u00e9mi Despr\u00e9s, packet switching network standards were developed by the International Telegraph and Telephone Consultative Committee (ITU-T) in the form of X.25 and related standards. X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET, the United Kingdom's high-speed national research and education network (NREN). The initial ITU Standard on X.25 was approved in March 1976. Existing networks, such as Telenet in the United States adopted X.25 as well as new public data networks, such as DATAPAC in Canada and TRANSPAC in France. X.25 was supplemented by the X.75 protocol which enabled internetworking between national PTT networks in Europe and commercial networks in North America.The British Post Office, Western Union International, and Tymnet collaborated to create the first international packet-switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.\nThe first public dial-in networks used asynchronous teleprinter (TTY) terminal protocols to reach a concentrator operated in the public network. Some networks, such as Telenet and CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.","completion":"Computer Network"}
{"prompt":"Power points (receptacles, plugs, wall sockets) need to be installed throughout the house in locations where power will be required. In many areas the installation must be done in compliance with standards and by a licensed or qualified electrician. Power points are typically located where there will be an appliance installed such as telephone, computers, television, home theater, security system, CCTV system.","completion":"Computer Network"}
{"prompt":"The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production.Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:\n\nDelivery Reports can be used to verify whether an address exists and if so, this indicates to a spammer that it is available to be spammed.\nIf the spammer uses a forged sender email address (email spoofing), then the innocent email address that was used can be flooded with NDRs from the many invalid email addresses the spammer may have attempted to mail. These NDRs then constitute spam from the ISP to the innocent user.In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns, and only work with email clients that support rendering of HTML. Many mail clients now default to not showing \"web content\". Webmail providers can also disrupt web bugs by pre-caching images.","completion":"Computer Network"}
{"prompt":"The original version of the protocol (now called Pure ALOHA, and the one implemented in ALOHAnet) was quite simple:  If you have data to send, send the data If, while you are transmitting data, you receive any data from another station, there has been a message collision.. All transmitting stations will need to try resending later.Pure ALOHA does not check whether the channel is busy before transmitting.. Since collisions can occur and data may have to be sent again, ALOHA cannot efficiently use 100% of the capacity of the communications channel.. How long a station waits until it retransmits, and the likelihood a collision occurs are interrelated, and both affect how efficiently the channel can be used.. This means that the concept of retransmit later is a critical aspect; The quality of the backoff scheme chosen significantly influences the efficiency of the protocol, the ultimate channel capacity, and the predictability of its behavior.. To assess Pure ALOHA, there is a need to predict its throughput, the rate of (successful) transmission of frames.. First make a few simplifying assumptions:  All frames have the same length.. Stations cannot generate a frame while transmitting or trying to transmit.. That is, while a station is sending or trying to resend a frame, it cannot be allowed to generate more frames to send.. The population of stations attempting to transmit (both new transmission and retransmissions) follows a Poisson distribution.Let T refer to the time needed to transmit one frame on the channel, and define frame-time as a unit of time equal to T. Let G refer to the mean used in the Poisson distribution over transmission-attempt amounts.. That is, on average, there are G transmission attempts per frame-time.. Consider what needs to happen for a frame to be transmitted successfully.. Let t refer to the time at which it is intended to send a frame.. It is preferable to use the channel for one frame-time beginning at t, and all other stations to refrain from transmitting during this time..","completion":"Computer Network"}
{"prompt":"Under DiffServ, all the policing and classifying are done at the boundaries between DiffServ domains. This means that in the core of the Internet, routers are unhindered by the complexities of collecting payment or enforcing agreements. That is, in contrast to IntServ, DiffServ requires no advance setup, no reservation, and no time-consuming end-to-end negotiation for each flow.\nThe details of how individual routers deal with the DS field are configuration specific, therefore it is difficult to predict end-to-end behavior. This is complicated further if a packet crosses two or more DiffServ domains before reaching its destination. From a commercial viewpoint, this means that it is impossible to sell different classes of end-to-end connectivity to end users, as one provider's Gold packet may be another's Bronze. DiffServ or any other IP-based QoS marking does not ensure the quality of the service or a specified service-level agreement (SLA). By marking the packets, the sender indicates that it wants the packets to be treated as a specific service, but there is no guarantee this happens. It is up to all the service providers and their routers in the path to ensure that their policies will take care of the packets in an appropriate fashion.","completion":"Computer Network"}
{"prompt":"A router is an internetworking device that forwards packets between networks by processing the addressing or routing information included in the packet.  The routing information is often processed in conjunction with the routing table.  A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks.","completion":"Computer Network"}
{"prompt":"Roberts engaged Howard Frank to consult on the topological design of the network. Frank made recommendations to increase throughput and reduce costs in a scaled-up network. By March 1970, the ARPANET reached the East Coast of the United States, when an IMP at BBN in Cambridge, Massachusetts was connected to the network. Thereafter, the ARPANET grew: 9 IMPs by June 1970 and 13 IMPs by December 1970, then 18 by September 1971 (when the network included 23 university and government hosts); 29 IMPs by August 1972, and 40 by September 1973. By June 1974, there were 46 IMPs, and in July 1975, the network numbered 57 IMPs. By 1981, the number was 213 host computers, with another host connecting approximately every twenty days.Support for inter-IMP circuits of up to 230.4 kbit\/s was added in 1970, although considerations of cost and IMP processing power meant this capability was not actively used.\nLarry Roberts saw the ARPANET and NPL projects as complementary and sought in 1970 to connect them via a satellite link. Peter Kirstein's research group at University College London (UCL) was subsequently chosen in 1971 in place of NPL for the UK connection. In June 1973, a transatlantic satellite link connected ARPANET to the Norwegian Seismic Array (NORSAR), via the Tanum Earth Station in Sweden, and onward via a terrestrial circuit to a TIP at UCL. UCL provided a gateway for interconnection of the ARPANET with British academic networks, the first international resource sharing network, and carried out some of the earliest experimental research work on internetworking.1971 saw the start of the use of the non-ruggedized (and therefore significantly lighter) Honeywell 316 as an IMP.\nIt could also be configured as a Terminal Interface Processor (TIP), which provided terminal server support for up to 63 ASCII serial terminals through a multi-line controller in place of one of the hosts. The 316 featured a greater degree of integration than the 516, which made it less expensive and easier to maintain. The 316 was configured with 40 kB of core memory for a TIP. The size of core memory was later increased, to 32 kB for the IMPs, and 56 kB for TIPs, in 1973.\nThe ARPANET was demonstrated at the International Conference on Computer Communications in October 1972. \nIn 1975, BBN introduced IMP software running on the Pluribus multi-processor. These appeared in a few sites. In 1981, BBN introduced IMP software running on its own C\/30 processor product.","completion":"Computer Network"}
{"prompt":"An augmenting path is a path (u1, u2, ..., uk) in the residual network, where u1 = s, uk = t, and for all ui, ui + 1 (cf (ui, ui + 1) > 0) (1 \u2264 i < k). More simply, an augmenting path is an available flow path from the source to the sink. A network is at maximum flow if and only if there is no augmenting path in the residual network Gf.\nThe bottleneck is the minimum residual capacity of all the edges in a given augmenting path. See example explained in the \"Example\" section of this article. The flow network is at maximum flow if and only if it has a bottleneck with a value equal to zero. If any augmenting path exists, its bottleneck weight will be greater than 0. In other words, if there is a bottleneck value greater than 0, then there is an augmenting path from the source to the sink. However, we know that if there is any augmenting path, then the network is not at maximum flow, which in turn means that, if there is a bottleneck value greater than 0, then the network is not at maximum flow.\nThe term \"augmenting the flow\" for an augmenting path means updating the flow f of each arc in this augmenting path to equal the capacity c of the bottleneck. Augmenting the flow corresponds to pushing additional flow along the augmenting path until there is no remaining available residual capacity in the bottleneck.","completion":"Computer Network"}
{"prompt":"There is active research to make non-classical computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.","completion":"Computer Network"}
{"prompt":"There are also hybrid systems that include an electronic ballot marking device (usually a touch screen system similar to a DRE) or other assistive technology to print a voter verified paper audit trail, then use a separate machine for electronic tabulation. Hybrid voting often includes both e-voting and mail-in paper ballots. Internet voting can use remote locations (voting from any Internet capable computer) or can use traditional polling locations with voting booths consisting of Internet connected voting systems.","completion":"Computer Network"}
{"prompt":"Steve Crocker formed a \"Networking Working Group\" in 1969 with Vint Cerf, who also joined an International Networking Working Group in 1972. These groups considered how to interconnect packet switching networks with different specifications, that is, internetworking. Stephen J. Lukasik directed DARPA to focus on internetworking research in the early 1970s. Research led by Bob Kahn at DARPA and Vint Cerf at Stanford University and later DARPA resulted in the formulation of the Transmission Control Program, which incorporated concepts from the French CYCLADES project directed by Louis Pouzin. Its specification was written by Cerf with Yogen Dalal and Carl Sunshine in December 1974 (RFC 675). The following year, testing began through concurrent implementations at Stanford, BBN and University College London. At first a monolithic design, the software was redesigned as a modular protocol stack in version 3 in 1978. Version 4 was installed in the ARPANET for production use in January 1983, replacing NCP.  The development of the complete Internet protocol suite by 1989, as outlined in RFC 1122 and RFC 1123, and partnerships with the telecommunication and computer industry laid the foundation for the adoption of TCP\/IP as a comprehensive protocol suite as the core component of the emerging Internet.","completion":"Computer Network"}
{"prompt":"Mellanox Technologies introduced the ConnectX-4 100GbE single and dual port adapter in November 2014. In the same period, Mellanox introduced availability of 100GbE copper and fiber cables. In June 2015, Mellanox introduced the Spectrum 10, 25, 40, 50 and 100GbE switch models.","completion":"Computer Network"}
{"prompt":"On July 18, 2006, a call for interest for a High Speed Study Group (HSSG) to investigate new standards for high speed Ethernet was held at the IEEE 802.3 plenary meeting in San Diego.The first 802.3 HSSG study group meeting was held in September 2006. In June 2007, a trade group called \"Road to 100G\" was formed after the NXTcomm trade show in Chicago.On December 5, 2007, the Project Authorization Request (PAR) for the P802.3ba 40 Gbit\/s and 100 Gbit\/s Ethernet Task Force was approved with the following project scope:\n\nThe purpose of this project is to extend the 802.3 protocol to operating speeds of 40 Gbit\/s and 100 Gbit\/s in order to provide a significant increase in bandwidth while maintaining maximum compatibility with the installed base of 802.3 interfaces, previous investment in research and development, and principles of network operation and management. The project is to provide for the interconnection of equipment satisfying the distance requirements of the intended applications.\n\nThe 802.3ba task force met for the first time in January 2008. This standard was approved at the June 2010 IEEE Standards Board meeting under the name IEEE Std 802.3ba-2010.The first 40 Gbit\/s Ethernet Single-mode Fibre PMD study group meeting was held in January 2010 and on March 25, 2010, the P802.3bg Single-mode Fibre PMD Task Force was approved for the 40 Gbit\/s serial SMF PMD.\n\nThe scope of this project is to add a single-mode fiber Physical Medium Dependent (PMD) option for serial 40 Gbit\/s operation by specifying additions to, and appropriate modifications of, IEEE Std 802.3-2008 as amended by the IEEE P802.3ba project (and any other approved amendment or corrigendum).\nOn June 17, 2010, the IEEE 802.3ba standard was approved. In March 2011, the IEEE 802.3bg standard was approved. On September 10, 2011, the P802.3bj 100 Gbit\/s Backplane and Copper Cable task force was approved.\nThe scope of this project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add 100 Gbit\/s 4-lane Physical Layer (PHY) specifications and management parameters for operation on backplanes and twinaxial copper cables, and specify optional Energy Efficient Ethernet (EEE) for 40 Gbit\/s and 100 Gbit\/s operation over backplanes and copper cables.\nOn May 10, 2013, the P802.3bm 40 Gbit\/s and 100 Gbit\/s Fiber Optic Task Force was approved.\nThis project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add 100 Gbit\/s Physical Layer (PHY) specifications and management parameters, using a four-lane electrical interface for operation on multimode and single-mode fiber optic cables, and to specify optional Energy Efficient Ethernet (EEE) for 40 Gbit\/s and 100 Gbit\/s operation over fiber optic cables. In addition, to add 40 Gbit\/s Physical Layer (PHY) specifications and management parameters for operation on extended reach (>10 km) single-mode fiber optic cables.\nAlso on May 10, 2013, the P802.3bq 40GBASE-T Task Force was approved.\nSpecify a Physical Layer (PHY) for operation at 40 Gbit\/s on balanced twisted-pair copper cabling, using existing Media Access Control, and with extensions to the appropriate physical layer management parameters.\nOn June 12, 2014, the IEEE 802.3bj standard was approved.On February 16, 2015, the IEEE 802.3bm standard was approved.On May 12, 2016, the IEEE P802.3cd Task Force started working to define next generation two-lane 100 Gbit\/s PHY.On May 14, 2018, the PAR for the IEEE P802.3ck Task Force was approved. The scope of this project is to specify additions to and appropriate modifications of IEEE Std 802.3 to add Physical Layer specifications and Management Parameters for 100 Gbit\/s, 200 Gbit\/s, and 400 Gbit\/s electrical interfaces based on 100 Gbit\/s signaling.On December 5, 2018, the IEEE-SA Board approved the IEEE 802.3cd standard.\nOn November 12, 2018, the IEEE P802.3ct Task Force started working to define PHY supporting 100 Gbit\/s operation on a single wavelength capable of at least 80 km over a DWDM system (using a combination of phase and amplitude modulation with coherent detection).In May 2019, the IEEE P802.3cu Task Force started working to define single-wavelength 100 Gbit\/s PHYs for operation over SMF (Single-Mode Fiber) with lengths up to at least 2 km (100GBASE-FR1) and 10 km (100GBASE-LR1).In June 2020, the IEEE P802.3db Task Force started working to define a physical layer specification that supports 100 Gbit\/s operation over 1 pair of MMF with lengths up to at least 50 m.On February 11, 2021, the IEEE-SA Board approved the IEEE 802.3cu standard.On June 16, 2021, the IEEE-SA Board approved the IEEE 802.3ct standard.On September 21, 2022, the IEEE-SA Board approved the IEEE 802.3ck and 802.3db standards.","completion":"Computer Network"}
{"prompt":"The Internet has achieved new relevance as a political tool. The presidential campaign of Howard Dean in 2004 in the United States was notable for its success in soliciting donation via the Internet. Many political groups use the Internet to achieve a new method of organizing for carrying out their mission, having given rise to Internet activism, most notably practiced by rebels in the Arab Spring. The New York Times suggested that social media websites, such as Facebook and Twitter, helped people organize the political revolutions in Egypt, by helping activists organize protests, communicate grievances, and disseminate information.Many have understood the Internet as an extension of the Habermasian notion of the public sphere, observing how network communication technologies provide something like a global civic forum. However, incidents of politically motivated Internet censorship have now been recorded in many countries, including western democracies.","completion":"Computer Network"}
{"prompt":"A thin client is a minimal sort of client.  Thin clients use the resources of the host computer. A thin client generally only presents processed data provided by an application server, which performs the bulk of any required data processing. A device using web application (such as Office Web Apps) is a thin client.","completion":"Computer Network"}
{"prompt":"A series of powerful cyberattacks began 27 June, 2017, that swamped websites of Ukrainian organizations, including banks, ministries, newspapers and electricity firms. In January 2022, Microsoft disclosed activity of a ransomware and DoS attack on various government agencies and organizations.","completion":"Computer Network"}
{"prompt":"A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer, a typical modern definition of a computer is: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" According to this definition, any device that processes information qualifies as a computer.","completion":"Computer Network"}
{"prompt":"40G Transceiver Form Factors\nThe QSFP+ form factor is specified for use with the 40 Gigabit Ethernet.  Copper direct attached cable (DAC) or optical modules are supported, see Figure 85\u201320 in the 802.3 spec. QSFP+ modules at 40Gbit\/s can also be used to provide four independent ports of 10 gigabit Ethernet.100G Transceiver Form Factors\nCFP modules use the 10-lane CAUI-10 electrical interface.CFP2 modules use the 10-lane CAUI-10 electrical interface or the 4-lane CAUI-4 electrical interface.CFP4 modules use the 4-lane CAUI-4 electrical interface.QSFP28 modules use the CAUI-4 electrical interface.SFP-DD or Small Form-factor Pluggable \u2013 Double Density modules use the 100GAUI-2 electrical interface.Cisco's CPAK optical module uses the four lane CEI-28G-VSR electrical interface.There are also CXP and HD module standards. CXP modules use the CAUI-10 electrical interface.","completion":"Computer Network"}
{"prompt":"Messaging applications have also changed how people communicate in the workplace. Enterprise messaging applications like Slack, TeleMessage, Teamnote and Yammer allow companies to enforce policies on how employees message at work and ensure secure storage of sensitive data. Message applications allow employees to separate work information from their personal emails and texts.\nMessaging applications may make workplace communication efficient, but they can also have consequences on productivity. A study at Slack showed on average, people spend 10 hours a day on Slack, which is about 67% more time than they spend using email.","completion":"Computer Network"}
{"prompt":"In the 1960s, computer scientists began developing systems for time-sharing of computer resources. J. C. R. Licklider proposed the idea of a universal network while working at Bolt Beranek & Newman and, later, leading the Information Processing Techniques Office (IPTO) at the Advanced Research Projects Agency (ARPA) of the United States Department of Defense (DoD). Research into packet switching, one of the fundamental Internet technologies, started in the work of Paul Baran in the early 1960s and, independently, Donald Davies in 1965. After the Symposium on Operating Systems Principles in 1967, packet switching from the proposed NPL network was incorporated into the design of the ARPANET, an experimental resource sharing network proposed by ARPA.ARPANET development began with two network nodes which were interconnected between the University of California, Los Angeles (UCLA) and SRI International (SRI) on 29 October 1969. The third site was at the University of California, Santa Barbara, followed by the University of Utah. In a sign of future growth, 15 sites were connected to the young ARPANET by the end of 1971. These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing. Thereafter, the ARPANET gradually developed into a decentralized communications network, connecting remote centers and military bases in the United States. Other user networks and research networks, such as the Merit Network and CYCLADES, were developed in the late 1960s and early 1970s.Early international collaborations for the ARPANET were rare. Connections were made in 1973 to the Norwegian Seismic Array (NORSAR), and to Peter Kirstein's research group at University College London, which provided a gateway to British academic networks, forming the first internetwork for resource sharing. ARPA projects, international working groups and commercial initiatives led to the development of various protocols and standards by which multiple separate networks could become a single network or \"a network of networks\". In 1974, Bob Kahn at DARPA and Vint Cerf at Stanford University published their ideas for \"A Protocol for Packet Network Intercommunication\". They used the term internet as a shorthand for internetwork in RFC 675, and later RFCs repeated this use. Kahn and Cerf credit Louis Pouzin with important influences on the resulting TCP\/IP design. National PTTs and commercial providers developed the X.25 standard and deployed it on public data networks.Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP\/IP) was standardized, which permitted worldwide proliferation of interconnected networks. TCP\/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit\/s and later at 1.5 Mbit\/s and 45 Mbit\/s. The NSFNet expanded into academic and research organizations in Europe, Australia, New Zealand and Japan in 1988\u201389. Although other network protocols such as UUCP and PTT public data networks had global reach well before this time, this marked the beginning of the Internet as an intercontinental network. Commercial Internet service providers (ISPs) emerged in 1989 in the United States and Australia. The ARPANET was decommissioned in 1990.\nSteady advances in semiconductor technology and optical networking created new economic opportunities for commercial involvement in the expansion of the network in its core and for delivering services to the public. In mid-1989, MCI Mail and Compuserve established connections to the Internet, delivering email and public access products to the half million users of the Internet. Just months later, on 1 January 1990, PSInet launched an alternate Internet backbone for commercial use; one of the networks that added to the core of the commercial Internet of later years. In March 1990, the first high-speed T1 (1.5 Mbit\/s) link between the NSFNET and Europe was installed between Cornell University and CERN, allowing much more robust communications than were capable with satellites. Six months later Tim Berners-Lee would begin writing WorldWideWeb, the first web browser, after two years of lobbying CERN management. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the HyperText Transfer Protocol (HTTP) 0.9, the HyperText Markup Language (HTML), the first Web browser (which was also an HTML editor and could access Usenet newsgroups and FTP files), the first HTTP server software (later known as CERN httpd), the first web server, and the first Web pages that described the project itself. In 1991 the Commercial Internet eXchange was founded, allowing PSInet to communicate with the other commercial networks CERFnet and Alternet. Stanford Federal Credit Union was the first financial institution to offer online Internet banking services to all of its members in October 1994. In 1996, OP Financial Group, also a cooperative bank, became the second online bank in the world and the first in Europe. By 1995, the Internet was fully commercialized in the U.S. when the NSFNet was decommissioned, removing the last restrictions on use of the Internet to carry commercial traffic.\nAs technology advanced and commercial opportunities fueled reciprocal growth, the volume of Internet traffic started experiencing similar characteristics as that of the scaling of MOS transistors, exemplified by Moore's law, doubling every 18 months. This growth, formalized as Edholm's law, was catalyzed by advances in MOS technology, laser light wave systems, and noise performance.Since 1995, the Internet has tremendously impacted culture and commerce, including the rise of near-instant communication by email, instant messaging, telephony (Voice over Internet Protocol or VoIP), two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking services, and online shopping sites. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1 Gbit\/s, 10 Gbit\/s, or more. The Internet continues to grow, driven by ever-greater amounts of online information and knowledge, commerce, entertainment and social networking services. During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%. This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network. As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30.2% of world population). It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication. By 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet.","completion":"Computer Network"}
{"prompt":"The MOS transistor underpinned the rapid growth of telecommunication bandwidth over the second half of the 20th century. To address the need for transmission capacity beyond that provided by radio, satellite and analog copper telephone lines, engineers developed optical communications systems based on fiber optic cables powered by lasers and optical amplifier techniques.\nThe concept of lasing arose from a 1917 paper by Albert Einstein, \"On the Quantum Theory of Radiation.\" Einstein expanded upon a dialog with Max Planck on how atoms absorb and emit light, part of a thought process that, with input from Erwin Schr\u00f6dinger, Werner Heisenberg and others, gave rise to Quantum Mechanics. Specifically, in his quantum theory, Einstein mathematically determined that light could be generated not only by spontaneous emission, such as the light emitted by an incandescent light or the Sun, but also by stimulated emission.\nForty years later, on November 13, 1957, Columbia University physics student Gordon Gould first realized how to make light by stimulated emission through a process of optical amplification. He coined the term LASER for this technology\u2014Light Amplification by Stimulated Emission of Radiation. Using Gould's light amplification method (patented as \"Optically Pumped Laser Amplifier\"), Theodore Maiman made the first working laser on May 16, 1960.Gould co-founded Optelecom, Inc. in 1973 to commercialize his inventions in optical fiber telecommunications. just as Corning Glass was producing the first commercial fiber optic cable in small quantities. Optelecom configured its own fiber lasers and optical amplifiers into the first commercial optical communication systems which it delivered to Chevron and the US Army Missile Defense. Three years later, GTE deployed the first optical telephone system in 1977 in Long Beach, California. By the early 1980s, optical networks powered by lasers, LED and optical amplifier equipment supplied by Bell Labs, NTT and Perelli were used by select universities and long-distance telephone providers.","completion":"Computer Network"}
{"prompt":"A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances.  A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI model: the physical layer, the data link layer, and the network layer.","completion":"Computer Network"}
{"prompt":"\"Connected\" light bulbs such as Lifx, Philips Hue, Samsung Smart Bulb, GE Link\nZigbee Light Link is the open standards protocol used by current major \"Connected\" light bulb vendors","completion":"Computer Network"}
{"prompt":"Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was most likely a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, likely livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.\n\nThe abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BCE. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.\nThe Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c.\u2009100 BCE. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab\u016b Rayh\u0101n al-B\u012br\u016bn\u012b in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BCE and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Ab\u016b Rayh\u0101n al-B\u012br\u016bn\u012b invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, c.\u20091000 AD.\nThe sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.\nThe planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.\n\nThe slide rule was invented around 1620\u20131630 by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.\nIn the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Mus\u00e9e d'Art et d'Histoire of Neuch\u00e2tel, Switzerland, and still operates.In 1831\u20131835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which, through a system of pulleys and cylinders and over, could predict the perpetual calendar for every year from 0 CE (that is, 1 BCE) to  4000 CE, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.\nThe differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.\nIn the 1890s, the Spanish engineer Leonardo Torres Quevedo began to develop a series of advanced analog machines that could solve real and complex roots of polynomials, which were published in 1901 by the Paris Academy of Sciences.","completion":"Computer Network"}
{"prompt":"Optical communications, in various forms, have been used for thousands of years. The ancient Greeks used a coded alphabetic system of signalling with torches developed by Cleoxenus, Democleitus and Polybius. In the modern era, semaphores and wireless solar telegraphs called heliographs were developed, using coded signals to communicate with their recipients.\nIn 1880, Alexander Graham Bell and his assistant Charles Sumner Tainter created the photophone, at Bell's newly established Volta Laboratory in Washington, DC. Bell considered it his most important invention. The device allowed for the transmission of sound on a beam of light. On June 3, 1880, Bell conducted the world's first wireless telephone transmission between two buildings, some 213 meters (700 feet) apart.Its first practical use came in military communication systems many decades later, first for optical telegraphy. German colonial troops used heliograph telegraphy transmitters during the Herero and Namaqua genocide starting in 1904, in German South-West Africa (today's Namibia) as did British, French, US or Ottoman signals.\n\nDuring the trench warfare of World War I when wire communications were often cut, German signals used three types of optical Morse transmitters called Blinkger\u00e4t, the intermediate type for distances of up to 4 km (2.5 miles) at daylight and of up to 8 km (5 miles) at night, using red filters for undetected communications. Optical telephone communications were tested at the end of the war, but not introduced at troop level. In addition, special blinkger\u00e4ts were used for communication with airplanes, balloons, and tanks, with varying success.A major technological step was to replace the Morse code by modulating optical waves in speech transmission. Carl Zeiss, Jena developed the Lichtsprechger\u00e4t 80\/80 (literal translation: optical speaking device) that the German army used in their World War II anti-aircraft defense units, or in bunkers at the Atlantic Wall.The invention of lasers in the 1960s revolutionized free-space optics. Military organizations were particularly interested and boosted their development. However, the technology lost market momentum when the installation of optical fiber networks for civilian uses was at its peak.\nMany simple and inexpensive consumer remote controls use low-speed communication using infrared (IR) light. This is known as consumer IR technologies.","completion":"Computer Network"}
{"prompt":"The ITU-T G.hn and IEEE Powerline standard, which provide high-speed (up to 1 Gbit\/s) local area networking over existing home wiring, are examples of home networking technology designed specifically for IPTV delivery. Recently, the IEEE passed proposal P1901 which grounded a standard within the Market for wireline products produced and sold by companies that are part of the HomePlug Alliance.  The IEEE is continuously working to push for P1901 to be completely recognized worldwide as the sole standard for all future products that are produced for Home Networking.\n\nHomePlug and HomePNA are associated standards\nUniversal Powerline Association","completion":"Computer Network"}
{"prompt":"DiffServ is a coarse-grained, class-based mechanism for traffic management. In contrast, IntServ is a fine-grained, flow-based mechanism. DiffServ relies on a mechanism to classify and mark packets as belonging to a specific class.  DiffServ-aware routers implement per-hop behaviors (PHBs), which define the packet-forwarding properties associated with a class of traffic.  Different PHBs may be defined to offer, for example, low-loss or low-latency service.\nRather than differentiating network traffic based on the requirements of an individual flow, DiffServ operates on the principle of traffic classification, placing each data packet into one of a limited number of traffic classes. Each router on the network is then configured to differentiate traffic based on its class. Each traffic class can be managed differently, ensuring preferential treatment for higher-priority traffic on the network. The premise of Diffserv is that complicated functions such as packet classification and policing can be carried out at the edge of the network by edge routers. Since no classification and policing is required in the core routers, functionality there can then be kept simple. Core routers simply apply PHB treatment to packets based on their markings. PHB treatment is achieved by core routers using a combination of scheduling policy and queue management policy.\nA group of routers that implement common, administratively defined DiffServ policies are referred to as a DiffServ domain.While DiffServ does recommend a standardized set of traffic classes, the DiffServ architecture does not incorporate predetermined judgments of what types of traffic should be given priority treatment.  DiffServ simply provides a framework to allow classification and differentiated treatment. The standard traffic classes (discussed below) serve to simplify interoperability between different networks and different vendors' equipment.","completion":"Computer Network"}
{"prompt":"Optical signal transmission over a nonlinear medium is principally an analog design problem. As such, it has evolved slower than digital circuit lithography (which generally progressed in step with Moore's law). This explains why 10 Gbit\/s transport systems existed since the mid-1990s, while the first forays into 100 Gbit\/s transmission happened about 15 years later \u2013 a 10x speed increase over 15 years is far slower than the 2x speed per 1.5 years typically cited for Moore's law.\nNevertheless, at least five firms (Ciena, Alcatel-Lucent, MRV, ADVA Optical and Huawei) made customer announcements for 100 Gbit\/s transport systems by August 2011, with varying degrees of capabilities. Although vendors claimed that 100 Gbit\/s light paths could use existing analog optical infrastructure, deployment of high-speed technology was tightly controlled and extensive interoperability tests were required before moving them into service.\nDesigning routers or switches which support 100 Gbit\/s interfaces is difficult. The need to process a 100 Gbit\/s stream of packets at line rate without reordering within IP\/MPLS microflows is one reason for this.\nAs of 2011, most components in the 100 Gbit\/s packet processing path (PHY chips, NPUs, memories) were not readily available off-the-shelf or require extensive qualification and co-design. Another problem is related to the low-output production of 100 Gbit\/s optical components, which were also not easily available \u2013 especially in pluggable, long-reach or tunable laser flavors.","completion":"Computer Network"}
{"prompt":"Shortest Path Bridging (SPB), specified in the IEEE 802.1aq standard and based on Dijkstra's algorithm, is a computer networking technology intended to simplify the creation and configuration of networks, while enabling multipath routing. It is a proposed replacement for Spanning Tree Protocol which blocks any redundant paths that could result in a switching loop. SPB allows all paths to be active with multiple equal-cost paths. SPB also increases the number of VLANs allowed on a layer-2 network.TRILL (Transparent Interconnection of Lots of Links) is the successor to Spanning Tree Protocol, both having been created by the same person, Radia Perlman. The catalyst for TRILL was an event at Beth Israel Deaconess Medical Center which began on 13 November 2002.  The concept of Rbridges [sic] was first proposed to the Institute of Electrical and Electronics Engineers in the year 2004, whom in 2005 rejected what came to be known as TRILL, and in the years 2006 through 2012 devised an incompatible variation known as Shortest Path Bridging.","completion":"Computer Network"}
{"prompt":"The communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture. As with any computer network, the Internet physically consists of routers, media (such as cabling and radio links), repeaters, modems etc. However, as an example of internetworking, many of the network nodes are not necessarily Internet equipment per se, the internet packets are carried by other full-fledged networking protocols with the Internet acting as a homogeneous networking standard, running across heterogeneous hardware, with the packets guided to their destinations by IP routers.","completion":"Computer Network"}
{"prompt":"The IANA function was originally performed by USC Information Sciences Institute (ISI), and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. ISI's Jonathan Postel managed the IANA, served as RFC Editor and performed other key roles until his premature death in 1998.As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by ISI's Paul Mockapetris in 1983. The Defense Data Network\u2014Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the \"growth of the Internet and its increasing globalization\" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of \"continental dimensions\"). Registries would be \"unbiased and widely recognized by network providers and subscribers\" within their region.\nThe RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry \/ registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.","completion":"Computer Network"}
{"prompt":"Energy is seen as the second infrastructure that could be attacked. It is broken down into two categories, electricity and natural gas. Electricity also known as electric grids power cities, regions, and households; it powers machines and other mechanisms used in day-to-day life. Using US as an example, in a conflict cyberterrorists can access data through the Daily Report of System Status that shows power flows throughout the system and can pinpoint the busiest sections of the grid. By shutting those grids down, they can cause mass hysteria, backlog, and confusion; also being able to locate critical areas of operation to further attacks in a more direct method. Cyberterrorists can access instructions on how to connect to the Bonneville Power Administration which helps direct them on how to not fault the system in the process. This is a major advantage that can be utilized when cyberattacks are being made because foreign attackers with no prior knowledge of the system can attack with the highest accuracy without drawbacks. Cyberattacks on natural gas installations go much the same way as it would with attacks on electrical grids. Cyberterrorists can shutdown these installations stopping the flow or they can even reroute gas flows to another section that can be occupied by one of their allies. There was a case in Russia with a gas supplier known as Gazprom, they lost control of their central switchboard which routes gas flow, after an inside operator and Trojan horse program bypassed security.The 2021 Colonial Pipeline cyberattack caused a sudden shutdown of the pipeline that carried 45% of the gasoline, diesel, and jet fuel consumed on the East Coast of the United States.\nWind farms, both onshore and offshore, are also at risk from cyberattacks. In February 2022, a German wind turbine maker, Enercon, lost remote connection to some 5,800 turbines following a large-scale disruption of satellite links. In April 2022, another company, Deutsche Windtechnik, also lost control of roughly 2,000 turbines because of a cyber-attack. While the wind turbines were not damaged during these incidents, these attacks illustrate just how vulnerable their computer systems are.","completion":"Computer Network"}
{"prompt":"Davies was appointed a Distinguished Fellow of the British Computer Society (BCS) in 1975 and was made a CBE in 1983, and later a Fellow of the Royal Society in 1987.He received the John Player Award from the BCS in 1974. and was awarded a medal by the John von Neumann Computer Society in Hungary in 1985.In 2000, Davies shared the inaugural IEEE Internet Award. In 2007, he was inducted into the National Inventors Hall of Fame, and in 2012 Davies was inducted into the Internet Hall of Fame by the Internet Society.NPL sponsors a gallery, opened in 2009, about the development of packet switching and \"Technology of the Internet\" at The National Museum of Computing.A blue plaque commemorating Davies was unveiled in Treorchy in July 2013.","completion":"Computer Network"}
{"prompt":"Robert Taylor was promoted to the head of the Information Processing Techniques Office (IPTO) at Defense Advanced Research Projects Agency (DARPA) in 1966. He intended to realize Licklider's ideas of an interconnected networking system. As part of the IPTO's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at University of California, Berkeley, and one for the Compatible Time-Sharing System project at Massachusetts Institute of Technology (MIT). Taylor's identified need for networking became obvious from the waste of resources apparent to him.\n\nFor each of these three terminals, I had three different sets of user commands. So if I was talking online with someone at S.D.C. and I wanted to talk to someone I knew at Berkeley or M.I.T. about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them.... \nI said, oh man, it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go where you have interactive computing. That idea is the ARPAnet.Bringing in Larry Roberts from MIT in January 1967, he initiated a project to build such a network. Roberts and Thomas Merrill had been researching computer time-sharing over wide area networks (WANs). Wide area networks emerged during the late 1950s and became established during the 1960s. At the first ACM Symposium on Operating Systems Principles in October 1967, Roberts presented a proposal for the \"ARPA net\", based on Wesley Clark's proposal to use Interface Message Processors (IMP) to create a message switching network.  At the conference, Roger Scantlebury presented Donald Davies' work on packet switching for data communications and mentioned the work of Paul Baran at RAND. Roberts incorporated the packet switching concepts into the ARPANET design and upgraded the proposed communications speed from 2.4 kbit\/s to 50 kbit\/s. Leonard Kleinrock subsequently developed the mathematical theory to model and measure the performance of this technology, building on his earlier work on the application of queueing theory to message switching systems.ARPA awarded the contract to build the network to Bolt Beranek & Newman. The \"IMP guys\", led by Frank Heart and including Bob Kahn, developed the routing, flow control, software design and network control. The first ARPANET link was established between the Network Measurement Center at the University of California, Los Angeles (UCLA) Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock, and the NLS system at Stanford Research Institute (SRI) directed by Douglas Engelbart in Menlo Park, California at 22:30 hours on October 29, 1969.\n\"We set up a telephone connection between us and the guys at SRI ...\", Kleinrock ... said in an interview: \"We typed the L and we asked on the phone,\n\"Do you see the L?\"\n\"Yes, we see the L,\" came the response.\nWe typed the O, and we asked, \"Do you see the O.\"\n\"Yes, we see the O.\"\nThen we typed the G, and the system crashed ...\nYet a revolution had begun\" .... \nBy December 1969, a four-node network was connected by adding the Culler-Fried Interactive Mathematics Center at the University of California, Santa Barbara followed by the University of Utah Graphics Department. In the same year, Taylor helped fund ALOHAnet, a system designed by professor Norman Abramson and others at the University of Hawai\u02bbi at M\u0101noa that transmitted data by radio between seven computers on four islands on Hawaii.Steve Crocker formed the \"Networking Working Group\" in 1969 at UCLA. He initiated and managed the Request for Comments (RFC) process, which is still used today for proposing and distributing contributions. RFC 1, entitled \"Host Software\", was written by Steve Crocker and published on April 7, 1969. The protocol for establishing links between network sites in the ARPANET, the Network Control Protocol (NCP), was completed in 1970. These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.\nEarly international collaborations via the ARPANET were sparse. Connections were made in 1973 to the Norwegian Seismic Array (NORSAR), via a satellite link at the Tanum Earth Station in Sweden, and to Peter Kirstein's research group at University College London, which provided a gateway to British academic networks, forming the first international heterogenous resource sharing network. By 1981, the number of hosts had grown to 213. The ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used.","completion":"Computer Network"}
{"prompt":"The field of technology available for telephony has broadened with the advent of new communication technologies. Telephony now includes the technologies of Internet services and mobile communication, including video conferencing.\nThe new technologies based on Internet Protocol (IP) concepts are often referred to separately as voice over IP (VoIP) telephony, also commonly referred to as IP telephony or Internet telephony. Unlike traditional phone service, IP telephony service is relatively unregulated by government. In the United States, the Federal Communications Commission (FCC) regulates phone-to-phone connections, but says they do not plan to regulate connections between a phone user and an IP telephony service provider.A specialization of digital telephony, Internet Protocol (IP) telephony involves the application of digital networking technology that was the foundation to the Internet to create, transmit, and receive telecommunications sessions over computer networks. Internet telephony is commonly known as voice over Internet Protocol (VoIP), reflecting the principle, but it has been referred with many other terms. VoIP has proven to be a disruptive technology that is rapidly replacing traditional telephone infrastructure technologies. As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 Newsweek article suggested that Internet telephony may be \"the next big thing\". As of 2006, many VoIP companies offer service to consumers and businesses.IP telephony uses an Internet connection and hardware IP phones, analog telephone adapters, or softphone computer applications to transmit conversations encoded as data packets. In addition to replacing plain old telephone service (POTS), IP telephony services compete with mobile phone services by offering free or lower cost connections via WiFi hotspots. VoIP is also used on private networks which may or may not have a connection to the global telephone network.","completion":"Computer Network"}
{"prompt":"A Bandwidth Broker in the framework of DiffServ is an agent that has some knowledge of an organization's priorities and policies and allocates bandwidth with respect to those policies. In order to achieve an end-to-end allocation of resources across separate domains, the Bandwidth Broker managing a domain will have to communicate with its adjacent peers, which allows end-to-end services to be constructed out of purely bilateral agreements.","completion":"Computer Network"}
{"prompt":"A multiport bridge connects multiple networks and operates transparently to decide on a frame-by-frame basis whether to forward traffic. Additionally, a multiport bridge must decide where to forward traffic. Like the simple bridge, a multiport bridge typically uses store and forward operation. The multiport bridge function serves as the basis for network switches.","completion":"Computer Network"}
{"prompt":"I\/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I\/O.\nI\/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I\/O. A 2016-era flat screen display contains its own computer circuitry.","completion":"Computer Network"}
{"prompt":"Personal computers such as desktops, laptops, netbooks, and tablets\nA network attached storage (NAS) device can be easily accessed via the CIFS or NFS protocols for general storage or for backup purposes.\nA print server can be used to share any directly connected printers with other computers on the network.\nIP phones or smartphones (when connected via Wi-Fi) utilizing VoIP technologies","completion":"Computer Network"}
{"prompt":"A nanoscale network has key components implemented at the nanoscale, including message carriers, and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for other communication techniques.","completion":"Computer Network"}
{"prompt":"An extranet is a network that is under the administrative control of a single organization but supports a limited connection to a specific external network.  For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers.  These other entities are not necessarily trusted from a security standpoint.  The network connection to an extranet is often, but not always, implemented via WAN technology.","completion":"Computer Network"}
{"prompt":"The first packet-switched computer networks, the NPL network and the ARPANET were interconnected in 1973 via University College London. The ARPANET used a backbone of routers called Interface Message Processors. Other packet-switched computer networks proliferated starting in the 1970s, eventually adopting TCP\/IP protocols, or being replaced by newer networks. The National Science Foundation created the National Science Foundation Network (NSFNET) in 1986 by funding six networking sites using 56kbit\/s interconnecting links, with peering to the ARPANET. In 1987, this new network was upgraded to 1.5Mbit\/s T1 links for thirteen sites. These sites included regional networks that in turn connected over 170 other networks. IBM, MCI and Merit upgraded the backbone to 45Mbit\/s bandwidth (T3) in 1991.  The combination of the ARPANET and NSFNET became known as the Internet. Within a few years, the dominance of the NSFNet backbone led to the decommissioning of the redundant ARPANET infrastructure in 1990.\nIn the early days of the Internet, backbone providers exchanged their traffic at government-sponsored network access points (NAPs), until the government privatized the Internet, and transferred the NAPs to commercial providers.","completion":"Computer Network"}
{"prompt":"Across all variations of 802.11, maximum achievable throughputs are given either based on measurements under ideal conditions or in the layer-2 data rates. However, this does not apply to typical deployments in which data is being transferred between two endpoints, of which at least one is typically connected to a wired infrastructure and the other endpoint is connected to an infrastructure via a wireless link.\n\nThis means that, typically, data frames pass an 802.11 (WLAN) medium and are being converted to 802.3 (Ethernet) or vice versa. Due to the difference in the frame (header) lengths of these two media, the application's packet size determines the speed of the data transfer. This means applications that use small packets (e.g., VoIP) create dataflows with high-overhead traffic (i.e., a low goodput). Other factors that contribute to the overall application data rate are the speed with which the application transmits the packets (i.e., the data rate) and, of course, the energy with which the wireless signal is received. The latter is determined by distance and by the configured output power of the communicating devices.The same references apply to the attached graphs that show measurements of UDP throughput. Each represents an average (UDP) throughput (please note that the error bars are there but barely visible due to the small variation) of 25 measurements. Each is with a specific packet size (small or large) and with a specific data rate (10 kbit\/s \u2013 100 Mbit\/s). Markers for traffic profiles of common applications are included as well. These figures assume there are no packet errors, which, if occurring, will lower the transmission rate further.","completion":"Computer Network"}
{"prompt":"Earth orbit is sufficiently nearby that conventional protocols can be used. For example, the International Space Station has been connected to the regular terrestrial Internet since January 22, 2010 when the first unassisted tweet was posted. However, the space station also serves as a useful platform to develop, experiment, and implement systems that make up the interplanetary Internet. NASA and the European Space Agency (ESA) have used an experimental version of the interplanetary Internet to control an educational rover, placed at the European Space Operations Centre in Darmstadt, Germany, from the International Space Station. The experiment used the DTN protocol to demonstrate technology that one day could enable Internet-like communications that can support habitats or infrastructure on another planet.","completion":"Computer Network"}
{"prompt":"Having originated among writers, the concept of cyberspace remains most popular in literature and film. Although artists working with other media have expressed interest in the concept, such as Roy Ascott, \"cyberspace\" in digital art is mostly used as a synonym for immersive virtual reality and remains more discussed than enacted.","completion":"Computer Network"}
{"prompt":"Terrestrial microwave \u2013 Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 50 km (30 mi) apart.\nCommunications satellites \u2013 Satellites communicate via microwave radio waves, which are not deflected by the Earth's atmosphere. The satellites are stationed in space, typically in geostationary orbit 35,786 km (22,236 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.\nCellular and PCS systems use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area has a low-power transmitter or radio relay antenna device to relay calls from one area to the next area.\nRadio and spread spectrum technologies \u2013 Wireless local area networks use a high-frequency radio technology similar to digital cellular and a low-frequency radio technology. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi.\nFree-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.","completion":"Computer Network"}
{"prompt":"A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it.  The signal may be reformed or retransmitted at a higher power level, to the other side of an obstruction possibly using a different transmission medium, so that the signal can cover longer distances without degradation. Commercial repeaters have extended RS-232 segments from 15 meters to over a kilometer. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.\nRepeaters work within the physical layer of the OSI model, that is, there is no end-to-end change in the physical protocol across the repeater, or repeater pair, even if a different physical layer may be used between the ends of the repeater, or repeater pair. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.\nA repeater with multiple ports is known as hub, an Ethernet hub in Ethernet networks, a USB hub in USB networks.\n\nUSB networks use hubs to form tiered-star topologies.\nEthernet hubs and repeaters in LANs have been mostly obsoleted by modern switches.","completion":"Computer Network"}
{"prompt":"In the early days of the Cold War, the U.S. and its allies developed an elaborate series of export control regulations designed to prevent a wide range of Western technology from falling into the hands of others, particularly the Eastern bloc. All export of technology classed as 'critical' required a license. CoCom was  organized to coordinate Western export controls.\nTwo types of technology were protected: technology associated only with weapons of war (\"munitions\") and dual use technology, which also had commercial applications. In the U.S., dual use technology export was controlled by the Department of Commerce, while munitions were controlled by the State Department. Since in the immediate post WWII period the market for cryptography was almost entirely military, the encryption technology (techniques as well as equipment and, after computers became important, crypto software) was included as a Category XIII item into the United States Munitions List. The multinational control of the export of cryptography on the Western side of the cold war divide was done via the mechanisms of CoCom.\nBy the 1960s, however, financial organizations were beginning to require strong commercial encryption on the rapidly growing field of wired money transfer. The U.S. Government's introduction of the Data Encryption Standard in 1975 meant that commercial uses of high quality encryption would become common, and serious problems of export control began to arise. Generally these were dealt with through case-by-case export license request proceedings brought by computer manufacturers, such as IBM, and by their large corporate customers.","completion":"Cryptography"}
{"prompt":"The original team has optimized Ed25519 for the x86-64 Nehalem\/Westmere processor family. Verification can be performed in batches of 64 signatures for even greater throughput. Ed25519 is intended to provide attack resistance comparable to quality 128-bit symmetric ciphers.Public keys are 256 bits long and signatures are 512 bits long.","completion":"Cryptography"}
{"prompt":"Jordan Kelley, founder of Robocoin, launched the first Bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.","completion":"Cryptography"}
{"prompt":"Camellia has been certified as a standard cipher by several standardization organizations:\nCRYPTREC\nNESSIE\nIETF\nAlgorithm\nRFC 3713: A Description of the Camellia Encryption Algorithm\nBlock cipher mode\nRFC 5528: Camellia Counter Mode and Camellia Counter with CBC-MAC Mode Algorithms\nS\/MIME\nRFC 3657: Use of the Camellia Encryption Algorithm in Cryptographic Message Syntax (CMS)\nXML Encryption\nRFC 4051: Additional XML Security Uniform Resource Identifiers (URIs)\nTLS\/SSL\nRFC 4132: Addition of Camellia Cipher Suites to Transport Layer Security (TLS)\nRFC 5932: Camellia Cipher Suites for TLS\nRFC 6367: Addition of the Camellia Cipher Suites to Transport Layer Security (TLS)\nIPsec\nRFC 4312: The Camellia Cipher Algorithm and Its Use With IPsec\nRFC 5529: Modes of Operation for Camellia for Use with IPsec\nKerberos\nRFC 6803: Camellia Encryption for Kerberos 5\nOpenPGP\nRFC 5581: The Camellia Cipher in OpenPGP\nRSA-KEM in CMS\nRFC 5990: Use of the RSA-KEM Key Transport Algorithm in the Cryptographic Message Syntax (CMS)\nPSKC\nRFC 6030: Portable Symmetric Key Container (PSKC)\nSmart grid\nRFC 6272: Internet Protocols for the Smart Grid\nISO\/IEC\nISO\/IEC 18033-3:2010 Information technology\u2014Security techniques\u2014Encryption algorithms\u2014Part 3: Block ciphers\nITU-T\nSecurity mechanisms and procedures for NGN (Y.2704)\nRSA Laboratories\nApproved cipher in the PKCS#11\nTV-Anytime Forum\nApproved cipher in TV-Anytime Rights Management and Protection Information for Broadcast Applications\nApproved cipher in Bi-directional Metadata Delivery Protection","completion":"Cryptography"}
{"prompt":"Given a composite number \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , exponent \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n   and number \n  \n    \n      \n        c\n        :=\n        \n          m\n          \n            e\n          \n        \n        (\n        \n          m\n          o\n          d\n        \n        \n        n\n        )\n      \n    \n    {\\displaystyle c:=m^{e}(\\mathrm {mod} \\;n)}\n  , the RSA problem is to find \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  .\nThe problem is conjectured to be hard, but becomes easy given the factorization of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . \nIn the RSA cryptosystem, \n  \n    \n      \n        (\n        n\n        ,\n        e\n        )\n      \n    \n    {\\displaystyle (n,e)}\n   is the public key, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is the encryption of message \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , and the factorization of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is the secret key used for decryption.","completion":"Cryptography"}
{"prompt":"Rectangular Cardan grilles can be placed in four positions.  The trellis or chessboard has only two positions, but it gave rise to a more sophisticated turning grille with four positions that can be rotated in two directions.\n\nBaron Edouard Fleissner von Wostrowitz, a retired Austrian cavalry colonel, described a variation on the chess board cipher in 1880 and his grilles were adopted by the German army during World War I. These grilles are often named after Fleissner, although he took his material largely from a German work, published in T\u00fcbingen in 1809, written by Kl\u00fcber who attributed this form of the grille to Cardano, as did Helen Fouch\u00e9 Gaines.Bauer notes that grilles were used in the 18th century, for example in 1745 in the administration of the Dutch Stadthouder William IV. Later, the mathematician C. F. Hindenburg studied turning grilles more systematically in 1796. '[they]are often called Fleissner grilles in ignorance of their historical origin.'\nOne form of the Fleissner (or Flei\u00dfner) grille makes 16 perforations in an 8x8 grid \u2013 4 holes in each quadrant.  If the squares in each quadrant are numbered 1 to 16, all 16 numbers must be used once only.  This allows many variations in placing the apertures.\nThe grille has four positions \u2013 North, East, South, West. Each position exposes 16 of the 64 squares.  The encipherer places the grille on a sheet and writes the first 16 letters of the message.  Then, turning the grille through 90 degrees, the second 16 are written, and so on until the grid is filled.\nIt is possible to construct grilles of different dimensions; however, if the number of squares in one quadrant is odd, even if the total is an even number, one quadrant or section must contain an extra perforation.  Illustrations of the Fleissner grille often take a 6x6 example for ease of space; the number of apertures in one quadrant is 9, so three quadrants contain 2 apertures and one quadrant must have 3.  There is no standard pattern of apertures: they are created by the user, in accordance with the above description, with the intention of producing a good mix.\nThe method gained wide recognition when Jules Verne used a turning grille as a plot device in his novel Mathias Sandorf, published in 1885. Verne had come across the idea in Fleissner's treatise Handbuch der Kryptographie which appeared in 1881.\n\nFleissner Grilles were constructed in various sizes during World War I and were used by the German Army at the end of 1916.  Each grille had a different code name:- 5x5 ANNA; 6X6 BERTA; 7X7 CLARA; 8X8 DORA; 9X9 EMIL; 10X10 FRANZ. Their security was weak, and they were withdrawn after four months.\nAnother method of indicating the size of the grille in use was to insert a key code at the start of the cipher text: E = 5; F = 6 and so on.  The grille can also be rotated in either direction and the starting position does not need to be NORTH.  Clearly the working method is by arrangement between sender and receiver and may be operated in accordance with a schedule.\nIn the following examples, two cipher texts contain the same message.  They are constructed from the example grille, beginning in the NORTH position, but one is formed by rotating the grille clockwise and the other anticlockwise.  The ciphertext is then taken off the grid in horizontal lines - but it could equally be taken off vertically.\nCLOCKWISE\nITIT ILOH GEHE TCDF LENS IIST FANB FSET EPES HENN URRE NEEN TRCG PR&I ODCT SLOE\nANTICLOCKWISE\nLEIT CIAH GTHE TIDF LENB IIET FONS FSST URES NEDN EPRE HEEN TRTG PROI ONEC SL&C\nIn 1925 Luigi Sacco of the Italian Signals Corps began writing a book on ciphers which included reflections on the codes of the Great War, Nozzioni di crittografia.  He observed that Fleissner's method could be applied to a fractionating cipher, such as a Delastelle Bifid or Four-Square, with considerable increase in security.\nGrille ciphers are also useful device for transposing Chinese characters; they avoid the transcription of words into alphabetic or syllabic characters to which other ciphers (for example, substitution ciphers) can be applied.\nAfter World War I, machine encryption made simple cipher devices obsolete, and grille ciphers fell into disuse except for amateur purposes. Yet, grilles provided seed ideas for transposition ciphers that are reflected in modern cryptography.","completion":"Cryptography"}
{"prompt":"His greatest contribution to the development of Islamic philosophy was his efforts to make Greek thought both accessible and acceptable to a Muslim audience. Al-Kindi carried out this mission from the House of Wisdom (Bayt al-Hikma), an institute of translation and learning patronized by the Abbasid Caliphs, in Baghdad. As well as translating many important texts, much of what was to become standard Arabic philosophical vocabulary originated with al-Kindi; indeed, if it had not been for him, the work of philosophers like al-Farabi, Avicenna, and al-Ghazali might not have been possible.In his writings, one of al-Kindi's central concerns was to demonstrate the compatibility between philosophy and natural theology on the one hand, and revealed or speculative theology on the other (though in fact he rejected speculative theology). Despite this, he did make clear that he believed revelation was a superior source of knowledge to reason because it guaranteed matters of faith that reason could not uncover. And while his philosophical approach was not always original, and was even considered clumsy by later thinkers (mainly because he was the first philosopher writing in the Arabic language), he successfully incorporated Aristotelian and (especially) neo-Platonist thought into an Islamic philosophical framework. This was an important factor in the introduction and popularization of Greek philosophy in the Muslim intellectual world.","completion":"Cryptography"}
{"prompt":"Key management concerns the generation, establishment, storage, usage and replacement of cryptographic keys. A key management system (KMS) typically includes three steps of establishing, storing and using keys. The base of security for the generation, storage, distribution, use and destruction of keys depends on successful key management protocols.","completion":"Cryptography"}
{"prompt":"GCM is proven secure in the concrete security model. It is secure when it is used with a block cipher that is indistinguishable from a random permutation; however, security depends on choosing a unique initialization vector for every encryption performed with the same key (see stream cipher attack). For any given key and initialization vector combination, GCM is limited to encrypting 239 \u2212 256 bits of plain text (64 GiB). NIST Special Publication 800-38D includes guidelines for initialization vector selection.\nThe authentication strength depends on the length of the authentication tag, like with all symmetric message authentication codes. The use of shorter authentication tags with GCM is discouraged. The bit-length of the tag, denoted t, is a security parameter. In general, t may be any one of the following five values: 128, 120, 112, 104, or 96. For certain applications, t may be 64 or 32, but the use of these two tag lengths constrains the length of the input data and the lifetime of the key. Appendix C in NIST SP 800-38D provides guidance for these constraints (for example, if t = 32 and the maximal packet size is 210 bytes, the authentication decryption function should be invoked no more than 211 times; if t = 64 and the maximal packet size is 215 bytes, the authentication decryption function should be invoked no more than 232 times).\nLike with any message authentication code, if the adversary chooses a t-bit tag at random, it is expected to be correct for given data with probability measure 2\u2212t. With GCM, however, an adversary can increase their likelihood of success by choosing tags with n words \u2013 the total length of the ciphertext plus any additional authenticated data (AAD) \u2013 with probability measure 2\u2212t by a factor of n. Although, one must bear in mind that these optimal tags are still dominated by the algorithm's survival measure 1 \u2212 n\u22c52\u2212t for arbitrarily large t. Moreover, GCM is neither well-suited for use with very short tag-lengths nor very long messages.\nFerguson and Saarinen independently described how an attacker can perform optimal attacks against GCM authentication, which meet the lower bound on its security. Ferguson showed that, if n denotes the total number of blocks in the encoding (the input to the GHASH function), then there is a method of constructing a targeted ciphertext forgery that is expected to succeed with a probability of approximately n\u22c52\u2212t. If the tag length t is shorter than 128, then each successful forgery in this attack increases the probability that subsequent targeted forgeries will succeed, and leaks information about the hash subkey, H. Eventually, H may be compromised entirely and the authentication assurance is completely lost.Independent of this attack, an adversary may attempt to systematically guess many different tags for a given input to authenticated decryption and thereby increase the probability that one (or more) of them, eventually, will be considered valid. For this reason, the system or protocol that implements GCM should monitor and, if necessary, limit the number of unsuccessful verification attempts for each key.\nSaarinen described GCM weak keys. This work gives some valuable insights into how polynomial hash-based authentication works. More precisely, this work describes a particular way of forging a GCM message, given a valid GCM message, that works with probability of about n\u22c52\u2212128 for messages that are n \u00d7 128 bits long. However, this work does not show a more effective attack than was previously known; the success probability in observation 1 of this paper matches that of lemma 2 from the INDOCRYPT 2004 analysis (setting w = 128 and l = n \u00d7 128). Saarinen also described a GCM variant Sophie Germain Counter Mode (SGCM) based on Sophie Germain primes.","completion":"Cryptography"}
{"prompt":"There are no known cryptanalytic attacks against ACHTERBAHN-128\/80 for the tabulated parameters that are faster than brute force attack. \nRecent analysis showed that attacks are possible if larger frame (packet) lengths are used in a communication protocol.\nThe cipher's authors recommend a maximum frame length of 244 bits. This value does however not imply practical limitations.","completion":"Cryptography"}
{"prompt":"Even when encryption correctly hides a message's content and it cannot be tampered with at rest or in transit, a message's length is a form of metadata that can still leak sensitive information about the message. For example, the well-known CRIME and BREACH attacks against HTTPS were side-channel attacks that relied on information leakage via the length of encrypted content. Traffic analysis is a broad class of techniques that often employs message lengths to infer sensitive implementation about traffic flows by aggregating information about a large number of messages.\nPadding a message's payload before encrypting it can help obscure the cleartext's true length, at the cost of increasing the ciphertext's size and introducing or increasing bandwidth overhead. Messages may be padded randomly or deterministically, with each approach having different tradeoffs. Encrypting and padding messages to form padded uniform random blobs or PURBs is a practice guaranteeing that the cipher text leaks no metadata about its cleartext's content, and leaks asymptotically minimal \n  \n    \n      \n        O\n        (\n        log\n        \u2061\n        log\n        \u2061\n        M\n        )\n      \n    \n    {\\displaystyle O(\\log \\log M)}\n   information via its length.","completion":"Cryptography"}
{"prompt":"Cryptographic experts have expressed concerns that the National Security Agency has inserted a kleptographic backdoor into at least one elliptic curve-based pseudo random generator. Internal memos leaked by former NSA contractor Edward Snowden suggest that the NSA put a backdoor in the Dual EC DRBG standard. One analysis of the possible backdoor concluded that an adversary in possession of the algorithm's secret key could obtain encryption keys given only 32 bytes of PRNG output.The SafeCurves project has been launched in order to catalog curves that are easy to implement securely and are designed in a fully publicly verifiable way to minimize the chance of a backdoor.","completion":"Cryptography"}
{"prompt":"There are a number of practical PRNGs that have been designed to be cryptographically secure, including\n\nthe Yarrow algorithm which attempts to evaluate the entropic quality of its inputs. Yarrow is used in macOS and other Apple OS' up until about Dec. 2019.  Apple has switched to Fortuna since then. (See \/dev\/random).\nthe ChaCha20 algorithm replaced RC4 in OpenBSD (version 5.4), NetBSD (version 7.0), and FreeBSD (version 12.0).\nChaCha20 also replaced SHA-1 in Linux in version 4.8.\nthe Fortuna algorithm, the successor to Yarrow, which does not attempt to evaluate the entropic quality of its inputs.  Fortuna is used in FreeBSD.  Apple changed to Fortuna for most or all Apple OS' beginning around Dec. 2019.\nthe function CryptGenRandom provided in Microsoft's Cryptographic Application Programming Interface\nISAAC based on a variant of the RC4 cipher\nLinear-feedback shift register tuned with evolutionary algorithm based on the NIST Statistical Test Suite.\narc4random\nAES-CTR DRBG is often used as a random number generator in systems that use AES encryption.\nANSI X9.17 standard (Financial Institution Key Management (wholesale)), which has been adopted as a FIPS standard as well. It takes as input a TDEA (keying option 2) key bundle k and (the initial value of) a 64-bit random seed s. Each time a random number is required it:\nObtains the current date\/time D to the maximum resolution possible.\nComputes a temporary value t = TDEAk(D)\nComputes the random value x = TDEAk(s \u2295 t), where \u2295 denotes bitwise exclusive or.\nUpdates the seed s = TDEAk(x \u2295 t)Obviously, the technique is easily generalized to any block cipher; AES has been suggested.","completion":"Cryptography"}
{"prompt":"Carlisle Adams, Entrust, Inc.\nFriedrich Bauer, Technische Universit\u00e4t M\u00fcnchen\nGerrit Bleumer, Francotyp-Postalia\nDan Boneh, Stanford University\nPascale Charpin, INRIA-Rocquencourt\nClaude Crepeau, McGill University\nYvo G. Desmedt, University College London (University of London)\nGrigory Kabatiansky, Institute for Information Transmission Problems\nBurt Kaliski, RSA Security\nPeter Landrock, University of Aarhus\nPatrick Drew McDaniel, Penn State University\nAlfred Menezes, University of Waterloo\nDavid Naccache, Gemplus\nChristof Paar, Ruhr-Universit\u00e4t Bochum\nBart Preneel, Katholieke Universiteit Leuven\nJean-Jacques Quisquater, Universit\u00e9 Catholique de Louvain\nKazue Sako, NEC Corporation\nBerry Schoenmakers, Technische Universiteit Eindhoven","completion":"Cryptography"}
{"prompt":"One important type of iterated block cipher known as a substitution\u2013permutation network (SPN) takes a block of the plaintext and the key as inputs and applies several alternating rounds consisting of a substitution stage followed by a permutation stage\u2014to produce each block of ciphertext output. The non-linear substitution stage mixes the key bits with those of the plaintext, creating Shannon's confusion. The linear permutation stage then dissipates redundancies, creating diffusion.A substitution box (S-box) substitutes a small block of input bits with another block of output bits. This substitution must be one-to-one, to ensure invertibility (hence decryption). A secure S-box will have the property that changing one input bit will change about half of the output bits on average, exhibiting what is known as the avalanche effect\u2014i.e. it has the property that each output bit will depend on every input bit.A permutation box (P-box) is a permutation of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.At each round, the round key (obtained from the key with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically XOR.Decryption is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).","completion":"Cryptography"}
{"prompt":"The history of cryptography began thousands of years ago. Cryptography uses a variety of different types of encryption. Earlier algorithms were performed by hand and are substantially different from modern algorithms, which are generally executed by a machine.","completion":"Cryptography"}
{"prompt":"One solution is to include the length of the message in the first block; in fact CBC-MAC has been proven secure as long as no two messages that are prefixes of each other are ever used and prepending the length is a special case of this. This can be problematic if the message length may not be known when processing begins.","completion":"Cryptography"}
{"prompt":"Many more modes of operation for block ciphers have been suggested. Some have been accepted, fully described (even standardized), and are in use. Others have been found insecure, and should never be used. Still others don't categorize as confidentiality, authenticity, or authenticated encryption \u2013 for example key feedback mode and Davies\u2013Meyer hashing.\nNIST maintains a list of proposed modes for block ciphers at Modes Development.Disk encryption often uses special purpose modes specifically designed for the application. Tweakable narrow-block encryption modes (LRW, XEX, and XTS) and wide-block encryption modes (CMC and EME) are designed to securely encrypt sectors of a disk (see disk encryption theory).\nMany modes use an initialization vector (IV) which, depending on the mode, may have requirements such as being only used once (a nonce) or being unpredictable ahead of its publication, etc.  Reusing an IV with the same key in CTR, GCM or OFB mode results in XORing the same keystream with two or more plaintexts, a clear misuse of a stream, with a catastrophic loss of security.  Deterministic authenticated encryption modes such as the NIST Key Wrap algorithm and the SIV (RFC 5297) AEAD mode do not require an IV as an input, and return the same ciphertext and authentication tag every time for a given plaintext and key.  Other IV misuse-resistant modes such as AES-GCM-SIV benefit from an IV input, for example in the maximum amount of data that can be safely encrypted with one key, while not failing catastrophically if the same IV is used multiple times.\nBlock ciphers can also be used in other cryptographic protocols. They are generally used in modes of operation similar to the block modes described here. As with all protocols, to be cryptographically secure, care must be taken to design these modes of operation correctly.\nThere are several schemes which use a block cipher to build a cryptographic hash function. See one-way compression function for descriptions of several such methods.\nCryptographically secure pseudorandom number generators (CSPRNGs) can also be built using block ciphers.\nMessage authentication codes (MACs) are often built from block ciphers. CBC-MAC, OMAC and PMAC are examples.","completion":"Cryptography"}
{"prompt":"Apart from cryptoviral extortion, there are other potential uses of cryptoviruses, such as deniable password snatching, cryptocounters,\nprivate information retrieval, and in secure communication between different instances of a distributed cryptovirus.","completion":"Cryptography"}
{"prompt":"Alternative representations of elliptic curves include:\n\nHessian curves\nEdwards curves\nTwisted curves\nTwisted Hessian curves\nTwisted Edwards curve\nDoubling-oriented Doche\u2013Icart\u2013Kohel curve\nTripling-oriented Doche\u2013Icart\u2013Kohel curve\nJacobian curve\nMontgomery curves","completion":"Cryptography"}
{"prompt":"EKMS NSA's Electronic Key Management System\nFNBDT NSA's secure narrow band voice standard\nFortezza encryption based on portable crypto token in PC Card format\nSTE secure telephone\nSTU-III older secure telephone\nTEMPEST prevents compromising emanations","completion":"Cryptography"}
{"prompt":"An example of a virus that informs the owner of the infected machine to pay a ransom is the virus nicknamed Tro_Ransom.A. This virus asks the owner of the infected machine to send $10.99 to a given account through Western Union.Virus.Win32.Gpcode.ag is a classic cryptovirus. This virus partially uses a version of 660-bit RSA and encrypts files with many different extensions. It instructs the owner of the machine to email a given mail ID if the owner desires the decryptor. If contacted by email, the user will be asked to pay a certain amount as ransom in return for the decryptor.","completion":"Cryptography"}
{"prompt":"Schneier is critical of digital rights management (DRM) and has said that it allows a vendor to increase lock-in. Proper implementation of control-based security for the user via trusted computing is very difficult, and security is not the same thing as control.Schneier insists that \"owning your data is a different way of thinking about data.\"","completion":"Cryptography"}
{"prompt":"DES is the archetypal block cipher\u2014an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bitstring of the same length. In the case of DES, the block size is 64 bits. DES also uses a key to customize the transformation, so that decryption can supposedly only be performed by those who know the particular key used to encrypt. The key ostensibly consists of 64 bits; however, only 56 of these are actually used by the algorithm. Eight bits are used solely for checking parity, and are thereafter discarded. Hence the effective key length is 56 bits.\nThe key is nominally stored or transmitted as 8 bytes, each with odd parity. According to ANSI X3.92-1981 (Now, known as ANSI INCITS 92\u20131981), section 3.5:\n\nOne bit in each 8-bit byte of the KEY may be utilized for error detection in key generation, distribution, and storage. Bits 8, 16,..., 64 are for use in ensuring that each byte is of odd parity.\nLike other block ciphers, DES by itself is not a secure means of encryption, but must instead be used in a mode of operation. FIPS-81 specifies several modes for use with DES. Further comments on the usage of DES are contained in FIPS-74.Decryption uses the same structure as encryption, but with the keys used in reverse order. (This has the advantage that the same hardware or software can be used in both directions.)","completion":"Cryptography"}
{"prompt":"In symmetric-key schemes, the encryption and decryption keys are the same. Communicating parties must have the same key in order to achieve secure communication. The German Enigma Machine utilized a new symmetric-key each day for encoding and decoding messages.\nIn public-key encryption schemes, the encryption key is published for anyone to use and encrypt messages. However, only the receiving party has access to the decryption key that enables messages to be read. Public-key encryption was first described in a secret document in 1973; beforehand, all encryption schemes were symmetric-key (also called private-key).:\u200a478\u200a Although published subsequently, the work of Diffie and Hellman was published in a journal with a large readership, and the value of the methodology was explicitly described. The method became known as the Diffie-Hellman key exchange.\nRSA (Rivest\u2013Shamir\u2013Adleman) is another notable public-key cryptosystem. Created in 1978, it is still used today for applications involving digital signatures. Using number theory, the RSA algorithm selects two prime numbers, which help generate both the encryption and decryption keys.A publicly available public-key encryption application called Pretty Good Privacy (PGP) was written in 1991 by Phil Zimmermann, and distributed free of charge with source code. PGP was purchased by Symantec in 2010 and is regularly updated.","completion":"Cryptography"}
{"prompt":"In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.","completion":"Cryptography"}
{"prompt":"The Lai\u2013Massey scheme offers security properties similar to those of the Feistel structure.. It also shares the advantage that the round function                                    F                          {\\displaystyle \\mathrm {F} }    does not have to be invertible.. Another similarity is that it also splits the input block into two equal pieces.. However, the round function is applied to the difference between the two, and the result is then added to both half blocks..","completion":"Cryptography"}
{"prompt":"Between 1949 and 2009, a bottle of cognac and three roses were left at Poe's original grave marker every January 19 by an unknown visitor affectionately referred to as the \"Poe Toaster\". Sam Porpora was a historian at the Westminster Church in Baltimore, where Poe is buried; he claimed on August 15, 2007, that he had started the tradition in 1949. Porpora said that the tradition began in order to raise money and enhance the profile of the church. His story has not been confirmed, and some details which he gave to the press are factually inaccurate. The Poe Toaster's last appearance was on January 19, 2009, the day of Poe's bicentennial.","completion":"Cryptography"}
{"prompt":"In 1838, Poe relocated to Philadelphia, where he lived at four different residences between 1838 and 1844, one of which at 532 N. 7th Street has been preserved as a National Historic Landmark.\nThat same year, Poe's novel The Narrative of Arthur Gordon Pym of Nantucket was published and widely reviewed. In the summer of 1839, he became assistant editor of Burton's Gentleman's Magazine. He published numerous articles, stories, and reviews, enhancing his reputation as a trenchant critic which he had established at the Messenger. Also in 1839, the collection Tales of the Grotesque and Arabesque was published in two volumes, though he made little money from it and it received mixed reviews.In June 1840, Poe published a prospectus announcing his intentions to start his own journal called The Stylus, although he originally intended to call it The Penn, since it would have been based in Philadelphia. He bought advertising space for his prospectus in the June 6, 1840, issue of Philadelphia's Saturday Evening Post: \"Prospectus of the Penn Magazine, a Monthly Literary journal to be edited and published in the city of Philadelphia by Edgar A. Poe.\" The journal was never produced before Poe's death.\nPoe left Burton's after about a year and found a position as writer and co-editor at Graham's Magazine, a successful monthly publication. In the last number of Graham's for 1841, Poe was among the co-signatories to an editorial note of celebration of the tremendous success the magazine had achieved in the past year: \"Perhaps the editors of no magazine, either in America or in Europe, ever sat down, at the close of a year, to contemplate the progress of their work with more satisfaction than we do now. Our success has been unexampled, almost incredible. We may assert without fear of contradiction that no periodical ever witnessed the same increase during so short a period.\"Around this time, Poe attempted to secure a position in the administration of John Tyler, claiming that he was a member of the Whig Party. He hoped to be appointed to the United States Custom House in Philadelphia with help from President Tyler's son Robert, an acquaintance of Poe's friend Frederick Thomas. Poe failed to show up for a meeting with Thomas to discuss the appointment in mid-September 1842, claiming to have been sick, though Thomas believed that he had been drunk. Poe was promised an appointment, but all positions were filled by others.One evening in January 1842, Virginia showed the first signs of consumption, or tuberculosis, while singing and playing the piano, which Poe described as breaking a blood vessel in her throat. She only partially recovered, and Poe began to drink more heavily under the stress of her illness. He left Graham's and attempted to find a new position, for a time angling for a government post. He returned to New York where he worked briefly at the Evening Mirror before becoming editor of the Broadway Journal, and later its owner. There Poe alienated himself from other writers by publicly accusing Henry Wadsworth Longfellow of plagiarism, though Longfellow never responded. On January 29, 1845, Poe's poem \"The Raven\" appeared in the Evening Mirror and became a popular sensation. It made Poe a household name almost instantly, though he was paid only $9 for its publication. It was concurrently published in The American Review: A Whig Journal under the pseudonym \"Quarles\".","completion":"Cryptography"}
{"prompt":"The Feistel construction is also used in cryptographic algorithms other than block ciphers. For example, the optimal asymmetric encryption padding (OAEP) scheme uses a simple Feistel network to randomize ciphertexts in certain asymmetric-key encryption schemes.\nA generalized Feistel algorithm can be used to create strong permutations on small domains of size not a power of two (see format-preserving encryption).","completion":"Cryptography"}
{"prompt":"The earliest known use of cryptography is found in non-standard hieroglyphs carved into the wall of a tomb from the Old Kingdom of Egypt circa 1900 BC. These are not thought to be serious attempts at secret communications, however, but rather to have been attempts at mystery, intrigue, or even amusement for literate onlookers.Some clay tablets from Mesopotamia somewhat later are clearly meant to protect information\u2014one dated near 1500 BC was found to encrypt a craftsman's recipe for pottery glaze, presumably commercially valuable. Furthermore, Hebrew scholars made use of simple monoalphabetic substitution ciphers (such as the Atbash cipher) beginning perhaps around 600 to 500 BC.In India around 400 BC to 200 AD, Mlecchita vikalpa or \"the art of understanding writing in cypher, and the writing of words in a peculiar way\" was documented in the Kama Sutra for the purpose of communication between lovers. This was also likely a simple substitution cipher. Parts of the Egyptian demotic Greek Magical Papyri were written in a cypher script.The ancient Greeks are said to have known of ciphers. The scytale transposition cipher was used by the Spartan military, but it is not definitively known whether the scytale was for encryption, authentication, or avoiding bad omens in speech. Herodotus tells us of secret messages physically concealed beneath wax on wooden tablets or as a tattoo on a slave's head concealed by regrown hair, although these are not properly examples of cryptography per se as the message, once known, is directly readable; this is known as steganography. Another Greek method was developed by Polybius (now called the \"Polybius Square\"). The Romans knew something of cryptography (e.g., the Caesar cipher and its variations).","completion":"Cryptography"}
{"prompt":"The first use of the term \"cryptograph\" (as opposed to \"cryptogram\") dates back to the 19th century\u2014originating from \"The Gold-Bug,\" a story by Edgar Allan Poe.Until modern times, cryptography referred almost exclusively to \"encryption\", which is the process of converting ordinary information (called plaintext) into an unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a \"key\". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext.  In formal mathematical terms, a \"cryptosystem\" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms that correspond to each key.  Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks.\nThere are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message. Data manipulation in symmetric systems is significantly faster than in asymmetric systems. Asymmetric systems use a \"public key\" to encrypt a message and a related \"private key\" to decrypt it. The advantage of asymmetric systems is that the public key can be freely published, allowing parties to establish secure communication without having a shared secret key.  In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie\u2013Hellman key exchange, RSA (Rivest\u2013Shamir\u2013Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).  Insecure symmetric algorithms include children's language tangling schemes such as Pig Latin or other cant, and all historical cryptographic schemes, however seriously intended, prior to the invention of the one-time pad early in the 20th century.\nIn colloquial use, the term \"code\" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, \"wallaby\" replaces \"attack at dawn\").  A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, a syllable, or a pair of letters, etc.) in order to produce a cyphertext.\nCryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to \"crack\" encryption algorithms or their implementations.\nSome use the terms \"cryptography\" and \"cryptology\" interchangeably in English, while others (including US military practice generally) use \"cryptography\" to refer specifically to the use and practice of cryptographic techniques and \"cryptology\" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which \"cryptology\" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics. Cryptolingusitics is especially used in military intelligence applications for deciphering foreign communications.","completion":"Cryptography"}
{"prompt":"In addition to linear and differential cryptanalysis, there is a growing catalog of attacks: truncated differential cryptanalysis, partial differential cryptanalysis, integral cryptanalysis, which encompasses square and integral attacks, slide attacks, boomerang attacks, the XSL attack, impossible differential cryptanalysis, and algebraic attacks. For a new block cipher design to have any credibility, it must demonstrate evidence of security against known attacks.","completion":"Cryptography"}
{"prompt":"Identity-based systems have a characteristic problem in operation. Suppose Alice and Bob are users of such a system. Since the information needed to find Alice's public key is completely determined by Alice's ID and the master public key, it is not possible to revoke Alice's credentials and issue new credentials without either (a) changing Alice's ID (usually a phone number or an email address which will appear in a corporate directory); or (b) changing the master public key and re-issuing private keys to all users, including Bob.This limitation may be overcome by including a time component (e.g. the current month) in the identity.","completion":"Cryptography"}
{"prompt":"In 2021, 17 states passed laws and resolutions concerning cryptocurrency regulation. The U.S. Securities and Exchange Commission (SEC) is considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, SEC Chairman Gary Gensler responded to Senator Elizabeth Warren's letter regarding cryptocurrency regulation and called for legislation focused on \"crypto trading, lending and DeFi platforms,\" because of how vulnerable the investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.On 19 October 2021, The first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker \"BITO.\" ProShares CEO Michael L. Sapir said the ETF would expose Bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that the approval of the \"BITO\" ETF by the SEC was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto as well as the hesitance to accept crypto from retail investors. This event would eventually open more opportunities for new capital and new people in this space.The United States Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.On 17 February 2022, the Justice department named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to aid in identification of and dealing with misuse of cryptocurrencies and other digital assets.The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict the growing and profitable industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, President Biden issued an executive order. Followed by the executive order, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released  to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the U.S. cryptocurrency regulation history.In February 2023, the Securities and Exchange Commission (SEC) ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the U.S. The case would impact other major crypto exchanges operating staking programs.On 23 March 2023, the U.S. Securities and Exchange Commission (SEC) issued an alert to investors stating that firms offering crypto asset securities may not be complying with U.S. laws. The SEC stated that unregistered offerings of crypto asset securities may not include important information.","completion":"Cryptography"}
{"prompt":"Despite their travails, Rejewski and Zygalski had fared better than some of their colleagues. Cadix's Polish military chiefs, Langer and Ci\u0119\u017cki, had also been captured\u2014by the Germans, as they tried to escape from France into Spain on the night of 10\u201311 March 1943\u2014along with three other Poles: Antoni Palluth, Edward Fokczy\u0144ski and Kazimierz Gaca. The first two became prisoners of war; the other three were sent as slave labourers to Germany, where Palluth and Fokczy\u0144ski perished. Despite the varyingly dire circumstances in which they were held, none of them\u2014Stefan Mayer emphasizes\u2014betrayed the secret of Enigma's decryption, thus making it possible for the Allies to continue exploiting this vital intelligence resource.Before the war, Palluth, a lecturer in the 1929 secret Pozna\u0144 University cryptology course, had been co-owner of AVA, which produced equipment for the Cipher Bureau, and knew many details of the decryption technology. In Warsaw, under German occupation, other Cipher Bureau workers were interrogated by German intelligence commissions, and some AVA workers were approached by German agents, but all kept silent about compromises to Enigma.","completion":"Cryptography"}
{"prompt":"In the view of al-Kindi, prophecy and philosophy were two different routes to arrive at the truth. He contrasts the two positions in four ways. Firstly, while a person must undergo a long period of training and study to become a philosopher, prophecy is bestowed upon someone by God. Secondly, the philosopher must arrive at the truth by his own devices (and with great difficulty), whereas the prophet has the truth revealed to him by God. Thirdly, the understanding of the prophet \u2013 being divinely revealed \u2013 is clearer and more comprehensive than that of the philosopher. Fourthly, the way in which the prophet is able to express this understanding to the ordinary people is superior. Therefore, al-Kindi says the prophet is superior in two fields: the ease and certainty with which he receives the truth, and the way in which he presents it. However, the crucial implication is that the content of the prophet's and the philosopher's knowledge is the same. This, says Adamson, demonstrates how limited the superiority al-Kindi afforded to prophecy was.In addition to this, al-Kindi adopted a naturalistic view of prophetic visions. He argued that, through the faculty of \"imagination\" as conceived of in Aristotelian philosophy, certain \"pure\" and well-prepared souls, were able to receive information about future events. Significantly, he does not attribute such visions or dreams to revelation from God, but instead explains that imagination enables human beings to receive the \"form\" of something without needing to perceive the physical entity to which it refers. Therefore, it would seem to imply that anyone who has purified themselves would be able to receive such visions. It is precisely this idea, amongst other naturalistic explanations of prophetic miracles that al-Ghazali attacks in his Incoherence of the Philosophers.","completion":"Cryptography"}
{"prompt":"The historical roots of cryptoeconomics can be traced to the rise of altcoins, prominent among them the Ethereum project, which in 2015 pioneered the integration of smart contracts into its blockchain, thereby enabling a wide range of DeFi applications.","completion":"Cryptography"}
{"prompt":"Poe was unable to support himself, so he enlisted in the United States Army as a private on May 27, 1827, using the name \"Edgar A. Perry\". He claimed that he was 22 years old even though he was 18. He first served at Fort Independence in Boston Harbor for five dollars a month. That year, he released his first book, a 40-page collection of poetry titled Tamerlane and Other Poems, attributed with the byline \"by a Bostonian\". Only 50 copies were printed, and the book received virtually no attention. Poe's regiment was posted to Fort Moultrie in Charleston, South Carolina, and traveled by ship on the brig Waltham on November 8, 1827. Poe was promoted to \"artificer\", an enlisted tradesman who prepared shells for artillery, and had his monthly pay doubled. He served for two years and attained the rank of Sergeant Major for Artillery, the highest rank that a non-commissioned officer could achieve; he then sought to end his five-year enlistment early. He revealed his real name and his circumstances to his commanding officer, Lieutenant Howard, who would allow Poe to be discharged only if he reconciled with Allan. Poe wrote a letter to Allan, who was unsympathetic and spent several months ignoring Poe's pleas; Allan may not have written to Poe even to make him aware of his foster mother's illness. Frances Allan died on February 28, 1829, and Poe visited the day after her burial. Perhaps softened by his wife's death, Allan agreed to support Poe's attempt to be discharged in order to receive an appointment to the United States Military Academy at West Point, New York.Poe was finally discharged on April 15, 1829, after securing a replacement to finish his enlisted term for him. Before entering West Point, he moved to Baltimore for a time to stay with his widowed aunt Maria Clemm, her daughter Virginia Eliza Clemm (Poe's first cousin), his brother Henry, and his invalid grandmother Elizabeth Cairnes Poe. In September of that year, Poe received \"the very first words of encouragement I ever remember to have heard\" in a review of his poetry by influential critic John Neal, prompting Poe to dedicate one of the poems to Neal in his second book Al Aaraaf, Tamerlane and Minor Poems, published in Baltimore in 1829.Poe traveled to West Point and matriculated as a cadet on July 1, 1830. In October 1830, Allan married his second wife Louisa Patterson. The marriage and bitter quarrels with Poe over the children born to Allan out of extramarital affairs led to the foster father finally disowning Poe. Poe decided to leave West Point by purposely getting court-martialed. On February 8, 1831, he was tried for gross neglect of duty and disobedience of orders for refusing to attend formations, classes, or church. He tactically pleaded not guilty to induce dismissal, knowing that he would be found guilty.Poe left for New York in February 1831 and released a third volume of poems, simply titled Poems. The book was financed with help from his fellow cadets at West Point, many of whom donated 75 cents to the cause, raising a total of $170. They may have been expecting verses similar to the satirical ones Poe had written about commanding officers. It was printed by Elam Bliss of New York, labeled as \"Second Edition\", and including a page saying, \"To the U.S. Corps of Cadets this volume is respectfully dedicated\". The book once again reprinted the long poems \"Tamerlane\" and \"Al Aaraaf\" but also six previously unpublished poems, including early versions of \"To Helen\", \"Israfel\", and \"The City in the Sea\". Poe returned to Baltimore to his aunt, brother, and cousin in March 1831. His elder brother Henry had been in ill health, in part due to problems with alcoholism, and he died on August 1, 1831.","completion":"Cryptography"}
{"prompt":"During a work-experience placement with Baltimore Technologies, Flannery was shown an unpublished paper by Michael Purser which outlined a new public-key cryptographic scheme using non-commutative multiplication. She was asked to write an implementation of this scheme in Mathematica.\nBefore this placement, Flannery had attended the 1998 ESAT Young Scientist and Technology Exhibition with a project describing already existing cryptographic techniques from the Caesar cipher to RSA. This had won her the Intel Student Award which included the opportunity to compete in the 1998 Intel International Science and Engineering Fair in the United States. Feeling that she needed some original work to add to her exhibition project, Flannery asked Michael Purser for permission to include work based on his cryptographic scheme.\nOn advice from her mathematician father, Flannery decided to use matrices to implement Purser's scheme as matrix multiplication has the necessary property of being non-commutative. As the resulting algorithm would depend on multiplication it would be a great deal faster than the RSA algorithm which uses an exponential step. For her Intel Science Fair project Flannery prepared a demonstration where the same plaintext was enciphered using both RSA and her new Cayley\u2013Purser algorithm and it did indeed show a significant time improvement.\nReturning to the ESAT Young Scientist and Technology Exhibition in 1999, Flannery formalised Cayley-Purser's runtime and analyzed a variety of known attacks, none of which were determined to be effective.\nFlannery did not make any claims that the Cayley\u2013Purser algorithm would replace RSA, knowing that any new cryptographic system would need to stand the test of time before it could be acknowledged as a secure system. The media were not so circumspect however and when she received first prize at the ESAT exhibition, newspapers around the world reported the story that a young girl genius had revolutionised cryptography.\nIn fact an attack on the algorithm was discovered shortly afterwards but she analyzed it and included it as an appendix in later competitions, including a Europe-wide competition in which she won a major award.","completion":"Cryptography"}
{"prompt":"During the German Invasion of Poland in September 1939, key Cipher Bureau personnel were evacuated southeast and \u2013 after the Soviets invaded eastern Poland on 17 September \u2013 into Romania, on the way destroying their cryptological equipment and documentation.  Eventually, crossing Yugoslavia and still-neutral Italy, they reached France. Some personnel of the Cipher Bureau's German section who had worked with Enigma, and most of the workers at the AVA Radio Company that had built Enigma doubles and cryptologic equipment for the German section, remained in Poland. Some were interrogated by the Gestapo, but no one gave away the secret of Polish mastery of Enigma decryption.\nAt PC Bruno, outside Paris, on 20 October 1939 the Poles resumed work on German Enigma ciphers in close collaboration with Britain's Government Code and Cypher School at Bletchley Park.In the interest of security, the Allied cryptological services, before sending their messages over a teleprinter line, encrypted them using Enigma doubles. Henri Braqueni\u00e9 often closed messages with a \"Heil Hitler!\". As late as December 1939, when Lt. Col. Gwido Langer, accompanied by Captain Braqueni\u00e9, visited London and Bletchley Park, the British asked that the Polish cryptologists be turned over to them. Langer, however, took the position that the Polish team must remain where the Polish Armed Forces were being formed \u2013 on French soil. The mathematicians might actually have reached Britain much earlier \u2013 and much more comfortably \u2013 than they eventually did; but in September 1939, when they went to the British embassy in Bucharest, Romania, they were brushed off by a preoccupied British diplomat.In January 1940, the British cryptanalyst Alan Turing spent several days at PC Bruno conferring with his Polish colleagues. He had brought the Poles a full set of Zygalski sheets that had been produced at Bletchley Park by John Jeffreys using Polish-supplied information. On 17 January 1940, the Poles made the first break into wartime Enigma traffic \u2013 that from 28 October 1939.During this period, until the collapse of France in June 1940, ultimately 83 percent of the Enigma keys that were found, were solved at Bletchley Park, the remaining 17 percent at PC Bruno. Rejewski commented:\n\nHow could it be otherwise, when there were three of us [Polish cryptologists] and [there were] at least several hundred British cryptologists, since about 10,000 people worked in Bletchley ... Besides, recovery of keys also depended on the amount of intercepted cipher material, and that amount was far greater on the British side than on the French side. Finally, in France (by contrast with the work in Poland) we ourselves not only sought for the daily keys, but after finding the key also read the messages. ... One can only be surprised that the Poles had as many as 17 percent of the keys to their credit.\nThe inter-Allied cryptologic collaboration prevented duplication of effort and facilitated discoveries. Before fighting had started in Norway in April 1940, the Polish-French team solved an uncommonly hard three-letter code used by the Germans to communicate with fighter and bomber squadrons and for exchange of meteorological data between aircraft and land.  The code had first appeared in December 1939, but the Polish cryptologists had been too preoccupied with Enigma to give the code much attention. With the German assault on the west impending, however, the breaking of the Luftwaffe code took on mounting urgency. The trail of the elusive code (whose system of letters changed every 24 hours) led back to Enigma. The first clue came from the British, who had noticed that the code's letters did not change randomly. If A changed to P, then elsewhere P was replaced by A. The British made no further headway, but the Poles realized that what was manifesting was Enigma's exclusivity principle that they had discovered in 1932. The Germans' carelessness meant that now the Poles, having after midnight solved Enigma's daily setting, could with no further effort also read the Luftwaffe signals.The Germans, just before opening their 10 May 1940 offensive in the west that would trample Belgium, Luxembourg and the Netherlands in order to reach the borders of France, once again changed their procedure for enciphering message keys, rendering the Zygalski sheets \"completely useless\" and temporarily defeating the joint British\u2013Polish cryptologic attacks on Enigma. According to Gustave Bertrand, \"It took superhuman day-and-night effort to overcome this new difficulty: on May 20, decryption resumed.\"Following the capitulation of France in June 1940, the Poles were evacuated to Algeria.  On October 1, 1940, they resumed work at Cadix, near Uz\u00e8s in unoccupied southern Vichy France, under the sponsorship of Gustave Bertrand.A little over two years later, on 8 November 1942, Bertrand learned from the BBC that the Allies had landed in French North Africa (\"Operation Torch\"). Knowing that in such an eventuality the Germans planned to occupy Vichy France, on 9 November he evacuated Cadix. Two days later, on 11 November, the Germans indeed marched into southern France.  On the morning of 12 November they occupied Cadix.Over the two years since its establishment in October 1940, Cadix had decrypted thousands of Wehrmacht, SS and Gestapo messages, originating not only from French territory but from across Europe, which provided invaluable intelligence to Allied commands and resistance movements.  Cadix had also decrypted thousands of Soviet messages.Having departed Cadix, the Polish personnel evaded the occupying Italian security police and German Gestapo and sought to escape France via Spain. Jerzy R\u00f3\u017cycki, Jan Grali\u0144ski and Piotr Smole\u0144ski had died in the January 1942 sinking, in the Mediterranean Sea, of a French passenger ship, the Lamorici\u00e8re, in which they had been returning to southern France from a tour of duty in Algeria.Marian Rejewski and Henryk Zygalski hiked over the Pyrenees with a guide (who robbed them at gunpoint) to the Spanish border, where they were arrested on 30 January 1943. They were incarcerated by the Spaniards for three months before being released, upon Red Cross intervention, on 4 May 1943. They then managed, by a circuitous land\u2013sea\u2013air route, to join the Polish Armed Forces in Britain, Rejewski and Zygalski were inducted into the Polish Army as privates (they would eventually be promoted to lieutenant) and put to work breaking German SS and SD hand ciphers at a Polish signals facility in Boxmoor. Because of their having been in occupied France, the British considered it too risky to invite them to work at Bletchley Park.Finally, with the end of the two mathematicians' cryptologic work at the close of World War II, the Cipher Bureau ceased to exist. From nearly its inception in 1931 until war's end in 1945, the Bureau, sometimes incorporated into aggregates under cryptonyms (PC Bruno and Cadix), had been essentially the same agency, with most of the same core personnel, carrying out much the same tasks; now it was extinguished. Neither Rejewski nor Zygalski would work again as cryptologists. In late 1946 Rejewski returned to his family in a devastated and politically altered Poland, to live there another 33 years until his death in February 1980. Zygalski would remain in England until his death in August 1978.","completion":"Cryptography"}
{"prompt":"In contrast to smart cards, which are simple devices performing a single function, personal computers are doing many things at once. Thus, it is much more difficult to perform electromagnetic side-channel attacks against them, due to high levels of noise and fast clock rates. Despite these issues, researchers in 2015 and 2016 showed attacks against a laptop using a near-field magnetic probe. The resulting signal, observed for only a few seconds, was filtered, amplified, and digitized for offline key extraction. Most attacks require expensive, lab-grade equipment, and require the attacker to be extremely close to the victim computer. However, some researchers were able to show attacks using cheaper hardware and from distances of up to half a meter. These attacks, however, required the collection of more traces than the more expensive attacks.","completion":"Cryptography"}
{"prompt":"The protocol is considered secure against eavesdroppers if G and g are chosen properly. In particular, the order of the group G must be large, particularly if the same group is used for large amounts of traffic. The eavesdropper has to solve the Diffie\u2013Hellman problem to obtain gab. This is currently considered difficult for groups whose order is large enough. An efficient algorithm to solve the discrete logarithm problem would make it easy to compute a or b and solve the Diffie\u2013Hellman problem, making this and many other public key cryptosystems insecure. Fields of small characteristic may be less secure.The order of G should have a large prime factor to prevent use of the Pohlig\u2013Hellman algorithm to obtain a or b. For this reason, a Sophie Germain prime q is sometimes used to calculate p = 2q + 1, called a safe prime, since the order of G is then only divisible by 2 and q. g is then sometimes chosen to generate the order q subgroup of G, rather than G, so that the Legendre symbol of ga never reveals the low order bit of a. A protocol using such a choice is for example IKEv2.g is often a small integer such as 2. Because of the random self-reducibility of the discrete logarithm problem a small g is equally secure as any other generator of the same group.\nIf Alice and Bob use random number generators whose outputs are not completely random and can be predicted to some extent, then it is much easier to eavesdrop.\nIn the original description, the Diffie\u2013Hellman exchange by itself does not provide authentication of the communicating parties and is thus vulnerable to a man-in-the-middle attack. Mallory (an active attacker executing the man-in-the-middle attack) may establish two distinct key exchanges, one with Alice and the other with Bob, effectively masquerading as Alice to Bob, and vice versa, allowing her to decrypt, then re-encrypt, the messages passed between them. Note that Mallory must continue to be in the middle, actively decrypting and re-encrypting messages every time Alice and Bob communicate. If she is ever absent, her previous presence is then revealed to Alice and Bob. They will know that all of their private conversations had been intercepted and decoded by someone in the channel. In most cases it will not help them get Mallory's private key, even if she used the same key for both exchanges.\nA method to authenticate the communicating parties to each other is generally needed to prevent this type of attack. Variants of Diffie\u2013Hellman, such as STS protocol, may be used instead to avoid these types of attacks.","completion":"Cryptography"}
{"prompt":"Symmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret.\n\nIn a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used\u2014a public key and a private key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as \"the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance\".In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. In a public-key encryption system, the public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie\u2013Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.\nThe X.509 standard defines the most commonly used format for public key certificates.Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.The Diffie\u2013Hellman and RSA algorithms, in addition to being the first publicly known examples of high-quality public-key algorithms, have been among the most widely used. Other asymmetric-key algorithms include the Cramer\u2013Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques.A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that was very similar in design rationale to RSA. In 1974, Malcolm J. Williamson is claimed to have developed the Diffie\u2013Hellman key exchange.\nPublic-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL\/TLS, many VPNs, etc.).Public-key algorithms are most often based on the computational complexity of \"hard\" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie\u2013Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.","completion":"Cryptography"}
{"prompt":"A cryptocurrency wallet is a means of storing the public and private \"keys\" (address) or seed which can be used to receive or spend the cryptocurrency. With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.\nThere exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with a software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.","completion":"Cryptography"}
{"prompt":"In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services. This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, Bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%. Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create Bitcoin and Ethereum.In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.","completion":"Cryptography"}
{"prompt":"Powers obey the usual algebraic identity bk\u2009+\u2009l = bk\u2009bl. In other words, the function\n\n  \n    \n      \n        f\n        :\n        \n          Z\n        \n        \u2192\n        G\n      \n    \n    {\\displaystyle f\\colon \\mathbf {Z} \\to G}\n  defined by f(k) = bk is a group homomorphism from the integers Z under addition onto the subgroup H of G generated by b. For all a in H, logb\u2009a exists. Conversely, logb\u2009a does not exist for a that are not in H.\nIf H is infinite, then logb\u2009a is also unique, and the discrete logarithm amounts to a group isomorphism\n\n  \n    \n      \n        \n          log\n          \n            b\n          \n        \n        :\n        H\n        \u2192\n        \n          Z\n        \n        .\n      \n    \n    {\\displaystyle \\log _{b}\\colon H\\to \\mathbf {Z} .}\n  On the other hand, if H is finite of order n, then logb\u2009a is unique only up to congruence modulo n, and the discrete logarithm amounts to a group isomorphism\n\n  \n    \n      \n        \n          log\n          \n            b\n          \n        \n        :\n        H\n        \u2192\n        \n          \n            Z\n          \n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\log _{b}\\colon H\\to \\mathbf {Z} _{n},}\n  where Zn denotes the additive group of integers modulo n.\nThe familiar base change formula for ordinary logarithms remains valid: If c is another generator of H, then\n\n  \n    \n      \n        \n          log\n          \n            c\n          \n        \n        \u2061\n        a\n        =\n        \n          log\n          \n            c\n          \n        \n        \u2061\n        b\n        \u22c5\n        \n          log\n          \n            b\n          \n        \n        \u2061\n        a\n        .\n      \n    \n    {\\displaystyle \\log _{c}a=\\log _{c}b\\cdot \\log _{b}a.}","completion":"Cryptography"}
{"prompt":"The Unique Label Cover problem is a constraint satisfaction problem, where each constraint \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   involves two variables \n  \n    \n      \n        x\n        ,\n        y\n      \n    \n    {\\displaystyle x,y}\n  , and for each value of \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   there is a unique value of \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   that satisfies \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\nDetermining whether all the constraints can be satisfied is easy, but the Unique Game Conjecture (UGC) postulates that determining whether almost all the constraints (\n  \n    \n      \n        (\n        1\n        \u2212\n        \u03b5\n        )\n      \n    \n    {\\displaystyle (1-\\varepsilon )}\n  -fraction, for any constant \n  \n    \n      \n        \u03b5\n        >\n        0\n      \n    \n    {\\displaystyle \\varepsilon >0}\n  ) can be satisfied or almost none of them (\n  \n    \n      \n        \u03b5\n      \n    \n    {\\displaystyle \\varepsilon }\n  -fraction) can be satisfied is NP-hard.\nApproximation problems are often known to be NP-hard assuming UGC; such problems are referred to as UG-hard. \nIn particular, assuming UGC there is a semidefinite programming algorithm that achieves optimal approximation guarantees for many important problems.","completion":"Cryptography"}
{"prompt":"The discovery of differential cryptanalysis is generally attributed to Eli Biham and Adi Shamir in the late 1980s, who published a number of attacks against various block ciphers and hash functions, including a theoretical weakness in the Data Encryption Standard (DES). It was noted by Biham and Shamir that DES was surprisingly resistant to differential cryptanalysis, but small modifications to the algorithm would make it much more susceptible.:\u200a8\u20139\u200aIn 1994, a member of the original IBM DES team, Don Coppersmith, published a paper stating that differential cryptanalysis was known to IBM as early as 1974, and that defending against differential cryptanalysis had been a design goal. According to author Steven Levy, IBM had discovered differential cryptanalysis on its own, and the NSA was apparently well aware of the technique. IBM kept some secrets, as Coppersmith explains: \"After discussions with NSA, it was decided that disclosure of the design considerations would reveal the technique of differential cryptanalysis, a powerful technique that could be used against many ciphers. This in turn would weaken the competitive advantage the United States enjoyed over other countries in the field of cryptography.\" Within IBM, differential cryptanalysis was known as the \"T-attack\" or \"Tickle attack\".While DES was designed with resistance to differential cryptanalysis in mind, other contemporary ciphers proved to be vulnerable. An early target for the attack was the FEAL block cipher. The original proposed version with four rounds (FEAL-4) can be broken using only eight chosen plaintexts, and even a 31-round version of FEAL is susceptible to the attack. In contrast, the scheme can successfully cryptanalyze DES with an effort on the order of 247 chosen plaintexts.","completion":"Cryptography"}
{"prompt":"Smartphones are of particular interest for electromagnetic side-channel attacks. Since the advent of mobile phone payment systems such as Apple Pay, e-commerce systems have become increasingly commonplace. Likewise, the amount of research dedicated to mobile phone security side channel attacks has also increased. Currently most attacks are proofs of concept that use expensive lab-grade signal processing equipment. One of these attacks demonstrated that a commercial radio receiver could detect mobile phone leakage up to three meters away.However, attacks using low-end consumer grade equipment have also shown successful. By using an external USB sound card and an induction coil salvaged from a wireless charging pad, researchers were able to extract a user's signing key in Android's OpenSSL and Apple's CommonCrypto implementations of ECDSA.","completion":"Cryptography"}
{"prompt":"The Elizabethan spymaster Sir Francis Walsingham (1530\u20131590) is reported to have used a \"trellis\" to conceal the letters of a plaintext in communication with his agents. However, he generally preferred the combined code-cipher method known as a nomenclator, which was the practical state-of-the-art in his day. The trellis was described as a device with spaces that was reversible. It appears to have been a transposition tool that produced something much like the Rail fence cipher and resembled a chess board.\nCardano is not known to have proposed this variation, but he was a chess player who wrote a book on gaming, so the pattern would have been familiar to him.  Whereas the ordinary Cardan grille has arbitrary perforations, if his method of cutting holes is applied to the white squares of a chess board a regular pattern results.\nThe encipherer begins with the board in the wrong position for chess.  Each successive letter of the message is written in a single square.  If the message is written vertically, it is taken off horizontally and vice versa.\nAfter filling in 32 letters, the board is turned through 90 degrees and another 32 letters written (note that flipping the board horizontally or vertically is the equivalent).  Shorter messages are filled with null letters (i.e., padding).  Messages longer than 64 letters require another turn of the board and another sheet of paper.  If the plaintext is too short, each square must be filled up entirely with nulls.\nJ M T H H D L I S I Y P S L U I A O W A E T I E E N W A P D E N E N E L G O O N N A I T E E F N K E R L O O N D D N T T E N R X\nThis transposition method produces an invariant pattern and is not satisfactorily secure for anything other than cursory notes.\n33, 5, 41, 13, 49, 21, 57, 29, 1, 37, 9, 45, 17, 53, 25, 61, 34, 6, 42, 14, 50, 22, 58, 30, 2, 38, 10, 46, 18, 54, 26, 62, 35, 7, 43, 15, 51, 23, 59, 31, 3, 39, 11, 47, 19, 55, 27, 63, 36, 8, 44, 16, 52, 24, 60, 32, 4, 40, 12, 48, 20, 56, 28, 64\nA second transposition is needed to obscure the letters.  Following the chess analogy, the route taken might be the knight's move. Or some other path can be agreed upon, such as a reverse spiral, together with a specific number of nulls to pad the start and end of a message.","completion":"Cryptography"}
{"prompt":"Non-repudiation, or more specifically non-repudiation of origin, is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.\nNote that these authentication, non-repudiation etc. properties rely on the secret key not having been revoked prior to its usage. Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an \"online\" check; e.g., checking a certificate revocation list or via the Online Certificate Status Protocol.  Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen. Of course, with stolen key pairs, the theft is often discovered only after the secret key's use, e.g., to sign a bogus certificate for espionage purpose.","completion":"Cryptography"}
{"prompt":"The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.","completion":"Cryptography"}
{"prompt":"Modern ciphers are more secure than classical ciphers and are designed  to withstand a wide range of attacks. An attacker should not be able to find the key used in a modern cipher, even if he knows any amount of plaintext and corresponding ciphertext. Modern encryption methods can be divided into the following categories:\n\nPrivate-key cryptography (symmetric key algorithm): the same key is used for encryption and decryption\nPublic-key cryptography (asymmetric key algorithm): two different keys are used for encryption and decryptionIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a public key is published and enables any sender to perform encryption, while a private key is kept secret by the receiver and enables only him to perform correct decryption.\nSymmetric key ciphers can be divided into block ciphers and stream ciphers. Block ciphers operate on fixed-length groups of bits, called blocks, with an unvarying transformation. Stream ciphers encrypt plaintext digits one at a time on a continuous stream of data and the transformation of successive digits varies during the encryption process.","completion":"Cryptography"}
{"prompt":"In World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers \u2013 including the Enigma machine and the Lorenz cipher \u2013 and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.Cryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.In practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers \u2013 the first electronic digital computers to be controlled by a program.","completion":"Cryptography"}
{"prompt":"Elliptic curves over finite fields are used in some cryptographic applications as well as for integer factorization. Typically, the general idea in these applications is that a known algorithm which makes use of certain finite groups is rewritten to use the groups of rational points of elliptic curves. For more see also:\n\nElliptic curve cryptography\nElliptic-curve Diffie\u2013Hellman key exchange\nSupersingular isogeny key exchange\nElliptic curve digital signature algorithm\nEdDSA digital signature algorithm\nDual EC DRBG random number generator\nLenstra elliptic-curve factorization\nElliptic curve primality proving","completion":"Cryptography"}
{"prompt":"Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person, but rather to one or more specific keys (or \"addresses\"). Thereby, Bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain. Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.","completion":"Cryptography"}
{"prompt":"The Germans made heavy use, in several variants, of an electromechanical rotor machine known as Enigma. Mathematician Marian Rejewski, at Poland's Cipher Bureau, in December 1932 deduced the detailed structure of the German Army Enigma, using mathematics and limited documentation supplied by Captain Gustave Bertrand of French military intelligence acquired from a German clerk. This was the greatest breakthrough in cryptanalysis in a thousand years and more, according to historian David Kahn. Rejewski and his mathematical Cipher Bureau colleagues, Jerzy R\u00f3\u017cycki and Henryk Zygalski, continued reading Enigma and keeping pace with the evolution of the German Army machine's components and encipherment procedures for some time. As the Poles' resources became strained by the changes being introduced by the Germans, and as war loomed, the Cipher Bureau, on the Polish General Staff's instructions, on 25 July 1939, at Warsaw, initiated French and British intelligence representatives into the secrets of Enigma decryption.\nSoon after the invasion of Poland by Germany on 1 September 1939, key Cipher Bureau personnel were evacuated southeastward; on 17 September, as the Soviet Union attacked Poland from the East, they crossed into Romania. From there they reached Paris, France; at PC Bruno, near Paris, they continued working toward breaking Enigma, collaborating with British cryptologists at Bletchley Park as the British got up to speed on their work breaking Enigma. In due course, the British cryptographers \u2013 whose ranks included many chess masters and mathematics dons such as Gordon Welchman, Max Newman, and Alan Turing (the conceptual founder of modern computing) \u2013  made substantial breakthroughs in the scale and technology of Enigma decryption.\nGerman code breaking in World War II also had some success, most importantly by breaking the Naval Cipher No. 3. This enabled them to track and sink Atlantic convoys. It was only Ultra intelligence that finally persuaded the admiralty to change their codes in June 1943. This is surprising given the success of the British Room 40 code breakers in the previous world war.\nAt the end of the War, on 19 April 1945, Britain's highest level civilian and military officials were told that they could never reveal that the German Enigma cipher had been broken because it would give the defeated enemy the chance to say they \"were not well and fairly beaten\".The German military also deployed several teleprinter stream ciphers. Bletchley Park called them the Fish ciphers; Max Newman and colleagues designed and deployed the Heath Robinson, and then the world's first programmable digital electronic computer, the Colossus, to help with their cryptanalysis. The German Foreign Office began to use the one-time pad in 1919; some of this traffic was read in World War II partly as the result of recovery of some key material in South America that was discarded without sufficient care by a German courier.\nThe Schl\u00fcsselger\u00e4t 41 was developed late in the war as a more secure replacement for Enigma, but only saw limited use.","completion":"Cryptography"}
{"prompt":"Cryptocurrency exchanges allow customers to trade cryptocurrencies for other assets, such as conventional fiat money, or to trade between different digital currencies.\nCrypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020 it was possible to arbitrage to find the difference in price across several markets.","completion":"Cryptography"}
{"prompt":"In 1997 a kind of triple DH was proposed by Simon Blake-Wilson, Don Johnson, Alfred Menezes in \"Key Agreement Protocols and their Security Analysis (1997)\", which was improved by C. Kudla and K. G. Paterson in \u201cModular Security Proofs for Key Agreement Protocols (2005)\u201d and shown to be secure. It's also used or mentioned in other variants. For example:\n\nExtended Triple Diffie-Hellman\nsci.crypt news group (from 18.08.2002)\nDouble Ratchet Algorithm\nSignal ProtocolThe long term secret keys of Alice and Bob are denoted by a and b respectively, with public keys A and B, as well as the ephemeral key pairs x, X and y, Y. Then protocol is:\n\nThe long term public keys need to be transferred somehow. That can be done beforehand in a separate, trusted channel, or the public keys can be encrypted using some partial key agreement to preserve anonymity. For more of such details as well as other improvements like side channel protection or explicit key confirmation, as well as early messages and additional password authentication, one could e.g. have a look at \"Advanced modular handshake for key agreement and optional authentication\"","completion":"Cryptography"}
{"prompt":"Following the 2015 Charlie Hebdo shooting, a terrorism attack, former UK Prime Minister David Cameron called for outlawing non-backdoored cryptography, saying that there should be no \"means of communication\" which \"we cannot read\". US president Barack Obama sided with Cameron on this. This call for action does not seem to have resulted in any legislation or changes in the status quo of non-backdoored cryptography being legal and available.","completion":"Cryptography"}
{"prompt":"In the asymptotic setting, a family of deterministic polynomial time computable functions                                    G                        k                             :         {         0         ,         1                    }                        k                             \u2192         {         0         ,         1                    }                        p             (             k             )                                     {\\displaystyle G_{k}\\colon \\{0,1\\}^{k}\\to \\{0,1\\}^{p(k)}}    for some polynomial p, is a pseudorandom number generator (PRNG, or PRG in some references), if it stretches the length of its input (                        p         (         k         )         >         k                 {\\displaystyle p(k)>k}    for any k), and if its output is computationally indistinguishable from true randomness, i.e..","completion":"Cryptography"}
{"prompt":"Collision resistance is desirable for several reasons.\n\nIn some digital signature systems, a party attests to a document by publishing a public key signature on a hash of the document. If it is possible to produce two documents with the same hash, an attacker could get a party to attest to one, and then claim that the party had attested to the other.\nIn some distributed content systems, parties compare cryptographic hashes of files in order to make sure they have the same version. An attacker who could produce two files with the same hash could trick users into believing they had the same version of a file when they in fact did not.","completion":"Cryptography"}
{"prompt":"The security of the ElGamal scheme depends on the properties of the underlying group \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   as well as any padding scheme used on the messages. If the computational Diffie\u2013Hellman assumption (CDH) holds in the underlying cyclic group \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  , then the encryption function is one-way.If the decisional Diffie\u2013Hellman assumption (DDH) holds in \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  , then\nElGamal achieves semantic security. Semantic security is not implied by the computational Diffie\u2013Hellman assumption alone. See Decisional Diffie\u2013Hellman assumption for a discussion of groups where the assumption is believed to hold.\nElGamal encryption is unconditionally malleable, and therefore is not secure under chosen ciphertext attack. For example, given an encryption \n  \n    \n      \n        (\n        \n          c\n          \n            1\n          \n        \n        ,\n        \n          c\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (c_{1},c_{2})}\n   of some (possibly unknown) message \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , one can easily construct a valid encryption \n  \n    \n      \n        (\n        \n          c\n          \n            1\n          \n        \n        ,\n        2\n        \n          c\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (c_{1},2c_{2})}\n   of the message \n  \n    \n      \n        2\n        m\n      \n    \n    {\\displaystyle 2m}\n  .\nTo achieve chosen-ciphertext security, the scheme must be further modified, or an appropriate padding scheme must be used. Depending on the modification, the DDH assumption may or may not be necessary.\nOther schemes related to ElGamal which achieve security against chosen ciphertext attacks have also been proposed. The Cramer\u2013Shoup cryptosystem is secure under chosen ciphertext attack assuming DDH holds for \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  . Its proof does not use the random oracle model. Another proposed scheme is DHIES, whose proof requires an assumption that is stronger than the DDH assumption.","completion":"Cryptography"}
{"prompt":"An initialization vector (IV) or starting variable (SV) is a block of bits that is used by several modes to randomize the encryption and hence to produce distinct ciphertexts even if the same plaintext is encrypted multiple times, without the need for a slower re-keying process.An initialization vector has different security requirements than a key, so the IV usually does not need to be secret. For most block cipher modes it is important that an initialization vector is never reused under the same key, i.e. it must be a cryptographic nonce. Many block cipher modes have stronger requirements, such as the IV must be random or pseudorandom. Some block ciphers have particular problems with certain initialization vectors, such as all zero IV generating no encryption (for some keys).\nIt is recommended to review relevant IV requirements for the particular block cipher mode in relevant specification, for example SP800-38A.\nFor CBC and CFB, reusing an IV leaks some information about the first block of plaintext, and about any common prefix shared by the two messages.\nFor OFB and CTR, reusing an IV causes key bitstream re-use, which breaks security. This can be seen because both modes effectively create a bitstream that is XORed with the plaintext, and this bitstream is dependent on the key and IV only.\nIn CBC mode, the IV must be unpredictable (random or pseudorandom) at encryption time; in particular, the (previously) common practice of re-using the last ciphertext block of a message as the IV for the next message is insecure (for example, this method was used by SSL 2.0). If an attacker knows the IV (or the previous block of ciphertext) before the next plaintext is specified, they can check their guess about plaintext of some block that was encrypted with the same key before (this is known as the TLS CBC IV attack).For some keys, an all-zero initialization vector may generate some block cipher modes (CFB-8, OFB-8) to get the internal state stuck at all-zero. For CFB-8, an all-zero IV and an all-zero plaintext, causes 1\/256 of keys to generate no encryption, plaintext is returned as ciphertext. For OFB-8, using all zero initialization vector will generate no encryption for 1\/256 of keys. OFB-8 encryption returns the plaintext unencrypted for affected keys.\nSome modes (such as AES-SIV and AES-GCM-SIV) are built to be more nonce-misuse resistant, i.e. resilient to scenarios in which the randomness generation is faulty or under the control of the attacker.  \n\nSynthetic initialization vectors (SIV) synthesize an internal IV by running a pseudo-random function (PRF) construction called S2V on the input (additional data and plaintext), preventing any external data from directly controlling the IV. External nonces \/ IV may be fed into S2V as an additional data field.\nAES-GCM-SIVs synthesize an internal IV by running POLYVAL Galois mode of authentication on input (additional data and plaintext), followed by an AES operation.","completion":"Cryptography"}
{"prompt":"This section is concerned with points P = (x, y) of E such that x is an integer.\nFor example, the equation y2 = x3 + 17 has eight integral solutions with y > 0:\n(x, y) = (\u22122, 3), (\u22121, 4), (2, 5), (4, 9), (8, 23), (43, 282), (52, 375), (5234, 378661).As another example, Ljunggren's equation, a curve whose Weierstrass form is y2 = x3 \u2212 2x, has only four solutions with y \u2265 0 :\n(x, y) = (0, 0), (\u22121, 1), (2, 2), (338, 6214).","completion":"Cryptography"}
{"prompt":"Like in normal counter mode, blocks are numbered sequentially, and then this block number is combined with an initialization vector (IV) and encrypted with a block cipher E, usually AES. The result of this encryption is then XORed with the plaintext to produce the ciphertext. Like all counter modes, this is essentially a stream cipher, and so it is essential that a different IV is used for each stream that is encrypted.\nThe ciphertext blocks are considered coefficients of a polynomial which is then evaluated at a key-dependent point H, using finite field arithmetic. The result is then encrypted, producing an authentication tag that can be used to verify the integrity of the data. The encrypted text then contains the IV, ciphertext, and authentication tag.","completion":"Cryptography"}
{"prompt":"Encryption is an important tool but is not sufficient alone to ensure the security or privacy of sensitive information throughout its lifetime. Most applications of encryption protect information only at rest or in transit, leaving sensitive data in clear text and potentially vulnerable to improper disclosure during processing, such as by a cloud service for example. Homomorphic encryption and secure multi-party computation are emerging techniques to compute on encrypted data; these techniques are general and Turing complete but incur high computational and\/or communication costs.\nIn response to encryption of data at rest, cyber-adversaries have developed new types of attacks. These more recent threats to encryption of data at rest include cryptographic attacks, stolen ciphertext attacks, attacks on encryption keys, insider attacks, data corruption or integrity attacks, data destruction attacks, and ransomware attacks. Data fragmentation and active defense data protection technologies attempt to counter some of these attacks, by distributing, moving, or mutating ciphertext so it is more difficult to identify, steal, corrupt, or destroy.","completion":"Cryptography"}
{"prompt":"In early 2020, the CrypTool project decided to merge with a similar project of the same name, CrypTools, founded in 2017 in Australia by Luka Lafaye de Micheaux, Arthur Guiot, and Lucas Gruwez. CrypTool, much older and known, thus completely \"absorbs\" the project under its name.\n\nThe first impact of this merger is the rebranding of the project. A new logo, a new website, and the new CTO version are announced. Currently, it's still in development. Another change was the targeted audience. Previously, CrypTool focused on (university) students, and CrypTools on developers and young people. It was therefore necessary to broaden the audience.\nOn May 15, 2020, in the midst of the COVID-19 pandemic, CrypTool announces the creation of tools to test Decentralized contact tracing protocols. A new page is added to CTO with technical description of the algorithms involved in DP-3T and Exposure Notification. In addition to this, CrypTool also announced the implementation of a page dedicated to raising awareness of the cryptographic means related to privacy in these protocols, called the Corona Tracing Animation. The newer page stands out for its new design and its accessibility to ordinary users.","completion":"Cryptography"}
{"prompt":"For a composite number \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , it is not known how to efficiently compute its Euler's totient function \n  \n    \n      \n        \u03d5\n        (\n        m\n        )\n      \n    \n    {\\displaystyle \\phi (m)}\n  . The Phi-hiding assumption postulates that it is hard to compute \n  \n    \n      \n        \u03d5\n        (\n        m\n        )\n      \n    \n    {\\displaystyle \\phi (m)}\n  , and furthermore even computing any prime factors of \n  \n    \n      \n        \u03d5\n        (\n        m\n        )\n      \n    \n    {\\displaystyle \\phi (m)}\n   is hard. This assumption is used in the Cachin\u2013Micali\u2013Stadler PIR protocol.","completion":"Cryptography"}
{"prompt":"Hamburg chose the Solinas trinomial prime base p = 2448 \u2212 2224 \u2212 1, calling it a \"Goldilocks\" prime \"because its form defines the golden ratio \u03c6 \u2261 2224\". The main advantage of a golden-ratio prime is fast Karatsuba multiplication.The curve Hamburg used is an untwisted Edwards curve\nEd: y2 + x2 = 1 \u2212 39081x2y2. The constant d = \u221239081 was chosen as the smallest absolute value that had the required mathematical properties, thus a nothing-up-my-sleeve number.\nCurve448 is constructed such that it avoids many potential implementation pitfalls.","completion":"Cryptography"}
{"prompt":"Deep Crack was designed by Cryptography Research, Inc., Advanced Wireless Technologies, and the EFF. The principal designer was Paul Kocher, president of Cryptography Research. Advanced Wireless Technologies built 1,856 custom ASIC DES chips (called Deep Crack or AWT-4500), housed on 29 circuit boards of 64 chips each. The boards were then fitted in six cabinets and mounted in a Sun-4\/470 chassis.\nThe search was coordinated by a single PC which assigned ranges of keys to the chips. The entire machine was capable of testing over 90 billion keys per second. It would take about 9 days to test every possible key at that rate. On average, the correct key would be found in half that time.\nIn 2006, another custom hardware attack machine was designed based on FPGAs. COPACOBANA (COst-optimized PArallel COdeBreaker) is able to crack DES at considerably lower cost. This advantage is mainly due to progress in integrated circuit technology. \nIn July 2012, security researchers David Hulton and Moxie Marlinspike unveiled a cloud computing tool for breaking the MS-CHAPv2 protocol by recovering the protocol's DES encryption keys by brute force.  This tool effectively allows members of the general public to recover a DES key from a known plaintext\u2013ciphertext pair in about 24 hours.","completion":"Cryptography"}
{"prompt":"In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the Bitcoin blockchain. Authors are also asked to include a personal Bitcoin address in the first page of their papers.","completion":"Cryptography"}
{"prompt":"Because digital signature algorithms cannot sign a large amount of data efficiently, most implementations use a hash function to reduce (\"compress\") the amount of data that needs to be signed down to a constant size. Digital signature schemes often become vulnerable to hash collisions as soon as the underlying hash function is practically broken; techniques like randomized (salted) hashing will buy extra time by requiring the harder preimage attack.The usual attack scenario goes like this:\n\nMallory creates two different documents A and B that have an identical hash value, i.e., a collision. Mallory seeks to deceive Bob into accepting document B, ostensibly from Alice.\nMallory sends document A to Alice, who agrees to what the document says, signs its hash, and sends the signature to Mallory.\nMallory attaches the signature from document A to document B.\nMallory then sends the signature and document B to Bob, claiming that Alice signed B. Because the digital signature matches document B's hash, Bob's software is unable to detect the substitution.In 2008, researchers used a chosen-prefix collision attack against MD5 using this scenario, to produce a rogue certificate authority certificate. They created two versions of a TLS public key certificate, one of which appeared legitimate and was submitted for signing by the RapidSSL certificate authority. The second version, which had the same MD5 hash, contained flags which signal web browsers to accept it as a legitimate authority for issuing arbitrary other certificates.","completion":"Cryptography"}
{"prompt":"After his brother's death, Poe began more earnest attempts to start his career as a writer, but he chose a difficult time in American publishing to do so. He was one of the first Americans to live by writing alone and was hampered by the lack of an international copyright law. American publishers often produced unauthorized copies of British works rather than paying for new work by Americans. The industry was also particularly hurt by the Panic of 1837. There was a booming growth in American periodicals around this time, fueled in part by new technology, but many did not last beyond a few issues. Publishers often refused to pay their writers or paid them much later than they promised, and Poe repeatedly resorted to humiliating pleas for money and other assistance.After his early attempts at poetry, Poe had turned his attention to prose, likely based on John Neal's critiques in The Yankee magazine. He placed a few stories with a Philadelphia publication and began work on his only drama Politian. The Baltimore Saturday Visiter awarded him a prize in October 1833 for his short story \"MS. Found in a Bottle\". The story brought him to the attention of John P. Kennedy, a Baltimorean of considerable means who helped Poe place some of his stories and introduced him to Thomas W. White, editor of the Southern Literary Messenger in Richmond.\nIn 1835, Poe became assistant editor of the 'Southern Literary Messenger, but White discharged him within a few weeks for being drunk on the job. Poe returned to Baltimore, where he obtained a license to marry his cousin Virginia on September 22, 1835, though it is unknown if they were married at that time. He was 26 and she was 13.\nPoe was reinstated by White after promising good behavior, and he returned to Richmond with Virginia and her mother. He remained at the Messenger until January 1837. During this period, Poe claimed that its circulation increased from 700 to 3,500. He published several poems, book reviews, critiques, and stories in the paper. On May 16, 1836, he and Virginia held a Presbyterian wedding ceremony performed by Amasa Converse at their Richmond boarding house, with a witness falsely attesting Clemm's age as 21.","completion":"Cryptography"}
{"prompt":"A number of modes of operation have been designed to combine secrecy and authentication in a single cryptographic primitive. Examples of such modes are , integrity-aware cipher block chaining (IACBC), integrity-aware parallelizable mode (IAPM), OCB, EAX, CWC, CCM, and GCM. Authenticated encryption modes are classified as single-pass modes or double-pass modes. Some single-pass authenticated encryption algorithms, such as OCB mode, are encumbered by patents, while others were specifically designed and released in a way to avoid such encumberment.\nIn addition, some modes also allow for the authentication of unencrypted associated data, and these are called AEAD (authenticated encryption with associated data) schemes. For example, EAX mode is a double-pass AEAD scheme while OCB mode is single-pass.","completion":"Cryptography"}
{"prompt":"This standard was published on 15 November 2013. It includes techniques for identity-based encryption, signatures, signcryption, key agreement, and proxy re-encryption, all based on bilinear pairings.","completion":"Cryptography"}
{"prompt":"While solving a monoalphabetic substitution cipher is easy, solving even a simple code is difficult. Decrypting a coded message is a little like trying to translate a document written in a foreign language, with the task basically amounting to building up a \"dictionary\" of the codegroups and the plaintext words they represent.\nOne fingerhold on a simple code is the fact that some words are more common than others, such as \"the\" or \"a\" in English. In telegraphic messages, the codegroup for \"STOP\" (i.e., end of sentence or paragraph) is usually very common. This helps define the structure of the message in terms of sentences, if not their meaning, and this is cryptanalytically useful.\nFurther progress can be made against a code by collecting many codetexts encrypted with the same code and then using information from other sources \n\nspies\nnewspapers\ndiplomatic cocktail party chat\nthe location from where a message was sent\nwhere it was being sent to (i.e., traffic analysis)\nthe time the message was sent,\nevents occurring before and after the message was sent\nthe normal habits of the people sending the coded messages\netc.For example, a particular codegroup found almost exclusively in messages from a particular army and nowhere else might very well indicate the commander of that army. A codegroup that appears in messages preceding an attack on a particular location may very well stand for that location.\nCribs can be an immediate giveaway to the definitions of codegroups. As codegroups are determined, they can gradually build up a critical mass, with more and more codegroups revealed from context and educated guesswork. One-part codes are more vulnerable to such educated guesswork than two-part codes, since if the codenumber \"26839\" of a one-part code is determined to stand for \"bulldozer\", then the lower codenumber \"17598\" will likely stand for a plaintext word that starts with \"a\" or \"b\". At least, for simple one part codes.\nVarious tricks can be used to \"plant\" or \"sow\" information into a coded message, for example by executing a raid at a particular time and location against an enemy, and then examining code messages sent after the raid. Coding errors are a particularly useful fingerhold into a code; people reliably make errors, sometimes disastrous ones. Planting data and exploiting errors works against ciphers as well.\n\nThe most obvious and, in principle at least, simplest way of cracking a code is to steal the codebook through bribery, burglary, or raiding parties \u2014 procedures sometimes glorified by the phrase \"practical cryptography\" \u2014 and this is a weakness for both codes and ciphers, though codebooks are generally larger and used longer than cipher keys. While a good code may be harder to break than a cipher, the need to write and distribute codebooks is seriously troublesome.Constructing a new code is like building a new language and writing a dictionary for it; it was an especially big job before computers. If a code is compromised, the entire task must be done all over again, and that means a lot of work for both cryptographers and the code users. In practice, when codes were in widespread use, they were usually changed on a periodic basis to frustrate codebreakers, and to limit the useful life of stolen or copied codebooks.\nOnce codes have been created, codebook distribution is logistically clumsy, and increases chances the code will be compromised. There is a saying that \"Three people can keep a secret if two of them are dead,\" (Benjamin Franklin - Wikiquote) and though it may be something of an exaggeration, a secret becomes harder to keep if it is shared among several people. Codes can be thought reasonably secure if they are only used by a few careful people, but if whole armies use the same codebook, security becomes much more difficult.\nIn contrast, the security of ciphers is generally dependent on protecting the cipher keys. Cipher keys can be stolen and people can betray them, but they are much easier to change and distribute.","completion":"Cryptography"}
{"prompt":"To Schneier, peer review and expert analysis are important for the security of cryptographic systems. Mathematical cryptography is usually not the weakest link in a security chain; effective security requires that cryptography be combined with other things.The term Schneier's law was coined by Cory Doctorow in a 2004 speech. The law is phrased as:\n\nAny person can invent a security system so clever that she or he can't think of how to break it.\nHe attributes this to Bruce Schneier, who wrote in 1998: \"Anyone, from the most clueless amateur to the best cryptographer, can create an algorithm that he himself can't break. It's not even hard. What is hard is creating an algorithm that no one else can break, even after years of analysis.\"Similar sentiments had been expressed by others before. In The Codebreakers, David Kahn states: \"Few false ideas have more firmly gripped the minds of so many intelligent men than the one that, if they just tried, they could invent a cipher that no one could break\", and in \"A Few Words On Secret Writing\", in July 1841, Edgar Allan Poe had stated: \"Few persons can be made to believe that it is not quite an easy thing to invent a method of secret writing which shall baffle investigation. Yet it may be roundly asserted that human ingenuity cannot concoct a cipher which human ingenuity cannot resolve.\"Schneier also coined the term \"kid sister cryptography\", writing in the Preface to Applied Cryptography that:\n\nThere are two kinds of cryptography in this world: cryptography that will stop your kid sister from reading your files, and cryptography that will stop major governments from reading your files.","completion":"Cryptography"}
{"prompt":"Bruce Schneier is the son of Martin Schneier, a Brooklyn Supreme Court judge. He grew up in the Flatbush neighborhood of Brooklyn, New York, attending P.S. 139 and Hunter College High School.After receiving a physics bachelor's degree from the University of Rochester in 1984, he went to American University in Washington, D.C., and got his master's degree in computer science in 1988. He was awarded an honorary Ph.D from the University of Westminster in London, England, in November 2011. The award was made by the Department of Electronics and Computer Science in recognition of Schneier's 'hard work and contribution to industry and public life'.\nSchneier was a founder and chief technology officer of Counterpane Internet Security (now BT Managed Security Solutions).  He worked for IBM once they acquired Resilient Systems where Schneier was CTO until he left at the end of June 2019.","completion":"Cryptography"}
{"prompt":"Cryptographic protocols can sometimes be verified formally on an abstract level. When it is done, there is a necessity to formalize the environment in which the protocol operates in order to identify threats. This is frequently done through the Dolev-Yao model.\nLogics, concepts and calculi used for formal reasoning of security protocols:\n\nBurrows\u2013Abadi\u2013Needham logic (BAN logic)\nDolev\u2013Yao model\n\u03c0-calculus\nProtocol composition logic (PCL)\nStrand spaceResearch projects and tools used for formal verification of security protocols:\n\nAutomated Validation of Internet Security Protocols and Applications (AVISPA) and follow-up project AVANTSSARConstraint Logic-based Attack Searcher (CL-AtSe)\nOpen-Source Fixed-Point Model-Checker (OFMC)\nSAT-based Model-Checker (SATMC)\nCasper\nCryptoVerif\nCryptographic Protocol Shapes Analyzer (CPSA)\nKnowledge In Security protocolS (KISS)\nMaude-NRL Protocol Analyzer (Maude-NPA)\nProVerif\nScyther\nTamarin Prover","completion":"Cryptography"}
{"prompt":"Confusion means that each binary digit (bit) of the ciphertext should depend on several parts of the key, obscuring the connections between the two.The property of confusion hides the relationship between the ciphertext and the key.\nThis property makes it difficult to find the key from the ciphertext and if a single bit in a key is changed, the calculation of  most or all of the bits in the ciphertext will be affected.\nConfusion increases the ambiguity of ciphertext and it is used by both block and stream ciphers.\nIn substitution\u2013permutation networks, confusion is provided by substitution boxes.","completion":"Cryptography"}
{"prompt":"Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's requirements, VASPs need to verify the identity of the beneficiary of the transfer.","completion":"Cryptography"}
{"prompt":"The strict avalanche criterion (SAC) is a formalization of the avalanche effect. It is satisfied if, whenever a single input bit is complemented, each of the output bits changes with a 50% probability. The SAC builds on the concepts of completeness and avalanche and was introduced by Webster and Tavares in 1985.Higher-order generalizations of SAC involve multiple input bits.\nBoolean functions which satisfy the highest order SAC are always bent functions,\nalso called maximally nonlinear functions,\nalso called \"perfect nonlinear\" functions.","completion":"Cryptography"}
{"prompt":"Cocks was educated at Manchester Grammar School and went on to study the Mathematical Tripos as an undergraduate at King's College, Cambridge. He continued as a PhD student at the University of Oxford, where he specialised in number theory under Bryan Birch, but left academia without finishing his doctorate.","completion":"Cryptography"}
{"prompt":"The Exponential Time Hypothesis (ETH) is a strengthening of \n  \n    \n      \n        P\n        \u2260\n        N\n        P\n      \n    \n    {\\displaystyle P\\neq NP}\n   hardness assumption, which conjectures that not only does the Boolean satisfiability problem not have a polynomial time algorithm, it furthermore requires exponential time (\n  \n    \n      \n        \n          2\n          \n            \u03a9\n            (\n            n\n            )\n          \n        \n      \n    \n    {\\displaystyle 2^{\\Omega (n)}}\n  ). \nAn even stronger assumption, known as the Strong Exponential Time Hypothesis (SETH) conjectures that \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -SAT requires \n  \n    \n      \n        \n          2\n          \n            (\n            1\n            \u2212\n            \n              \u03b5\n              \n                k\n              \n            \n            )\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{(1-\\varepsilon _{k})n}}\n   time, where \n  \n    \n      \n        \n          lim\n          \n            k\n            \u2192\n            \u221e\n          \n        \n        \n          \u03b5\n          \n            k\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle \\lim _{k\\rightarrow \\infty }\\varepsilon _{k}=0}\n  . \nETH, SETH, and related computational hardness assumptions allow for deducing fine-grained complexity results, e.g. results that distinguish polynomial time and quasi-polynomial time, or even \n  \n    \n      \n        \n          n\n          \n            1.99\n          \n        \n      \n    \n    {\\displaystyle n^{1.99}}\n   versus \n  \n    \n      \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n^{2}}\n  .\nSuch assumptions are also useful in parametrized complexity.","completion":"Cryptography"}
{"prompt":"Suppose Alice wants to send a signed message to Bob.. Initially, they must agree on the curve parameters                         (                                 CURVE                             ,         G         ,         n         )                 {\\displaystyle ({\\textrm {CURVE}},G,n)}   .. In addition to the field and equation of the curve, we need                         G                 {\\displaystyle G}   , a base point of prime order on the curve;                         n                 {\\displaystyle n}    is the multiplicative order of the point                         G                 {\\displaystyle G}   .. The order                         n                 {\\displaystyle n}    of the base point                         G                 {\\displaystyle G}    must be prime..","completion":"Cryptography"}
{"prompt":"Every operation performed by a computer emits electromagnetic radiation and different operations emit radiation at different frequencies. In electromagnetic side-channel attacks, an attacker is only interested in a few frequencies at which encryption is occurring. Signal processing is responsible for isolating these frequencies from the vast multitude of extraneous radiation and noise. To isolate certain frequencies, a bandpass filter, which blocks frequencies outside of a given range, must be applied to the electromagnetic trace. Sometimes, the attacker does not know which frequencies encryption is performed at. In this case, the trace can be represented as a spectrogram, which can help determine which frequencies are most prevalent at different points of execution. Depending on the device being attacked and the level of noise, several filters may need to be applied.","completion":"Cryptography"}
{"prompt":"In 2001, Barak et al., showing that black-box obfuscation is impossible, also proposed the idea of an indistinguishability obfuscator, and constructed an inefficient one.Although this notion seemed relatively weak, Goldwasser and Rothblum (2007) showed that an efficient indistinguishability obfuscator would be a best-possible obfuscator, and any best-possible obfuscator would be an indistinguishability obfuscator. (However, for inefficient obfuscators, no best-possible obfuscator exists unless the polynomial hierarchy collapses to the second level.)\nAn open-source software implementation of an IO candidate was created in 2015.","completion":"Cryptography"}
{"prompt":"For Bob to authenticate Alice's signature, he must have a copy of her public-key curve point                                    Q                        A                                     {\\displaystyle Q_{A}}   .. Bob can verify                                    Q                        A                                     {\\displaystyle Q_{A}}    is a valid curve point as follows:  Check that                                    Q                        A                                     {\\displaystyle Q_{A}}    is not equal to the identity element O, and its coordinates are otherwise valid.. Check that                                    Q                        A                                     {\\displaystyle Q_{A}}    lies on the curve..","completion":"Cryptography"}
{"prompt":"A nonce is an arbitrary number used only once in a cryptographic communication, in the spirit of a nonce word. They are often random or pseudo-random numbers. Many nonces also include a timestamp to ensure exact timeliness, though this requires clock synchronisation between organisations. The addition of a client nonce (\"cnonce\") helps to improve the security in some ways as implemented in digest access authentication. To ensure that a nonce is used only once, it should be time-variant (including a suitably fine-grained timestamp in its value), or generated with enough random bits to ensure a  insignificantly low chance of repeating a previously generated value. Some authors define pseudo-randomness (or unpredictability) as a requirement for a nonce.Nonce is a word dating back to Middle English for something only used once or temporarily (often with the construction \"for the nonce\"). It descends from the construction \"then anes\" (\"the one [purpose]\"). A false etymology claiming it to mean \"number used once\" is incorrect. In Britain the term may be avoided as \"nonce\" in modern British English means a paedophile.","completion":"Cryptography"}
{"prompt":"On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.","completion":"Cryptography"}
{"prompt":"Block ciphers may be evaluated according to multiple criteria in practice. Common factors include:\nKey parameters, such as its key size and block size, both of which provide an upper bound on the security of the cipher.\nThe estimated security level, which is based on the confidence gained in the block cipher design after it has largely withstood major efforts in cryptanalysis over time, the design's mathematical soundness, and the existence of practical or certificational attacks.\nThe cipher's complexity and its suitability for implementation in hardware or software. Hardware implementations may measure the complexity in terms of gate count or energy consumption, which are important parameters for resource-constrained devices.\nThe cipher's performance in terms of processing throughput on various platforms, including its memory requirements.\nThe cost of the cipher refers to licensing requirements that may apply due to intellectual property rights.\nThe flexibility of the cipher includes its ability to support multiple key sizes and block lengths.","completion":"Cryptography"}
{"prompt":"The most common characters are Alice and Bob. Eve, Mallory, and Trent are also common names, and have fairly well-established \"personalities\" (or functions). The names often use alliterative mnemonics (for example, Eve, \"eavesdropper\"; Mallory, \"malicious\") where different players have different motives. Other names are much less common and more flexible in use. Sometimes the genders are alternated: Alice, Bob, Carol, Dave, Eve, etc.\nFor interactive proof systems there are other characters:","completion":"Cryptography"}
{"prompt":"As of 2011 and since 2004, the law for trust in the digital economy (French: Loi pour la confiance dans l'\u00e9conomie num\u00e9rique; abbreviated LCEN) mostly liberalized the use of cryptography.\nAs long as cryptography is only used for authentication and integrity purposes, it can be freely used. The cryptographic key or the nationality of the entities involved in the transaction do not matter. Typical e-business websites fall under this liberalized regime.\nExportation and importation of cryptographic tools to or from foreign countries must be either declared (when the other country is a member of the European Union) or requires an explicit authorization (for countries outside the EU).","completion":"Cryptography"}
{"prompt":"Al-Kindi took his view of the solar system from Ptolemy, who placed the Earth at the centre of a series of concentric spheres, in which the known heavenly bodies (the Moon, Mercury, Venus, the Sun, Mars, Jupiter, and the stars) are embedded. In one of his treatises on the subject, he says that these bodies are rational entities, whose circular motion is in obedience to and worship of God. Their role, al-Kindi believes, is to act as instruments for divine providence. He furnishes empirical evidence as proof for this assertion; different seasons are marked by particular arrangements of the planets and stars (most notably the sun); the appearance and manner of people varies according to the arrangement of heavenly bodies situated above their homeland.However, he is ambiguous when it comes to the actual process by which the heavenly bodies affect the material world. One theory he posits in his works is from Aristotle, who conceived that the movement of these bodies causes friction in the sub-lunar region, which stirs up the primary elements of earth, fire, air and water, and these combine to produce everything in the material world. An alternative view found in the treatise On Rays (De radiis) is that the planets exercise their influence in straight lines; but this treatise, written by a Latin author, probably around the middle of the 13th century, is apocryphal. In each of these, two fundamentally different views of physical interaction are presented; action by contact and action at a distance. This dichotomy is duplicated in his writings on optics.Some of the notable astrological works by al-Kindi include:\nThe Book of the Judgement of the Stars, including The Forty Chapters, on questions and elections.\nOn the Stellar Rays (spurious)\nSeveral epistles on weather and meteorology, including De mutatione temporum, (\"On the Changing of the Weather\").\nTreatise on the Judgement of Eclipses.\nTreatise on the Dominion of the Arabs and its Duration (used to predict the end of Arab rule).\nThe Choices of Days (on elections).\nOn the Revolutions of the Years (on mundane astrology and natal revolutions).\nDe Signis Astronomiae Applicitis as Mediciam 'On the Signs of Astronomy as applied to Medicine'\nTreatise on the Spirituality of the Planets.","completion":"Cryptography"}
{"prompt":"Public key encryption schemes based on the Diffie\u2013Hellman key exchange have been proposed. The first such scheme is the ElGamal encryption. A more modern variant is the Integrated Encryption Scheme.","completion":"Cryptography"}
{"prompt":"In a transposition cipher, the letters themselves are kept unchanged, but their order within the message is scrambled according to some well-defined scheme. Many transposition ciphers are done according to a geometric design. A simple (and once again easy to crack) encryption would be to write every word backwards. For example, \"Hello my name is Alice.\" would now be \"olleH ym eman si ecilA.\" A scytale is a machine that aids in the transposition of methods.\nIn a columnar cipher, the original message is arranged in a rectangle, from left to right and top to bottom. Next, a key is chosen and used to assign a number to each column in the rectangle to determine the order of rearrangement. The number corresponding to the letters in the key is determined by their place in the alphabet, i.e. A is 1, B is 2, C is 3, etc. For example, if the key word is CAT and the message is THE SKY IS BLUE, the message would be arranged thus:\n\n                         C A T\n                         3 1 20\n                         T H E\n                         S K Y\n                         I S B\n                         L U E\n\nNext, the letters are taken in numerical order and that is how the message is transposed. The column under A is taken first, then the column under C, then the column under T, as a result the message \"The sky is blue\" has become: HKSUTSILEYBE\nIn the Chinese cipher's method of transposing, the letters of the message are written from right to left, down and up columns to scramble the letters. Then, starting in the first row, the letters are taken in order to get the new ciphertext. For example, if the message needed to be enciphered was THE DOG RAN FAR, the Chinese cipher would look like this:\n\n                           R R G T\n                           A A O H\n                           F N D E\n\nThe cipher text then reads: RRGT AAOH FNDE\nMany transposition ciphers are similar to these two examples, usually involving rearranging the letters into rows or columns and then taking them in a systematic way to transpose the letters. Other examples include the Vertical Parallel and the Double Transposition Cipher.\nMore complex algorithms can be formed by mixing substitution and transposition in a product cipher; modern block ciphers such as DES iterate through several stages of substitution and transposition.","completion":"Cryptography"}
{"prompt":"The Birch and Swinnerton-Dyer conjecture (BSD) is one of the Millennium problems of the Clay Mathematics Institute.. The conjecture relies on analytic and arithmetic objects defined by the elliptic curve in question.. At the analytic side, an important ingredient is a function of a complex variable, L, the Hasse\u2013Weil zeta function of E over Q.. This function is a variant of the Riemann zeta function and Dirichlet L-functions..","completion":"Cryptography"}
{"prompt":"Schneier warns about misplaced trust in blockchain and the lack of use cases, calling blockchain a solution in search of a problem.\n\nHe goes on to say that cryptocurrencies are useless and are only used by speculators looking for quick riches.","completion":"Cryptography"}
{"prompt":"As with many cryptographic schemes, na\u00efve use of ciphers and other protocols may lead to attacks being possible, reducing the effectiveness of the cryptographic protection (or even rendering it useless). We present attacks which are possible due to using the CBC-MAC incorrectly.","completion":"Cryptography"}
{"prompt":"Boak, David G. A History of U.S. Communications Security (Volumes I and II); the David G. Boak Lectures, National Security Agency (NSA), 1973, A frank, detailed, and often humorous series of lectures delivered to new NSA hires by a long time insider, largely declassified as of 2015.\nCallimahos, Lambros D. and Friedman, William F. Military Cryptanalytics. A (partly) declassified text intended as a training manual for NSA cryptanalysts.\nFriedman, William F., Six Lectures on Cryptology, National Cryptology School, U.S. National Security Agency, 1965, declassified 1977, 1984\nFriedman, William F. (October 14, 1940). \"Preliminary Historical Report on the Solution of the Type \"B\" Machine\" (PDF). Archived from the original (PDF) on April 4, 2013. (How the Japanese Purple cipher was broken, declassified 2001)","completion":"Cryptography"}
{"prompt":"Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate which is defined when the system is created and which is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency. In the case of cryptocurrency, companies or governments cannot produce new units, and have not so far provided backing for other firms, banks or corporate entities which hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.Within a proof-of-work system such as Bitcoin, the safety, integrity and balance of ledgers is maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme. In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.\nMost cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation. Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.","completion":"Cryptography"}
{"prompt":"Cryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech.","completion":"Cryptography"}
{"prompt":"The security of a key is dependent on how a key is exchanged between parties. Establishing a secured communication channel is necessary so that outsiders cannot obtain the key. A key establishment scheme (or key exchange) is used to transfer an encryption key among entities. Key agreement and key transport are the two types of a key exchange scheme that are used to be  remotely exchanged between entities . In a key agreement scheme, a secret key, which is used between the sender and the receiver to encrypt and decrypt information, is set up to be sent indirectly. All parties exchange information (the shared secret) that permits each party to derive the secret key material. In a key transport scheme, encrypted keying material that is chosen by the sender is transported to the receiver. Either symmetric key or asymmetric key techniques can be used in both schemes.The Diffie\u2013Hellman key exchange and Rivest-Shamir-Adleman (RSA) are the most two widely used key exchange algorithms. In 1976, Whitfield Diffie and Martin Hellman constructed the Diffie\u2013Hellman algorithm, which was the first public key algorithm. The Diffie\u2013Hellman key exchange protocol allows key exchange over an insecure channel by electronically generating a shared key between two parties. On the other hand, RSA is a form of the asymmetric key system which consists of three steps: key generation, encryption, and decryption.Key confirmation delivers an assurance between the key confirmation recipient and provider that the shared keying materials are correct and established. The National Institute of Standards and Technology recommends key confirmation to be integrated into a key establishment scheme to validate its implementations.","completion":"Cryptography"}
{"prompt":"Hash flooding (also known as HashDoS) is a denial of service attack that uses hash collisions to exploit the worst-case (linear probe) runtime of hash table lookups. It was originally described in 2003. To execute such an attack, the attacker sends the server multiple pieces of data that hash to the same value and then tries to get the server to perform slow lookups. As the main focus of hash functions used in hash tables was speed instead of security, most major programming languages were affected, with new vulnerabilities of this class still showing up a decade after the original presentation.To prevent hash flooding without making the hash function overly complex, newer keyed hash functions are introduced, with the security objective that collisions are hard to find as long as the key is unknown. They may be slower than previous hashes, but are still much easier to compute than cryptographic hashes. As of 2021, Daniel J. Bernstein's SipHash (2012) is the most widely-used hash function in this class. (Non-keyed \"simple\" hashes remain safe to use as long as the application's hash table is not controllable from the outside.)\nIt is possible to perform an analogous attack to fill up Bloom filters using a (partial) preimage attack.","completion":"Cryptography"}
{"prompt":"Note that there are two standardization efforts for EdDSA, one from IETF, an informational RFC 8032 and one from NIST as part of FIPS 186-5. The differences between the standards have been analyzed, and test vectors are available.","completion":"Cryptography"}
{"prompt":"Much of the theoretical work in cryptography concerns cryptographic primitives\u2014algorithms with basic cryptographic properties\u2014and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.","completion":"Cryptography"}
{"prompt":"Al-Farahidi's eschewing of material wealth has been noted by a number of biographers. In his old age, the son of Habib ibn al-Muhallab and reigning governor of the Muhallabids offered al-Farahidi a pension and requested that the latter tutor the former's son. Al-Farahidi declined, stating that he was wealthy though possessing no money, as true poverty lay not in a lack of money, but in the soul. The governor reacted by rescinding the pension, an act to which al-Farahidi responded with the following lines of poetry:\n\n\"He, Who formed me with a mouth, engaged to give me nourishment till such a time as He takes me to Himself. Thou hast refused me a trifling sum, but that refusal will not increase thy wealth.\"Embarrassed, the governor then responded with an offer to renew the pension and double the rate, which al-Farahidi still greeted with a lukewarm reception. Al-Farahidi's apathy about material wealth was demonstrated in his habit of quoting Akhtal's famous stanza: \"If thou wantest treasures, thou wilt find none equal to a virtuous conduct.\"Al-Farahidi distinguished himself via his philosophical views as well. He reasoned that a man's intelligence peaked at the age of forty \u2013 the age when the Islamic prophet Muhammad began his call \u2013 and began to diminish after sixty, the point at which Muhammad died. He also believed that a person was at their peak intelligence at the clearest part of dawn.In regard to the field of grammar, al-Farahidi held the realist views common among early Arab linguists yet rare among both later and modern times. Rather than holding the rules of grammar as he and his students described them to be absolute rules, al-Farahidi saw the Arabic language as the natural, instinctual speaking habits of the Bedouin; if the descriptions of scholars such as himself differed from how the Arabs of the desert naturally spoke, then the cause was a lack of knowledge on the scholar's part as the unspoken, unwritten natural speech of pure Arabs was the final determiner. Al-Farahidi was distinguished, however, in his view that the Arabic alphabet included 29 letters rather than 28 and that each letter represented a fundamental characteristic of people or animals. His classification of 29 letters was due to his consideration of the combination of L\u0101m and Alif as a separate third letter from the two individual parts.","completion":"Cryptography"}
{"prompt":"A5\/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard.\nSecurity researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn't feel this way, and the algorithm as now fielded is a French design.\"According to professor Jan Arild Audestad, at the standardization process which started in 1982, A5\/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now estimated that 128 bits would in fact also still be secure as of 2014. Audestad, Peter van der Arend, and Thomas Haug say that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 56 bits. In general, a key of length 56 is \n  \n    \n      \n        \n          2\n          \n            128\n            \u2212\n            56\n          \n        \n        =\n        \n          2\n          \n            72\n          \n        \n        =\n        4.7\n        \u00d7\n        \n          10\n          \n            21\n          \n        \n      \n    \n    {\\displaystyle 2^{128-56}=2^{72}=4.7\\times 10^{21}}\n   times easier to break than a key of length 128.","completion":"Cryptography"}
{"prompt":"The use of elliptic curves in cryptography was suggested independently by Neal Koblitz and Victor S. Miller in 1985. Elliptic curve cryptography algorithms entered wide use in 2004 to 2005.\nIn 1999, NIST recommended fifteen elliptic curves. Specifically, FIPS 186-4 has ten recommended finite fields:\n\nFive prime fields \n  \n    \n      \n        \n          \n            F\n          \n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {F} _{p}}\n   for certain primes p of sizes 192, 224, 256, 384, and 521 bits. For each of the prime fields, one elliptic curve is recommended.\nFive binary fields \n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              2\n              \n                m\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {F} _{2^{m}}}\n   for m equal 163, 233, 283, 409, and 571. For each of the binary fields, one elliptic curve and one Koblitz curve was selected.The NIST recommendation thus contains a total of five prime curves and ten binary curves. The curves were chosen for optimal security and implementation efficiency.At the RSA Conference 2005, the National Security Agency (NSA) announced Suite B, which exclusively uses ECC for digital signature generation and key exchange. The suite is intended to protect both classified and unclassified national security systems and information. National Institute of Standards and Technology (NIST) has endorsed elliptic curve cryptography in its Suite B set of recommended algorithms, specifically elliptic-curve Diffie\u2013Hellman (ECDH) for key exchange and Elliptic Curve Digital Signature Algorithm (ECDSA) for digital signature.  The NSA allows their use for protecting information classified up to top secret with 384-bit keys.Recently, a large number of cryptographic primitives based on bilinear mappings on various elliptic curve groups, such as the Weil and Tate pairings, have been introduced. Schemes based on these primitives provide efficient identity-based encryption as well as pairing-based signatures, signcryption, key agreement, and proxy re-encryption.Elliptic curve cryptography is used successfully in numerous popular protocols, such as Transport Layer Security and Bitcoin.","completion":"Cryptography"}
{"prompt":"IFES (Integer Factorization Encryption Scheme): Essentially RSA encryption with Optimal Asymmetric Encryption Padding (OAEP).\nDL\/ECIES (Discrete Logarithm\/Elliptic Curve Integrated Encryption Scheme): Essentially the \"DHAES\" variant of ElGamal encryption.\nIFES-EPOC (Integer Factorization Encryption Scheme, EPOC version)","completion":"Cryptography"}
{"prompt":"As with elliptic-curve cryptography in general, the bit size of the private key believed to be needed for ECDSA is about twice the size of the security level, in bits. For example, at a security level of 80 bits\u2014meaning an attacker requires a maximum of about \n  \n    \n      \n        \n          2\n          \n            80\n          \n        \n      \n    \n    {\\displaystyle 2^{80}}\n   operations to find the private key\u2014the size of an ECDSA private key would be 160 bits.  On the other hand, the signature size is the same for both DSA and ECDSA: approximately \n  \n    \n      \n        4\n        t\n      \n    \n    {\\displaystyle 4t}\n   bits, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n   is the exponent in the formula \n  \n    \n      \n        \n          2\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle 2^{t}}\n  , that is, about 320 bits for a security level of 80 bits, which is equivalent to \n  \n    \n      \n        \n          2\n          \n            80\n          \n        \n      \n    \n    {\\displaystyle 2^{80}}\n   operations.","completion":"Cryptography"}
{"prompt":"There exist groups for which computing discrete logarithms is apparently difficult.  In some cases (e.g. large prime order subgroups of  groups (Zp)\u00d7) there is not only no efficient algorithm known for the worst case, but the average-case complexity can be shown to be about as hard as the worst case using random self-reducibility.At the same time, the inverse problem of discrete exponentiation is not difficult (it can be computed efficiently using exponentiation by squaring, for example).  This asymmetry is analogous to the one between integer factorization and integer multiplication. Both asymmetries (and other possibly one-way functions) have been exploited in the construction of cryptographic systems.\nPopular choices for the group G in discrete logarithm cryptography (DLC) are the cyclic groups (Zp)\u00d7 (e.g. ElGamal encryption, Diffie\u2013Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (see Elliptic curve cryptography).\nWhile there is no publicly known algorithm for solving the discrete logarithm problem in general, the first three steps of the number field sieve algorithm only depend on the group G, not on the specific elements of G whose finite log is desired. By precomputing these three steps for a specific group, one need only carry out the last step, which is much less computationally expensive than the first three, to obtain a specific logarithm in that group.It turns out that much Internet traffic uses one of a handful of groups that are of order 1024 bits or less, e.g. cyclic groups with order of the Oakley primes specified in RFC 2409. The Logjam attack used this vulnerability to compromise a variety of Internet services that allowed the use of groups whose order was a 512-bit prime number, so called export grade.The authors of the Logjam attack estimate that the much more difficult precomputation needed to solve the discrete log problem for a 1024-bit prime would be within the budget of a large national intelligence agency such as the U.S. National Security Agency (NSA). The Logjam authors speculate that precomputation against widely reused 1024 DH primes is behind claims in leaked NSA documents that NSA is able to break much of current cryptography.","completion":"Cryptography"}
{"prompt":"Ed448 is the EdDSA signature scheme using SHAKE256 and Curve448 defined in RFC 8032. It has also been approved in the final version the FIPS 186-5 standard.","completion":"Cryptography"}
{"prompt":"As of 2009, non-military cryptography exports from the U.S. are controlled by the Department of Commerce's Bureau of Industry and Security. Some restrictions still exist, even for mass market products, particularly with regard to export to \"rogue states\" and terrorist organizations. Militarized encryption equipment, TEMPEST-approved electronics, custom cryptographic software, and even cryptographic consulting services still require an export license (pp. 6\u20137). Furthermore, encryption registration with the BIS is required for the export of \"mass market encryption commodities, software and components with encryption exceeding 64 bits\" (75 FR 36494). In addition, other items require a one-time review by or notification to BIS prior to export to most countries. For instance, the BIS must be notified before open-source cryptographic software is made publicly available on the Internet, though no review is required.  Export regulations have been relaxed from pre-1996 standards, but are still complex. Other countries, notably those participating in the Wassenaar Arrangement, have similar restrictions.","completion":"Cryptography"}
{"prompt":"In 2007, IFSB was published. In 2010, S-FSB was published, which is 30% faster than the original.\nIn 2011, D. J. Bernstein and Tanja Lange published RFSB, which is 10x faster than the original FSB-256.\n RFSB was shown to run very fast on the Spartan 6 FPGA, reaching throughputs of around 5 Gbit\/s.>","completion":"Cryptography"}
{"prompt":"While the fact that circuits that emit high-frequency signals may leak secret information was known since 1982 by the NSA, it was classified until 2000, which was right around the time that the first electromagnetic attack against encryption was shown by researchers. Since then, many more complex attacks have been introduced.","completion":"Cryptography"}
{"prompt":"The Merkle\u2013Damg\u00e5rd construction is proven to base its security only on the security of the used compression function. So we only need to show that the compression function \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   is secure.\nA cryptographic hash function needs to be secure in three different aspects:\n\nPre-image resistance: Given a Hash h it should be hard to find a message m such that Hash(m)=h\nSecond pre-image resistance:\tGiven a message m1 it should be hard to find a message m2 such that Hash(m1) = Hash(m2)\nCollision resistance: It should be hard to find two different messages m1 and m2 such that Hash(m1)=Hash(m2)Note that if an adversary can find a second pre-image, then it can certainly find a collision. This means that if we can prove our system to be collision resistant, it will certainly be second-pre-image resistant.\nUsually in cryptography hard means something like \u201calmost certainly beyond the reach of any adversary who must be prevented from breaking the system\u201d. We will however need a more exact meaning of the word hard. We will take hard to mean \u201cThe runtime of any algorithm that finds a collision or pre-image will depend exponentially on size of the hash value\u201d. This means that by relatively small additions to the hash size, we can quickly reach high security.","completion":"Cryptography"}
{"prompt":"Hessian curve\nEdwards curve\nTwisted curve\nTwisted Hessian curve\nTwisted Edwards curve\nDoubling-oriented Doche\u2013Icart\u2013Kohel curve\nTripling-oriented Doche\u2013Icart\u2013Kohel curve\nJacobian curve\nMontgomery curve","completion":"Cryptography"}
{"prompt":"Leslie Lamport invented hash-based signatures in 1979. The XMSS (eXtended Merkle Signature Scheme) and SPHINCS hash-based signature schemes were introduced in 2011 and 2015, respectively. XMSS was developed by a team of researchers under the direction of Johannes Buchmann and is based both on Merkle's seminal scheme and on the 2007 Generalized Merkle Signature Scheme (GMSS). A multi-tree variant of XMSS, XMSSMT, was described in 2013.","completion":"Cryptography"}
{"prompt":"Closely related to the Unique Label Cover problem is the Small Set Expansion (SSE) problem: Given a graph \n  \n    \n      \n        G\n        =\n        (\n        V\n        ,\n        E\n        )\n      \n    \n    {\\displaystyle G=(V,E)}\n  , find a small set of vertices (of size \n  \n    \n      \n        n\n        \n          \/\n        \n        log\n        \u2061\n        (\n        n\n        )\n      \n    \n    {\\displaystyle n\/\\log(n)}\n  ) whose edge expansion is minimal. \nIt is known that if SSE is hard to approximate, then so is Unique Label Cover. Hence, the Small Set Expansion Hypothesis, which postulates that SSE is hard to approximate, is a stronger (but closely related) assumption than the Unique Game Conjecture.\nSome approximation problems are known to be SSE-hard (i.e. at least as hard as approximating SSE).","completion":"Cryptography"}
{"prompt":"The output feedback (OFB) mode makes a block cipher into a synchronous stream cipher.  It generates keystream blocks, which are then XORed with the plaintext blocks to get the ciphertext.  Just as with other stream ciphers, flipping a bit in the ciphertext produces a flipped bit in the plaintext at the same location.  This property allows many error-correcting codes to function normally even when applied before encryption.\nBecause of the symmetry of the XOR operation, encryption and decryption are exactly the same:\n\n  \n    \n      \n        \n          C\n          \n            j\n          \n        \n        =\n        \n          P\n          \n            j\n          \n        \n        \u2295\n        \n          O\n          \n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle C_{j}=P_{j}\\oplus O_{j},}\n  \n\n  \n    \n      \n        \n          P\n          \n            j\n          \n        \n        =\n        \n          C\n          \n            j\n          \n        \n        \u2295\n        \n          O\n          \n            j\n          \n        \n        ,\n      \n    \n    {\\displaystyle P_{j}=C_{j}\\oplus O_{j},}\n  \n\n  \n    \n      \n        \n          O\n          \n            j\n          \n        \n        =\n        \n          E\n          \n            K\n          \n        \n        (\n        \n          I\n          \n            j\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle O_{j}=E_{K}(I_{j}),}\n  \n\n  \n    \n      \n        \n          I\n          \n            j\n          \n        \n        =\n        \n          O\n          \n            j\n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle I_{j}=O_{j-1},}\n  \n\n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        =\n        \n          IV\n        \n        .\n      \n    \n    {\\displaystyle I_{0}={\\text{IV}}.}\n  \nEach output feedback block cipher operation depends on all previous ones, and so cannot be performed in parallel.  However, because the plaintext or ciphertext is only used for the final XOR, the block cipher operations may be performed in advance, allowing the final step to be performed in parallel once the plaintext or ciphertext is available.\nIt is possible to obtain an OFB mode keystream by using CBC mode with a constant string of zeroes as input. This can be useful, because it allows the usage of fast hardware implementations of CBC mode for OFB mode encryption.\nUsing OFB mode with a partial block as feedback like CFB mode reduces the average cycle length by a factor of 232 or more. A mathematical model proposed by Davies and Parkin and substantiated by experimental results showed that only with full feedback an average cycle length near to the obtainable maximum can be achieved. For this reason, support for truncated feedback was removed from the specification of OFB.","completion":"Cryptography"}
{"prompt":"The propagating cipher block chaining or plaintext cipher-block chaining mode was designed to cause small changes in the ciphertext to propagate indefinitely when decrypting, as well as when encrypting. In PCBC mode, each block of plaintext is XORed with both the previous plaintext block and the previous ciphertext block before being encrypted. Like with CBC mode, an initialization vector is used in the first block.\nUnlike CBC, decrypting PCBC with the incorrect IV (initialization vector) causes all blocks of plaintext to be corrupt.\n\nEncryption and decryption algorithms are as follows:\n\n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n        =\n        \n          E\n          \n            K\n          \n        \n        (\n        \n          P\n          \n            i\n          \n        \n        \u2295\n        \n          P\n          \n            i\n            \u2212\n            1\n          \n        \n        \u2295\n        \n          C\n          \n            i\n            \u2212\n            1\n          \n        \n        )\n        ,\n        \n          P\n          \n            0\n          \n        \n        \u2295\n        \n          C\n          \n            0\n          \n        \n        =\n        I\n        V\n        ,\n      \n    \n    {\\displaystyle C_{i}=E_{K}(P_{i}\\oplus P_{i-1}\\oplus C_{i-1}),P_{0}\\oplus C_{0}=IV,}\n  \n\n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n        =\n        \n          D\n          \n            K\n          \n        \n        (\n        \n          C\n          \n            i\n          \n        \n        )\n        \u2295\n        \n          P\n          \n            i\n            \u2212\n            1\n          \n        \n        \u2295\n        \n          C\n          \n            i\n            \u2212\n            1\n          \n        \n        ,\n        \n          P\n          \n            0\n          \n        \n        \u2295\n        \n          C\n          \n            0\n          \n        \n        =\n        I\n        V\n        .\n      \n    \n    {\\displaystyle P_{i}=D_{K}(C_{i})\\oplus P_{i-1}\\oplus C_{i-1},P_{0}\\oplus C_{0}=IV.}\n  PCBC is used in Kerberos v4 and WASTE, most notably, but otherwise is not common. On a message encrypted in PCBC mode, if two adjacent ciphertext blocks are exchanged, this does not affect the decryption of subsequent blocks. For this reason, PCBC is not used in Kerberos v5.","completion":"Cryptography"}
{"prompt":"The historical Edgar Allan Poe has appeared as a fictionalized character, often in order to represent the \"mad genius\" or \"tormented artist\" and in order to exploit his personal struggles. Many such depictions also blend in with characters from his stories, suggesting that Poe and his characters share identities. Often, fictional depictions of Poe use his mystery-solving skills in such novels as The Poe Shadow by Matthew Pearl.","completion":"Cryptography"}
{"prompt":"Al-Kindi denied the possibility of transmuting base metals into precious metals such as gold and silver, a position that was later attacked by the Persian chemist and physician Abu Bakr al-Razi (c.\u2009865 \u2013 c.\u2009925).One work attributed to al-Kindi, variously known as the Kit\u0101b al-Taraffuq f\u012b l-\u02bfi\u1e6dr (\"The Book of Gentleness on Perfume\") or the Kit\u0101b K\u012bmiy\u0101\u02be al-\u02bfi\u1e6dr wa-l-ta\u1e63\u02bf\u012bd\u0101t (\"The Book of the Chemistry of Perfume and Distillations\"), contains one of the earliest known references to the distillation of wine. The work also describes the distillation process for extracting rose oils, and provides recipes for 107 different kinds of perfumes.","completion":"Cryptography"}
{"prompt":"To encrypt a message addressed to Alice in a hybrid cryptosystem, Bob does the following:\n\nObtains Alice's public key.\nGenerates a fresh symmetric key for the data encapsulation scheme.\nEncrypts the message under the data encapsulation scheme, using the symmetric key just generated.\nEncrypts the symmetric key under the key encapsulation scheme, using Alice's public key.\nSends both of these ciphertexts to Alice.To decrypt this hybrid ciphertext, Alice does the following:\n\nUses her private key to decrypt the symmetric key contained in the key encapsulation segment.\nUses this symmetric key to decrypt the message contained in the data encapsulation segment.","completion":"Cryptography"}
{"prompt":"South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework. The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of Bitcoin. Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.","completion":"Cryptography"}
{"prompt":"Since Merkle's initial scheme, numerous hash-based signature schemes with performance improvements have been introduced. Recent ones include the XMSS, the Leighton\u2013Micali (LMS), the SPHINCS and the BPQS schemes. Most hash-based signature schemes are stateful, meaning that signing requires updating the secret key, unlike conventional digital signature schemes. For stateful hash-based signature schemes, signing requires keeping state of the used one-time keys and making sure they are never reused. The XMSS, LMS and BPQS  schemes are stateful, while the SPHINCS scheme is stateless. SPHINCS signatures are larger than XMSS and LMS signatures. BPQS has been designed specifically for blockchain systems. Additionally to the WOTS+ one-time signature scheme, SPHINCS also uses a few-time (hash-based) signature scheme called HORST. HORST is an improvement of an older few-time signature scheme, HORS (Hash to Obtain Random Subset).The stateful hash-based schemes XMSS and XMSSMT are specified in RFC 8391 (XMSS: eXtended Merkle Signature Scheme)\n.\nLeighton\u2013Micali Hash-Based Signatures are specified in RFC 8554. Practical improvements have been proposed in the literature that alleviate the concerns introduced by stateful schemes. Hash functions appropriate for these schemes include SHA-2, SHA-3 and BLAKE.","completion":"Cryptography"}
{"prompt":"Boomerang attack\nBrute-force attack\nDavies' attack\nDifferential cryptanalysis\nImpossible differential cryptanalysis\nImprobable differential cryptanalysis\nIntegral cryptanalysis\nLinear cryptanalysis\nMeet-in-the-middle attack\nMod-n cryptanalysis\nRelated-key attack\nSandwich attack\nSlide attack\nXSL attack","completion":"Cryptography"}
{"prompt":"According to Arab bibliographer Ibn al-Nadim, al-Kindi wrote at least two hundred and sixty books, contributing heavily to geometry (thirty-two books), medicine and philosophy (twenty-two books each), logic (nine books), and physics (twelve books). Although most of his books have been lost over the centuries, a few have survived in the form of Latin translations by Gerard of Cremona, and others have been rediscovered in Arabic manuscripts; most importantly, twenty-four of his lost works were located in the mid-twentieth century in a Turkish library.","completion":"Cryptography"}
{"prompt":"Certain types of encryption, by their mathematical properties, cannot be defeated by brute force. An example of this is one-time pad cryptography, where every cleartext bit has a corresponding key from a truly random sequence of key bits. A 140 character one-time-pad-encoded string subjected to a brute-force attack would eventually reveal every 140 character string possible, including the correct answer \u2013 but of all the answers given, there would be no way of knowing which was the correct one. Defeating such a system, as was done by the Venona project, generally relies not on pure cryptography, but upon mistakes in its implementation, such as the key pads not being truly random, intercepted keypads, or operators making mistakes.","completion":"Cryptography"}
{"prompt":"Given elements \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n   from a group \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  , the discrete log problem asks for an integer \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   such that \n  \n    \n      \n        a\n        =\n        \n          b\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle a=b^{k}}\n  .\nThe discrete log problem is not known to be comparable to integer factorization, but their computational complexities are closely related.\nMost cryptographic protocols related to the discrete log problem actually rely on the stronger Diffie\u2013Hellman assumption: given group elements \n  \n    \n      \n        g\n        ,\n        \n          g\n          \n            a\n          \n        \n        ,\n        \n          g\n          \n            b\n          \n        \n      \n    \n    {\\displaystyle g,g^{a},g^{b}}\n  , where \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   is a generator and \n  \n    \n      \n        a\n        ,\n        b\n      \n    \n    {\\displaystyle a,b}\n   are random integers, it is hard to find \n  \n    \n      \n        \n          g\n          \n            a\n            \u22c5\n            b\n          \n        \n      \n    \n    {\\displaystyle g^{a\\cdot b}}\n  . Examples of protocols that use this assumption include the original Diffie\u2013Hellman key exchange, as well as the ElGamal encryption (which relies on the yet stronger Decisional Diffie\u2013Hellman (DDH) variant).","completion":"Cryptography"}
{"prompt":"Any cryptographic hash function, such as SHA-2 or SHA-3, may be used in the calculation of an HMAC; the resulting MAC algorithm is termed HMAC-X, where X is the hash function used (e.g. HMAC-SHA256 or HMAC-SHA3-512). The cryptographic strength of the HMAC depends upon the cryptographic strength of the underlying hash function, the size of its hash output, and the size and quality of the key.HMAC uses two passes of hash computation. Before either pass, the secret key is used to derive two keys \u2013 inner and outer. Next, the first pass of the hash algorithm produces an internal hash derived from the message and the inner key. The second pass produces the final HMAC code derived from the inner hash result and the outer key. Thus the algorithm provides better immunity against length extension attacks.\nAn iterative hash function (one that uses the Merkle\u2013Damg\u00e5rd construction) breaks up a message into blocks of a fixed size and iterates over them with a compression function. For example, SHA-256 operates on 512-bit blocks. The size of the output of HMAC is the same as that of the underlying hash function (e.g., 256 and 512 bits in the case of SHA-256 and SHA3-512, respectively), although it can be truncated if desired.\nHMAC does not encrypt the message. Instead, the message (encrypted or not) must be sent alongside the HMAC hash. Parties with the secret key will hash the message again themselves, and if it is authentic, the received and computed hashes will match.\nThe definition and analysis of the HMAC construction was first published in 1996 in a paper by Mihir Bellare, Ran Canetti, and Hugo Krawczyk, and they also wrote RFC 2104 in 1997. The 1996 paper also defined a nested variant called NMAC (Nested MAC). FIPS PUB 198 generalizes and standardizes the use of HMACs. HMAC is used within the IPsec, SSH and TLS protocols and for JSON Web Tokens.","completion":"Cryptography"}
{"prompt":"Technically speaking, a digital signature applies to a string of bits, whereas humans and applications \"believe\" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. WYSIWYS (What You See Is What You Sign) means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by Peter Landrock and Torben Pedersen to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.","completion":"Cryptography"}
{"prompt":"In a reverse brute-force attack, a single (usually common) password is tested against multiple usernames or encrypted files. The process may be repeated for a select few passwords. In such a strategy, the attacker is not targeting a specific user.","completion":"Cryptography"}
{"prompt":"Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.\nAnother instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).","completion":"Cryptography"}
{"prompt":"In the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.\nIn the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.In many jurisdictions, the legal status of forced disclosure remains unclear.\nThe 2016 FBI\u2013Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected.\nAs a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).","completion":"Cryptography"}
{"prompt":"The original Cardano Grille was a literary device for gentlemen's private correspondence.  Any suspicion of its use can lead to discoveries of hidden messages where no hidden messages exist at all, thus confusing the cryptanalyst. Letters and numbers in a random grid can take shape without substance. Obtaining the grille itself is a chief goal of the attacker.\nBut all is not lost if a grille copy can't be obtained. The later variants of the Cardano grille present problems which are common to all transposition ciphers. Frequency analysis will show a normal distribution of letters, and will suggest the language in which the plaintext was written. The problem, easily stated though less easily accomplished, is to identify the transposition pattern and so decrypt the ciphertext.  Possession of several messages written using the same grille is a considerable aid.\nGaines, in her standard work on hand ciphers and their cryptanalysis, gave a lengthy account of transposition ciphers, and devoted a chapter to the turning grille.","completion":"Cryptography"}
{"prompt":"Al-Farahidi's first work was in the study of Arabic prosody, a field for which he is credited as the founder. Reportedly, he performed the Hajj pilgrimage to Mecca while a young man and prayed to God that he be inspired with knowledge no one else had. When he returned to Basra shortly thereafter, he overheard the rhythmic beating of a blacksmith on an anvil and he immediately wrote down fifteen metres around the periphery of five circles, which were accepted as the basis of the field and still accepted as such in Arabic language prosody today. Three of the meters were not known to Pre-Islamic Arabia, suggesting that al-Farahidi may have invented them himself. He never mandated, however, that all Arab poets must necessarily follow his rules without question, and even he was said to have knowingly broken the rules at times.","completion":"Cryptography"}
{"prompt":"Many modern symmetric block ciphers are based on Feistel networks. Feistel networks were first seen commercially in IBM's Lucifer cipher, designed by Horst Feistel and Don Coppersmith in 1973. Feistel networks gained respectability when the U.S. Federal Government adopted the DES (a cipher based on Lucifer, with changes made by the NSA) in 1976. Like other components of the DES, the iterative nature of the Feistel construction makes implementing the cryptosystem in hardware easier (particularly on the hardware available at the time of DES's design).","completion":"Cryptography"}
{"prompt":"A message may have letterhead or a handwritten signature identifying its sender, but letterheads and handwritten signatures can be copied and pasted onto forged messages.\nEven legitimate messages may be modified in transit.If a bank's central office receives a letter claiming to be from a branch office with instructions to change the balance of an account, the central bankers need to be sure, before acting on the instructions, that they were actually sent by a branch banker, and not forged\u2014whether a forger fabricated the whole letter, or just modified an existing letter in transit by adding some digits.\nWith a digital signature scheme, the central office can arrange beforehand to have a public key on file whose private key is known only to the branch office.\nThe branch office can later sign a message and the central office can use the public key to verify the signed message was not a forgery before acting on it.\nA forger who doesn't know the sender's private key can't sign a different message, or even change a single digit in an existing message without making the recipient's signature verification fail.Encryption can hide the content of the message from an eavesdropper, but encryption on its own may not let recipient verify the message's authenticity, or even detect selective modifications like changing a digit\u2014if the bank's offices simply encrypted the messages they exchange, they could still be vulnerable to forgery.\nIn other applications, such as software updates, the messages are not secret\u2014when a software author publishes an patch for all existing installations of the software to apply, the patch itself is not secret, but computers running the software must verify the authenticity of the patch before applying it, lest they become victims to malware.","completion":"Cryptography"}
{"prompt":"The mid-1970s saw two major public (i.e., non-secret) advances. First was the publication of the draft Data Encryption Standard in the U.S. Federal Register on 17 March 1975. The proposed DES cipher was submitted by a research group at IBM, at the invitation of the National Bureau of Standards (now NIST), in an effort to develop secure electronic communication facilities for businesses such as banks and other large financial organizations. After advice and modification by the NSA, acting behind the scenes, it was adopted and published as a Federal Information Processing Standard Publication in 1977 (currently at FIPS 46-3). DES was the first publicly accessible cipher to be 'blessed' by a national agency such as the NSA. The release of its specification by NBS stimulated an explosion of public and academic interest in cryptography.\nThe aging DES was officially replaced by the Advanced Encryption Standard (AES) in 2001 when NIST announced FIPS 197. After an open competition, NIST selected Rijndael, submitted by two Belgian cryptographers, to be the AES. DES, and more secure variants of it (such as Triple DES), are still used today, having been incorporated into many national and organizational standards. However, its 56-bit key-size has been shown to be insufficient to guard against brute force attacks (one such attack, undertaken by the cyber civil-rights group Electronic Frontier Foundation in 1997, succeeded in 56 hours.) As a result, use of straight DES encryption is now without doubt insecure for use in new cryptosystem designs, and messages protected by older cryptosystems using DES, and indeed all messages sent since 1976 using DES, are also at risk. Regardless of DES' inherent quality, the DES key size (56-bits) was thought to be too small by some even in 1976, perhaps most publicly by Whitfield Diffie. There was suspicion that government organizations even then had sufficient computing power to break DES messages; clearly others have achieved this capability.","completion":"Cryptography"}
{"prompt":"Early versions of Microsoft's PPTP virtual private network software used the same RC4 key for the sender and the receiver (later versions had other problems). In any case where a stream cipher like RC4 is used twice with the same key, it is open to ciphertext-only attack. See: stream cipher attack\nWired Equivalent Privacy (WEP), the first security protocol for Wi-Fi, proved vulnerable to several attacks, most of them ciphertext-only.\nGSM's A5\/1 and A5\/2\nSome modern cipher designs have later been shown to be vulnerable to ciphertext-only attacks. For example, Akelarre.\nA cipher whose key space is too small is subject to brute force attack with access to nothing but ciphertext by simply trying all possible keys. All that is needed is some way to distinguish valid plaintext from random noise, which is easily done for natural languages when the ciphertext is longer than the unicity distance. One example is DES, which only has 56-bit keys. All too common current examples are commercial security products that derive keys for otherwise impregnable ciphers like AES from a user-selected password. Since users rarely employ passwords with anything close to the  entropy of the cipher's key space, such systems are often quite easy to break in practice using only ciphertext. The 40-bit CSS cipher used to encrypt DVD video discs can always be broken with this method, as all that is needed is to look for MPEG-2 video data.","completion":"Cryptography"}
{"prompt":"The sender begins by generating a random natural number s and computing:\n\n  \n    \n      \n        \u03b4\n        =\n        \n          \u03b3\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle \\delta =\\gamma ^{s}}\n  \n\n  \n    \n      \n        \u03f5\n        =\n        \n          \u03b4\n          \n            \u2212\n            1\n          \n        \n        \u03b1\n        \u03b4\n      \n    \n    {\\displaystyle \\epsilon =\\delta ^{-1}\\alpha \\delta }\n  \n\n  \n    \n      \n        \u03ba\n        =\n        \n          \u03b4\n          \n            \u2212\n            1\n          \n        \n        \u03b2\n        \u03b4\n      \n    \n    {\\displaystyle \\kappa =\\delta ^{-1}\\beta \\delta }\n  Then, to encrypt a message, each message block is encoded as a number (as in RSA) and they are placed four at a time as elements of a plaintext matrix \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n  . Each \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is encrypted using:\n\n  \n    \n      \n        \n          \u03bc\n          \u2032\n        \n        =\n        \u03ba\n        \u03bc\n        \u03ba\n        .\n      \n    \n    {\\displaystyle \\mu '=\\kappa \\mu \\kappa .}\n  Then \n  \n    \n      \n        \n          \u03bc\n          \u2032\n        \n      \n    \n    {\\displaystyle \\mu '}\n   and \n  \n    \n      \n        \u03f5\n      \n    \n    {\\displaystyle \\epsilon }\n   are sent to the receiver.","completion":"Cryptography"}
{"prompt":"Informally, a block cipher is secure in the standard model if an attacker cannot tell the difference between the block cipher (equipped with a random key) and a random permutation.\nTo be a bit more precise, let E be an n-bit block cipher. We imagine the following game:\n\nThe person running the game flips a coin.\nIf the coin lands on heads, he chooses a random key K and defines the function f = EK.\nIf the coin lands on tails, he chooses a random permutation \u03c0 on the set of n-bit strings and defines the function f = \u03c0.\nThe attacker chooses an n-bit string X, and the person running the game tells him the value of f(X).\nStep 2 is repeated a total of q times. (Each of these q interactions is a query.)\nThe attacker guesses how the coin landed. He wins if his guess is correct.The attacker, which we can model as an algorithm, is called an adversary. The function f (which the adversary was able to query) is called an oracle.\nNote that an adversary can trivially ensure a 50% chance of winning simply by guessing at random (or even by, for example, always guessing \"heads\"). Therefore, let PE(A) denote the probability that adversary A wins this game against E, and define the advantage of A as 2(PE(A) \u2212 1\/2). It follows that if A guesses randomly, its advantage will be 0; on the other hand, if A always wins, then its advantage is 1. The block cipher E is a pseudo-random permutation (PRP) if no adversary has an advantage significantly greater than 0, given specified restrictions on q and the adversary's running time. If in Step 2 above adversaries have the option of learning f\u22121(X) instead of f(X) (but still have only small advantages) then E is a strong PRP (SPRP). An adversary is non-adaptive if it chooses all q values for X before the game begins (that is, it does not use any information gleaned from previous queries to choose each X as it goes).\nThese definitions have proven useful for analyzing various modes of operation. For example, one can define a similar game for measuring the security of a block cipher-based encryption algorithm, and then try to show (through a reduction argument) that the probability of an adversary winning this new game is not much more than PE(A) for some A. (The reduction typically provides limits on q and the running time of A.) Equivalently, if PE(A) is small for all relevant A, then no attacker has a significant probability of winning the new game. This formalizes the idea that the higher-level algorithm inherits the block cipher's security.","completion":"Cryptography"}
{"prompt":"The table below shows the support of various stream ciphers. Stream ciphers are defined as using plain text digits that are combined with a pseudorandom cipher digit stream. Stream ciphers are typically faster than block ciphers and may have lower hardware complexity, but may be more susceptible to attacks.","completion":"Cryptography"}
{"prompt":"There are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.","completion":"Cryptography"}
{"prompt":"Poe had a keen interest in cryptography. He had placed a notice of his abilities in the Philadelphia paper Alexander's Weekly (Express) Messenger, inviting submissions of ciphers which he proceeded to solve. In July 1841, Poe had published an essay called \"A Few Words on Secret Writing\" in Graham's Magazine. Capitalizing on public interest in the topic, he wrote \"The Gold-Bug\" incorporating ciphers as an essential part of the story. Poe's success with cryptography relied not so much on his deep knowledge of that field (his method was limited to the simple substitution cryptogram) as on his knowledge of the magazine and newspaper culture. His keen analytical abilities, which were so evident in his detective stories, allowed him to see that the general public was largely ignorant of the methods by which a simple substitution cryptogram can be solved, and he used this to his advantage. The sensation that Poe created with his cryptography stunts played a major role in popularizing cryptograms in newspapers and magazines.Two ciphers he published in 1841 under the name \"W. B. Tyler\" were not solved until 1992 and 2000 respectively. One was a quote from Joseph Addison's play Cato; the other is probably based on a poem by Hester Thrale.Poe had an influence on cryptography beyond increasing public interest during his lifetime. William Friedman, America's foremost cryptologist, was heavily influenced by Poe. Friedman's initial interest in cryptography came from reading \"The Gold-Bug\" as a child, an interest that he later put to use in deciphering Japan's PURPLE code during World War II.","completion":"Cryptography"}
{"prompt":"Encryption is used in the 21st century to protect digital data and information systems. As computing power increased over the years, encryption technology has only become more advanced and secure. However, this advancement in technology has also exposed a potential limitation of today's encryption methods.\nThe length of the encryption key is an indicator of the strength of the encryption method. For example, the original encryption key, DES (Data Encryption Standard), was 56 bits, meaning it had 2^56 combination possibilities. With today's computing power, a 56-bit key is no longer secure, being vulnerable to brute force attacks.Quantum computing utilizes properties of quantum mechanics in order to process large amounts of data simultaneously. Quantum computing has been found to achieve computing speeds thousands of times faster than today's supercomputers. This computing power presents a challenge to today's encryption technology. For example, RSA encryption utilizes the multiplication of very large prime numbers to create a semiprime number for its public key. Decoding this key without its private key requires this semiprime number to be factored, which can take a very long time to do with modern computers. It would take a supercomputer anywhere between weeks to months to factor in this key. However, quantum computing can use quantum algorithms to factor this semiprime number in the same amount of time it takes for normal computers to generate it. This would make all data protected by current public-key encryption vulnerable to quantum computing attacks. Other encryption techniques like elliptic curve cryptography and symmetric key encryption are also vulnerable to quantum computing.While quantum computing could be a threat to encryption security in the future, quantum computing as it currently stands is still very limited. Quantum computing currently is not commercially available, cannot handle large amounts of code, and only exists as computational devices, not computers. Furthermore, quantum computing advancements will be able to be utilized in favor of encryption as well. The National Security Agency (NSA) is currently preparing post-quantum encryption standards for the future. Quantum encryption promises a level of security that will be able to counter the threat of quantum computing.","completion":"Cryptography"}
{"prompt":"The following example illustrates how a shared key is established.. Suppose Alice wants to establish a shared key with Bob, but the only channel available for them may be eavesdropped by a third party.. Initially, the domain parameters (that is,                         (         p         ,         a         ,         b         ,         G         ,         n         ,         h         )                 {\\displaystyle (p,a,b,G,n,h)}    in the prime case or                         (         m         ,         f         (         x         )         ,         a         ,         b         ,         G         ,         n         ,         h         )                 {\\displaystyle (m,f(x),a,b,G,n,h)}    in the binary case) must be agreed upon..","completion":"Cryptography"}
{"prompt":"In their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures:\nIn a key-only attack, the attacker is only given the public verification key.\nIn a known message attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.\nIn an adaptive chosen message attack, the attacker first learns signatures on arbitrary messages of the attacker's choice.They also describe a hierarchy of attack results:\nA total break results in the recovery of the signing key.\nA universal forgery attack results in the ability to forge signatures for any message.\nA selective forgery attack results in a signature on a message of the adversary's choice.\nAn existential forgery merely results in some valid message\/signature pair not already known to the adversary.The strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.","completion":"Cryptography"}
{"prompt":"The powers of 10 are\n\n  \n    \n      \n        \u2026\n        ,\n        0.001\n        ,\n        0.01\n        ,\n        0.1\n        ,\n        1\n        ,\n        10\n        ,\n        100\n        ,\n        1000\n        ,\n        \u2026\n        .\n      \n    \n    {\\displaystyle \\ldots ,0.001,0.01,0.1,1,10,100,1000,\\ldots .}\n  For any number a in this list, one can compute log10\u2009a. For example, log10\u200910000 = 4, and log10\u20090.001 = \u22123. These are instances of the discrete logarithm problem.\nOther base-10 logarithms in the real numbers are not instances of the discrete logarithm problem, because they involve non-integer exponents. For example, the equation log10\u200953 = 1.724276\u2026 means that 101.724276\u2026 = 53. While integer exponents can be defined in any group using products and inverses, arbitrary real exponents, such as this 1.724276\u2026, require other concepts such as the exponential function.\nIn group-theoretic terms, the powers of 10 form a cyclic group G under multiplication, and 10 is a generator for this group. The discrete logarithm log10\u2009a is defined for any a in G.","completion":"Cryptography"}
{"prompt":"The Eliminating Abusive and Rampant Neglect of Interactive Technologies (EARN IT) Act of 2020 provides for a 19-member National Commission which will develop a set of \"best practice\" guidelines to which technology providers will have to conform in order to \"earn\" immunity (traditionally provided 'automatically' by Section 230 of the Communications Decency Act) to liability for child sexual abuse material on their platforms. Proponents present it as a way to tackle child sexual abuse material on internet platforms, but it has been criticized by advocates of encryption because it is likely that the \"best practices\" devised by the commission will include refraining from using end-to-end encryption, as such encryption would make it impossible to screen for illegal content.","completion":"Cryptography"}
{"prompt":"Al-Kindi theorized that there was a separate, incorporeal and universal intellect (known as the \"First Intellect\"). It was the first of God's creation and the intermediary through which all other things came into creation. Aside from its obvious metaphysical importance, it was also crucial to al-Kindi's epistemology, which was influenced by Platonic realism.According to Plato, everything that exists in the material world corresponds to certain universal forms in the heavenly realm. These forms are really abstract concepts such as a species, quality or relation, which apply to all physical objects and beings. For example, a red apple has the quality of \"redness\" derived from the appropriate universal. However, al-Kindi says that human intellects are only potentially able to comprehend these. This potential is actualized by the First Intellect, which is perpetually thinking about all of the universals. He argues that the external agency of this intellect is necessary by saying that human beings cannot arrive at a universal concept merely through perception. In other words, an intellect cannot understand the species of a thing simply by examining one or more of its instances. According to him, this will only yield an inferior \"sensible form\", and not the universal form which we desire. The universal form can only be attained through contemplation and actualization by the First Intellect.The analogy he provides to explain his theory is that of wood and fire. Wood, he argues, is potentially hot (just as a human is potentially thinking about a universal), and therefore requires something else which is already hot (such as fire) to actualize this. This means that for the human intellect to think about something, the First Intellect must already be thinking about it. Therefore, he says that the First Intellect must always be thinking about everything. Once the human intellect comprehends a universal by this process, it becomes part of the individual's \"acquired intellect\" and can be thought about whenever he or she wishes.","completion":"Cryptography"}
{"prompt":"One of the simplest settings for discrete logarithms is the group (Zp)\u00d7. This is the group of multiplication modulo the prime p. Its elements are  congruence classes modulo p, and the group product of two elements may be obtained by ordinary integer multiplication of the elements followed by reduction modulo p.\nThe kth power of one of the numbers in this group may be computed by finding its kth power as an integer and then finding the remainder after division by p. When the numbers involved are large, it is more efficient to reduce modulo p multiple times during the computation. Regardless of the specific algorithm used, this operation is called modular exponentiation. For example, consider (Z17)\u00d7. To compute 34 in this group, compute 34 = 81, and then divide 81 by 17, obtaining a remainder of 13. Thus 34 = 13 in the group (Z17)\u00d7.\nThe discrete logarithm is just the inverse operation. For example, consider the equation 3k \u2261 13 (mod 17) for k. From the example above, one solution is k = 4, but it is not the only solution. Since 316 \u2261 1 (mod 17)\u2014as follows from Fermat's little theorem\u2014it also follows that if n is an integer then 34+16n \u2261 34 \u00d7 (316)n \u2261 13 \u00d7 1n \u2261 13 (mod 17). Hence the equation has infinitely many solutions of the form 4 + 16n. Moreover, because 16 is the smallest positive integer m satisfying 3m \u2261 1 (mod 17), these are the only solutions. Equivalently, the set of all possible solutions can be expressed by the constraint that k \u2261 4 (mod 16).","completion":"Cryptography"}
{"prompt":"Many modern block ciphers and hashes are ARX algorithms\u2014their round function involves only three operations: (A) modular addition, (R) rotation with fixed rotation amounts, and (X) XOR. Examples include ChaCha20, Speck, XXTEA, and BLAKE. Many authors draw an ARX network, a kind of data flow diagram, to illustrate such a round function.These ARX operations are popular because they are relatively fast and cheap in hardware and software, their implementation can be made extremely simple, and also because they run in constant time, and therefore are immune to timing attacks. The rotational cryptanalysis technique attempts to attack such round functions.","completion":"Cryptography"}
{"prompt":"Born in 718 in Oman, southern Arabia, to Azdi parents of modest means, al-Farahidi became a leading grammarian of Basra in Iraq. In Basra, he studied Islamic traditions and philology under Abu 'Amr ibn al-'Ala' with Aiy\u016bb al-Sakhtiy\u0101ni, \u2018\u0100\u1e63m al-A\u1e25wal, al-\u2018Aww\u0101m b. \u1e24awshab, etc. His teacher Ayyub persuaded him to renounce the Ab\u0101\u1e0di doctrine and convert to Sunni orthodoxy; Among his pupils were Sibawayh, al-Na\u1e0dr b. Shumail, and al-Layth b. al-Mu\u1e93affar b. Na\u1e63r. Known for his piety and frugality, he was a companion of J\u0101bir ibn Zayd, the founder of ibadism. It was said his parents were converts to Islam, and that his father was the first to be named \"Ahmad\" after the time of Prophet Muhammad. His nickname, \"Farahidi\", differed from his tribal name and derived from an ancestor named Furhud (Young Lion); plural farahid.  He refused lavish gifts from rulers, or to indulge in the slander and gossip his fellow Arab and Persian rival scholars were wont, and he performed annual pilgrimage to Mecca. He lived in a small reed house in Basra and once remarked that when his door was shut, his mind did not go beyond it. He taught linguistics, and some of his students became wealthy teachers. Al-Farahidi's main income was falconry and a garden inherited from his father. Two dates of death are cited, 786 and 791 CE. The story goes that it was theoretical contemplation that brought about his death. On the particular day, while he was deeply absorbed in contemplation of a system of accounting to save his maidservant from being cheated by the green grocer, he wandered into a mosque and there he absent-mindedly bumped into a pillar and was fatally injured.","completion":"Cryptography"}
{"prompt":"The Broadway Journal failed in 1846, and Poe moved to a cottage in Fordham, New York, in the Bronx. That home, now known as the Edgar Allan Poe Cottage, was relocated in later years to a park near the southeast corner of the Grand Concourse and Kingsbridge Road. Nearby, Poe befriended the Jesuits at St. John's College, now Fordham University. Virginia died at the cottage on January 30, 1847. Biographers and critics often suggest that Poe's frequent theme of the \"death of a beautiful woman\" stems from the repeated loss of women throughout his life, including his wife.Poe was increasingly unstable after his wife's death. He attempted to court poet Sarah Helen Whitman, who lived in Providence, Rhode Island. Their engagement failed, purportedly because of Poe's drinking and erratic behavior. There is also strong evidence that Whitman's mother intervened and did much to derail the relationship. Poe then returned to Richmond and resumed a relationship with his childhood sweetheart Sarah Elmira Royster.","completion":"Cryptography"}
{"prompt":"Poe's writing reflects his literary theories, which he presented in his criticism and also in essays such as \"The Poetic Principle\". He disliked didacticism and allegory, though he believed that meaning in literature should be an undercurrent just beneath the surface. Works with obvious meanings, he wrote, cease to be art. He believed that work of quality should be brief and focus on a specific single effect. To that end, he believed that the writer should carefully calculate every sentiment and idea.Poe describes his method in writing \"The Raven\" in the essay \"The Philosophy of Composition\", and he claims to have strictly followed this method. It has been questioned whether he really followed this system, however. T. S. Eliot said: \"It is difficult for us to read that essay without reflecting that if Poe plotted out his poem with such calculation, he might have taken a little more pains over it: the result hardly does credit to the method.\" Biographer Joseph Wood Krutch described the essay as \"a rather highly ingenious exercise in the art of rationalization\".","completion":"Cryptography"}
{"prompt":"Schneier, Bruce. Applied Cryptography, John Wiley & Sons, 1994. ISBN 0-471-59756-2\nSchneier, Bruce. Protect Your Macintosh, Peachpit Press, 1994. ISBN 1-56609-101-2\nSchneier, Bruce. E-Mail Security, John Wiley & Sons, 1995. ISBN 0-471-05318-X\nSchneier, Bruce. Applied Cryptography, Second Edition, John Wiley & Sons, 1996. ISBN 0-471-11709-9\nSchneier, Bruce; Kelsey, John; Whiting, Doug; Wagner, David; Hall, Chris; Ferguson, Niels. The Twofish Encryption Algorithm, John Wiley & Sons, 1996. ISBN 0-471-35381-7\nSchneier, Bruce; Banisar, David. The Electronic Privacy Papers, John Wiley & Sons, 1997. ISBN 0-471-12297-1\nSchneier, Bruce. Secrets and Lies: Digital Security in a Networked World, John Wiley & Sons, 2000. ISBN 0-471-25311-1\nSchneier, Bruce. Beyond Fear: Thinking Sensibly About Security in an Uncertain World, Copernicus Books, 2003. ISBN 0-387-02620-7\nFerguson, Niels; Schneier, Bruce. Practical Cryptography, John Wiley & Sons, 2003. ISBN 0-471-22357-3\nSchneier, Bruce. Secrets and Lies: Digital Security in a Networked World, John Wiley & Sons, 2004. ISBN 978-0-471-45380-2\nSchneier, Bruce. Schneier on Security, John Wiley & Sons, 2008. ISBN 978-0-470-39535-6\nFerguson, Niels; Schneier, Bruce; Kohno, Tadayoshi. Cryptography Engineering, John Wiley & Sons, 2010. ISBN 978-0-470-47424-2\nSchneier, Bruce. Liars and Outliers: Enabling the Trust that Society Needs to Thrive, John Wiley & Sons, 2012. ISBN 978-1-118-14330-8\nSchneier, Bruce. Carry On: Sound Advice from Schneier on Security, John Wiley & Sons, 2013. ISBN 978-1118790816\nSchneier, Bruce. Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World, W. W. Norton & Company, 2015. ISBN 978-0-393-24481-6\nSchneier, Bruce. Click Here to Kill Everybody: Security and Survival in a Hyper-connected World, W. W. Norton & Company, 2018. ISBN 978-0-393-60888-5\nSchneier, Bruce. We Have Root: Even More Advice from Schneier on Security, John Wiley & Sons, 2019.  ISBN 978-1119643012\nSchneier, Bruce. A Hacker\u2019s Mind: How the Powerful Bend Society\u2019s Rules, and How to Bend them Back, W. W. Norton & Company, 2023.  ISBN 978-0-393-86666-7","completion":"Cryptography"}
{"prompt":"A one-time code is a prearranged word, phrase or symbol that is intended to be used only once to convey a simple message, often the signal to execute or abort some plan or confirm that it has succeeded or failed.  One-time codes are often designed to be included in what would appear to be an innocent conversation. Done properly they are almost impossible to detect, though a trained analyst monitoring the communications of someone who has already aroused suspicion might be able to recognize a comment like \"Aunt Bertha has gone into labor\" as having an ominous meaning. Famous example of one time codes include:\n\nIn the Bible, Jonathan prearranges a code with David, who is going into hiding from Jonathan's father, King Saul. If, during archery practice, Jonathan tells the servant retrieving arrows \"the arrows are on this side of you,\" it's safe for David to return to court, if the command is \"the arrows are beyond you,\" David must flee.\n\"One if by land; two if by sea\" in \"Paul Revere's Ride\" made famous in the poem by Henry Wadsworth Longfellow\n\"Climb Mount Niitaka\" - the signal to Japanese planes to begin the attack on Pearl Harbor\nDuring World War II the British Broadcasting Corporation's overseas service frequently included \"personal messages\" as part of its regular broadcast schedule. The seemingly nonsensical stream of messages read out by announcers were actually one time codes intended for Special Operations Executive (SOE) agents operating behind enemy lines. An example might be \"The princess wears red shoes\" or \"Mimi's cat is asleep under the table\". Each code message was read out twice. By such means, the French Resistance were instructed to start sabotaging rail and other transport links the night before D-day.\n\"Over all of Spain, the sky is clear\" was a signal (broadcast on radio) to start the nationalist military revolt in Spain on July 17, 1936.Sometimes messages are not prearranged and rely on shared knowledge hopefully known only to the recipients. An example is the telegram sent to U.S. President Harry Truman, then at the Potsdam Conference to meet with Soviet premier Joseph Stalin, informing Truman of the first successful test of an atomic bomb. \n\n\"Operated on this morning. Diagnosis not yet complete but results seem satisfactory and already exceed expectations. Local press release necessary as interest extends great distance. Dr. Groves pleased. He returns tomorrow. I will keep you posted.\"See also one-time pad, an unrelated cypher algorithm","completion":"Cryptography"}
{"prompt":"Al-Farahidi's Kitab al-Muamma \"Book of Cryptographic Messages\", was the first book on cryptography and cryptanalysis written by a linguist. The lost work contains many \"firsts\", including the use of permutations and combinations to list all possible Arabic words with and without vowels. Later Arab cryptographers explicitly resorted to al-Farahidi's phonological analysis for calculating letter frequency in their own works. His work on cryptography influenced al-Kindi (c. 801\u2013873), who discovered the method of cryptanalysis by frequency analysis.","completion":"Cryptography"}
{"prompt":"The initiative involved four standard algorithms: a data encryption algorithm called Skipjack, along with the Clipper chip that included the Skipjack algorithm, a digital signature algorithm, Digital Signature Algorithm (DSA), a hash function, SHA-1, and a key exchange protocol.  Capstone's first implementation was in the Fortezza PCMCIA card. All Capstone components were designed to provide 80-bit security.The initiative encountered massive resistance from the cryptographic community, and eventually the US government abandoned the effort.  The main reasons for this resistance were concerns about Skipjack's design, which was classified, and the use of key escrow in the Clipper chip.","completion":"Cryptography"}
{"prompt":"The structure and properties of Feistel ciphers have been extensively analyzed by cryptographers.\nMichael Luby and Charles Rackoff analyzed the Feistel cipher construction and proved that if the round function is a cryptographically secure pseudorandom function, with Ki used as the seed, then 3 rounds are sufficient to make the block cipher a pseudorandom permutation, while 4 rounds are sufficient to make it a \"strong\" pseudorandom permutation (which means that it remains pseudorandom even to an adversary who gets oracle access to its inverse permutation). Because of this very important result of Luby and Rackoff, Feistel ciphers are sometimes called Luby\u2013Rackoff block ciphers.\nFurther theoretical work has generalized the construction somewhat and given more precise bounds for security.","completion":"Cryptography"}
{"prompt":"The simplest (and not to be used anymore) of the encryption modes is the electronic codebook (ECB) mode (named after conventional physical codebooks). The message is divided into blocks, and each block is encrypted separately.\n\nThe disadvantage of this method is a lack of diffusion. Because ECB encrypts identical plaintext blocks into identical ciphertext blocks, it does not hide data patterns well.\nECB is not recommended for use in cryptographic protocols.A striking example of the degree to which ECB can leave plaintext data patterns in the ciphertext can be seen when ECB mode is used to encrypt a bitmap image which uses large areas of uniform color. While the color of each individual pixel is encrypted, the overall image may still be discerned, as the pattern of identically colored pixels in the original remains in the encrypted version.\n\nECB mode can also make protocols without integrity protection even more susceptible to replay attacks, since each block gets decrypted in exactly the same way.","completion":"Cryptography"}
{"prompt":"Every modern cipher attempts to provide protection against ciphertext-only attacks. The vetting process for a new cipher design standard usually takes many years and includes exhaustive testing of large quantities of ciphertext for any statistical departure from random noise. See: Advanced Encryption Standard process. Also, the field of steganography evolved, in part, to develop methods like mimic functions that allow one piece of data to adopt the statistical profile of another. Nonetheless, poor cipher usage or reliance on home-grown proprietary algorithms that have not been subject to thorough scrutiny has resulted in many computer-age encryption systems that are still subject to ciphertext-only attack. Examples include:","completion":"Cryptography"}
{"prompt":"Solving RSD, we are in the opposite situation as when hashing. Using the same values as in the previous example, we are given \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   separated into \n  \n    \n      \n        w\n        =\n        3\n      \n    \n    {\\displaystyle w=3}\n   sub-blocks and a string \n  \n    \n      \n        r\n        =\n        1111\n      \n    \n    {\\displaystyle r=1111}\n  . We are asked to find in each sub-block exactly one column such that they would all sum to \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  . The expected answer is thus \n  \n    \n      \n        \n          s\n          \n            1\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle s_{1}=1}\n  , \n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle s_{2}=0}\n  , \n  \n    \n      \n        \n          s\n          \n            3\n          \n        \n        =\n        3\n      \n    \n    {\\displaystyle s_{3}=3}\n  . This is known to be hard to compute for large matrices.\nIn 2-RNSD we want to find in each sub-block not one column, but two or zero such that they would sum up to 0000 (and not to \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  ). In the example, we might use column (counting from 0) 2 and 3 from \n  \n    \n      \n        \n          H\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle H_{1}}\n  , no column from \n  \n    \n      \n        \n          H\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle H_{2}}\n   column 0 and 2 from \n  \n    \n      \n        \n          H\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle H_{3}}\n  . More solutions are possible, for example might use no columns from \n  \n    \n      \n        \n          H\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle H_{3}}\n  .","completion":"Cryptography"}
{"prompt":"In encryption, confidential information (called the \"plaintext\") is sent securely to a recipient by the sender first converting it into an unreadable form (\"ciphertext\") using an encryption algorithm.  The ciphertext is sent through an insecure channel to the recipient.  The recipient decrypts the ciphertext by applying an inverse decryption algorithm, recovering the plaintext.  To decrypt the ciphertext, the recipient requires a secret knowledge from the sender, usually a string of letters, numbers, or bits, called a cryptographic key.  The concept is that even if an unauthorized person gets access to the ciphertext during transmission, without the secret key they cannot convert it back to plaintext.\nEncryption has been used throughout history to send important military, diplomatic and commercial messages, and today is very widely used in computer networking to protect email and internet communication.\nThe goal of cryptanalysis is for a third party, a cryptanalyst, to gain as much information as possible about the original (\"plaintext\"), attempting to \"break\" the encryption to read the ciphertext and learning the secret key so future messages can be decrypted and read.  A mathematical technique to do this is called a cryptographic attack. Cryptographic attacks can be characterized in a number of ways:","completion":"Cryptography"}
{"prompt":"The attack is completely successful if the corresponding plaintexts can be deduced, or even better, the key.  The ability to obtain any information at all about the underlying plaintext beyond what was pre-known to the attacker is still considered a success. For example, if an adversary is sending ciphertext continuously to maintain traffic-flow security, it would be very useful to be able to distinguish real messages from nulls. Even making an informed guess of the existence of real messages would facilitate traffic analysis.  \nIn the history of cryptography, early ciphers, implemented using pen-and-paper, were routinely broken using ciphertexts alone. Cryptographers developed statistical techniques for attacking ciphertext, such as frequency analysis. Mechanical encryption devices such as Enigma made these attacks much more difficult (although, historically, Polish cryptographers were able to mount a successful ciphertext-only cryptanalysis of the Enigma by exploiting an insecure protocol for indicating the message settings). More advanced ciphertext-only attacks on the Enigma were mounted in Bletchley Park during World War II, by intelligently guessing plaintexts corresponding to intercepted ciphertexts.","completion":"Cryptography"}
{"prompt":"The UK and US employed large numbers of women in their code-breaking operation, with close to 7,000 reporting to Bletchley Park  \nand 11,000 to the separate US Army and Navy operations, around Washington, DC. By tradition in Japan and Nazi doctrine in Germany, women were excluded from war work, at least until late in the war. Even after encryption systems were broken, large amounts of work were needed to respond to changes made, recover daily key settings for multiple networks, and intercept, process, translate, prioritize and analyze the huge volume of enemy messages generated in a global conflict. A few women, including Elizabeth Friedman and Agnes Meyer Driscoll, had been major contributors to US code-breaking in the 1930s and the Navy and Army began actively recruiting top graduates of women's colleges shortly before the attack on Pearl Harbor. Liza Mundy argues that this disparity in utilizing the talents of women between the Allies and Axis made a strategic difference in the war.:\u200ap.29","completion":"Cryptography"}
{"prompt":"There are more than thirty treatises attributed to al-Kindi in the field of medicine, in which he was chiefly influenced by the ideas of Galen. His most important work in this field is probably De Gradibus, in which he demonstrates the application of mathematics to medicine, particularly in the field of pharmacology. For example, he developed a mathematical scale to quantify the strength of a drug, and a system (based on the phases of the moon) that would allow a doctor to determine in advance the most critical days of a patient's illness. According to Plinio Prioreschi, this was the first attempt at serious quantification in medicine.","completion":"Cryptography"}
{"prompt":"Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, Bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (Bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022. The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later. The Wall Street Journal has commented that the crypto sector has become \"intertwined\" with the rest of the capital markets and \"sensitive to the same forces that drive tech stocks and other risk assets\", such as inflation forecasts.","completion":"Cryptography"}
{"prompt":"ElGamal encryption is probabilistic, meaning that a single plaintext can be encrypted to many possible ciphertexts, with the consequence that a general ElGamal encryption produces a 1:2 expansion in size from plaintext to ciphertext.\nEncryption under ElGamal requires two exponentiations; however, these exponentiations are independent of the message and can be computed ahead of time if needed. Decryption requires one exponentiation and one computation of a group inverse, which can, however, be easily combined into just one exponentiation.","completion":"Cryptography"}
{"prompt":"For any cipher, the most basic method of attack is brute force\u2014trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 256 bits to 56 bits to fit on a single chip.\nIn academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented\u2014or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: \"There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES.\" The machine brute-forced a key in a little more than 2 days' worth of searching.\nThe next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well.  One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware\u2014see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. SciEngines RIVYERA held the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has further lowered this time.\nIn 2012, David Hulton and Moxie Marlinspike announced a system with 48 Xilinx Virtex-6 LX240T FPGAs, each FPGA containing 40 fully pipelined DES cores running at 400 MHz, for a total capacity of  768 gigakeys\/sec. The system can exhaustively search the entire 56-bit DES key space in about 26 hours and this service is offered for a fee online.","completion":"Cryptography"}
{"prompt":"One of the main differences between a digital signature and a written signature is that the user does not \"see\" what they sign. The user application presents a hash code to be signed by the digital signing algorithm using the private key. An attacker who gains control of the user's PC can possibly replace the user application with a foreign substitute, in effect replacing the user's own communications with those of the attacker. This could allow a malicious application to trick a user into signing any document by displaying the user's original on-screen, but presenting the attacker's own documents to the signing application.\nTo protect against this scenario, an authentication system can be set up between the user's application (word processor, email client, etc.) and the signing application. The general idea is to provide some means for both the user application and signing application to verify each other's integrity. For example, the signing application may require all requests to come from digitally signed binaries.","completion":"Cryptography"}
{"prompt":"An ink signature could be replicated from one document to another by copying the image manually or digitally, but to have credible signature copies that can resist some scrutiny is a significant manual or technical skill, and to produce ink signature copies that resist professional scrutiny is very difficult.\nDigital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document. Paper contracts sometimes have the ink signature block on the last page, and the previous pages may be replaced after a signature is applied.  Digital signatures can be applied to an entire document, such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered, but this can also be achieved by signing with ink and numbering all pages of the contract.","completion":"Cryptography"}
{"prompt":"The earliest modes of operation, ECB, CBC, OFB, and CFB (see below for all), date back to 1981 and were specified in FIPS 81, DES Modes of Operation. In 2001, the US National Institute of Standards and Technology (NIST) revised its list of approved modes of operation by including AES as a block cipher and adding CTR mode in SP800-38A, Recommendation for Block Cipher Modes of Operation. Finally, in January, 2010, NIST added XTS-AES in SP800-38E, Recommendation for Block Cipher Modes of Operation: The XTS-AES Mode for Confidentiality on Storage Devices. Other confidentiality modes exist which have not been approved by NIST. For example, CTS is ciphertext stealing mode and available in many popular cryptographic libraries.\nThe block cipher modes ECB, CBC, OFB, CFB, CTR, and XTS provide confidentiality, but they do not protect against accidental modification or malicious tampering. Modification or tampering can be detected with a separate message authentication code such as CBC-MAC, or a digital signature. The cryptographic community recognized the need for dedicated integrity assurances and NIST responded with HMAC, CMAC, and GMAC. HMAC was approved in 2002 as FIPS 198, The Keyed-Hash Message Authentication Code (HMAC), CMAC was released in 2005 under SP800-38B, Recommendation for Block Cipher Modes of Operation: The CMAC Mode for Authentication, and GMAC was formalized in 2007 under SP800-38D, Recommendation for Block Cipher Modes of Operation: Galois\/Counter Mode (GCM) and GMAC.\nThe cryptographic community observed that compositing (combining)  a confidentiality mode with an authenticity mode could be difficult and error prone.  They therefore began to supply modes which combined confidentiality and data integrity into a single cryptographic primitive (an encryption algorithm). These combined modes are referred to as authenticated encryption, AE or \"authenc\". Examples of AE modes are CCM (SP800-38C), GCM (SP800-38D), CWC, EAX, IAPM, and OCB.\nModes of operation are defined by a number of national and internationally recognized standards bodies. Notable standards organizations include NIST, ISO (with ISO\/IEC 10116), the IEC, the IEEE, ANSI, and the IETF.","completion":"Cryptography"}
{"prompt":"An idiot code is a code that is created by the parties using it. This type of communication is akin to the hand signals used by armies in the field.\nExample: Any sentence where 'day' and 'night' are used means 'attack'. The location mentioned in the following sentence specifies the location to be attacked.\n\nPlaintext: Attack X.\nCodetext: We walked day and night through the streets but couldn't find it! Tomorrow we'll head into X.An early use of the term appears to be by George Perrault, a character in the science fiction book Friday by Robert A. Heinlein:\n\nThe simplest sort [of code] and thereby impossible to break. The first ad told the person or persons concerned to carry out number seven or expect number seven or it said something about something designated as seven. This one says the same with respect to code item number ten. But the meaning of the numbers cannot be deduced through statistical analysis because the code can be changed long before a useful statistical universe can be reached. It's an idiot code... and an idiot code can never be broken if the user has the good sense not to go too often to the well.Terrorism expert Magnus Ranstorp said that the men who carried out the September 11 attacks on the United States used basic e-mail and what he calls \"idiot code\" to discuss their plans.","completion":"Cryptography"}
{"prompt":"The following pseudocode demonstrates how HMAC may be implemented. The block size is 512 bits (64 bytes) when using one of the following hash functions: SHA-1, MD5, RIPEMD-128.\nfunction hmac is\n    input:\n        key:        Bytes    \/\/ Array of bytes\n        message:    Bytes    \/\/ Array of bytes to be hashed\n        hash:       Function \/\/ The hash function to use (e.g. SHA-1)\n        blockSize:  Integer  \/\/ The block size of the hash function (e.g. 64 bytes for SHA-1)\n        outputSize: Integer  \/\/ The output size of the hash function (e.g. 20 bytes for SHA-1)\n\n    \/\/ Compute the block sized key\n    block_sized_key = computeBlockSizedKey(key, hash, blockSize)\n\n    o_key_pad \u2190 block_sized_key xor [0x5c blockSize]   \/\/ Outer padded key\n    i_key_pad \u2190 block_sized_key xor [0x36 blockSize]   \/\/ Inner padded key\n\n    return  hash(o_key_pad \u2225 hash(i_key_pad \u2225 message))\n\nfunction computeBlockSizedKey is\n    input:\n        key:        Bytes    \/\/ Array of bytes\n        hash:       Function \/\/ The hash function to use (e.g. SHA-1)\n        blockSize:  Integer  \/\/ The block size of the hash function (e.g. 64 bytes for SHA-1)\n \n    \/\/ Keys longer than blockSize are shortened by hashing them\n    if (length(key) > blockSize) then\n        key = hash(key)\n\n    \/\/ Keys shorter than blockSize are padded to blockSize by padding with zeros on the right\n    if (length(key) < blockSize) then\n        return  Pad(key, blockSize) \/\/ Pad key with zeros to make it blockSize bytes long\n\n    return  key","completion":"Cryptography"}
{"prompt":"FIPS PUB 31 Guidelines for Automatic Data Processing Physical Security and  Risk Management 1974\nFIPS PUB 46-3 Data Encryption Standard (DES) 1999\nFIPS PUB 73 Guidelines for Security of Computer Applications 1980\nFIPS PUB 74 Guidelines for Implementing and Using the NBS Data Encryption Standard 1981\nFIPS PUB 81 DES Modes of Operation 1980\nFIPS PUB 102 Guideline for Computer Security Certification and Accreditation  1983\nFIPS PUB 112 Password Usage 1985, defines 10 factors to be considered in  access control systems that  are based on passwords\nFIPS PUB 113 Computer Data Authentication 1985, specifies a Data Authentication Algorithm  (DAA) based on DES, adopted by the Department of Treasury and the  banking community to protect electronic fund transfers.\nFIPS PUB 140-2 Security Requirements for Cryptographic Modules 2001, defines four increasing security levels\nFIPS PUB 171 Key Management Using ANSI X9.17 (ANSI X9.17-1985) 1992, based on DES\nFIPS PUB 180-2 Secure Hash Standard (SHS) 2002 defines the SHA family\nFIPS PUB 181 Automated Password Generator (APG) 1993\nFIPS PUB 185 Escrowed Encryption Standard (EES) 1994, a key escrow system that provides for decryption  of telecommunications when lawfully authorized.\nFIPS PUB 186-2 Digital Signature Standard (DSS) 2000\nFIPS PUB 190 Guideline for the Use of Advanced Authentication Technology Alternatives  1994\nFIPS PUB 191 Guideline for the Analysis of local area network Security 1994\nFIPS PUB 196 Entity Authentication Using Public Key Cryptography 1997\nFIPS PUB 197 Advanced Encryption Standard (AES) 2001\nFIPS PUB 198 The Keyed-Hash Message Authentication Code (HMAC) 2002","completion":"Cryptography"}
{"prompt":"David Kahn was born in New York City to Florence Abraham Kahn, a glass manufacturer, and Jesse Kahn, a lawyer.\nKahn has said he traces his interest in cryptography to reading Fletcher Pratt's Secret and Urgent as a boy. Kahn is a founding editor of the Cryptologia journal.  In 1969, Kahn married Susanne Fiedler; they are now divorced. They have two sons, Oliver and Michael.He attended Bucknell University. After graduation, he worked as a reporter at Newsday.  He also served as an editor at the International Herald Tribune in Paris in the 1960s.\nIt was during this period that he wrote an article for the New York Times Magazine about two defectors from the National Security Agency. It was the origin of his monumental book, The Codebreakers.","completion":"Cryptography"}
{"prompt":"This table denotes, if a cryptography library provides the technical requisites for FIPS 140, and the status of their FIPS 140 certification (according to NIST's CMVP search, modules in process list and implementation under test list).","completion":"Cryptography"}
{"prompt":"Recovering the private key                         \u03c7                 {\\displaystyle \\chi }    from                         \u03b3                 {\\displaystyle \\gamma }    is computationally infeasible, at least as hard as finding square roots mod n (see quadratic residue).. It could be recovered from                         \u03b1                 {\\displaystyle \\alpha }    and                         \u03b2                 {\\displaystyle \\beta }    if the system                         \u03c7         \u03b2         =                    \u03b1                        \u2212             1                             \u03c7                 {\\displaystyle \\chi \\beta =\\alpha ^{-1}\\chi }    could be solved, but the number of solutions to this system is large as long as elements in the group have a large order, which can be guaranteed for almost every element..","completion":"Cryptography"}
{"prompt":"A second party, Bob, encrypts a message \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   to Alice under her public key \n  \n    \n      \n        (\n        G\n        ,\n        q\n        ,\n        g\n        ,\n        h\n        )\n      \n    \n    {\\displaystyle (G,q,g,h)}\n   as follows:\n\nMap the message \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n   to an element \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   of \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n   using a reversible mapping function.\nChoose an integer \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   randomly from \n  \n    \n      \n        {\n        1\n        ,\n        \u2026\n        ,\n        q\n        \u2212\n        1\n        }\n      \n    \n    {\\displaystyle \\{1,\\ldots ,q-1\\}}\n  .\nCompute \n  \n    \n      \n        s\n        :=\n        \n          h\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle s:=h^{y}}\n  . This is called the shared secret.\nCompute \n  \n    \n      \n        \n          c\n          \n            1\n          \n        \n        :=\n        \n          g\n          \n            y\n          \n        \n      \n    \n    {\\displaystyle c_{1}:=g^{y}}\n  .\nCompute \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        :=\n        m\n        \u22c5\n        s\n      \n    \n    {\\displaystyle c_{2}:=m\\cdot s}\n  .\nBob sends the ciphertext \n  \n    \n      \n        (\n        \n          c\n          \n            1\n          \n        \n        ,\n        \n          c\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (c_{1},c_{2})}\n   to Alice.Note that if one knows both the ciphertext \n  \n    \n      \n        (\n        \n          c\n          \n            1\n          \n        \n        ,\n        \n          c\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (c_{1},c_{2})}\n   and the plaintext \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , one can easily find the shared secret \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , since \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        \u22c5\n        \n          m\n          \n            \u2212\n            1\n          \n        \n        =\n        s\n      \n    \n    {\\displaystyle c_{2}\\cdot m^{-1}=s}\n  . Therefore, a new \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and hence a new \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   is generated for every message to improve security. For this reason, \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   is also called an ephemeral key.","completion":"Cryptography"}
{"prompt":"Both the block size of the hash function and the output size are completely scalable.\nThe speed can be adjusted by adjusting the number of bitwise operations used by FSB per input bit.\nThe security can be adjusted by adjusting the output size.\nBad instances exist and one must take care when choosing the matrix \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  .\nThe matrix used in the compression function may grow large in certain situations. This might be a limitation when trying to use FSB on memory constrained devices. This problem was solved in the related hash function called Improved FSB, which is still provably secure, but relies on slightly stronger assumptions.","completion":"Cryptography"}
{"prompt":"The main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter some fixed number of positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (c.\u20091900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.\nThe Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military). Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair. More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.\nIn India, the 2000-year-old Kamasutra of V\u0101tsy\u0101yana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.In Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the \u0161\u0101h-dab\u012br\u012bya (literally \"King's script\") which was used for official correspondence, and the r\u0101z-sahar\u012bya which was used to communicate secret messages with other countries.David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\nCiphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as Alkindus) in the 9th century, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.\nLanguage letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.\nEssentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel that implemented a partial realization of his invention. In the Vigen\u00e8re cipher, a polyalphabetic cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigen\u00e8re cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim\u2014'the enemy knows the system'.\nDifferent physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher. In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption\/decryption devices were invented early in the 20th century, and several patented, among them rotor machines\u2014famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.","completion":"Cryptography"}
{"prompt":"Notable uses of Ed25519 include OpenSSH, GnuPG and various alternatives, and the signify tool by OpenBSD. Usage of Ed25519 (and Ed448) in the SSH protocol has been standardized. In 2023 the final version of the FIPS 186-5 standard included deterministic Ed25519 as an approved signature scheme.\nApple Watch and iPhone use Ed25519 keys for IKEv2 mutual authentication\nBotan\nCryptoNote cryptocurrency protocol\nDropbear SSH\nI2Pd implementation of EdDSA\nJava Development Kit 15\nLibgcrypt\nMinisign and Minisign Miscellanea for macOS\nNaCl \/ libsodium\nOpenSSL 1.1.1\nPython - A slow but concise alternate implementation, does not include side-channel attack protection\nSupercop reference implementation (C language with inline assembler)\nVirgil PKI uses Ed25519 keys by default\nwolfSSL","completion":"Cryptography"}
{"prompt":"In October 2017, Deputy Attorney General Rod Rosenstein called for key escrow under the euphemism \"responsible encryption\" as a solution to the ongoing problem of \"going dark\". This refers to wiretapping court orders and police measures becoming ineffective as strong end-to-end encryption is increasingly added to widespread messenger products. Rosenstein suggested key escrow would provide their customers with a way to recover their encrypted data if they forget their password, so that it is not lost forever. From a law enforcement perspective, this would allow a judge to issue a search warrant instructing the company to decrypt the data; without escrow or other undermining of encryption it is impossible for a service provider to comply with this request. In contrast to previous proposals, the decentralized storage of keys by companies instead of government agencies is claimed to be an additional safeguard.","completion":"Cryptography"}
{"prompt":"Identity-based systems allow any party to generate a public key from a known identity value, such as an ASCII string.  A trusted third party, called the private key generator (PKG), generates the corresponding private keys.  To operate, the PKG first publishes a master public key, and retains the corresponding master private key (referred to as master key).  Given the master public key, any party can compute a public key corresponding to the identity ID by combining the master public key with the identity value.  To obtain a corresponding private key, the party authorized to use the identity ID contacts the PKG, which uses the master private key to generate the private key for the identity ID.","completion":"Cryptography"}
{"prompt":"Although Camellia is patented, it is available under a royalty-free license.  This has allowed the Camellia cipher to become part of the OpenSSL Project, under an open-source license, since November 2006. It has also allowed it to become part of the Mozilla's NSS (Network Security Services) module.","completion":"Cryptography"}
{"prompt":"Significant books on cryptography include:\n\nAumasson, Jean-Philippe (2017), Serious Cryptography: A Practical Introduction to Modern Encryption. No Starch Press, 2017, ISBN 9781593278267.[1] Presents modern cryptography in a readable way, suitable for practitioners, software engineers, and others who want to learn practice-oriented cryptography. Each chapter includes a discussion of common implementation mistakes using real-world examples and details what could go wrong and how to avoid these pitfalls.\nAumasson, Jean-Philippe (2021), Crypto Dictionary: 500 Tasty Tidbits for the Curious Cryptographer. No Starch Press, 2021, ISBN 9781718501409.[2] Ultimate desktop dictionary with hundreds of definitions organized alphabetically for all things cryptographic. The book also includes discussions of the threat that quantum computing is posing to current cryptosystems and a nod to post-quantum algorithms, such as lattice-based cryptographic schemes.\nBertram, Linda A. \/ Dooble, Gunther van: Transformation of Cryptography - Fundamental concepts of Encryption, Milestones, Mega-Trends and sustainable Change in regard to Secret Communications and its Nomenclatura, 2019, ISBN 978-3749450749.\nCandela, Rosario (1938). The Military Cipher of Commandant Bazeries. New York: Cardanus Press, This book detailed the cracking of a famous code from 1898 created by Commandant Bazeries, a brilliant French Army Cryptanalyst.\nFalconer, John (1685). Cryptomenysis Patefacta, or Art of Secret Information Disclosed Without a Key. One of the earliest English texts on cryptography.\nFerguson, Niels, and Schneier, Bruce (2003). Practical Cryptography, Wiley, ISBN 0-471-22357-3.  A cryptosystem design consideration primer. Covers both algorithms and protocols. This is an in-depth consideration of one cryptographic problem, including paths not taken and some reasons why. At the time of its publication, most of the material was not otherwise available in a single source. Some was not otherwise available at all. According to the authors, it is (in some sense) a follow-up to Applied Cryptography.\nGaines, Helen Fouch\u00e9 (1939). Cryptanalysis, Dover, ISBN 0-486-20097-3. Considered one of the classic books on the subject, and includes many sample ciphertext for practice. It reflects public amateur practice as of the inter-War period. The book was compiled as one of the first projects of the American Cryptogram Association.\nGoldreich, Oded (2001 and 2004). Foundations of Cryptography. Cambridge University Press. Presents the theoretical foundations of cryptography in a detailed and comprehensive manner. A must-read for anyone interested in the theory of cryptography.\nKatz, Jonathan and Lindell, Yehuda (2007 and 2014). Introduction to Modern Cryptography, CRC Press. Presents modern cryptography at a level appropriate for undergraduates, graduate students, or practitioners. Assumes mathematical maturity but presents all the necessary mathematical and computer science background.\nKonheim, Alan G. (1981). Cryptography: A Primer, John Wiley & Sons, ISBN 0-471-08132-9. Written by one of the IBM team who developed DES.\nMao, Wenbo (2004). Modern Cryptography Theory and Practice ISBN 0-13-066943-1. An up-to-date book on cryptography. Touches on provable security, and written with students and practitioners in mind.\nMel, H.X., and Baker, Doris (2001). Cryptography Decrypted, Addison Wesley ISBN 0-201-61647-5. This technical overview of basic cryptographic components (including extensive diagrams and graphics) explains the evolution of cryptography from the simplest concepts to some modern concepts. It details the basics of symmetric key, and asymmetric key ciphers, MACs, SSL, secure mail and IPsec. No math background is required, though there's some coverage of the mathematics underlying public key\/private key crypto in the appendix.\nA. J. Menezes, P. C. van Oorschot, and S. A. Vanstone (1996) Handbook of Applied Cryptography ISBN 0-8493-8523-7. Equivalent to Applied Cryptography in many ways, but somewhat more mathematical. For the technically inclined. Covers few meta-cryptographic topics, such as crypto system design. This is currently (2004) regarded as the standard reference work in technical cryptography.\nPaar, Christof and Jan Pelzl (2009). Understanding Cryptography: A Textbook for Students and Practitioners, Springer, ISBN 978-3-642-04100-6. Very accessible introduction to applied cryptography which covers most schemes of practical relevance. The focus is on being a textbook, i.e., it has pedagogical approach, many problems and further reading sections. The main target audience are readers without a background in pure mathematics.\nPatterson, Wayne (1987). Mathematical Cryptology for Computer Scientists and Mathematicians, Rowman & Littlefield, ISBN 0-8476-7438-X\nRosulek, Mike  (2018). The Joy of Cryptography Presents modern cryptography at a level appropriate for undergraduates.\nSchneier, Bruce (1996). Applied Cryptography, 2 ed, Wiley, (ISBN 0-471-11709-9).  Survey of mostly obsolete cryptography with some commentary on 1990s legal environment.  Aimed at engineers without mathematical background, including source code for obsolete ciphers.  Lacks guidance for choosing cryptographic components and combining them into protocols and engineered systems.  Contemporaneously influential on a generation of engineers, hackers, and cryptographers.  Supplanted by Cryptography Engineering.\nSmart, Nigel (2004). Cryptography: An introduction ISBN 0-07-709987-7. Similar in intent to Applied Cryptography but less comprehensive. Covers more modern material and is aimed at undergraduates covering topics such as number theory and group theory not generally covered in cryptography books.\nStinson, Douglas (2005). Cryptography: Theory and Practice ISBN 1-58488-508-4.  Covers topics in a textbook style but with more mathematical detail than is usual.\nYoung, Adam L. and  Moti Yung (2004). Malicious Cryptography: Exposing Cryptovirology, ISBN 0764568469, ISBN 9780764568466, John Wiley & Sons. Covers topics regarding use of cryptography as an attack tool in systems as was introduced in the 1990s: Kleptography which deals with hidden subversion of cryptosystems, and, more generally, Cryptovirology which predicted Ransomware in which cryptography is used as a tool to disable computing systems, in a way that is reversible only by the attacker, generally requiring ransom payment(s).\nWashington, Lawrence C. (2003). Elliptic Curves: Number Theory and Cryptography ISBN 1-58488-365-0.  A book focusing on elliptic curves, beginning at an undergraduate level (at least for those who have had a course on abstract algebra), and progressing into much more advanced topics, even at the end touching on Andrew Wiles' proof of the Taniyama\u2013Shimura conjecture which led to the proof of Fermat's Last Theorem.\nWelsh, Dominic (1988). Codes and Cryptography, Oxford University Press, A brief textbook intended for undergraduates. Some coverage of fundamental information theory. Requires some mathematical maturity; is well written, and otherwise accessible.","completion":"Cryptography"}
{"prompt":"In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on \"suicide watch\". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.","completion":"Cryptography"}
{"prompt":"Crypto-microeconomics is concerned with the individual and enterprise usages of cryptocurrencies and DeFi transactions. A strong majority of USA adults have heard about major cryptocurrencies (Bitcoin, Ether), and 16% say they personally have invested in, traded, or otherwise used one. More than 300 million people use cryptocurrency worldwide, and approximately 46 million Americans have invested in Bitcoin.","completion":"Cryptography"}
{"prompt":"Cryptographic techniques enable cryptocurrency technologies, such as distributed ledger technologies (e.g., blockchains), which finance cryptoeconomics applications such as decentralized finance (DeFi). Key cryptographic techniques that enable cryptocurrencies and cryptoeconomics include, but are not limited to: cryptographic keys, cryptographic hash function, asymmetric (public key) encryption, Multi-Factor Authentication (MFA), End-to-End Encryption (E2EE), and Zero Knowledge Proofs (ZKP).","completion":"Cryptography"}
{"prompt":"Most cryptographic applications require random numbers, for example:\n\nkey generation\nnonces\nsalts in certain signature schemes, including ECDSA, RSASSA-PSSThe \"quality\" of the randomness required for these applications varies.\nFor example, creating a nonce in some protocols needs only uniqueness.\nOn the other hand, the generation of a master key requires a higher quality, such as more entropy. And in the case of one-time pads, the information-theoretic guarantee of perfect secrecy only holds if the key material comes from a true random source with high entropy, and thus any kind of pseudorandom number generator is insufficient.\nIdeally, the generation of random numbers in CSPRNGs uses entropy obtained from a high-quality source, generally the operating system's randomness API. However, unexpected correlations have been found in several such ostensibly independent processes. From an information-theoretic point of view, the amount of randomness, the entropy that can be generated, is equal to the entropy provided by the system. But sometimes, in practical situations, more random numbers are needed than there is entropy available. Also, the processes to extract randomness from a running system are slow in actual practice.  In such instances, a CSPRNG can sometimes be used. A CSPRNG can \"stretch\" the available entropy over more bits.","completion":"Cryptography"}
{"prompt":"Entering a PIN code to activate the smart card commonly requires a numeric keypad. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a keystroke logger, potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often EAL3 certified.","completion":"Cryptography"}
{"prompt":"FSB is a speed-up version of syndrome-based hash function (SB). In the case of SB the compression function is very similar to the encoding function of Niederreiter's version of McEliece cryptosystem. Instead of using the parity check matrix of a permuted Goppa code, SB uses a random matrix \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  . From the security point of view this can only strengthen the system.","completion":"Cryptography"}
{"prompt":"Schneier has criticized security approaches that try to prevent any malicious incursion, instead arguing that designing systems to fail well is more important. The designer of a system should not underestimate the capabilities of an attacker, as technology may make it possible in the future to do things that are not possible at the present. Under Kerckhoffs's Principle, the need for one or more parts of a cryptographic system to remain secret increases the fragility of the system; whether details about a system should be obscured depends upon the availability of persons who can make use of the information for beneficial uses versus the potential for attackers to misuse the information.\nSecrecy and security aren't the same, even though it may seem that way. Only bad security relies on secrecy; good security works even if all the details of it are public.","completion":"Cryptography"}
{"prompt":"In the Arab world al-Farahidi had become a household name by the time he died, and become almost as mythic a figure as Abu al-Aswad al-Du'ali in Arabic philology. He was the first to codify the complex metres of Arabic poetry, and an outstanding genius of the Muslim world. Sibawayh and al-Asma'i were among his students, with the former having been more indebted to al-Farahidi than to any other teacher. Ibn al-Nadim, the 10th-century bibliophile biographer from Basra, reports that in fact Sibawayh's \"Kitab\" (Book), was a collaborative work of forty-two authors, but also that the principles and subjects in the \"Kitab\" were based on those of al-Farahidi. He is quoted by Sibawayh 608 times, more than any other authority. Throughout the Kitab Sibawayh says \"I asked him\" or \"he said\", without naming the person referred to by the pronoun, however, it is clear that he refers to al-Farahidi. Both the latter and the former are historically the earliest and most significant figures in the formal recording of the Arabic language.Al-Farahidi was also well versed in astronomy, mathematics, Islamic law, music theory and Muslim prophetic tradition. His prowess in the Arabic language was said to be drawn, first and foremost, from his vast knowledge of Muslim prophetic tradition as well as exegesis of the Qur'an. The Al Khalil Bin Ahmed Al Farahidi School of Basic Education in Rustaq, Oman is named after him.","completion":"Cryptography"}
{"prompt":"CrypTool has received several international awards as an educational program, such as the TeleTrusT Special Award 2004, EISA 2004, IT Security Award NRW 2004, and Selected Landmark in the Land of Ideas 2008 award.","completion":"Cryptography"}
{"prompt":"Barak et al. (2001) proved that an inefficient indistinguishability obfuscator exists for circuits; that is, the lexicographically first circuit that computes the same function. If P = NP holds, then an indistinguishability obfuscator exists, even though no other kind of cryptography would also exist.A candidate construction of IO with provable security under concrete hardness assumptions relating to multilinear maps was published by Garg et al. (2013), but this assumption was later invalidated. (Previously, Garg, Gentry, and Halevi (2012) had constructed a candidate version of a multilinear map based on heuristic assumptions.)\nStarting from 2016, Lin began to explore constructions of IO based on less strict versions of multilinear maps, constructing a candidate based on maps of degree up to 30, and eventually a candidate based on maps of degree up to 3. Finally, in 2020, Jain, Lin, and Sahai proposed a construction of IO based on the symmetric external Diffie-Helman, learning with errors, and learning plus noise assumptions, as well as the existence of a super-linear stretch pseudorandom generator in the function class NC0. (The existence of pseudorandom generators in NC0 (even with sub-linear stretch) was a long-standing open problem until 2006.) It is possible that this construction could be broken with quantum computing, but there is an alternative construction that may be secure even against that (although the latter relies on less established security assumptions).","completion":"Cryptography"}
{"prompt":"IPsec Virtual Private Network (VPN) and more\nIEEE P1363 covers most aspects of public-key cryptography\nTransport Layer Security (formerly SSL)\nSSH secure Telnet and more\nContent Scrambling System (CSS, the DVD encryption standard, broken by DeCSS)\nKerberos authentication standard\nRADIUS authentication standard\nANSI X9.59 electronic payment standard\nCommon Criteria Trusted operating system standard\nCRYPTREC Japanese Government's cryptography recommendations","completion":"Cryptography"}
{"prompt":"No childhood home of Poe is still standing, including the Allan family's Moldavia estate. The oldest standing home in Richmond, the Old Stone House, is in use as the Edgar Allan Poe Museum, though Poe never lived there. The collection includes many items that Poe used during his time with the Allan family, and also features several rare first printings of Poe works. 13 West Range is the dorm room that Poe is believed to have used while studying at the University of Virginia in 1826; it is preserved and available for visits. Its upkeep is overseen by a group of students and staff known as the Raven Society.The earliest surviving home in which Poe lived is at 203 North Amity St. in Baltimore, which is preserved as the Edgar Allan Poe House and Museum. Poe is believed to have lived in the home at the age of 23 when he first lived with Maria Clemm and Virginia and possibly his grandmother and possibly his brother William Henry Leonard Poe. It is open to the public and is also the home of the Edgar Allan Poe Society.\nWhile in Philadelphia between 1838 and 1844, Poe lived at at least four different residences, including the Indian Queen Hotel at 15 S. 4th Street, at a residence at 16th and Locust Streets, at 2502 Fairmount Street, and then in the Spring Garden section of the city at 532 N. 7th Street, a residence that has been preserved by the National Park Service as the Edgar Allan Poe National Historic Site. Poe's final home in Bronx, New York City, is preserved as the Edgar Allan Poe Cottage.In Boston, a commemorative plaque on Boylston Street is several blocks away from the actual location of Poe's birth. The house which was his birthplace at 62 Carver Street no longer exists; also, the street has since been renamed \"Charles Street South\". A \"square\" at the intersection of Broadway, Fayette, and Carver Streets had once been named in his honor, but it disappeared when the streets were rearranged. In 2009, the intersection of Charles and Boylston Streets (two blocks north of his birthplace) was designated \"Edgar Allan Poe Square\".In March 2014, fundraising was completed for construction of a permanent memorial sculpture, known as Poe Returning to Boston, at this location. The winning design by Stefanie Rocknak depicts a life-sized Poe striding against the wind, accompanied by a flying raven; his suitcase lid has fallen open, leaving a \"paper trail\" of literary works embedded in the sidewalk behind him. The public unveiling on October 5, 2014, was attended by former U.S. poet laureate Robert Pinsky.Other Poe landmarks include a building on the Upper West Side, where Poe temporarily lived when he first moved to New York City. A plaque suggests that Poe wrote \"The Raven\" here. On Sullivan's Island in Charleston County, South Carolina, the setting of Poe's tale \"The Gold-Bug\" and where Poe served in the Army in 1827 at Fort Moultrie, there is a restaurant called Poe's Tavern. In the Fell's Point section of Baltimore, a bar still stands where legend says that Poe was last seen drinking before his death. Known as \"The Horse You Came in On\", local lore insists that a ghost whom they call \"Edgar\" haunts the rooms above.","completion":"Cryptography"}
{"prompt":"Cryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible.\nIn some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.","completion":"Cryptography"}
{"prompt":"Although cryptography has a long and complex history, it wasn't until the 19th century that it developed anything more than ad hoc approaches to either encryption or cryptanalysis (the science of finding weaknesses in crypto systems). Examples of the latter include Charles Babbage's Crimean War era work on mathematical cryptanalysis of polyalphabetic ciphers, redeveloped and published somewhat later by the Prussian Friedrich Kasiski.  Understanding of cryptography at this time typically consisted of hard-won rules of thumb; see, for example, Auguste Kerckhoffs' cryptographic writings in the latter 19th century.  Edgar Allan Poe used systematic methods to solve ciphers in the 1840s. In particular he placed a notice of his abilities in the Philadelphia paper Alexander's Weekly (Express) Messenger, inviting submissions of ciphers, most of which he proceeded to solve. His success created a public stir for some months. He later wrote an essay on methods of cryptography which proved useful as an introduction for novice British cryptanalysts attempting to break German codes and ciphers during World War I, and a famous story, The Gold-Bug, in which cryptanalysis was a prominent element.\nCryptography, and its misuse, were involved in the execution of Mata Hari and in Dreyfus' conviction and imprisonment, both in the early 20th century. Cryptographers were also involved in exposing the machinations which had led to the Dreyfus affair; Mata Hari, in contrast, was shot.\nIn World War I the Admiralty's Room 40 broke German naval codes and played an important role in several naval engagements during the war, notably in detecting major German sorties into the North Sea that led to the battles of Dogger Bank and Jutland as the British fleet was sent out to intercept them. However, its most important contribution was probably in decrypting the Zimmermann Telegram, a cable from the German Foreign Office sent via Washington to its ambassador Heinrich von Eckardt in Mexico which played a major part in bringing the United States into the war.\nIn 1917, Gilbert Vernam proposed a teleprinter cipher in which a previously prepared key, kept on paper tape, is combined character by character with the plaintext message to produce the cyphertext. This led to the development of electromechanical devices as cipher machines, and to the only unbreakable cipher, the one time pad.\nDuring the 1920s, Polish naval-officers assisted the Japanese military with code and cipher development.\nMathematical methods proliferated in the period prior to World War II (notably in William F. Friedman's application of statistical techniques to cryptanalysis and cipher development and in Marian Rejewski's initial break into the German Army's version of the Enigma system in 1932).","completion":"Cryptography"}
{"prompt":"Diffusion means that if we change a single bit of the plaintext, then about half of the bits in the ciphertext should change, and similarly, if we change one bit of the ciphertext, then about half of the plaintext bits should change. This is equivalent to the expectation that encryption schemes exhibit an avalanche effect.\nThe purpose of diffusion is to hide the statistical relationship between the ciphertext and the plain text. For example, diffusion ensures that any patterns in the plaintext, such as redundant bits, are not apparent in the ciphertext. Block ciphers achieve this by \"diffusing\" the information about the plaintext's structure across the rows and columns of the cipher.\nIn substitution\u2013permutation networks, diffusion is provided by permutation boxes (a.k.a. permutation layer). In the beginning of the 21st century a consensus had appeared where the designers preferred the permutation layer to consist of linear Boolean functions, although nonlinear functions can be used, too.","completion":"Cryptography"}
{"prompt":"The development of CrypTool started in 1998. Originally developed by German companies and universities, it is an open-source project since 2001. More than sixty people worldwide contribute regularly to the project. Contributions as software plugins came from universities or schools in the following towns: Belgrad, Berlin, Bochum, Brisbane, Darmstadt, Dubai, Duisburg-Essen, Eindhoven, Hagenberg, Jena, Kassel, Klagenfurt, Koblenz, London, Madrid, Mannheim, San Jose, Siegen, Utrecht, Warsaw.Currently 4 versions of CrypTool are maintained and developed: The CrypTool 1 (CT1) software is available in 6 languages (English, German, Polish, Spanish, Serbian, and French). CrypTool 2 (CT2) is available in 3 languages (English, German, Russian). All others, JCrypTool (JCT) and CrypTool-Online (CTO), are available only in English and German.The goal of the CrypTool project is to make users aware of how cryptography can help against network security threats and to explain the underlying concepts of cryptology.CrypTool 1 (CT1) is written in C++ and designed for the Microsoft Windows operating system. A port of CT1 to Linux with Qt4 was started, but there is no progress anymore.In 2007, development began on two additional projects, both based on a pure-plugin architecture, to serve as successors to the original CrypTool program. Both successors regularly publish new stable versions:\n\nCrypTool 2 (built with C#\/.NET\/WPF) (abbreviated CT2)uses the concept of visual programming to clarify cryptographic processes. Currently, CT2 contains more than 150 crypto functions.\n\nJCrypTool 1.0 (built with Java\/Eclipse\/RCP\/SWT) (abbreviated JCT)runs on Windows, macOS, and Linux, and offers both a document-centric and a function-centric perspective. Currently, JCT contains more than 100 crypto functions. One of its focal points are modern digital signatures (like Merkle trees and SPHINCS).","completion":"Cryptography"}
{"prompt":"Camellia is a Feistel cipher with either 18 rounds (when using 128-bit keys) or 24 rounds (when using 192- or 256-bit keys). Every six rounds, a logical transformation layer is applied: the so-called \"FL-function\" or its inverse. Camellia uses four 8\u00d78-bit S-boxes with input and output affine transformations and logical operations. The cipher also uses input and output key whitening. The diffusion layer uses a linear transformation based on a matrix with a branch number of 5.","completion":"Cryptography"}
{"prompt":"For a cubic curve not in Weierstrass normal form, we can still define a group structure by designating one of its nine inflection points as the identity O. In the projective plane, each line will intersect a cubic at three points when accounting for multiplicity. For a point P, \u2212P is defined as the unique third point on the line passing through O and P. Then, for any P and Q, P + Q is defined as \u2212R where R is the unique third point on the line containing P and Q.\nFor an example of the group law over a non-Weierstrass curve, see Hessian curves.","completion":"Cryptography"}
{"prompt":"Although the formal definition of an elliptic curve requires some background in algebraic geometry, it is possible to describe some features of elliptic curves over the real numbers using only introductory algebra and geometry.\nIn this context, an elliptic curve is a plane curve defined by an equation of the form\n\n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            3\n          \n        \n        +\n        a\n        x\n        +\n        b\n      \n    \n    {\\displaystyle y^{2}=x^{3}+ax+b}\n  after a linear change of variables (a and b are real numbers). This type of equation is called a Weierstrass equation, and said to be in Weierstrass form, or Weierstrass normal form.\nThe definition of elliptic curve also requires that the curve be non-singular. Geometrically, this means that the graph has no cusps, self-intersections, or isolated points. Algebraically, this holds if and only if the discriminant, \n  \n    \n      \n        \u0394\n      \n    \n    {\\displaystyle \\Delta }\n  , is not equal to zero.\n\n  \n    \n      \n        \u0394\n        =\n        \u2212\n        16\n        \n          (\n          \n            4\n            \n              a\n              \n                3\n              \n            \n            +\n            27\n            \n              b\n              \n                2\n              \n            \n          \n          )\n        \n        \u2260\n        0\n      \n    \n    {\\displaystyle \\Delta =-16\\left(4a^{3}+27b^{2}\\right)\\neq 0}\n  (Although the factor \u221216 is irrelevant to whether or not the curve is non-singular, this definition of the discriminant is useful in a more advanced study of elliptic curves.)The real graph of a non-singular curve has two components if its discriminant is positive, and one component if it is negative. For example, in the graphs shown in figure to the right, the discriminant in the first case is 64, and in the second case is \u2212368.","completion":"Cryptography"}
{"prompt":"In the special case where b is the identity element 1 of the group G, the discrete logarithm logb\u2009a is undefined for a other than 1, and every integer k is a discrete logarithm for a = 1.","completion":"Cryptography"}
{"prompt":"In a treatise entitled as Risala fi l-Illa al-Failali l-Madd wa l-Fazr (Treatise on the Efficient Cause of the Flow and Ebb), al-Kindi presents a theory on tides which \"depends on the changes which take place in bodies owing to the rise and fall of temperature.\" In order to support his argument, he gave a description of a scientific experiment as follows:\n\nOne can also observe by the senses... how in consequence of extreme cold air changes into water. To do this, one takes a glass bottle, fills it completely with snow, and closes its end carefully. Then one determines its weight by weighing. One places it in a container... which has previously been weighed. On the surface of the bottle the air changes into water, and appears upon it like the drops on large porous pitchers, so that a considerable amount of water gradually collects inside the container. One then weighs the bottle, the water and the container, and finds their weight greater than previously, which proves the change. [...] Some foolish persons are of opinion that the snow exudes through the glass. This is impossible. There is no process by which water or snow can be made to pass through glass.\nIn explaining the natural cause of the wind, and the difference for its directions based on time and location, he wrote:\nWhen the sun is in its northern declination northerly places will heat up and it will be cold towards the south. Then the northern air will expand in a southerly direction because of the heat due to the contraction of the southern air. Therefore most of the summer winds are merits and most of the winter winds are not.","completion":"Cryptography"}
{"prompt":"Synthetic initialization vector (SIV) is a nonce-misuse resistant block cipher mode.\nSIV synthesizes an internal IV using the pseudorandom function S2V. S2V is a keyed hash is based on CMAC, and the input to the function is:\n\nAdditional authenticated data (zero, one or many AAD fields are supported)\nPlaintext\nAuthentication key (K1).SIV encrypts the S2V output and the plaintext using AES-CTR, keyed with the encryption key (K2).\nSIV can support external nonce-based authenticated encryption, in which case one of the authenticated data fields is utilized for this purpose. RFC5297 specifies that for interoperability purposes the last authenticated data field should be used external nonce.\nOwing to the use of two keys, the authentication key K1 and encryption key K2, naming schemes for SIV AEAD-variants may lead to some confusion; for example AEAD_AES_SIV_CMAC_256 refers to AES-SIV with two AES-128 keys and not AES-256.","completion":"Cryptography"}
{"prompt":"Kitab al-Ayn was the first dictionary written for the Arabic language. \"Ayn\" is the deepest letter in Arabic, and \"ayn\" may also mean a water source in the desert. Its title, \"the source\", reflects its author's goal to derive the etymological origins of Arabic vocabulary and lexicography.","completion":"Cryptography"}
{"prompt":"The CrypTool project also includes the website CrypTool-Online, launched in 2009. This website allows users to try cryptographic methods directly within a browser on a PC or on a smartphone (using JavaScript), without the need to download and install software. \nThis site aims to present the topic in an easy and attractive way for new users and young people.  Advanced tasks still require the offline versions of CrypTool.","completion":"Cryptography"}
{"prompt":"On 17 March 1975, the proposed DES was published in the Federal Register. Public comments were requested, and in the following year two open workshops were held to discuss the proposed standard. There was criticism received from public-key cryptography pioneers Martin Hellman and Whitfield Diffie, citing a shortened key length and the mysterious \"S-boxes\" as evidence of improper interference from the NSA. The suspicion was that the algorithm had been covertly weakened by the intelligence agency so that they\u2014but no one else\u2014could easily read encrypted messages. Alan Konheim (one of the designers of DES) commented, \"We sent the S-boxes off to Washington. They came back and were all different.\" The United States Senate Select Committee on Intelligence reviewed the NSA's actions to determine whether there had been any improper involvement. In the unclassified summary of their findings, published in 1978, the Committee wrote:\n\nIn the development of DES, NSA convinced IBM that a reduced key size was sufficient; indirectly assisted in the development of the S-box structures; and certified that the final DES algorithm was, to the best of their knowledge, free from any statistical or mathematical weakness.\nHowever, it also found that\n\nNSA did not tamper with the design of the algorithm in any way. IBM invented and designed the algorithm, made all pertinent decisions regarding it, and concurred that the agreed upon key size was more than adequate for all commercial applications for which the DES was intended.\nAnother member of the DES team, Walter Tuchman, stated \"We developed the DES algorithm entirely within IBM using IBMers. The NSA did not dictate a single wire!\"\nIn contrast, a declassified NSA book on cryptologic history states:\n\nIn 1973 NBS solicited private industry for a data encryption standard (DES). The first offerings were disappointing, so NSA began working on its own algorithm. Then Howard Rosenblum, deputy director for research and engineering, discovered that Walter Tuchman of IBM was working on a modification to Lucifer for general use. NSA gave Tuchman a clearance and brought him in to work jointly with the Agency on his Lucifer modification.\"\nand\n\nNSA worked closely with IBM to strengthen the algorithm against all except brute-force attacks and to strengthen substitution tables, called S-boxes. Conversely, NSA tried to convince IBM to reduce the length of the key from 64 to 48 bits. Ultimately they compromised on a 56-bit key.\nSome of the suspicions about hidden weaknesses in the S-boxes were allayed in 1990, with the independent discovery and open publication by Eli Biham and Adi Shamir of differential cryptanalysis, a general method for breaking block ciphers. The S-boxes of DES were much more resistant to the attack than if they had been chosen at random, strongly suggesting that IBM knew about the technique in the 1970s. This was indeed the case; in 1994, Don Coppersmith published some of the original design criteria for the S-boxes. According to Steven Levy, IBM Watson researchers discovered differential cryptanalytic attacks in 1974 and were asked by the NSA to keep the technique secret. Coppersmith explains IBM's secrecy decision by saying, \"that was because [differential cryptanalysis] can be a very powerful tool, used against many schemes, and there was concern that such information in the public domain could adversely affect national security.\" Levy quotes Walter Tuchman: \"[t]hey asked us to stamp all our documents confidential... We actually put a number on each one and locked them up in safes, because they were considered U.S. government classified. They said do it. So I did it\". Bruce Schneier observed that \"It took the academic community two decades to figure out that the NSA 'tweaks' actually improved the security of DES.\"","completion":"Cryptography"}
{"prompt":"Ed25519 is the EdDSA signature scheme using SHA-512 (SHA-2) and Curve25519 where\n\n  \n    \n      \n        q\n        =\n        \n          2\n          \n            255\n          \n        \n        \u2212\n        19\n        ,\n      \n    \n    {\\displaystyle q=2^{255}-19,}\n  \n\n  \n    \n      \n        E\n        \n          \/\n        \n        \n          \n            F\n          \n          \n            q\n          \n        \n      \n    \n    {\\displaystyle E\/\\mathbb {F} _{q}}\n   is the twisted Edwards curve\n\n  \n    \n      \n        \u2113\n        =\n        \n          2\n          \n            252\n          \n        \n        +\n        27742317777372353535851937790883648493\n      \n    \n    {\\displaystyle \\ell =2^{252}+27742317777372353535851937790883648493}\n   and \n  \n    \n      \n        c\n        =\n        3\n      \n    \n    {\\displaystyle c=3}\n  \n\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is the unique point in \n  \n    \n      \n        E\n        (\n        \n          \n            F\n          \n          \n            q\n          \n        \n        )\n      \n    \n    {\\displaystyle E(\\mathbb {F} _{q})}\n   whose \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   coordinate is \n  \n    \n      \n        4\n        \n          \/\n        \n        5\n      \n    \n    {\\displaystyle 4\/5}\n   and whose \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   coordinate is positive.\"positive\" is defined in terms of bit-encoding:\n\"positive\" coordinates are even coordinates (least significant bit is cleared)\n\"negative\" coordinates are odd coordinates (least significant bit is set)\n\n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   is SHA-512, with \n  \n    \n      \n        b\n        =\n        256\n      \n    \n    {\\displaystyle b=256}\n  .The curve \n  \n    \n      \n        E\n        (\n        \n          \n            F\n          \n          \n            q\n          \n        \n        )\n      \n    \n    {\\displaystyle E(\\mathbb {F} _{q})}\n   is birationally equivalent to the Montgomery curve known as Curve25519. The equivalence is","completion":"Cryptography"}
{"prompt":"The goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.\nIt is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible.\nThere are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved).\n\nCryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations. This is a considerable improvement over brute force attacks.\nPublic-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time (P) using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, to achieve an equivalent strength of encryption, techniques that depend upon the difficulty of factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.\nWhile pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, they may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, rubber-hose cryptanalysis or torture) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin.","completion":"Cryptography"}
{"prompt":"The Caesar Cipher is one of the earliest known cryptographic systems. Julius Caesar used a cipher that shifts the letters in the alphabet in place by three and wrapping the remaining letters to the front to write to Marcus Tullius Cicero in approximately 50 BC.[11]Historical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as ROT13) and transposition ciphers (such as a Rail Fence Cipher). For example, \"GOOD DOG\" can be encrypted as \"PLLX XLP\" where \"L\" substitutes for \"O\", \"P\" for \"G\", and \"X\" for \"D\" in the message.  Transposition of the letters \"GOOD DOG\" can result in \"DGOGDOO\". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.William Shakespeare often used the concept of ciphers in his writing to symbolize nothingness. In Shakespeare's Henry V, he relates one of the accounting methods that brought the Arabic Numeral system and zero to Europe, to the human imagination. The actors who perform this play were not at the battles of Henry V's reign, so they represent absence. In another sense, ciphers are important to people who work with numbers, but they do not hold value. Shakespeare used this concept to outline how those who counted and identified the dead from the battles used that information as a political weapon, furthering class biases and xenophobia.In the 1640s, the Parliamentarian commander, Edward Montagu, 2nd Earl of Manchester, developed ciphers to send coded messages to his allies during the English Civil War.Simple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigen\u00e8re) which changed the substitution alphabet for every letter.  For example, \"GOOD DOG\" can be encrypted as \"PLSX TWF\" where \"L\", \"S\", and \"W\" substitute for \"O\".  With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.\nDuring the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of \"additive\" substitution.  In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.","completion":"Cryptography"}
{"prompt":"Other operations often used in block ciphers include data-dependent rotations as in RC5 and RC6, a substitution box implemented as a lookup table as in Data Encryption Standard and Advanced Encryption Standard, a permutation box, and multiplication as in IDEA.","completion":"Cryptography"}
{"prompt":"Let G be any group. Denote its group operation by multiplication and its identity element by 1. Let b be any element of G. For any positive integer k, the expression bk denotes the product of b with itself k times:\n\n  \n    \n      \n        \n          b\n          \n            k\n          \n        \n        =\n        \n          \n            \n              \n                b\n                \u22c5\n                b\n                \u22ef\n                b\n              \n              \u23df\n            \n          \n          \n            k\n            \n            \n              factors\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle b^{k}=\\underbrace {b\\cdot b\\cdots b} _{k\\;{\\text{factors}}}.}\n  Similarly, let b\u2212k denote the product of b\u22121 with itself k times. For k = 0, the kth power is the identity: b0 = 1.\nLet a also be an element of G. An integer k that solves the equation bk = a is termed a discrete logarithm (or simply logarithm, in this context) of a to the base b. One writes k = logb\u2009a.","completion":"Cryptography"}
{"prompt":"One of the main differences between a cloud based digital signature service and a locally provided one is risk.  Many risk averse companies, including governments, financial and medical institutions,  and payment processors require more secure standards, like FIPS 140-2 level 3 and FIPS 201 certification, to ensure the signature is validated and secure.","completion":"Cryptography"}
{"prompt":"During World War II, Japan used a cipher  machine for diplomatic communications; the United States was able to crack it and read its messages, mostly because the \"key values\" used were insufficiently random.","completion":"Cryptography"}
{"prompt":"Widely used theoretical encryption schemes are mathematically secure, yet this type of security does not consider their physical implementations, and thus, do not necessarily protect against side-channel attacks. Therefore, the vulnerability lies in the code itself, and it is the specific implementation that is shown to be insecure. Luckily, many of the vulnerabilities shown have since been patched. Vulnerable implementations include, but are definitely not limited to, the following:\n\nLibgcrypt \u2013 cryptographic library of GnuPG, implementation of ECDH public-key encryption algorithm (since patched)\nGnuPG implementation of 4096-bit RSA (since patched)\nGnuPG implementation of 3072-bit ElGamal (since patched)\nGMP implementation of 1024-bit RSA\nOpenSSL implementation of 1024-bit RSA","completion":"Cryptography"}
{"prompt":"It was common to encipher a message after first encoding it, to increase the difficulty of  cryptanalysis. With a numerical code, this was commonly done with an \"additive\" - simply a long key number which was digit-by-digit added to the code groups, modulo 10. Unlike the codebooks, additives would be changed frequently. The famous Japanese Navy code, JN-25, was of this design.","completion":"Cryptography"}
{"prompt":"There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.","completion":"Cryptography"}
{"prompt":"The above groups can be described algebraically as well as geometrically.. Given the curve y2 = x3 + ax + b over the field K (whose characteristic we assume to be neither 2 nor 3), and points P = (xP, yP) and Q = (xQ, yQ) on the curve, assume first that xP \u2260 xQ (case 1)..","completion":"Cryptography"}
{"prompt":"S. Goldwasser, S. Micali and C. Rackoff, \"The knowledge complexity of interactive proof systems\", SIAM Journal on Computing, 18, 1989, pp. 186\u2013208.\nC. Rackoff and D. Simon, \"Non-interactive zero-knowledge proof of knowledge and the chosen cipertext attack\", in Proceedings of Crypto 91, pp. 433\u2013444.\nC. Rackoff and D. Simon, \"Cryptographic defense against traffic analysis\", in Proceedings of the 25th ACM Symposium on Theory of Computing, May 1993, pp. 672\u2013681.","completion":"Cryptography"}
{"prompt":"Encrypt-last-block CBC-MAC (ECBC-MAC) is defined as CBC-MAC-ELB(m, (k1, k2)) = E(k2, CBC-MAC(k1, m)). Compared to the other discussed methods of extending CBC-MAC to variable-length messages, encrypt-last-block has the advantage of not needing to know the length of the message until the end of the computation.","completion":"Cryptography"}
{"prompt":"A wide variety of cryptographic protocols go beyond the traditional goals of data confidentiality, integrity, and authentication to also secure a variety of other desired characteristics of computer-mediated collaboration. Blind signatures can be used for digital cash and digital credentials to prove that a person holds an attribute or right without revealing that person's identity or the identities of parties that person transacted with.  Secure digital timestamping can be used to prove that data (even if confidential) existed at a certain time.  Secure multiparty computation can be used to compute answers (such as determining the highest bid in an auction) based on confidential data (such as private bids), so that when the protocol is complete the participants know only their own input and the answer. End-to-end auditable voting systems provide sets of desirable privacy and auditability properties for conducting e-voting.  Undeniable signatures include interactive protocols that allow the signer to prove a forgery and limit who can verify the signature.  Deniable encryption augments standard encryption by making it impossible for an attacker to mathematically prove the existence of a plain text message. Digital mixes create hard-to-trace communications.","completion":"Cryptography"}
{"prompt":"Santha and Vazirani proved that several bit streams with weak randomness can be combined to produce a higher-quality quasi-random bit stream.\nEven earlier, John von Neumann proved that a simple algorithm can remove a considerable amount of the bias in any bit stream, which should be applied to each bit stream before using any variation of the Santha\u2013Vazirani design.","completion":"Cryptography"}
{"prompt":"Let \n  \n    \n      \n        m\n        \n      \n    \n    {\\displaystyle m\\!}\n    be the plaintext message that Alice wants to secretly transmit to Bob and let \n  \n    \n      \n        \n          E\n          \n            k\n          \n        \n        \n      \n    \n    {\\displaystyle E_{k}\\!}\n   be the encryption cipher, where \n  \n    \n      \n        \n          \n          \n            k\n          \n        \n        \n      \n    \n    {\\displaystyle _{k}\\!}\n   is a cryptographic key. Alice must first transform the plaintext into ciphertext, \n  \n    \n      \n        c\n        \n      \n    \n    {\\displaystyle c\\!}\n  , in order to securely send the message to Bob, as follows:\n\n  \n    \n      \n        c\n        =\n        \n          E\n          \n            k\n          \n        \n        (\n        m\n        )\n        .\n        \n      \n    \n    {\\displaystyle c=E_{k}(m).\\!}\n  In a symmetric-key system, Bob knows Alice's encryption key. Once the message is encrypted, Alice can safely transmit it to Bob (assuming no one else knows the key). In order to read Alice's message, Bob must decrypt the ciphertext using \n  \n    \n      \n        \n          \n            \n              E\n              \n                k\n              \n            \n          \n          \n            \u2212\n            1\n          \n        \n        \n      \n    \n    {\\displaystyle {E_{k}}^{-1}\\!}\n   which is known as the decryption cipher, \n  \n    \n      \n        \n          D\n          \n            k\n          \n        \n        :\n        \n      \n    \n    {\\displaystyle D_{k}:\\!}\n  \n\n  \n    \n      \n        \n          D\n          \n            k\n          \n        \n        (\n        c\n        )\n        =\n        \n          D\n          \n            k\n          \n        \n        (\n        \n          E\n          \n            k\n          \n        \n        (\n        m\n        )\n        )\n        =\n        m\n        .\n        \n      \n    \n    {\\displaystyle D_{k}(c)=D_{k}(E_{k}(m))=m.\\!}\n  Alternatively, in a non-symmetric key system, everyone, not just Alice and Bob, knows the encryption key; but the decryption key cannot be inferred from the encryption key. Only Bob knows the decryption key \n  \n    \n      \n        \n          D\n          \n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle D_{k},}\n   and decryption proceeds as\n\n  \n    \n      \n        \n          D\n          \n            k\n          \n        \n        (\n        c\n        )\n        =\n        m\n        .\n      \n    \n    {\\displaystyle D_{k}(c)=m.}","completion":"Cryptography"}
{"prompt":"The resources required for a brute-force attack grow exponentially with increasing key size, not linearly. Although U.S. export regulations historically restricted key lengths to 56-bit symmetric keys (e.g. Data Encryption Standard), these restrictions are no longer in place, so modern symmetric algorithms typically use computationally stronger 128- to 256-bit keys.\nThere is a physical argument that a 128-bit symmetric key is computationally secure against brute-force attack. The Landauer limit implied by the laws of physics sets a lower limit on the energy required to perform a computation of kT  \u00b7  ln 2 per bit erased in a computation, where T is the temperature of the computing device in kelvins, k is the Boltzmann constant, and the natural logarithm of 2 is about 0.693 (0.6931471805599453). No irreversible computing device can use less energy than this, even in principle.  Thus, in order to simply flip through the possible values for a 128-bit symmetric key (ignoring doing the actual computing to check it) would, theoretically, require 2128 \u2212 1 bit flips on a conventional processor.  If it is assumed that the calculation occurs near room temperature (\u2248300 K), the Von Neumann-Landauer Limit can be applied to estimate the energy required as \u22481018 joules, which is equivalent to consuming 30 gigawatts of power for one year. This is equal to 30\u00d7109 W\u00d7365\u00d724\u00d73600 s = 9.46\u00d71017 J or 262.7 TWh (about 0.1% of the yearly world energy production). The full actual computation \u2013 checking each key to see if a solution has been found \u2013 would consume many times this amount. Furthermore, this is simply the energy requirement for cycling through the key space; the actual time it takes to flip each bit is not considered, which is certainly greater than 0 (see Bremermann's limit).However, this argument assumes that the register values are changed using conventional set and clear operations, which inevitably generate entropy. It has been shown that computational hardware can be designed not to encounter this theoretical obstruction (see reversible computing), though no such computers are known to have been constructed.\nAs commercial successors of governmental ASIC solutions have become available, also known as custom hardware attacks,  two emerging technologies have proven their capability in the brute-force attack of certain ciphers. One is modern graphics processing unit (GPU) technology, the other is the field-programmable gate array (FPGA) technology.  GPUs benefit from their wide availability and price-performance benefit, FPGAs from their energy efficiency per cryptographic operation. Both technologies try to transport the benefits of parallel processing to brute-force attacks. In case of GPUs some hundreds, in the case of FPGA some thousand processing units making them much better suited to cracking passwords than conventional processors.\nVarious publications in the fields of cryptographic analysis have proved the energy efficiency of today's FPGA technology, for example, the COPACOBANA FPGA Cluster computer consumes the same energy as a single PC (600 W), but performs like 2,500 PCs for certain algorithms. A number of firms provide hardware-based FPGA cryptographic analysis solutions from a single FPGA PCI Express card up to dedicated FPGA computers.  WPA and WPA2 encryption have successfully been brute-force attacked by reducing the workload by a factor of 50 in comparison to conventional CPUs and some hundred in case of FPGAs.\n\nAdvanced Encryption Standard (AES) permits the use of 256-bit keys. Breaking a symmetric 256-bit key by brute force requires 2128 times more computational power than a 128-bit key. One of the fastest supercomputers in 2019 has a speed of 100 petaFLOPS which could theoretically check 100 million (1014) AES keys per second (assuming 1000 operations per check), but would still require 3.67\u00d71055 years to exhaust the 256-bit key space.An underlying assumption of a brute-force attack is that the complete key space was used to generate keys, something that relies on an effective random number generator, and that there are no defects in the algorithm or its implementation.  For example, a number of systems that were originally thought to be impossible to crack by brute force have nevertheless been cracked because the key space to search through was found to be much smaller than originally thought, because of a lack of entropy in their pseudorandom number generators. These include Netscape's implementation of Secure Sockets Layer (SSL) (cracked by Ian Goldberg and David Wagner in 1995) and a Debian\/Ubuntu edition of OpenSSL discovered in 2008 to be flawed.  A similar lack of implemented entropy led to the breaking of Enigma's code.","completion":"Cryptography"}
{"prompt":"Cryptocurrency advertisements have been banned on the following platforms:\n\nGoogle - Ended August 2021\nTwitter\nFacebook - Ended December 2021\nBing - Ended June 2022\nSnapchat\nLinkedIn\nMailChimp\nBaidu\nTencent\nWeibo\nLine\nYandex","completion":"Cryptography"}
{"prompt":"GCM combines the well-known counter mode of encryption with the new Galois mode of authentication.. The key feature is the ease of parallel computation of the Galois field multiplication used for authentication.. This feature permits higher throughput than encryption algorithms, like CBC, which use chaining modes.. The GF(2128) field used is defined by the polynomial                                    x                        128                             +                    x                        7                             +                    x                        2                             +         x         +         1                 {\\displaystyle x^{128}+x^{7}+x^{2}+x+1}   The authentication tag is constructed by feeding blocks of data into the GHASH function and encrypting the result..","completion":"Cryptography"}
{"prompt":"Cryptocurrencies use various timestamping schemes to \"prove\" the validity of transactions added to the blockchain ledger without the need for a trusted third party.\nThe first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.\nAnother method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.","completion":"Cryptography"}
{"prompt":"A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF. Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.However, in 2021, there was a backlash against donations in Bitcoin because of the environmental emissions it caused. Some agencies stopped accepting Bitcoin and others turned to \"greener\" cryptocurrencies. The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: \"As the amount of energy needed to run Bitcoin became clearer, this policy became no longer tenable.\"In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.","completion":"Cryptography"}
{"prompt":"As the popularity and demand for online currencies has increased since the inception of Bitcoin in 2009, so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.\nTransactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them. White supremacist Richard Spencer went as far as to declare Bitcoin the \"currency of the alt-right\".","completion":"Cryptography"}
{"prompt":"Bertram, Linda A. \/ Dooble, Gunther van \/ et al. (Eds.): Nomenclatura: Encyclopedia of modern Cryptography and Internet Security - From AutoCrypt and Exponential Encryption to Zero-Knowledge-Proof Keys, 2019, ISBN 9783746066684.\nPiper, Fred and Sean Murphy, Cryptography : A Very Short Introduction ISBN 0-19-280315-8 This book outlines the major goals, uses, methods, and developments in cryptography.","completion":"Cryptography"}
{"prompt":"Al-Kindi was the first major writer on optics since antiquity. Roger Bacon placed him in the first rank after Ptolemy as a writer on the topic. In the apocryphal work known as De radiis stellarum, is developed the theory \"that everything in the world ... emits rays in every direction, which fill the whole world.\" This theory of the active power of rays had an influence on later scholars such as Ibn al-Haytham, Robert Grosseteste and Roger Bacon.Two major theories of optics appear in the writings of al-Kindi; Aristotelian and Euclidean. Aristotle had believed that in order for the eye to perceive an object, both the eye and the object must be in contact with a transparent medium (such as air) that is filled with light. When these criteria are met, the \"sensible form\" of the object is transmitted through the medium to the eye. On the other hand, Euclid proposed that vision occurred in straight lines when \"rays\" from the eye reached an illuminated object and were reflected back. As with his theories on Astrology, the dichotomy of contact and distance is present in al-Kindi's writings on this subject as well.\nThe factor which al-Kindi relied upon to determine which of these theories was most correct was how adequately each one explained the experience of seeing. For example, Aristotle's theory was unable to account for why the angle at which an individual sees an object affects his perception of it. For example, why a circle viewed from the side will appear as a line. According to Aristotle, the complete sensible form of a circle should be transmitted to the eye and it should appear as a circle. On the other hand, Euclidean optics provided a geometric model that was able to account for this, as well as the length of shadows and reflections in mirrors, because Euclid believed that the visual \"rays\" could only travel in straight lines (something which is commonly accepted in modern science). For this reason, al-Kindi considered the latter preponderant.Al-Kindi's primary optical treatise \"De aspectibus\" was later translated into Latin. This work, along with Alhazen's Optics and the Arabic translations of Ptolemy and Euclid's Optics, were the main Arabic texts to affect the development of optical investigations in Europe, most notably those of Robert Grosseteste, Vitello and Roger Bacon.","completion":"Cryptography"}
{"prompt":"Some computational problems are assumed to be hard on average over a particular distribution of instances.\nFor example, in the planted clique problem, the input is a random graph sampled, by sampling an Erd\u0151s\u2013R\u00e9nyi random graph and then \"planting\" a random \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -clique, i.e. connecting \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   uniformly random nodes (where \n  \n    \n      \n        2\n        \n          log\n          \n            2\n          \n        \n        \u2061\n        n\n        \u226a\n        k\n        \u226a\n        \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2\\log _{2}n\\ll k\\ll {\\sqrt {n}}}\n  ), and the goal is to find the planted \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  -clique (which is unique w.h.p.).\nAnother important example is Feige's Hypothesis, which is a computational hardness assumption about random instances of 3-SAT (sampled to maintain a specific ratio of clauses to variables).\nAverage-case computational hardness assumptions are useful for proving average-case hardness in applications like statistics, where there is a natural distribution over inputs.\nAdditionally, the planted clique hardness assumption has also been used to distinguish between polynomial and quasi-polynomial worst-case time complexity of other problems,\nsimilarly to the Exponential Time Hypothesis.","completion":"Cryptography"}
{"prompt":"In his Kitab al-Fihrist (Catalogue), Ibn al-Nadim recounts the various names attached to the transmission of Kitab al-'Ayn, i.e. the isnad (chain of authorities).  He begins with Durustuyah's account that it was al-Kasrawi who said that al-Zaj al-Muhaddath had said that al-Khalil had explained the concept and structure of his dictionary to al-Layth b. al-Muzaffar b. Nasr b. Sayyar, had dictated edited portions to al-Layth and they had reviewed its preparation together. Ibn al-Nadim writes that a manuscript in the possession of Da'laj had probably belonged originally to Ibn al-'Ala al-Sijistani, who according to Durustuyah had been a member of a circle of scholars who critiqued the book. In this group was Abu Talib al-Mufaddal ibn Slamah, 'Abd Allah ibn Muhammad al-Karmani, Abu Bakr ibn Durayd and al-Huna'i al-Dawsi.","completion":"Cryptography"}
{"prompt":"Section 69 of the Information Technology Act, 2000 (as amended in 2008) authorizes Indian government officials or policemen to listen in on any phone calls, read any SMS messages or emails, or monitor the websites that anyone visits, without requiring a warrant.:\u200a2\u200a (However, this is a violation of article 21 of the Constitution of India.:\u200a2\u200a) This section also enables the central government of India or a state government of India to compel any agency to decrypt information.:\u200a4\u200aAccording to the Information Technology (Intermediaries Guidelines) Rules, 2011, intermediaries are required to provide information to Indian government agencies for investigative or other purposes.:\u200a2\u200aISP license holders are freely allowed to use encryption keys up to 40 bits. Beyond that, they are required to obtain written permission and to deposit the decryption key with the Department of Telecommunications.:\u200a2\u20133\u200aPer the 2012 SEBI Master Circular for Stock Exchange or Cash Market (issued by the Securities and Exchange Board of India), it is the responsibility of stock exchanges to maintain data reliability and confidentiality through the use of encryption.:\u200a3\u200a Per Reserve Bank of India guidance issued in 2001, banks must use at least 128-bit SSL to protect browser-to-bank communication; they must also encrypt sensitive data internally.:\u200a3\u200aElectronics, including cryptographic products, is one of the categories of dual-use items in the Special Chemicals, Organisms, Materials, Equipment and Technologies (SCOMET; part of the Foreign Trade (Development & Regulation Act), 1992). However, this regulation does not specify which cryptographic products are subject to export controls.:\u200a3","completion":"Cryptography"}
{"prompt":"Normally a block cipher applies a fixed sequence of primitive mathematical or logical operators (such as additions, XORs, etc.) on the plaintext and secret key in order to produce the ciphertext.  An attacker uses this knowledge to search for weaknesses in the cipher which may allow the recovery of the plaintext. \nFROG's design philosophy is to hide the exact sequence of primitive operations even though the cipher itself is known. While other ciphers use the secret key only as data (which are combined with the plain text to produce the cipher text), FROG uses the key both as data and as instructions on how to combine these data. In effect an expanded version of the key is used by FROG as a program. FROG itself operates as an interpreter that applies this key-dependent program on the plain text to produce the cipher text. Decryption works by applying the same program in reverse on the cipher text.","completion":"Cryptography"}
{"prompt":"Shahram Khazaei, Simon Fischer, and Willi Meier give a cryptanalysis of the ASG allowing various tradeoffs between time complexity and the amount of output needed to mount the attack, e.g. with asymptotic complexity \n  \n    \n      \n        O\n        (\n        \n          L\n          \n            2\n          \n        \n        \n          .2\n          \n            2\n            L\n            \n              \/\n            \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(L^{2}.2^{2L\/3})}\n   and \n  \n    \n      \n        O\n        (\n        \n          2\n          \n            2\n            L\n            \n              \/\n            \n            3\n          \n        \n        )\n      \n    \n    {\\displaystyle O(2^{2L\/3})}\n   bits, where \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n   is the size of the shortest of the three LFSRs.","completion":"Cryptography"}
{"prompt":"Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman, Robert J. Shiller, Joseph Stiglitz, Richard Thaler, James Heckman, Thomas Sargent, Angus Deaton, and Oliver Hart; and by central bank officials including Alan Greenspan, Agust\u00edn Carstens, V\u00edtor Const\u00e2ncio, and Nout Wellink.The investors Warren Buffett and George Soros have respectively characterized it as a \"mirage\" and a \"bubble\"; while the business executives Jack Ma and J.P. Morgan Chase CEO Jamie Dimon have called it a \"bubble\" and a \"fraud\", respectively, although Jamie Dimon later said he regretted dubbing Bitcoin a fraud. BlackRock CEO Laurence D. Fink called Bitcoin an \"index of money laundering\".In June 2022, Bill Gates said that cryptocurrencies are \"100% based on greater fool theory\".Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he\/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.","completion":"Cryptography"}
{"prompt":"Authentication protocols may use nonces to ensure that old communications cannot be reused in replay attacks. For instance, nonces are used in HTTP digest access authentication to calculate an MD5 digest of the password. The nonces are different each time the 401 authentication challenge response code is presented, thus making replay attacks virtually impossible. The scenario of ordering products over the Internet can provide an example of the usefulness of nonces in replay attacks. An attacker could take the encrypted information and\u2014without needing to decrypt\u2014could continue to send a particular order to the supplier, thereby ordering products over and over again under the same name and purchase information. The nonce is used to give 'originality' to a given message so that if the company receives any other orders from the same person with the same nonce, it will discard those as invalid orders.\nA nonce may be used to ensure security for a stream cipher.  Where the same key is used for more than one message and then a different nonce is used to ensure that the keystream is different for different messages encrypted with that key; often the message number is used.\nSecret nonce values are used by the Lamport signature scheme as a signer-side secret which can be selectively revealed for comparison to public hashes for signature creation and verification.","completion":"Cryptography"}
{"prompt":"The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Each block typically contains a hash pointer as a link to a previous block, a timestamp and transaction data. By design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.\nBlockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.","completion":"Cryptography"}
{"prompt":"Figure 3 illustrates the key schedule for encryption\u2014the algorithm which generates the subkeys. Initially, 56 bits of the key are selected from the initial 64 by Permuted Choice 1 (PC-1)\u2014the remaining eight bits are either discarded or used as parity check bits. The 56 bits are then divided into two 28-bit halves; each half is thereafter treated separately. In successive rounds, both halves are rotated left by one or two bits (specified for each round), and then 48 subkey bits are selected by Permuted Choice 2 (PC-2)\u201424 bits from the left half, and 24 from the right. The rotations (denoted by \"<<<\" in the diagram) mean that a different set of bits is used in each subkey; each bit is used in approximately 14 out of the 16 subkeys.\nThe key schedule for decryption is similar\u2014the subkeys are in reverse order compared to encryption. Apart from that change, the process is the same as for encryption. The same 28 bits are passed to all rotation boxes.","completion":"Cryptography"}
{"prompt":"In February 2014, the world's largest Bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 Bitcoins belonging to their clients. This added up to approximately 7% of all Bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a Bitcoin fell from a high of about $1,160 in December to under $400 in February.On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70M using a hijacked company computer.On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year. Customers were still granted access to 75% of their assets.\nIn May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers. Exchanges lost an estimated $18m and Bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.\nOn 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release. Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The U.S. Securities and Exchange Commission separately brought a civil enforcement action against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold \"investment contracts representing shares in the profits they claimed would be generated\" from mining.In January 2018, Japanese exchange Coincheck reported that hackers had stolen $530M worth of cryptocurrencies.In June 2018, South Korean exchange Coinrail was hacked, losing over $37M worth of cryptos. The hack worsened an already ongoing cryptocurrency selloff by an additional $42 billion.On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in cryptocurrency stolen.A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets \"typically find their way to illegal markets and are used to fund further criminal activity\".According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, the following three categories make up the majority of illicit cryptocurrency uses: \"(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself.\" The report concludes that \"for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed\" and that \"the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses\".According to the UK 2020 national risk assessment\u2014a comprehensive assessment of money laundering and terrorist financing risk in the UK\u2014the risk of using cryptoassets such as Bitcoin for money laundering and terrorism financing is assessed as \"medium\" (from \"low\" in the previous 2017 report). Legal scholars suggested that the money laundering opportunities may be more perceived than real. Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.In December 2021, Monkey Kingdom - a NFT project based in Hong Kong lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.","completion":"Cryptography"}
{"prompt":"Integral cryptanalysis is a cryptanalytic attack that is particularly applicable to block ciphers based on substitution\u2013permutation networks. Unlike differential cryptanalysis, which uses pairs of chosen plaintexts with a fixed XOR difference, integral cryptanalysis uses sets or even multisets of chosen plaintexts of which part is held constant and another part varies through all possibilities. For example, an attack might use 256 chosen plaintexts that have all but 8 of their bits the same, but all differ in those 8 bits. Such a set necessarily has an XOR sum of 0, and the XOR sums of the corresponding sets of ciphertexts provide information about the cipher's operation. This contrast between the differences between pairs of texts and the sums of larger sets of texts inspired the name \"integral cryptanalysis\", borrowing the terminology of calculus.","completion":"Cryptography"}
{"prompt":"Counter with cipher block chaining message authentication code (counter with CBC-MAC; CCM) is an authenticated encryption algorithm designed to provide both authentication and confidentiality.  CCM mode is only defined for block ciphers with a block length of 128 bits.","completion":"Cryptography"}
{"prompt":"As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, University of Chicago, and Stanford are publishing electronic student transcripts with digital signatures.\nBelow are some common reasons for applying a digital signature to communications:","completion":"Cryptography"}
{"prompt":"Despite the criticisms, DES was approved as a federal standard in November 1976, and published on 15 January 1977 as FIPS PUB 46, authorized for use on all unclassified data. It was subsequently reaffirmed as the standard in 1983, 1988 (revised as FIPS-46-1), 1993 (FIPS-46-2), and again in 1999 (FIPS-46-3), the latter prescribing \"Triple DES\" (see below). On 26 May 2002, DES was finally superseded by the Advanced Encryption Standard (AES), following a public competition. On 19 May 2005, FIPS 46-3 was officially withdrawn, but NIST has approved Triple DES through the year 2030 for sensitive government information.The algorithm is also specified in ANSI X3.92 (Today X3 is known as INCITS and ANSI X3.92 as ANSI INCITS 92), NIST SP 800-67 and ISO\/IEC 18033-3 (as a component of TDEA).\nAnother theoretical attack, linear cryptanalysis, was published in 1994, but it was the Electronic Frontier Foundation's DES cracker in 1998 that demonstrated that DES could be attacked very practically, and highlighted the need for a replacement algorithm. These and other methods of cryptanalysis are discussed in more detail later in this article.\nThe introduction of DES is considered to have been a catalyst for the academic study of cryptography, particularly of methods to crack block ciphers. According to a NIST retrospective about DES,\n\nThe DES can be said to have \"jump-started\" the nonmilitary study and development of encryption algorithms. In the 1970s there were very few cryptographers, except for those in military or intelligence organizations, and little academic study of cryptography. There are now many active academic cryptologists, mathematics departments with strong programs in cryptography, and commercial information security companies and consultants. A generation of cryptanalysts has cut its teeth analyzing (that is, trying to \"crack\") the DES algorithm. In the words of cryptographer Bruce Schneier, \"DES did more to galvanize the field of cryptanalysis than anything else. Now there was an algorithm to study.\" An astonishing share of the open literature in cryptography in the 1970s and 1980s dealt with the DES, and the DES is the standard against which every symmetric key algorithm since has been compared.","completion":"Cryptography"}
{"prompt":"The classified National Security Agency program TEMPEST focuses on both the spying on systems by observing electromagnetic radiation and the securing of equipment to protect against such attacks.\nThe Federal Communications Commission outlines the rules regulating the unintended emissions of electronic devices in Part 15 of the Code of Federal Regulations Title 47. The FCC does not provide a certification that devices do not produce excess emissions, but instead relies on a self-verification procedure.","completion":"Cryptography"}
{"prompt":"Immediately after Poe's death, his literary rival Rufus Wilmot Griswold wrote a slanted high-profile obituary under a pseudonym, filled with falsehoods that cast Poe as a lunatic, and which described him as a person who \"walked the streets, in madness or melancholy, with lips moving in indistinct curses, or with eyes upturned in passionate prayers, (never for himself, for he felt, or professed to feel, that he was already damned)\".The long obituary appeared in the New York Tribune, signed \"Ludwig\" on the day that Poe was buried in Baltimore. It was further published throughout the country. The obituary began, \"Edgar Allan Poe is dead. He died in Baltimore the day before yesterday. This announcement will startle many, but few will be grieved by it.\" \"Ludwig\" was soon identified as Griswold, an editor, critic, and anthologist who had borne a grudge against Poe since 1842. Griswold somehow became Poe's literary executor and attempted to destroy his enemy's reputation after his death.Griswold wrote a biographical article of Poe called \"Memoir of the Author\", which he included in an 1850 volume of the collected works. There he depicted Poe as a depraved, drunken, drug-addled madman and included Poe's letters as evidence. Many of his claims were either lies or distortions; for example, it is seriously disputed that Poe was a drug addict. Griswold's book was denounced by those who knew Poe well, including John Neal, who published an article defending Poe and attacking Griswold as a \"Rhadamanthus, who is not to be bilked of his fee, a thimble-full of newspaper notoriety\". Griswold's book nevertheless became a popularly accepted biographical source. This was in part because it was the only full biography available and was widely reprinted, and in part because readers thrilled at the thought of reading works by an \"evil\" man. Letters that Griswold presented as proof were later revealed as forgeries.","completion":"Cryptography"}
{"prompt":"Plaintext in the new unabridged: An examination of the definitions on cryptology in Webster's Third New International Dictionary (Crypto Press 1963)\nThe Codebreakers \u2013 The Story of Secret Writing (ISBN 978-0-684-83130-5) (1967)\nHitler's Spies: German Military Intelligence in World War II (Macmillan 1978) (ISBN 0-02-560610-7)\nThe Codebreakers \u2013 The Story of Secret Writing Revised edition (ISBN 0-684-83130-9) (1996)\nCryptology goes Public (Council on Foreign Relations 1979)\nNotes & correspondence on the origin of polyalphabetic substitution (1980)\nCodebreaking in World Wars I and II: The major successes and failures, their causes and their effects (Cambridge University Press 1980)\nKahn on Codes: Secrets of the New Cryptology (Macmillan 1984) (ISBN 978-0-02-560640-1)\nCryptology: Machines, History and Methods by Cipher Deavours and David Kahn (Artech House 1989) (ISBN 978-0-89006-399-6)\nSeizing the Enigma: The Race to Break the German U-Boats Codes, 1939\u20131943 (Houghton Mifflin 1991) (ISBN 978-0-395-42739-2)\nThe Reader of Gentlemen's Mail: Herbert O. Yardley and the Birth of American Codebreaking (Yale University Press 2004) (ISBN 978-0-300-09846-4)\nHow I Discovered World War II's Greatest Spy and Other Stories of Intelligence and Code, Boca Raton : CRC Press, Taylor & Francis Group, 2014. ISBN 978-1466561991","completion":"Cryptography"}
{"prompt":"Until 1937 the Cipher Bureau's German section, BS-4, had been housed in the Polish General Staff building \u2013 the stately 18th-century \"Saxon Palace\" \u2013 in Warsaw. That year BS-4 moved into specially constructed new facilities in the Kabaty Woods near Pyry, south of Warsaw.  There, working conditions were incomparably better than in the cramped quarters at the General Staff building.The move was dictated as well by requirements of security. Germany's Abwehr was always looking for potential traitors among the military and civilian workers at the General Staff building. Strolling agents, even if lacking access to the Staff building, could observe personnel entering and leaving, and photograph them with concealed miniature cameras. Annual Abwehr intelligence assignments for German agents in Warsaw placed a priority on securing informants at the Polish General Staff.","completion":"Cryptography"}
{"prompt":"The algorithm can be described as first performing a Diffie\u2013Hellman key exchange to establish a shared secret \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , then using this as a one-time pad for encrypting the message. ElGamal encryption is performed in three phases: the key generation, the encryption, and the decryption. The first is purely key exchange, whereas the latter two mix key exchange computations with message computations.","completion":"Cryptography"}
{"prompt":"Let                                    F                          {\\displaystyle \\mathrm {F} }    be the round function and let                                    K                        0                             ,                    K                        1                             ,         \u2026         ,                    K                        n                                     {\\displaystyle K_{0},K_{1},\\ldots ,K_{n}}    be the sub-keys for the rounds                         0         ,         1         ,         \u2026         ,         n                 {\\displaystyle 0,1,\\ldots ,n}    respectively..","completion":"Cryptography"}
{"prompt":"For the purposes of this article, an elliptic curve is a plane curve over a finite field (rather than the real numbers) which consists of the points satisfying the equation:\n\n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            3\n          \n        \n        +\n        a\n        x\n        +\n        b\n        ,\n        \n      \n    \n    {\\displaystyle y^{2}=x^{3}+ax+b,\\,}\n  along with a distinguished point at infinity, denoted \u221e. The coordinates here are to be chosen from a fixed finite field of characteristic not equal to 2 or 3, or the curve equation would be somewhat more complicated.\nThis set of points, together with the group operation of elliptic curves, is an abelian group, with the point at infinity as an identity element. The structure of the group is inherited from the divisor group of the underlying algebraic variety:\n\n  \n    \n      \n        \n          \n            D\n            i\n            v\n          \n          \n            0\n          \n        \n        (\n        E\n        )\n        \u2192\n        \n          \n            P\n            i\n            c\n          \n          \n            0\n          \n        \n        (\n        E\n        )\n        \u2243\n        E\n        ,\n        \n      \n    \n    {\\displaystyle \\mathrm {Div} ^{0}(E)\\to \\mathrm {Pic} ^{0}(E)\\simeq E,\\,}","completion":"Cryptography"}
{"prompt":"If both the key encapsulation and data encapsulation schemes in a hybrid cryptosystem are secure against adaptive chosen ciphertext attacks, then the hybrid scheme inherits that property as well. However, it is possible to construct a hybrid scheme secure against adaptive chosen ciphertext attacks even if the key encapsulation has a slightly weakened security definition (though the security of the data encapsulation must be slightly stronger).","completion":"Cryptography"}
{"prompt":"A classical example of a cryptosystem is the Caesar cipher.  A more contemporary example is the RSA cryptosystem.\nAnother example of a cryptosystem is the Advanced Encryption Standard (AES). AES is a widely used symmetric encryption algorithm that has become the standard for securing data in various applications.","completion":"Cryptography"}
{"prompt":"A block cipher by itself allows encryption only of a single data block of the cipher's block length. For a variable-length message, the data must first be partitioned into separate cipher blocks. In the simplest case, known as electronic codebook (ECB) mode, a message is first split into separate blocks of the cipher's block size (possibly extending the last block with padding bits), and then each block is encrypted and decrypted independently. However, such a naive method is generally insecure because equal plaintext blocks will always generate equal ciphertext blocks (for the same key), so patterns in the plaintext message become evident in the ciphertext output.To overcome this limitation, several so-called block cipher modes of operation have been designed and specified in national recommendations such as NIST 800-38A and BSI TR-02102 and international standards such as ISO\/IEC 10116. The general concept is to use randomization of the plaintext data based on an additional input value, frequently called an initialization vector, to create what is termed probabilistic encryption. In the popular cipher block chaining (CBC) mode, for encryption to be secure the initialization vector passed along with the plaintext message must be a random or pseudo-random value, which is added in an exclusive-or manner to the first plaintext block before it is encrypted. The resultant ciphertext block is then used as the new initialization vector for the next plaintext block. In the cipher feedback (CFB) mode, which emulates a self-synchronizing stream cipher, the initialization vector is first encrypted and then added to the plaintext block. The output feedback (OFB) mode repeatedly encrypts the initialization vector to create a key stream for the emulation of a synchronous stream cipher. The newer counter (CTR) mode similarly creates a key stream, but has the advantage of only needing unique and not (pseudo-)random values as initialization vectors; the needed randomness is derived internally by using the initialization vector as a block counter and encrypting this counter for each block.From a security-theoretic point of view, modes of operation must provide what is known as semantic security. Informally, it means that given some ciphertext under an unknown key one cannot practically derive any information from the ciphertext (other than the length of the message) over what one would have known without seeing the ciphertext. It has been shown that all of the modes discussed above, with the exception of the ECB mode, provide this property under so-called chosen plaintext attacks.","completion":"Cryptography"}
{"prompt":"David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.The invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers, by Al-Kindi, an Arab mathematician, sometime around AD 800, proved to be the single most significant cryptanalytic advance until World War II. Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), in which he described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic. An important contribution of Ibn Adlan (1187\u20131268) was on sample size for use of frequency analysis.In early medieval England between the years 800\u20131100, substitution ciphers were frequently used by scribes as a playful and clever way to encipher notes, solutions to riddles, and colophons. The ciphers tend to be fairly straightforward, but sometimes they deviate from an ordinary pattern, adding to their complexity, and possibly also to their sophistication. This period saw vital and significant cryptographic experimentation in the West.\nAhmad al-Qalqashandi (AD 1355\u20131418) wrote the Subh al-a 'sha, a 14-volume encyclopedia which included a section on cryptology. This information was attributed to Ibn al-Durayhim who lived from AD 1312 to 1361, but whose writings on cryptography have been lost. The list of ciphers in this work included both substitution and transposition, and for the first time, a cipher with multiple substitutions for each plaintext letter (later called homophonic substitution). Also traced to Ibn al-Durayhim is an exposition on and a worked example of cryptanalysis, including the use of tables of letter frequencies and sets of letters which cannot occur together in one word.\nThe earliest example of the homophonic substitution cipher is the one used by Duke of Mantua in the early 1400s. Homophonic cipher replaces each letter with multiple symbols depending on the letter frequency. The cipher is ahead of the time because it combines monoalphabetic and polyalphabetic features.\nEssentially all ciphers remained vulnerable to the cryptanalytic technique of frequency analysis until the development of the polyalphabetic cipher, and many remained so thereafter. The polyalphabetic cipher was most clearly explained by Leon Battista Alberti around AD 1467, for which he was called the \"father of Western cryptology\". Johannes Trithemius, in his work Poligraphia, invented the tabula recta, a critical component of the Vigen\u00e8re cipher. Trithemius also wrote the Steganographia. The French cryptographer Blaise de Vigen\u00e8re devised a practical polyalphabetic system which bears his name, the Vigen\u00e8re cipher.In Europe, cryptography became (secretly) more important as a consequence of political competition and religious revolution. For instance, in Europe during and after the Renaissance, citizens of the various Italian states\u2014the Papal States and the Roman Catholic Church included\u2014were responsible for rapid proliferation of cryptographic techniques, few of which reflect understanding (or even knowledge) of Alberti's polyalphabetic advance. \"Advanced ciphers\", even after Alberti, were not as advanced as their inventors\/developers\/users claimed (and probably even they themselves believed). They were frequently broken. This over-optimism may be inherent in cryptography, for it was then \u2013 and remains today \u2013 difficult in principle to know how vulnerable one's own system is. In the absence of knowledge, guesses and hopes are predictably common.\nCryptography, cryptanalysis, and secret-agent\/courier betrayal featured in the Babington plot during the reign of Queen Elizabeth I which led to the execution of Mary, Queen of Scots. Robert Hooke suggested in the chapter Of Dr. Dee's Book of Spirits, that John Dee made use of Trithemian steganography, to conceal his communication with Queen Elizabeth I.The chief cryptographer of King Louis XIV of France was Antoine Rossignol; he and his family created what is known as the Great Cipher because it remained unsolved from its initial use until 1890, when French military cryptanalyst, \u00c9tienne Bazeries solved it. An encrypted message from the time of the Man in the Iron Mask (decrypted just prior to 1900 by \u00c9tienne Bazeries) has shed some, regrettably non-definitive, light on the identity of that real, if legendary and unfortunate, prisoner.\nOutside of Europe, after the Mongols brought about the end of the Islamic Golden Age, cryptography remained comparatively undeveloped. Cryptography in Japan seems not to have been used until about 1510, and advanced techniques were not known until after the opening of the country to the West beginning in the 1860s.","completion":"Cryptography"}
{"prompt":"Brute-force attacks work by calculating every possible combination that could make up a password and testing it to see if it is the correct password. As the password's length increases, the amount of time, on average, to find the correct password increases exponentially.","completion":"Cryptography"}
{"prompt":"The public developments of the 1970s broke the near monopoly on high quality cryptography held by government organizations (see S Levy's Crypto for a journalistic account of some of the policy controversy of the time in the US). For the first time ever, those outside government organizations had access to cryptography not readily breakable by anyone (including governments). Considerable controversy, and conflict, both public and private, began more or less immediately, sometimes called the crypto wars. They have not yet subsided. In many countries, for example, export of cryptography is subject to restrictions. Until 1996 export from the U.S. of cryptography using keys longer than 40 bits (too small to be very secure against a knowledgeable attacker) was sharply limited. As recently as 2004, former FBI Director Louis Freeh, testifying before the 9\/11 Commission, called for new laws against public use of encryption.\nOne of the most significant people favoring strong encryption for public use was Phil Zimmermann. He wrote and then in 1991 released PGP (Pretty Good Privacy), a very high quality crypto system. He distributed a freeware version of PGP when he felt threatened by legislation then under consideration by the US Government that would require backdoors to be included in all cryptographic products developed within the US. His system was released worldwide shortly after he released it in the US, and that began a long criminal investigation of him by the US Government Justice Department for the alleged violation of export restrictions.  The Justice Department eventually dropped its case against Zimmermann, and the freeware distribution of PGP has continued around the world. PGP even eventually became an open Internet standard (RFC 2440 or OpenPGP).","completion":"Cryptography"}
{"prompt":"Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road. The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the U.S., Bitcoins are labelled as \"virtual assets\". This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.","completion":"Cryptography"}
{"prompt":"Al-Kindi was a master of many different areas of thought and was held to be one of the greatest philosophers. His influence in the fields of physics, mathematics, medicine, philosophy, and music were far-reaching and lasted for several centuries. Ibn al-Nadim in his Kitab al-Fihrist praised al-Kindi and his work stating:\n\nThe best man of his time, unique in his knowledge of all the ancient sciences. He is called the Philosopher of the Arabs. His books deal with different sciences, such as logic, philosophy, geometry, arithmetic, astronomy etc. We have connected him with the natural philosophers because of his prominence in Science.\nAl-Kindi's major contribution was his establishment of philosophy in the Islamic world and his efforts in trying to harmonize the philosophical investigation along with the Islamic theology and creed. The philosophical texts which were translated under his supervision would become the standard texts in the Islamic world for centuries to come, even after his influence has been eclipsed by later Philosophers.Al-Kindi was also an important figure in medieval Europe. Several of his books got translated into Latin influencing western authors like Robert Grosseteste and Roger Bacon. The Italian Renaissance scholar Geralomo Cardano (1501\u20131575) considered him one of the twelve greatest minds.In 1986, the Royal Commission for Riyadh City inaugurated the Al Kindi Plaza in the Diplomatic Quarter district of Riyadh, Saudi Arabia.","completion":"Cryptography"}
{"prompt":"Bitcoin, along with other cryptocurrencies, has been described as an economic bubble by many economists, including Robert Shiller, Joseph Stiglitz, Richard Thaler,  Paul Krugman, and Nouriel Roubini. In addition, Bitcoin and other cryptocurrencies have been criticized for the amount of electricity required for cryptocurrency \u201cmining\u201d (blockchain transaction validation), and for their being used to purchase illegal goods.","completion":"Cryptography"}
{"prompt":"In 1980, he returned to Israel, joining the faculty of Mathematics and Computer Science at the Weizmann Institute. Starting from 2006, he is also an invited professor at \u00c9cole Normale Sup\u00e9rieure in Paris.\nIn addition to RSA, Shamir's other numerous inventions and contributions to cryptography include the Shamir secret sharing scheme, the breaking of the Merkle-Hellman knapsack cryptosystem, visual cryptography, and the TWIRL and TWINKLE factoring devices. Together with Eli Biham, he discovered differential cryptanalysis in the late 1980s, a general method for attacking block ciphers. It later emerged that differential cryptanalysis was already known \u2014 and kept a secret \u2014 by both IBM and the National Security Agency (NSA).Shamir has also made contributions to computer science outside of cryptography, such as finding the first linear time algorithm for 2-satisfiability and showing the equivalence of the complexity classes PSPACE and IP.","completion":"Cryptography"}
{"prompt":"Ehrsam, Meyer, Smith and Tuchman invented the cipher block chaining (CBC) mode of operation in 1976. In CBC mode, each block of plaintext is XORed with the previous ciphertext block before being encrypted. This way, each ciphertext block depends on all plaintext blocks processed up to that point. To make each message unique, an initialization vector must be used in the first block.\n\nIf the first block has index 1, the mathematical formula for CBC encryption is\n\n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n        =\n        \n          E\n          \n            K\n          \n        \n        (\n        \n          P\n          \n            i\n          \n        \n        \u2295\n        \n          C\n          \n            i\n            \u2212\n            1\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle C_{i}=E_{K}(P_{i}\\oplus C_{i-1}),}\n  \n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        I\n        V\n        ,\n      \n    \n    {\\displaystyle C_{0}=IV,}\n  while the mathematical formula for CBC decryption is\n\n  \n    \n      \n        \n          P\n          \n            i\n          \n        \n        =\n        \n          D\n          \n            K\n          \n        \n        (\n        \n          C\n          \n            i\n          \n        \n        )\n        \u2295\n        \n          C\n          \n            i\n            \u2212\n            1\n          \n        \n        ,\n      \n    \n    {\\displaystyle P_{i}=D_{K}(C_{i})\\oplus C_{i-1},}\n  \n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        I\n        V\n        .\n      \n    \n    {\\displaystyle C_{0}=IV.}","completion":"Cryptography"}
{"prompt":"This property results in the cipher's security degrading quadratically, and needs to be taken into account when selecting a block size. There is a trade-off though as large block sizes can result in the algorithm becoming inefficient to operate. Earlier block ciphers such as the DES have typically selected a 64-bit block size, while newer designs such as the AES support block sizes of 128 bits or more, with some ciphers supporting a range of different block sizes.","completion":"Cryptography"}
{"prompt":"During his lifetime, Poe was mostly recognized as a literary critic. Fellow critic James Russell Lowell called him \"the most discriminating, philosophical, and fearless critic upon imaginative works who has written in America\", suggesting\u2014rhetorically\u2014that he occasionally used prussic acid instead of ink. Poe's caustic reviews earned him the reputation of being a \"tomahawk man\". A favorite target of Poe's criticism was Boston's acclaimed poet Henry Wadsworth Longfellow, who was often defended by his literary friends in what was later called \"The Longfellow War\". Poe accused Longfellow of \"the heresy of the didactic\", writing poetry that was preachy, derivative, and thematically plagiarized. Poe correctly predicted that Longfellow's reputation and style of poetry would decline, concluding, \"We grant him high qualities, but deny him the Future\".Poe was also known as a writer of fiction and became one of the first American authors of the 19th century to become more popular in Europe than in the United States. Poe is particularly respected in France, in part due to early translations by Charles Baudelaire. Baudelaire's translations became definitive renditions of Poe's work in Continental Europe.Poe's early detective fiction tales featuring C. Auguste Dupin laid the groundwork for future detectives in literature. Sir Arthur Conan Doyle said, \"Each [of Poe's detective stories] is a root from which a whole literature has developed.... Where was the detective story until Poe breathed the breath of life into it?\" The Mystery Writers of America have named their awards for excellence in the genre the \"Edgars\". Poe's work also influenced science fiction, notably Jules Verne, who wrote a sequel to Poe's novel The Narrative of Arthur Gordon Pym of Nantucket called An Antarctic Mystery, also known as The Sphinx of the Ice Fields. Science fiction author H. G. Wells noted, \"Pym tells what a very intelligent mind could imagine about the south polar region a century ago\". In 2013, The Guardian cited Pym as one of the greatest novels ever written in the English language, and noted its influence on later authors such as Doyle, Henry James, B. Traven, and David Morrell.Horror author and historian H. P. Lovecraft was heavily influenced by Poe's horror tales, dedicating an entire section of his long essay, \"Supernatural Horror in Literature\", to his influence on the genre. In his letters, Lovecraft described Poe as his \"God of Fiction\". Lovecraft's earlier stories express a significant influence from Poe. A later work, At the Mountains of Madness, quotes him and was influenced by The Narrative of Arthur Gordon Pym of Nantucket. Lovecraft also made extensive use of Poe's unity of effect in his fiction. Alfred Hitchcock once said, \"It's because I liked Edgar Allan Poe's stories so much that I began to make suspense films\". Many references to Poe's works are present in Vladimir Nabokov's novels.Like many famous artists, Poe's works have spawned imitators. One trend among imitators of Poe has been claims by clairvoyants or psychics to be \"channeling\" poems from Poe's spirit. One of the most notable of these was Lizzie Doten, who published Poems from the Inner Life in 1863, in which she claimed to have \"received\" new compositions by Poe's spirit. The compositions were re-workings of famous Poe poems such as \"The Bells\", but which reflected a new, positive outlook.Poe has also received criticism. This is partly because of the negative perception of his personal character and its influence upon his reputation. William Butler Yeats was occasionally critical of Poe and once called him \"vulgar\". Transcendentalist Ralph Waldo Emerson reacted to \"The Raven\" by saying, \"I see nothing in it\", and derisively referred to Poe as \"the jingle man\". Aldous Huxley wrote that Poe's writing \"falls into vulgarity\" by being \"too poetical\"\u2014the equivalent of wearing a diamond ring on every finger.It is believed that only twelve copies have survived of Poe's first book Tamerlane and Other Poems. In December 2009, one copy sold at Christie's auctioneers in New York City for $662,500, a record price paid for a work of American literature.","completion":"Cryptography"}
{"prompt":"M. Liskov, R. Rivest, and D. Wagner have described a generalized version of block ciphers called \"tweakable\" block ciphers. A tweakable block cipher accepts a second input called the tweak along with its usual plaintext or ciphertext input. The tweak, along with the key, selects the permutation computed by the cipher. If changing tweaks is sufficiently lightweight (compared with a usually fairly expensive key setup operation), then some interesting new operation modes become possible. The disk encryption theory article describes some of these modes.","completion":"Cryptography"}
{"prompt":"Most digital signature schemes share the following goals regardless of cryptographic theory or legal provision:\n\nQuality algorithms: Some public-key algorithms are known to be insecure, as practical attacks against them have been discovered.\n\nQuality implementations: An implementation of a good algorithm (or protocol) with mistake(s) will not work.\n\nUsers (and their software) must carry out the signature protocol properly.\n\nThe private key must remain private: If the private key becomes known to any other party, that party can produce perfect digital signatures of anything.\n\nThe public key owner must be verifiable: A public key associated with Bob actually came from Bob. This is commonly done using a public key infrastructure (PKI) and the public key\u2194user association is attested by the operator of the PKI (called a certificate authority). For 'open' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected public key certificate), the possibility of mistaken attestation is non-trivial. Commercial PKI operators have suffered several publicly known problems. Such mistakes could lead to falsely signed, and thus wrongly attributed, documents. 'Closed' PKI systems are more expensive, but less easily subverted in this way.Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.\nLegislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and\/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in Utah in the United States, followed closely by the states Massachusetts and California. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying cryptographic engineering, and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable.\nAdoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on interoperability, algorithm choice, key lengths, and so on what the engineering is attempting to provide.","completion":"Cryptography"}
{"prompt":"A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.\nNode owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.","completion":"Cryptography"}
{"prompt":"Conventional methods for permanently deleting data from a storage device involve overwriting the device's whole content with zeros, ones, or other patterns \u2013 a process which can take a significant amount of time, depending on the capacity and the type of storage medium. Cryptography offers a way of making the erasure almost instantaneous. This method is called crypto-shredding. An example implementation of this method can be found on iOS devices, where the cryptographic key is kept in a dedicated 'effaceable storage'. Because the key is stored on the same device, this setup on its own does not offer full privacy or security protection if an unauthorized person gains physical access to the device.","completion":"Cryptography"}
{"prompt":"In a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:\n\nComputational power available, i.e., the computing power which can be brought to bear on the problem. It is important to note that average performance\/capacity of a single computer is not the only factor to consider. An adversary can use multiple computers at once, for instance, to increase the speed of exhaustive search for a key (i.e., \"brute force\" attack) substantially.\nKey size, i.e., the size of key used to encrypt a message. As the key size increases, so does the complexity of exhaustive search to the point where it becomes impractical to crack encryption directly.Since the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.\nAn example of this process can be found at Key Length which uses multiple reports to suggest that a symmetrical cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 256 bits, all have similar difficulty at present.\nClaude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.","completion":"Cryptography"}
{"prompt":"Al-Kindi authored works on a number of important mathematical subjects, including arithmetic, geometry, the Hindu numbers, the harmony of numbers, lines and multiplication with numbers, relative quantities, measuring proportion and time, and numerical procedures and cancellation. He also wrote four volumes, On the Use of the Hindu Numerals  (Arabic: \u0643\u062a\u0627\u0628 \u0641\u064a \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u0627\u0644\u0623\u0639\u062f\u0627\u062f \u0627\u0644\u0647\u0646\u062f\u064a\u0629 Kit\u0101b f\u012b Isti`m\u0101l al-'A`d\u0101d al-Hind\u012byyah) which contributed greatly to diffusion of the Hindu system of numeration in the Middle-East and the West. In geometry, among other works, he wrote on the theory of parallels. Also related to geometry were two works on optics. One of the ways in which he made use of mathematics as a philosopher was to attempt to disprove the eternity of the world by demonstrating that actual infinity is a mathematical and logical absurdity.","completion":"Cryptography"}
{"prompt":"DL\/ECKAS-DH1 and DL\/ECKAS-DH2 (Discrete Logarithm\/Elliptic Curve Key Agreement Scheme, Diffie\u2013Hellman version): This includes both traditional Diffie\u2013Hellman and elliptic curve Diffie\u2013Hellman.\nDL\/ECKAS-MQV (Discrete Logarithm\/Elliptic Curve Key Agreement Scheme, Menezes\u2013Qu\u2013Vanstone version)","completion":"Cryptography"}
{"prompt":"In 2013, The New York Times stated that Dual Elliptic Curve Deterministic Random Bit Generation (or Dual_EC_DRBG) had been included as a NIST national standard due to the influence of NSA, which had included a deliberate weakness in the algorithm and the recommended elliptic curve. RSA Security in September 2013 issued an advisory recommending that its customers discontinue using any software based on Dual_EC_DRBG. In the wake of the exposure of Dual_EC_DRBG as \"an NSA undercover operation\", cryptography experts have also expressed concern over the security of the NIST recommended elliptic curves, suggesting a return to encryption based on non-elliptic-curve groups.\n\nAdditionally, in August 2015, the NSA announced that it plans to replace Suite B with a new cipher suite due to concerns about quantum computing attacks on ECC.","completion":"Cryptography"}
{"prompt":"A secure block cipher can be converted into a CSPRNG by running it in counter mode using, for example, a special construct that NIST in SP 800-90A calls CTR_DRBG.\nA cryptographically secure hash might also be a base of a good CSPRNG, using, for example, a construct that NIST calls Hash_DRBG.\nAn HMAC primitive can be used as a base of a CSPRNG, for example, as part of the construct that NIST calls HMAC_DRBG.","completion":"Cryptography"}
{"prompt":"The chart below depicts who knows what, again with non-secret values in blue, and secret values in red. Here Eve is an eavesdropper \u2013 she watches what is sent between Alice and Bob, but she does not alter the contents of their communications.\n\ng, public (primitive root) base, known to Alice, Bob, and Eve. g = 5\np, public (prime) modulus, known to Alice, Bob, and Eve. p = 23\na, Alice's private key, known only to Alice. a = 6\nb, Bob's private key known only to Bob. b = 15\nA, Alice's public key, known to Alice, Bob, and Eve. A = ga mod p = 8\nB, Bob's public key, known to Alice, Bob, and Eve. B = gb mod p = 19Now s is the shared secret key and it is known to both Alice and Bob, but not to Eve. Note that it is not helpful for Eve to compute AB, which equals ga + b mod p.\nNote: It should be difficult for Alice to solve for Bob's private key or for Bob to solve for Alice's private key. If it is not difficult for Alice to solve for Bob's private key (or vice versa), then an eavesdropper, Eve, may simply substitute her own private \/ public key pair, plug Bob's public key into her private key, produce a fake shared secret key, and solve for Bob's private key (and use that to solve for the shared secret key). Eve may attempt to choose a public \/ private key pair that will make it easy for her to solve for Bob's private key.","completion":"Cryptography"}
{"prompt":"One attack on KASUMI, a block cipher used in 3GPP, is a related-key rectangle attack which breaks the full eight rounds of the cipher faster than exhaustive search (Biham et al., 2005). The attack requires 254.6 chosen plaintexts, each of which has been encrypted under one of four related keys and has a time complexity equivalent to 276.1 KASUMI encryptions.","completion":"Cryptography"}
{"prompt":"In case of an offline attack where the attacker has gained access to the encrypted material, one can try key combinations without the risk of discovery or interference. In case of online attacks, database and directory administrators can deploy countermeasures such as limiting the number of attempts that a password can be tried, introducing time delays between successive attempts, increasing the answer's complexity (e.g., requiring a CAPTCHA answer or employing multi-factor authentication), and\/or locking accounts out after unsuccessful login attempts.  Website administrators may prevent a particular IP address from trying more than a predetermined number of password attempts against any account on the site.","completion":"Cryptography"}
{"prompt":"Below is a list of cryptographic libraries that provide support for ECDSA:\n\nBotan\nBouncy Castle\ncryptlib\nCrypto++\nCrypto API (Linux)\nGnuTLS\nlibgcrypt\nLibreSSL\nmbed TLS\nMicrosoft CryptoAPI\nOpenSSL\nwolfCrypt","completion":"Cryptography"}
{"prompt":"This specification includes key agreement, signature, and encryption\nschemes using several mathematical approaches: integer factorization,\ndiscrete logarithm, and elliptic curve discrete logarithm.","completion":"Cryptography"}
{"prompt":"Transaction fees for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction. The currency holder can choose a specific transaction fee, while network entities process transactions in order of highest offered fee to lowest. Cryptocurrency exchanges can simplify the process for currency holders by offering priority alternatives and thereby determine which fee will likely cause the transaction to be processed in the requested time.For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while Bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845, while for Bitcoin it corresponded to $0.659.Some cryptocurrencies have no transaction fees, and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.","completion":"Cryptography"}
{"prompt":"Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers. A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades. A 2019 report by Bitwise Asset Management claimed that 95% of all Bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.","completion":"Cryptography"}
{"prompt":"In 2005, Curve25519 was first released by Daniel J. Bernstein.In 2013, interest began to increase considerably when it was discovered that the NSA had potentially implemented a backdoor into the P-256 curve based Dual_EC_DRBG algorithm. While not directly related, suspicious aspects of the NIST's P curve constants led to concerns that the NSA had chosen values that gave them an advantage in breaking the encryption.\n\"I no longer trust the constants. I believe the NSA has manipulated them through their relationships with industry.\"\n\nSince 2013, Curve25519 has become the de facto alternative to P-256, being used in a wide variety of applications. Starting in 2014, OpenSSH defaults to Curve25519-based ECDH and GnuPG adds support for Ed25519 keys for signing and encryption. Behavior for general SSH protocol is still being standardized as of 2018.In 2017, NIST announced that Curve25519 and Curve448 would be added to Special Publication 800-186, which specifies approved elliptic curves for use by the US Federal Government. Both are described in RFC 7748. A 2019 draft of \"FIPS 186-5\" notes the intention to allow usage of Ed25519 for digital signatures. A 2019 draft of Special Publication 800-186 notes the intention to allow usage of Curve25519.In 2018, DKIM specification was amended so as to allow signatures with this algorithm.Also in 2018, RFC 8446 was published as the new Transport Layer Security v1.3 standard. It recommends support for X25519, Ed25519, X448, and Ed448 algorithms.","completion":"Cryptography"}
{"prompt":"The Americans referred to the intelligence resulting from cryptanalysis, perhaps especially that from the Purple machine, as 'Magic'. The British eventually settled on 'Ultra' for intelligence resulting from cryptanalysis, particularly that from message traffic protected by the various Enigmas. An earlier British term for Ultra had been 'Boniface' in an attempt to suggest, if betrayed, that it might have an individual agent as a source.\n\nAllied cipher machines used in World War II included the British TypeX and the American SIGABA; both were electromechanical rotor designs similar in spirit to the Enigma, albeit with major improvements. Neither is known to have been broken by anyone during the War. The Poles used the Lacida machine, but its security was found to be less than intended (by Polish Army cryptographers in the UK), and its use was discontinued. US troops in the field used the M-209 and the still less secure M-94 family machines. British SOE agents initially used 'poem ciphers' (memorized poems were the encryption\/decryption keys), but later in the War, they began to switch to one-time pads.\nThe VIC cipher (used at least until 1957 in connection with Rudolf Abel's NY spy ring) was a very complex hand cipher, and is claimed to be the most complicated known to have been used by the Soviets, according to David Kahn in Kahn on Codes. For the decrypting of Soviet ciphers (particularly when one-time pads were reused), see Venona project.","completion":"Cryptography"}
{"prompt":"The simplest and the original implementation, later formalized as Finite Field Diffie-Hellman in RFC 7919, of the protocol uses the multiplicative group of integers modulo p, where p is prime, and g is a primitive root modulo p. These two values are chosen in this way to ensure that the resulting shared secret can take on any value from 1 to p\u20131.. Here is an example of the protocol, with non-secret values in blue, and secret values in red.. Alice and Bob publicly agree to use a modulus p = 23 and base g = 5 (which is a primitive root modulo 23)..","completion":"Cryptography"}
{"prompt":"Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a message authentication code (MAC) or a digital signature usually done by a hashing algorithm or a PGP signature. Authenticated encryption algorithms are designed to provide both encryption and integrity protection together. Standards for cryptographic software and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single error in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See for example traffic analysis, TEMPEST, or Trojan horse.Integrity protection mechanisms such as MACs and digital signatures must be applied to the ciphertext when it is first created, typically on the same device used to compose the message, to protect a message end-to-end along its full transmission path; otherwise, any node between the sender and the encryption agent could potentially tamper with it. Encrypting at the time of creation is only secure if the encryption device itself has correct keys and has not been tampered with. If an endpoint device has been configured to trust a root certificate that an attacker controls, for example, then the attacker can both inspect and tamper with encrypted data by performing a man-in-the-middle attack anywhere along the message's path. The common practice of TLS interception by network operators represents a controlled and institutionally sanctioned form of such an attack, but countries have also attempted to employ such attacks as a form of control and censorship.","completion":"Cryptography"}
{"prompt":"Crypto-macroeconomics is concerned with the regional, national, and international regulation of cryptocurrencies and DeFi transactions. The Group of Seven governments' interest in cryptocurrencies became evident in August 2014, when the United Kingdom Treasury commissioned a study of cryptocurrencies and their potential role in the UK economy, and issued its final report in January 2021. In June 2021, El Salvador became the first country to accept Bitcoin as legal tender. In August 2021, Cuba followed with a legal resolution to recognize and regulate cryptocurrencies such as Bitcoin. However, in September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal, completing a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.","completion":"Cryptography"}
{"prompt":"Schneier is a proponent of full disclosure, i.e. making security issues public.\n\nIf researchers don't go public, things don\u2019t get fixed. Companies don't see it as a security problem; they see it as a PR problem.","completion":"Cryptography"}
{"prompt":"While viruses in the wild have used cryptography in the past, the only purpose of such usage of cryptography was to avoid detection by antivirus software. For example, the tremor virus used polymorphism as a defensive technique in an attempt to avoid detection by anti-virus software. Though cryptography does assist in such cases to enhance the longevity of a virus, the capabilities of cryptography are not used in the payload. The One-half virus was amongst the first viruses known to have encrypted affected files.","completion":"Cryptography"}
{"prompt":"In 2006, Hellman suggested the algorithm be called Diffie\u2013Hellman\u2013Merkle key exchange in recognition of Ralph Merkle's contribution to the invention of public-key cryptography (Hellman, 2006), writing:\n\nThe system...has since become known as Diffie\u2013Hellman key exchange. While that system was first described in a paper by Diffie and me, it is a public key distribution system, a concept developed by Merkle, and hence should be called 'Diffie\u2013Hellman\u2013Merkle key exchange' if names are to be associated with it. I hope this small pulpit might help in that endeavor to recognize Merkle's equal contribution to the invention of public key cryptography.","completion":"Cryptography"}
{"prompt":"The provable security of FSB means that finding collisions is NP-complete. But the proof is a reduction to a problem with asymptotically hard worst-case complexity. This offers only limited security assurance as there still can be an algorithm that easily solves the problem for a subset of the problem space. For example, there exists a linearization method that can be used to produce collisions of in a matter of seconds on a desktop PC for early variants of FSB with claimed 2^128 security. It is shown that the hash function offers minimal pre-image or collision resistance when the message space is chosen in a specific way.","completion":"Cryptography"}
{"prompt":"In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea. Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.","completion":"Cryptography"}
{"prompt":"A fault injection attack involves stressing the transistors responsible for encryption tasks to generate faults that will then be used as input for analysis. The stress can be an electromagnetic pulse (EM pulse or laser pulse).\nPractical fault injection consists of using an electromagnetic probe connected to a pulser or a laser generating a disturbance of a similar length to the processor's cycle time (of the order of a nanosecond). The energy transferred to the chip may be sufficient to burn out certain components of the chip, so the voltage of the pulser (a few hundred volts) and the positioning of the probe must be finely calibrated. For greater precision, the chips are often decapsulated (chemically eroded to expose the bare silicon).","completion":"Cryptography"}
{"prompt":"Proof-of-work cryptocurrencies, such as Bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity in order to make verification costly enough to accurately validate public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they further must consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities. In 2018, Bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using Bitcoin as a means of payment. However, the efficiency of the Bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.","completion":"Cryptography"}
{"prompt":"Galois\/counter mode (GCM) combines the well-known counter mode of encryption with the new Galois mode of authentication. The key feature is the ease of parallel computation of the Galois field multiplication used for authentication. This feature permits higher throughput than encryption algorithms.\nGCM is defined for block ciphers with a block size of 128 bits. Galois message authentication code (GMAC) is an authentication-only variant of the GCM which can form an incremental message authentication code. Both GCM and GMAC can accept initialization vectors of arbitrary length. GCM can take full advantage of parallel processing and implementing GCM can make efficient use of an instruction pipeline or a hardware pipeline. The CBC mode of operation incurs pipeline stalls that hamper its efficiency and performance.\nLike in CTR, blocks are numbered sequentially, and then this block number is combined with an IV and encrypted with a block cipher E, usually AES. The result of this encryption is then XORed with the plaintext to produce the ciphertext. Like all counter modes, this is essentially a stream cipher, and so it is essential that a different IV is used for each stream that is encrypted.\n\nThe ciphertext blocks are considered coefficients of a polynomial which is then evaluated at a key-dependent point H, using finite field arithmetic. The result is then encrypted, producing an authentication tag that can be used to verify the integrity of the data. The encrypted text then contains the IV, ciphertext, and authentication tag.","completion":"Cryptography"}
{"prompt":"A family of functions {hk : {0, 1}m(k) \u2192 {0, 1}l(k)} generated by some algorithm G is a family of collision-resistant hash functions, if |m(k)| > |l(k)| for any k, i.e., hk compresses the input string, and every hk can be computed within polynomial time given k, but for any probabilistic polynomial algorithm A, we have\n\nPr [k \u2190 G(1n), (x1, x2) \u2190 A(k, 1n) s.t. x1 \u2260 x2 but hk(x1) = hk(x2)] < negl(n),where negl(\u00b7) denotes some negligible function, and n is the security parameter.","completion":"Cryptography"}
{"prompt":"Bamford, James, The Puzzle Palace: A Report on America's Most Secret Agency (1982)(ISBN 0-14-006748-5), and the more recent Body of Secrets: Anatomy of the Ultra-Secret National Security Agency (2001). The first is one of a very few books about the US Government's NSA. The second is also about NSA but concentrates more on its history. There is some very interesting material in Body of Secrets about US attempts (the TICOM mission) to investigate German cryptographic efforts immediately as WW II wound down.\nGustave Bertrand, Enigma ou la plus grande \u00e9nigme de la guerre 1939\u20131945 (Enigma:  the Greatest Enigma of the War of 1939\u20131945), Paris, 1973. The first public disclosure in the West of the breaking of Enigma, by the chief of French military cryptography prior to WW II. The first public disclosure anywhere was made in the first edition of Bitwa o tajemnice by the late W\u0142adys\u0142aw Kozaczuk.\nJames Gannon, Stealing Secrets, Telling Lies:  How Spies and Codebreakers Helped Shape the Twentieth Century, Washington, D.C., Brassey's, 2001:  an overview of major 20th-century episodes in cryptology and espionage, particularly strong regarding the misappropriation of credit for conspicuous achievements.\nKahn, David \u2013 The Codebreakers (1967) (ISBN 0-684-83130-9) A single-volume source for cryptographic history, at least for events up to the mid-'60s (i.e., to just before DES and the public release of asymmetric key cryptography). The added chapter on more recent developments (in the most recent edition) is quite thin. Kahn has written other books and articles on cryptography, and on cryptographic history. They are very highly regarded.\nKozaczuk, W\u0142adys\u0142aw, Enigma:  How the German Machine Cipher Was Broken, and How It Was Read by the Allies in World War II, edited and translated by Christopher Kasparek, Frederick, MD, 1984: a history of cryptological efforts against Enigma, concentrating on the contributions of Polish mathematicians Marian Rejewski, Jerzy R\u00f3\u017cycki and Henryk Zygalski; of particular interest to specialists will be several technical appendices by Rejewski.\nLevy, Steven \u2013 Crypto: How the Code Rebels Beat the Government\u2014Saving Privacy in the Digital Age (2001) (ISBN 0-14-024432-8):  a journalistic overview of the development of public cryptographic techniques and the US regulatory context for cryptography. This is an account of a major policy conflict.\nSingh, Simon, The Code Book (ISBN 1-85702-889-9):  an anecdotal introduction to the history of cryptography. Covers more recent material than does even the revised edition of Kahn's The Codebreakers. Clearly written and quite readable. The included cryptanalytic contest has been won and the prize awarded, but the cyphertexts are still worth attempting.\nBauer, F L, Decrypted Secrets, This book is unusual. It is both a history of cryptography, and a discussion of mathematical topics related to cryptography. In his review, David Kahn said he thought it the best book he'd read on the subject. It is essentially two books, in more or less alternating chapters. Originally in German, and the translation shows it in places. Some surprising content, e.g., in the discussion of President Edgar Hoover's Secretary of State, Henry Stimson.\nBudiansky, Stephen, Battle of Wits:  a one-volume history of cryptography in WW II. It is well written, well researched, and responsible. Technical material (e.g., a description of the cryptanalysis of Enigma) is limited, but clearly presented.\nBudiansky, Stephen, Code Warriors: NSA's Codebreakers and the Secret Intelligence War Against the Soviet Union (Knopf, 2016). (ISBN 0385352662): A sweeping, in-depth history of NSA, whose famous \u201ccult of silence\u201d has left the agency shrouded in mystery for decades.\nPrados, John \u2013 Combined Fleet Decoded, An account of cryptography in the Pacific Theatre of World War II with special emphasis on the Japanese side. Reflects extensive research in Japanese sources and recently available US material. Contains material not previously accessible or unavailable.\nMarks, Leo, Between Silk and Cyanide: a Codemaker's Story, 1941\u20131945, (HarperCollins, 1998). (ISBN 0-684-86780-X). A humorous but informative account of code-making and -breaking in Britain's WWII Special Operations Executive.\nMundy, Liza, Code Girls, (Hachette Books, 2017) (ISBN 978-0-316-35253-6) An account of some of the thousands of women recruited for U.S. cryptologic work before and during World War II, including top analysts such as Elizebeth Smith Friedman and Agnes Meyer Driscoll, lesser known but outstanding contributors like Genevieve Grotjan Feinstein and Ann Zeilinger Caracristi, and many others, and how the women made a strategic difference in the war.\nYardley, Herbert, The American Black Chamber (ISBN 0-345-29867-5),  a classic 1931 account of American code-breaking during and after World War I; and Chinese Black Chamber: An Adventure in Espionage (ISBN 0-395-34648-7), about Yardley's work with the Chinese government in the years just before World War II.  Yardley has an enduring reputation for embellishment, and some of the material in these books is less than reliable.   The American Black Chamber was written after the New York operation Yardley ran was shut down by Secretary of State Henry L. Stimson and the US Army, on the grounds that \"gentlemen don't read each other's mail\".","completion":"Cryptography"}
{"prompt":"When a block cipher is used in a given mode of operation, the resulting algorithm should ideally be about as secure as the block cipher itself. ECB (discussed above) emphatically lacks this property: regardless of how secure the underlying block cipher is, ECB mode can easily be attacked. On the other hand, CBC mode can be proven to be secure under the assumption that the underlying block cipher is likewise secure. Note, however, that making statements like this requires formal mathematical definitions for what it means for an encryption algorithm or a block cipher to \"be secure\". This section describes two common notions for what properties a block cipher should have. Each corresponds to a mathematical model that can be used to prove properties of higher-level algorithms, such as CBC.\nThis general approach to cryptography \u2013 proving higher-level algorithms (such as CBC) are secure under explicitly stated assumptions regarding their components (such as a block cipher) \u2013 is known as provable security.","completion":"Cryptography"}
{"prompt":"One digital signature scheme (of many) is based on RSA. To create signature keys, generate an RSA key pair containing a modulus, N, that is the product of two random secret distinct large primes, along with integers, e and d, such that e d \u2261 1 (mod \u03c6(N)), where \u03c6 is Euler's totient function. The signer's public key consists of N and e, and the signer's secret key contains d.\nUsed directly, this type of signature scheme is vulnerable to key-only existential forgery attack. To create a forgery, the attacker picks a random signature \u03c3 and uses the verification procedure to determine the message, m, corresponding to that signature. In practice, however, this type of signature is not used directly, but rather, the message to be signed is first hashed to produce a short digest, that is then padded to larger width comparable to N, then signed with the reverse trapdoor function. This forgery attack, then, only produces the padded hash function output that corresponds to \u03c3, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, hash-then-sign (an idealized version of that practice where hash and padding combined have close to N possible outputs), this form of signature is existentially unforgeable, even against a chosen-plaintext attack.There are several reasons to sign such a hash (or message digest) instead of the whole document.\n\nFor efficiency\nThe signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.\nFor compatibility\nMessages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number N). A hash function can be used to convert an arbitrary input into the proper format.\nFor integrity\nWithout the hash function, the text \"to be signed\" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly. However, the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order.","completion":"Cryptography"}
{"prompt":"\"Movie-plot threat\" is a term Schneier coined that refers to very specific and dramatic terrorist attack scenarios, reminiscent of the behavior of terrorists in movies, rather than what terrorists actually do in the real world. Security measures created to protect against movie plot threats do not provide a higher level of real security, because such preparation only pays off if terrorists choose that one particular avenue of attack, which may not even be feasible. Real-world terrorists would also be likely to notice the highly specific security measures, and simply attack in some other way. The specificity of movie plot threats gives them power in the public imagination, however, so even extremely unrealistic security theater countermeasures may receive strong support from the public and legislators. Among many other examples of movie plot threats, Schneier described banning baby carriers from subways, for fear that they may contain explosives. Starting in April 2006, Schneier has had an annual contest to create the most fantastic movie-plot threat. In 2015, during the 8th and as of 17 February 2022 the last one, he mentioned that the contest may have run its course.","completion":"Cryptography"}
{"prompt":"Abu Yusuf Yaqub ibn Ishaq al-Sabbah Al-Kindi, (A Manuscript on Deciphering Cryptographic Messages), 9th century included first known explanation of frequency analysis cryptanalysis\nMichel de Nostredame, (16th century prophet famed since 1555 for prognostications), known widely for his \"Les Propheties\" sets of quatrains composed from four languages into a ciphertext, deciphered in a series called \"Rise to Consciousness\" (Deschausses, M., Outskirts Press, Denver, CO, Nov 2008).\nRoger Bacon (English friar and polymath), Epistle on the secret Works of Art and Nullity of Magic, 13th century, possibly the first  European work on cryptography since Classical times, written in Latin and not widely available then or now\nJohannes Trithemius, Steganographia (\"Hidden Writing\"), written ca. 1499; pub 1606, banned by the Catholic Church 1609 as alleged discussion of magic, see Polygraphiae (below).\nJohannes Trithemius, Polygraphiae Libri Sex (\"Six Books on Polygraphy\"), 1518, first printed book on cryptography (thought to really be about magic by some observers at the time)\nGiovan Battista Bellaso, La cifra del. Sig. Giovan Battista Bellaso, 1553, first pub of the cypher widely misattributed to Vigen\u00e8re.\nGiambattista della Porta, De Furtivis Literarum Notis (\"On concealed characters in writing\"), 1563.\nBlaise de Vigen\u00e8re, Traicte de Chiffres, 1585.\nGustavus Selenus, Cryptomenytics, 1624, (modern era English trans by J W H Walden)\nJohn Wilkins, Mercury, 1647, earliest printed book in English about cryptography\nJohann Ludwig Kl\u00fcber, Kryptographik Lehrbuch der Geheimschreibekunst (\"Cryptology: Instruction Book on the Art of Secret Writing\"), 1809.\nFriedrich Kasiski, Die Geheimschriften und die Dechiffrierkunst (\"Secret writing and the Art of Deciphering\"), pub 1863, contained the first public description of a technique for cryptanalyzing polyalphabetic cyphers.\nEtienne Bazeries, Les Chiffres secrets d\u00e9voil\u00e9s (\"Secret ciphers unveiled\") about 1900.\n\u00c9mile Victor Th\u00e9odore Myszkowski, Cryptographie ind\u00e9chiffrable: bas\u00e9e sur de nouvelles combinaisons rationelles (\"Unbreakable cryptography\"), published 1902.\nWilliam F. Friedman and others, the Riverbank Publications, a series of pamphlets written during and after World War I that are considered seminal to modern cryptanalysis, including no. 22 on the Index of Coincidence.","completion":"Cryptography"}
{"prompt":"While modern ciphers like AES and the higher quality asymmetric ciphers are widely considered unbreakable, poor designs and implementations are still sometimes adopted and there have been important cryptanalytic breaks of deployed crypto systems in recent years. Notable examples of broken crypto designs include the first Wi-Fi encryption scheme WEP, the Content Scrambling System used for encrypting and controlling DVD use, the A5\/1 and A5\/2 ciphers used in GSM cell phones, and the CRYPTO1 cipher used in the widely deployed MIFARE Classic smart cards from NXP Semiconductors, a spun off division of Philips Electronics. All of these are symmetric ciphers. Thus far, not one of the mathematical ideas underlying public key cryptography has been proven to be 'unbreakable', and so some future mathematical analysis advance might render systems relying on them insecure. While few informed observers foresee such a breakthrough, the key size recommended for security as best practice keeps increasing as increased computing power required for breaking codes becomes cheaper and more available. Quantum computers, if ever constructed with enough capacity, could break existing public key algorithms and efforts are underway to develop and standardize post-quantum cryptography.\nEven without breaking encryption in the traditional sense, side-channel attacks can be mounted that exploit information gained from the way a computer system is implemented, such as cache memory usage, timing information, power consumption, electromagnetic leaks or even sounds emitted. Newer cryptographic algorithms are being developed that make such attacks more difficult.","completion":"Cryptography"}
{"prompt":"Like most public key systems, the ElGamal cryptosystem is usually used as part of a hybrid cryptosystem, where the message itself is encrypted using a symmetric cryptosystem, and ElGamal is then used to encrypt only the symmetric key. This is because asymmetric cryptosystems like ElGamal are usually slower than symmetric ones for the same level of security, so it is faster to encrypt the message, which can be arbitrarily large, with a symmetric cipher, and then use ElGamal only to encrypt the symmetric key, which usually is quite small compared to the size of the message.","completion":"Cryptography"}
{"prompt":"We start with a compression function \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   with parameters \n  \n    \n      \n        \n          n\n          ,\n          r\n          ,\n          w\n        \n      \n    \n    {\\displaystyle {n,r,w}}\n   such that  \n  \n    \n      \n        n\n        >\n        w\n      \n    \n    {\\displaystyle n>w}\n   and \n  \n    \n      \n        w\n        log\n        \u2061\n        (\n        n\n        \n          \/\n        \n        w\n        )\n        >\n        r\n      \n    \n    {\\displaystyle w\\log(n\/w)>r}\n  . This function will only work on messages with length \n  \n    \n      \n        s\n        =\n        w\n        log\n        \u2061\n        (\n        n\n        \n          \/\n        \n        w\n        )\n      \n    \n    {\\displaystyle s=w\\log(n\/w)}\n  ; \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   will be the size of the output. Furthermore, we want \n  \n    \n      \n        n\n        ,\n        r\n        ,\n        w\n        ,\n        s\n      \n    \n    {\\displaystyle n,r,w,s}\n   and \n  \n    \n      \n        log\n        \u2061\n        (\n        n\n        \n          \/\n        \n        w\n        )\n      \n    \n    {\\displaystyle \\log(n\/w)}\n   to be natural numbers, where \n  \n    \n      \n        log\n      \n    \n    {\\displaystyle \\log }\n   denotes the binary logarithm. The reason for \n  \n    \n      \n        w\n        \u22c5\n        log\n        \u2061\n        (\n        n\n        \n          \/\n        \n        w\n        )\n        >\n        r\n      \n    \n    {\\displaystyle w\\cdot \\log(n\/w)>r}\n   is that we want \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   to be a compression function, so the input must be larger than the output. We will later use the Merkle\u2013Damg\u00e5rd construction to extend the domain to inputs of arbitrary lengths.\nThe basis of this function consists of a (randomly chosen) binary \n  \n    \n      \n        r\n        \u00d7\n        n\n      \n    \n    {\\displaystyle r\\times n}\n   matrix \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   which acts on a message of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   bits by matrix multiplication. Here we encode the \n  \n    \n      \n        w\n        log\n        \u2061\n        (\n        n\n        \n          \/\n        \n        w\n        )\n      \n    \n    {\\displaystyle w\\log(n\/w)}\n  -bit message as a vector in \n  \n    \n      \n        (\n        \n          \n            F\n          \n          \n            2\n          \n        \n        \n          )\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle (\\mathbf {F} _{2})^{n}}\n  , the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -dimensional vector space over the field of two elements, so the output will be a message of \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   bits.\nFor security purposes as well as to get a faster hash speed we want to use only \u201cregular words of weight \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \u201d as input for our matrix.","completion":"Cryptography"}
{"prompt":"Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the Automotive Network Exchange for the automobile industry and the SAFE-BioPharma Association for the healthcare industry.","completion":"Cryptography"}
{"prompt":"Differential cryptanalysis is usually a chosen plaintext attack, meaning that the attacker must be able to obtain ciphertexts for some set of plaintexts of their choosing. There are, however, extensions that would allow a known plaintext or even a ciphertext-only attack. The basic method uses pairs of plaintexts related by a constant difference. Difference can be defined in several ways, but the eXclusive OR (XOR) operation is usual. The attacker then computes the differences of the corresponding ciphertexts, hoping to detect statistical patterns in their distribution. The resulting pair of differences is called a differential. Their statistical properties depend upon the nature of the S-boxes used for encryption, so the attacker analyses differentials \n  \n    \n      \n        (\n        \n          \u0394\n          \n            x\n          \n        \n        ,\n        \n          \u0394\n          \n            y\n          \n        \n        )\n      \n    \n    {\\displaystyle (\\Delta _{x},\\Delta _{y})}\n   where\n\n(and \u2295 denotes exclusive or) for each such S-box S. In the basic attack, one particular ciphertext difference is expected to be especially frequent. In this way, the cipher can be distinguished from random. More sophisticated variations allow the key to be recovered faster than an exhaustive search.\nIn the most basic form of key recovery through differential cryptanalysis, an attacker requests the ciphertexts for a large number of plaintext pairs, then assumes that the differential holds for at least r \u2212 1 rounds, where r is the total number of rounds. The attacker then deduces which round keys (for the final round) are possible, assuming the difference between the blocks before the final round is fixed. When round keys are short, this can be achieved by simply exhaustively decrypting the ciphertext pairs one round with each possible round key. When one round key has been deemed a potential round key considerably more often than any other key, it is assumed to be the correct round key.\nFor any particular cipher, the input difference must be carefully selected for the attack to be successful. An analysis of the algorithm's internals is undertaken; the standard method is to trace a path of highly probable differences through the various stages of encryption, termed a differential characteristic.\nSince differential cryptanalysis became public knowledge, it has become a basic concern of cipher designers. New designs are expected to be accompanied by evidence that the algorithm is resistant to this attack and many including the Advanced Encryption Standard, have been proven secure against the attack.","completion":"Cryptography"}
{"prompt":"Originating from the Arabic word for zero \u0635\u0641\u0631 (sifr), the word \"cipher\" spread to Europe as part of the Arabic numeral system during the Middle Ages. The Roman numeral system lacked the concept of zero, and this limited advances in mathematics. In this transition, the word was adopted into Medieval Latin as cifra, and then into Middle French as cifre. This eventually led to the English word cipher (minority spelling cypher). One theory for how the term came to refer to encoding is that the concept of zero was confusing to Europeans, and so the term came to refer to a message or communication that was not easily understood.The term cipher was later also used to refer to any Arabic digit, or to calculation using them, so encoding text in the form of Arabic numerals is literally converting the text to \"ciphers\".","completion":"Cryptography"}
{"prompt":"By World War II, mechanical and electromechanical cipher machines were in wide use, although\u2014where such machines were impractical\u2014code books and manual systems continued in use. Great advances were made in both cipher design and cryptanalysis, all in secrecy. Information about this period has begun to be declassified as the official British 50-year secrecy period has come to an end, as US archives have slowly opened, and as assorted memoirs and articles have appeared.","completion":"Cryptography"}
{"prompt":"Public key ciphers\nSignature\nDSA\nECDSA\nEdDSA\nRSA-PSS\nRSASSA-PKCS1-v1_5\nConfidentiality\nRSA-OAEP\nKey exchange\nDH\nECDH\nSymmetric key ciphers\n64-bit block ciphers\nN\/A\n128-bit block ciphers\nAES\nCamellia\nStream ciphers\nKCipher-2\nHash functions\nSHA-256\nSHA-384\nSHA-512\nSHA-512\/256\nSHA3-256\nSHA3-384\nSHA3-512\nSHAKE128\nSHAKE256\nModes of operation\nEncryption modes\nCBC\nCFB\nCTR\nOFB\nXTS\nAuthenticated encryption modes\nCCM\nGCM\nMessage authentication codes\nCMAC\nHMAC\nAuthenticated encryption\nChaCha20-Poly1305\nEntity authentication\nISO\/IEC 9798-2\nISO\/IEC 9798-3\nISO\/IEC 9798-4","completion":"Cryptography"}
{"prompt":"Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.Another instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).","completion":"Cryptography"}
{"prompt":"The used keys can either be ephemeral or static (long term) key, but could even be mixed, so called semi-static DH. These variants have different properties and hence different use cases. An overview over many variants and some also discussions can for example be found in NIST SP 800-56A. A basic list:\n\nephemeral, ephemeral: Usually used for key agreement. Provides forward secrecy, but no authenticity.\nstatic, static: Would generate a long term shared secret. Does not provide forward secrecy, but implicit authenticity. Since the keys are static it would for example not protect against replay-attacks.\nephemeral, static: For example, used in ElGamal encryption or Integrated Encryption Scheme (IES). If used in key agreement it could provide implicit one-sided authenticity (the ephemeral side could verify the authenticity of the static side). No forward secrecy is provided.It is possible to use ephemeral and static keys in one key agreement to provide more security as for example shown in NIST SP 800-56A, but it is also possible to combine those in a single DH key exchange, which is then called triple DH (3-DH).","completion":"Cryptography"}
{"prompt":"Cryptography is widely used on the internet to help protect user-data and prevent eavesdropping. To ensure secrecy during transmission, many systems use private key cryptography to protect transmitted information. With public-key systems, one can maintain secrecy without a master key or a large number of keys. But, some algorithms like Bitlocker and Veracrypt are generally not private-public key cryptography. For example, Veracrypt uses a password hash to generate the single private key. However, it can be configured to run in public-private key systems. The C++ opensource encryption library OpenSSL provides free and opensource encryption software and tools. The most commonly used encryption cipher suit is AES, as it has hardware acceleration for all x86 based processors that has AES-NI. A close contender is ChaCha20-Poly1305, which is a stream cipher, however it is commonly used for mobile devices as they are ARM based which does not feature AES-NI instruction set extension.","completion":"Cryptography"}
{"prompt":"There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as Bitcoin result in high up-front costs to miners in the form of specialized hardware and software. Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.","completion":"Cryptography"}
{"prompt":"Poe's best-known fiction works are Gothic horror, adhering to the genre's conventions to appeal to the public taste. His most recurring themes deal with questions of death, including its physical signs, the effects of decomposition, concerns of premature burial, the reanimation of the dead, and mourning. Many of his works are generally considered part of the dark romanticism genre, a literary reaction to transcendentalism which Poe strongly disliked. He referred to followers of the transcendental movement as \"Frog-Pondians\", after the pond on Boston Common, and ridiculed their writings as \"metaphor\u2014run mad,\" lapsing into \"obscurity for obscurity's sake\" or \"mysticism for mysticism's sake\". Poe once wrote in a letter to Thomas Holley Chivers that he did not dislike transcendentalists, \"only the pretenders and sophists among them\".Beyond horror, Poe also wrote satires, humor tales, and hoaxes. For comic effect, he used irony and ludicrous extravagance, often in an attempt to liberate the reader from cultural conformity. \"Metzengerstein\" is the first story that Poe is known to have published and his first foray into horror, but it was originally intended as a burlesque satirizing the popular genre. Poe also reinvented science fiction, responding in his writing to emerging technologies such as hot air balloons in \"The Balloon-Hoax\".Poe wrote much of his work using themes aimed specifically at mass-market tastes. To that end, his fiction often included elements of popular pseudosciences, such as phrenology and physiognomy.","completion":"Cryptography"}
{"prompt":"Note: CTR mode (CM) is also known as integer counter mode (ICM) and segmented integer counter (SIC) mode.Like OFB, counter mode turns a block cipher into a stream cipher. It generates the next keystream block by encrypting successive values of a \"counter\". The counter can be any function which produces a sequence which is guaranteed not to repeat for a long time, although an actual increment-by-one counter is the simplest and most popular. The usage of a simple deterministic input function used to be controversial; critics argued that \"deliberately exposing a cryptosystem to a known systematic input represents an unnecessary risk\". However, today CTR mode is widely accepted, and any problems are considered a weakness of the underlying block cipher, which is expected to be secure regardless of systemic bias in its input. Along with CBC, CTR mode is one of two block cipher modes recommended by Niels Ferguson and Bruce Schneier.CTR mode was introduced by Whitfield Diffie and Martin Hellman in 1979.CTR mode has similar characteristics to OFB, but also allows a random-access property during decryption. CTR mode is well suited to operate on a multi-processor machine, where blocks can be encrypted in parallel. Furthermore, it does not suffer from the short-cycle problem that can affect OFB.If the IV\/nonce is random, then they can be combined with the counter using any invertible operation (concatenation, addition, or XOR) to produce the actual unique counter block for encryption. In case of a non-random nonce (such as a packet counter), the nonce and counter should be concatenated (e.g., storing the nonce in the upper 64 bits and the counter in the lower 64 bits of a 128-bit counter block). Simply adding or XORing the nonce and counter into a single value would break the security under a chosen-plaintext attack in many cases, since the attacker may be able to manipulate the entire IV\u2013counter pair to cause a collision.  Once an attacker controls the IV\u2013counter pair and plaintext, XOR of the ciphertext with the known plaintext would yield a value that, when XORed with the ciphertext of the other block sharing the same IV\u2013counter pair, would decrypt that block.Note that the nonce in this diagram is equivalent to the initialization vector (IV) in the other diagrams. However, if the offset\/location information is corrupt, it will be impossible to partially recover such data due to the dependence on byte offset.","completion":"Cryptography"}
{"prompt":"Unlike most other DLP systems (where it is possible to use the same procedure for squaring and multiplication), the EC addition is significantly different for doubling (P = Q) and general addition (P \u2260 Q) depending on the coordinate system used. Consequently, it is important to counteract side-channel attacks (e.g., timing or simple\/differential power analysis attacks) using, for example, fixed pattern window (a.k.a. comb) methods (note that this does not increase computation time). Alternatively one can use an Edwards curve; this is a special family of elliptic curves for which doubling and addition can be done with the same operation. Another concern for ECC-systems is the danger of fault attacks, especially when running on smart cards.","completion":"Cryptography"}
{"prompt":"Most block cipher algorithms are classified as iterated block ciphers which means that they transform fixed-size blocks of plaintext into identically sized blocks of ciphertext, via the repeated application of an invertible transformation known as the round function, with each iteration referred to as a round.Usually, the round function R takes different round keys Ki as a second input, which is derived from the original key:\n\n  \n    \n      \n        \n          M\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            \n              K\n              \n                i\n              \n            \n          \n        \n        (\n        \n          M\n          \n            i\n            \u2212\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle M_{i}=R_{K_{i}}(M_{i-1})}\n  where \n  \n    \n      \n        \n          M\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle M_{0}}\n   is the plaintext and \n  \n    \n      \n        \n          M\n          \n            r\n          \n        \n      \n    \n    {\\displaystyle M_{r}}\n   the ciphertext, with r being the number of rounds.\nFrequently, key whitening is used in addition to this. At the beginning and the end, the data is modified with key material (often with XOR, but simple arithmetic operations like adding and subtracting are also used):\n\n  \n    \n      \n        \n          M\n          \n            0\n          \n        \n        =\n        M\n        \u2295\n        \n          K\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle M_{0}=M\\oplus K_{0}}\n  \n\n  \n    \n      \n        \n          M\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            \n              K\n              \n                i\n              \n            \n          \n        \n        (\n        \n          M\n          \n            i\n            \u2212\n            1\n          \n        \n        )\n        \n        ;\n        \n        i\n        =\n        1\n        \u2026\n        r\n      \n    \n    {\\displaystyle M_{i}=R_{K_{i}}(M_{i-1})\\;;\\;i=1\\dots r}\n  \n\n  \n    \n      \n        C\n        =\n        \n          M\n          \n            r\n          \n        \n        \u2295\n        \n          K\n          \n            r\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle C=M_{r}\\oplus K_{r+1}}\n  Given one of the standard iterated block cipher design schemes, it is fairly easy to construct a block cipher that is cryptographically secure, simply by using a large number of rounds. However, this will make the cipher inefficient. Thus, efficiency is the most important additional design criterion for professional ciphers. Further, a good block cipher is designed to avoid side-channel attacks, such as branch prediction and input-dependent memory accesses that might leak secret data via the cache state or the execution time. In addition, the cipher should be concise, for small hardware and software implementations. Finally, the cipher should be easily crypt analyzable, such that it can be shown how many rounds the cipher needs to be reduced to so that the existing cryptographic attacks would work \u2013 and, conversely, that it can be shown that the number of actual rounds is large enough to protect against them.","completion":"Cryptography"}
{"prompt":"An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S., and Canada, have indicated that if a coin or token is an \"investment contract\" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of \"tokens\") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often Bitcoin or Ether.According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a \"balanced approach\" to ICO projects and would allow \"legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system.\" In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.","completion":"Cryptography"}
{"prompt":"Some modes such as the CBC mode only operate on complete plaintext blocks. Simply extending the last block of a message with zero bits is insufficient since it does not allow a receiver to easily distinguish messages that differ only in the number of padding bits. More importantly, such a simple solution gives rise to very efficient padding oracle attacks. A suitable padding scheme is therefore needed to extend the last plaintext block to the cipher's block size. While many popular schemes described in standards and in the literature have been shown to be vulnerable to padding oracle attacks, a solution that adds a one-bit and then extends the last block with zero-bits, standardized as \"padding method 2\" in ISO\/IEC 9797-1, has been proven secure against these attacks.","completion":"Cryptography"}
{"prompt":"Diffie\u2013Hellman key agreement is not limited to negotiating a key shared by only two participants. Any number of users can take part in an agreement by performing iterations of the agreement protocol and exchanging intermediate data (which does not itself need to be kept secret). For example, Alice, Bob, and Carol could participate in a Diffie\u2013Hellman agreement as follows, with all operations taken to be modulo p:\n\nThe parties agree on the algorithm parameters p and g.\nThe parties generate their private keys, named a, b, and c.\nAlice computes ga mod p and sends it to Bob.\nBob computes (ga)b mod p = gab mod p and sends it to Carol.\nCarol computes (gab)c mod p = gabc mod p and uses it as her secret.\nBob computes gb mod p and sends it to Carol.\nCarol computes (gb)c mod p = gbc mod p and sends it to Alice.\nAlice computes (gbc)a mod p = gbca mod p = gabc mod p and uses it as her secret.\nCarol computes gc mod p and sends it to Alice.\nAlice computes (gc)a mod p = gca mod p and sends it to Bob.\nBob computes (gca)b mod p = gcab mod p = gabc mod p and uses it as his secret.An eavesdropper has been able to see ga mod p, gb mod p, gc mod p, gab mod p, gac mod p, and gbc mod p, but cannot use any combination of these to efficiently reproduce gabc mod p.\nTo extend this mechanism to larger groups, two basic principles must be followed:\n\nStarting with an \"empty\" key consisting only of g, the secret is made by raising the current value to every participant's private exponent once, in any order (the first such exponentiation yields the participant's own public key).\nAny intermediate value (having up to N-1 exponents applied, where N is the number of participants in the group) may be revealed publicly, but the final value (having had all N exponents applied) constitutes the shared secret and hence must never be revealed publicly. Thus, each user must obtain their copy of the secret by applying their own private key last (otherwise there would be no way for the last contributor to communicate the final key to its recipient, as that last contributor would have turned the key into the very secret the group wished to protect).These principles leave open various options for choosing in which order participants contribute to keys. The simplest and most obvious solution is to arrange the N participants in a circle and have N keys rotate around the circle, until eventually every key has been contributed to by all N participants (ending with its owner) and each participant has contributed to N keys (ending with their own). However, this requires that every participant perform N modular exponentiations.\nBy choosing a more desirable order, and relying on the fact that keys can be duplicated, it is possible to reduce the number of modular exponentiations performed by each participant to log2(N) + 1 using a divide-and-conquer-style approach, given here for eight participants:\n\nParticipants A, B, C, and D each perform one exponentiation, yielding gabcd; this value is sent to E, F, G, and H. In return, participants A, B, C, and D receive gefgh.\nParticipants A and B each perform one exponentiation, yielding gefghab, which they send to C and D, while C and D do the same, yielding gefghcd, which they send to A and B.\nParticipant A performs an exponentiation, yielding gefghcda, which it sends to B; similarly, B sends gefghcdb to A. C and D do similarly.\nParticipant A performs one final exponentiation, yielding the secret gefghcdba = gabcdefgh, while B does the same to get gefghcdab = gabcdefgh; again, C and D do similarly.\nParticipants E through H simultaneously perform the same operations using gabcd as their starting point.Once this operation has been completed all participants will possess the secret gabcdefgh, but each participant will have performed only four modular exponentiations, rather than the eight implied by a simple circular arrangement.","completion":"Cryptography"}
{"prompt":"Public key ciphers\nSignature\nN\/A\nConfidentiality\nRSAES-PKCS1-v1_5\nKey exchange\nN\/A\nSymmetric key ciphers\n64-bit block ciphers\n3-key Triple DES\n128-bit block ciphers\nN\/A\nStream ciphers\nN\/A\nHash functions\nRIPEMD-160\nSHA-1\nModes of operation\nEncryption modes\nN\/A\nAuthenticated encryption modes\nN\/A\nMessage authentication codes\nCBC-MAC\nAuthenticated encryption\nN\/A\nEntity authentication\nN\/A","completion":"Cryptography"}
{"prompt":"Short stories\n\nPoetry\n\nOther works\n\nPolitian (1835) \u2013 Poe's only play\nThe Narrative of Arthur Gordon Pym of Nantucket (1838) \u2013 Poe's only complete novel\nThe Journal of Julius Rodman (1840) \u2013 Poe's second, unfinished novel\n\"The Balloon-Hoax\" (1844) \u2013 A journalistic hoax printed as a true story\n\"The Philosophy of Composition\" (1846) \u2013 Essay\nEureka: A Prose Poem (1848) \u2013 Essay\n\"The Poetic Principle\" (1848) \u2013 Essay\n\"The Light-House\" (1849) \u2013 Poe's last, incomplete work","completion":"Cryptography"}
{"prompt":"The boomerang attack is based on differential cryptanalysis.. In differential cryptanalysis, an attacker exploits how differences in the input to a cipher (the plaintext) can affect the resultant difference at the output (the ciphertext).. A high probability \"differential\" (that is, an input difference that will produce a likely output difference) is needed that covers all, or nearly all, of the cipher.. The boomerang attack allows differentials to be used which cover only part of the cipher.. The attack attempts to generate a so-called \"quartet\" structure at a point halfway through the cipher.. For this purpose, say that the encryption action, E, of the cipher can be split into two consecutive stages, E0 and E1, so that E(M) = E1(E0(M)), where M is some plaintext message..","completion":"Cryptography"}
{"prompt":"Credential recycling is the hacking practice of re-using username and password combinations gathered in previous brute-force attacks. A special form of credential recycling is pass the hash, where unsalted hashed credentials are stolen and re-used without first being brute forced.","completion":"Cryptography"}
{"prompt":"On October 23, 2017, Shaanan Cohney, Matthew Green, and Nadia Heninger, cryptographers at The University of Pennsylvania and Johns Hopkins University released details of the DUHK (Don't Use Hard-coded Keys) attack on WPA2 where hardware vendors use a hardcoded seed key for the ANSI X9.31 RNG algorithm, stating \"an attacker can brute-force encrypted data to discover the rest of the encryption parameters and deduce the master encryption key used to encrypt web sessions or virtual private network (VPN) connections.\"","completion":"Cryptography"}
{"prompt":"Al-Kindi was born in Kufa to an aristocratic family of the Arab Kinda tribe, descended from the chieftain al-Ash'ath ibn Qays, a contemporary of the Islamic Prophet Muhammad. The family belonged to the most prominent families of the tribal nobility of Kufa in the early Islamic period, until it lost much of its power following the revolt of Abd al-Rahman ibn Muhammad ibn al-Ash'ath. His father Ishaq was the governor of Basrah, and al-Kindi received his preliminary education there. He later went to complete his studies in Baghdad, where he was patronized by the Abbasid caliphs al-Ma'mun (r.\u2009813\u2013833) and al-Mu'tasim (r.\u2009833\u2013842). On account of his learning and aptitude for study, al-Ma'mun appointed him to the House of Wisdom, a recently established centre for the translation of Greek philosophical and scientific texts, in Baghdad. He was also well known for his beautiful calligraphy, and at one point was employed as a calligrapher by Caliph al-Mutawakkil (r.\u2009847\u2013861).When al-Ma'mun died, his brother, al-Mu'tasim became caliph. Al-Kindi's position would be enhanced under al-Mu'tasim, who appointed him as a tutor to his son. But on the accession of al-Wathiq (r.\u2009842\u2013847), and especially of al-Mutawakkil, al-Kindi's star waned.  There are various theories concerning this: some attribute al-Kindi's downfall to scholarly rivalries at the House of Wisdom; others refer to al-Mutawakkil's often violent persecution of unorthodox Muslims (as well as of non-Muslims); at one point al-Kindi was beaten and his library temporarily confiscated. Henry Corbin, an authority on Islamic studies, says that in 873, al-Kindi died \"a lonely man\", in Baghdad during the reign of al-Mu'tamid (r.\u2009870\u2013892).After his death, al-Kindi's philosophical works quickly fell into obscurity and many of them were lost even to later Islamic scholars and historians. Felix Klein-Franke suggests a number of reasons for this: aside from the militant orthodoxy of al-Mutawakkil, the Mongols also destroyed countless libraries during their invasion. However, he says the most probable cause of this was that his writings never found popularity amongst subsequent influential philosophers such as al-Farabi and Avicenna, who ultimately overshadowed him. His philosophical career peaked under al-Mu'tasim, to whom al-Kindi dedicated his most famous work, On First Philosophy, and whose son Ahmad was tutored by al-Kindi.","completion":"Cryptography"}
{"prompt":"In the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and \"dual-use\" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S\/MIME. Many Internet users don't realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally don't find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.","completion":"Cryptography"}
{"prompt":"Alice decrypts a ciphertext                         (                    c                        1                             ,                    c                        2                             )                 {\\displaystyle (c_{1},c_{2})}    with her private key                         x                 {\\displaystyle x}    as follows:  Compute                         s         :=                    c                        1                                   x                                     {\\displaystyle s:=c_{1}^{x}}   ..","completion":"Cryptography"}
{"prompt":"Kahn was awarded a doctorate (D.Phil) from Oxford University in 1974, in modern German history under the supervision of the then Regius professor of modern history, Hugh Trevor-Roper.\nKahn continued his work as a reporter and op-ed editor for Newsday until 1998 and served as a journalism professor at New York University.\nDespite past differences between Kahn and the National Security Agency over the information in The Codebreakers, Kahn was selected in 1995 to become NSA's scholar-in-residence. On October 26, 2010, Kahn attended a ceremony at NSA's National Cryptologic Museum (NCM) to commemorate his donation of his lifetime collection of cryptologic books, memorabilia, and artifacts to the museum and its library. The collection is housed at the NCM library and is non-circulating (that is, items cannot be checked out or loaned out), but photocopying and photography of items in the collection are allowed.\nKahn lives (as of 2012) in New York City. He has lived in Washington, D.C.; Paris, France; Freiburg, Germany; Oxford, England; and Great Neck, New York.","completion":"Cryptography"}
{"prompt":"The curve used is \n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            3\n          \n        \n        +\n        486662\n        \n          x\n          \n            2\n          \n        \n        +\n        x\n      \n    \n    {\\displaystyle y^{2}=x^{3}+486662x^{2}+x}\n  , a Montgomery curve, over the prime field defined by the prime number \n  \n    \n      \n        \n          2\n          \n            255\n          \n        \n        \u2212\n        19\n      \n    \n    {\\displaystyle 2^{255}-19}\n   (hence the numeric \"25519\" in the name), and it uses the base point \n  \n    \n      \n        x\n        =\n        9\n      \n    \n    {\\displaystyle x=9}\n  . This point generates a cyclic subgroup whose order is the prime \n  \n    \n      \n        \n          2\n          \n            252\n          \n        \n        +\n        27742317777372353535851937790883648493\n      \n    \n    {\\displaystyle 2^{252}+27742317777372353535851937790883648493}\n  . This subgroup has a co-factor of \n  \n    \n      \n        8\n      \n    \n    {\\displaystyle 8}\n  , meaning the number of elements in the subgroup is \n  \n    \n      \n        1\n        \n          \/\n        \n        8\n      \n    \n    {\\displaystyle 1\/8}\n   that of the elliptic curve group. Using a prime order subgroup prevents mounting a Pohlig\u2013Hellman algorithm attack.The protocol uses compressed elliptic point (only X coordinates), so it allows efficient use of the Montgomery ladder for ECDH, using only XZ coordinates.Curve25519 is constructed such that it avoids many potential implementation pitfalls.By design, Curve25519 is immune to timing attacks, and it accepts any 32-byte string as a valid public key and does not require validating that a given point belongs to the curve, or is generated by the base point.The curve is birationally equivalent to a twisted Edwards curve used in the Ed25519 signature scheme.","completion":"Cryptography"}
{"prompt":"The Cardan grille was invented as a method of secret writing. The word cryptography became the more familiar term for secret communications from the middle of the 17th century.  Earlier, the word steganography was common. The other general term for secret writing was cypher - also spelt cipher.  There is a modern distinction between cryptography and steganography\nSir Francis Bacon gave three fundamental conditions for ciphers.  Paraphrased, these are:\n\na cipher method should not be difficult to use\nit should not be possible for others to recover the plaintext (called 'reading the cipher')\nin some cases, the presence of messages should not be suspectedIt is difficult to fulfil all three conditions simultaneously.  Condition 3 applies to steganography.  Bacon meant that a cipher message should, in some cases, not appear to be a cipher at all. The original  Cardan Grille met that aim.\nVariations on the Cardano original, however, were not intended to fulfill condition 3 and generally failed to meet condition 2 as well. But, few  if any ciphers have ever achieved this second condition, so the point is generally a cryptanalyst's delight whenever the grille ciphers are used.\nThe attraction of a grille cipher for users lies in its ease of use (condition 1).  In short, it's very simple.","completion":"Cryptography"}
{"prompt":"An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017. The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price \u2013 or were out of stock. A GTX 1070 Ti which was released at a price of $450 sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model) was released at an MSRP of $250, and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPU's as soon as they are available.Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris B\u00f6hles, PR manager for Nvidia in the German region, said: \"Gamers come first for Nvidia.\"","completion":"Cryptography"}
{"prompt":"Lower Rejection Rate: As a Fiat-Shamir lattice signature scheme, BLISS improves upon previous ones by replacing uniform and discrete Gaussian sampling with bimodal samples, thereby reducing sampling rejection rate.\nMemory-Efficient Gaussian Sampling: In the paper describing BLISS, the authors constructed a discrete Gaussian sampler of arbitrary standard deviation, from a sampler of a fixed standard deviation then rejecting samples based on pre-computed Bernoulli constants.\nSignature Compression: As the coefficients of the signature polynomials are distributed according to discrete Gaussian, the final signature can be compressed using Huffman coding.","completion":"Cryptography"}
{"prompt":"According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind Bitcoin. Early Bitcoin supporter Roger Ver said: \"At first, almost everyone who got involved did so for philosophical reasons. We saw Bitcoin as a great idea, as a way to separate money from the state.\" Economist Paul Krugman argues that cryptocurrencies like Bitcoin are \"something of a cult\" based in \"paranoid fantasies\" of government power.David Golumbia says that the ideas influencing Bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism. Steve Bannon, who owns a \"good stake\" in Bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.Bitcoin's founder, Satoshi Nakamoto, has supported the idea that cryptocurrencies go well with libertarianism. \"It's very attractive to the libertarian viewpoint if we can explain it properly,\" Nakamoto said in 2008.According to the European Central Bank, the decentralization of money offered by Bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined, in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.","completion":"Cryptography"}
{"prompt":"Encryption in modern times is achieved by using algorithms that have a key to encrypt and decrypt information. These keys convert the messages and data into \"digital gibberish\" through encryption and then return them to the original form through decryption. In general, the longer the key is, the more difficult it is to crack the code. This holds true because deciphering an encrypted message by brute force would require the attacker to try every possible key. To put this in context, each binary unit of information, or bit, has a value of 0 or 1. An 8-bit key would then have 256 or 2^8 possible keys. A 56-bit key would have 2^56, or 72 quadrillion, possible keys to try and decipher the message. With modern technology, cyphers using keys with these lengths are becoming easier to decipher.  DES, an early US Government approved cypher, has an effective key length of 56 bits, and test messages using that cypher have been broken by brute force key search. However, as technology advances, so does the quality of encryption. Since World War II, one of the most notable advances in the study of cryptography is the introduction of the asymmetric key cyphers (sometimes termed public-key cyphers).  These are algorithms which use two mathematically related keys for encryption of the same message. Some of these algorithms permit publication of one of the keys, due to it being extremely difficult to determine one key simply from knowledge of the other.Beginning around 1990, the use of the Internet for commercial purposes and the introduction of commercial transactions over the Internet called for a widespread standard for encryption. Before the introduction of the Advanced Encryption Standard (AES), information sent over the Internet, such as financial data, was encrypted if at all, most commonly using the Data Encryption Standard (DES). This had been approved by NBS (a US Government agency) for its security, after public call for, and a competition among, candidates for such a cypher algorithm. DES was approved for a short period, but saw extended use due to complex wrangles over the use by the public of high quality encryption.  DES was finally replaced by the AES after another public competition organized by the NBS successor agency, NIST.  Around the late 1990s to early 2000s, the use of public-key algorithms became a more common approach for encryption, and soon a hybrid of the two schemes became the most accepted way for e-commerce operations to proceed. Additionally, the creation of a new protocol known as the Secure Socket Layer, or SSL, led the way for online transactions to take place. Transactions ranging from purchasing goods to online bill pay and banking used SSL. Furthermore, as wireless Internet connections became more common among households, the need for encryption grew, as a level of security was needed in these everyday situations.","completion":"Cryptography"}
{"prompt":"On 8 May 1919 Lt. J\u00f3zef Serafin Stanslicki established a Polish Army \"Cipher Section\" (Sekcja Szyfr\u00f3w), precursor to the \"Cipher Bureau\" (Biuro Szyfr\u00f3w). The Cipher Section reported to the Polish General Staff and contributed substantially to Poland's defense by J\u00f3zef Pi\u0142sudski's forces during the Polish-Soviet War of 1919\u20131921, thereby helping preserve Poland's independence, recently regained in the wake of World War I. The Cipher Section's purview included both ciphers and codes. In Polish the term \"cipher\" (szyfr) loosely refers to both these two principal categories of cryptography. (Compare the opposite practice in English, which loosely refers to both codes and ciphers as \"codes\".)\nDuring the Polish\u2013Soviet War (1919\u20131921), approximately a hundred Russian ciphers were broken by a sizable cadre of Polish cryptologists who included army Lieutenant Jan Kowalewski and three world-famous professors of mathematics \u2013 Stefan Mazurkiewicz, Wac\u0142aw Sierpi\u0144ski, and Stanis\u0142aw Le\u015bniewski. Russian army staffs were still following the same disastrously ill-disciplined signals-security procedures as had Tsarist army staffs during World War I, to the decisive advantage of their German enemy. As a result, during the Polish-Soviet War the Polish military were regularly kept informed by Russian signals stations about the movements of Russian armies and their intentions and operational orders.The Soviet staffs, according to Polish Colonel Mieczys\u0142aw \u015acie\u017cy\u0144ski, \n\nhad not the slightest hesitation about sending any and all messages of an operational nature by means of radiotelegraphy; there were periods during the war when, for purposes of operational communications and for purposes of command by higher staffs, no other means of communication whatever were used, messages being transmitted either entirely \"in clear\"  (plaintext) or encrypted by means of such an incredibly uncomplicated system that for our trained specialists reading the messages was child's play. The same held for the chitchat of personnel at radiotelegraphic stations, where discipline was disastrously lax.\nIn the crucial month of August 1920 alone, Polish cryptologists decrypted 410 signals:\n\nfrom Soviet General Mikhail Tukhachevsky, commander of the northern front\nfrom Leon Trotsky,  Soviet commissar of war\nfrom commanders of armies, for example:\nthe commander of the RKKA IV Army, Yevgenii Nikolaievich Sergeev\nthe commander of the 1st Cavalry Army, Semyon Budionny\nthe commander of the 3rd Cavalry Corps, Gai\nfrom the staffs of the XII, XV and XVI Armies\nfrom the staffs of:\nthe Mozyr Group (named after the Belarusian city)\nthe Zolochiv Group (after the Ukrainian town)\nthe Yakir Group [after General Iona Emmanuilovich Yakir]\nfrom the 2, 4, 7, 10, 11, 12, 16, 17, 18, 24, 27, 41, 44, 45, 53, 54, 58 and 60 Infantry Divisions\nfrom the 8 Cavalry Divisionetc.The intercepts were as a rule decrypted the same day, or at latest the next day, and were immediately sent to the Polish General Staff's Section II (Intelligence) and operational section.  The more important signals were read in their entirety by the Chief of the General Staff, and even by the Commander in Chief, Marshal J\u00f3zef Pi\u0142sudski. Interception and reading of the signals provided Polish intelligence with entire Russian operational orders. The Poles were able to follow the whole operation of Budionny's Cavalry Army in the second half of August 1920 with incredible precision, just by monitoring his radiotelegraphic correspondence with Tukhachevsky, including the famous and historic conflict between the two Russian commanders.The intercepts even included an order from Trotsky to the revolutionary council of war of the Western Front, confirming Tukhachevsky's operational orders, thus giving them the authority of the supreme chief of the Soviet armed forces. An entire operational order from Tukhachevsky to Budionny was intercepted on 19 August and read on 20 August, stating the tasks of all of Tukhachevsky's armies, of which only the essence had previously been known.\u015acie\u017cy\u0144ski surmises that the Soviets must likewise have intercepted Polish operational signals; but he doubts that this would have availed them much, since Polish cryptography \"stood abreast of modern cryptography\" and since only a small number of Polish higher headquarters were equipped with radio stations, of which there was a great shortage; and finally, Polish headquarters were more cautious than the Russians and almost every Polish division had the use of a land line.Polish cryptologists enjoyed generous support under the command of Col. Tadeusz Schaetzel, chief of the Polish General Staff's Section II (Intelligence). They worked at Warsaw's radio station WAR, one of two Polish long-range radio transmitters at the time. The Polish cryptologists' work led, among many other things, to the discovery of a large gap on the Red Army's left flank, which enabled Poland's Marshal J\u00f3zef Pi\u0142sudski to drive a war-winning wedge into that gap during the August 1920 Battle of Warsaw.The discovery of the Cipher Bureau's archives, decades after the Polish-Soviet War, has borne out \u015acie\u017cy\u0144ski's assertion\n\nthat ... radio intelligence ... furnished [the Polish Commander-in-Chief, J\u00f3zef Pi\u0142sudski], in the years 1919\u20131920 ... the most complete and... current intelligence on all aspects of the functioning of the Red Army, especially of units operating on the anti-Polish front, that it was radio intelligence that to a large degree determined the course of all ... military operations conducted by Poland in 1920 \u2013 from the January fighting at Ovruch, through the March operation against Mozyr and Kiev, the April operation in Ukraine, the battles with Tukhachevsky's first and second offensives in Belarus, the battles with Budionny's Cavalry Army, the Battle of Brody, to the Battles of Warsaw, Lw\u00f3w and the Niemen.","completion":"Cryptography"}
{"prompt":"Many worst-case computational problems are known to be hard or even complete for some complexity class \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , in particular NP-hard (but often also PSPACE-hard, PPAD-hard, etc.). This means that they are at least as hard as any problem in the class \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  . \nIf a problem is \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  -hard (with respect to polynomial time reductions), then it cannot be solved by a polynomial-time algorithm unless the computational hardness assumption \n  \n    \n      \n        P\n        \u2260\n        C\n      \n    \n    {\\displaystyle P\\neq C}\n   is false.","completion":"Cryptography"}
{"prompt":"A Feistel network uses a round function, a function which takes two inputs \u2013  a data block and a subkey \u2013  and returns one output of the same size as the data block. In each round, the round function is run on half of the data to be encrypted, and its output is XORed with the other half of the data. This is repeated a fixed number of times, and the final output is the encrypted data. An important advantage of Feistel networks compared to other cipher designs such as substitution\u2013permutation networks is that the entire operation is guaranteed to be invertible (that is, encrypted data can be decrypted), even if the round function is not itself invertible. The round function can be made arbitrarily complicated, since it does not need to be designed to be invertible.:\u200a465\u200a :\u200a347\u200a Furthermore, the encryption and decryption operations are very similar, even identical in some cases, requiring only a reversal of the key schedule. Therefore, the size of the code or circuitry required to implement such a cipher is nearly halved.","completion":"Cryptography"}
{"prompt":"Adi Shamir was born in Tel Aviv. He  received a Bachelor of Science (BSc) degree in mathematics from Tel Aviv University in 1973 and obtained an MSc and PhD in computer science from the Weizmann Institute in 1975 and 1977 respectively.\nHe spent a year as a postdoctoral researcher at the University of Warwick and did research at Massachusetts Institute of Technology (MIT) from 1977 to 1980.","completion":"Cryptography"}
{"prompt":"In the context of cryptography, encryption serves as a mechanism to ensure confidentiality. Since data may be visible on the Internet, sensitive information such as passwords and personal communication may be exposed to potential interceptors. The process of encrypting and decrypting messages involves keys. The two main types of keys in cryptographic systems are symmetric-key and public-key (also known as asymmetric-key).Many complex cryptographic algorithms often use simple modular arithmetic in their implementations.","completion":"Cryptography"}
{"prompt":"According to al-Kindi, the goal of metaphysics is knowledge of God. For this reason, he does not make a clear distinction between philosophy and theology, because he believes they are both concerned with the same subject. Later philosophers, particularly al-Farabi and Avicenna, would strongly disagree with him on this issue, by saying that metaphysics is actually concerned with being qua being, and as such, the nature of God is purely incidental.Central to al-Kindi's understanding of metaphysics is God's absolute oneness, which he considers an attribute uniquely associated with God (and therefore not shared with anything else). By this he means that while we may think of any existent thing as being \"one\", it is in fact both \"one\" and many\". For example, he says that while a body is one, it is also composed of many different parts. A person might say \"I see an elephant\", by which he means \"I see one elephant\", but the term 'elephant' refers to a species of animal that contains many. Therefore, only God is absolutely one, both in being and in concept, lacking any multiplicity whatsoever. Some feel this understanding entails a very rigorous negative theology because it implies that any description which can be predicated to anything else, cannot be said about God.In addition to absolute oneness, al-Kindi also described God as the Creator. This means that He acts as both a final and efficient cause. Unlike later Muslim Neo-Platonic philosophers (who asserted that the universe existed as a result of God's existence \"overflowing\", which is a passive act), al-Kindi conceived of God as an active agent. In fact, of God as the agent, because all other intermediary agencies are contingent upon Him. The key idea here is that God \"acts\" through created intermediaries, which in turn \"act\" on one another \u2013 through a chain of cause and effect \u2013 to produce the desired result. In reality, these intermediary agents do not \"act\" at all, they are merely a conduit for God's own action. This is especially significant in the development of Islamic philosophy, as it portrayed the \"first cause\" and \"unmoved mover\" of Aristotelian philosophy as compatible with the concept of God according to Islamic revelation.","completion":"Cryptography"}
{"prompt":"Smalltalk was one of many object-oriented programming languages based on Simula. Smalltalk is also one of the most influential programming languages. Virtually all of the object-oriented languages that came after\u2014Flavors, CLOS, Objective-C, Java, Python, Ruby, and many others\u2014were influenced by Smalltalk. Smalltalk was also one of the most popular languages for agile software development methods, rapid application development (RAD) or prototyping, and software design patterns. The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.\nSmalltalk emerged from a larger program of Advanced Research Projects Agency (ARPA) funded research that in many ways defined the modern world of computing. In addition to Smalltalk, working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s. Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.Smalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the model\u2013view\u2013controller (MVC) pattern for user interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification.  Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.In addition to the MVC pattern, the Smalltalk language and environment were highly influential in the history of the graphical user interface (GUI) and the what you see is what you get (WYSIWYG) user interface, font editors, and desktop metaphors for UI design.  The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the integrated development environments, starting with Lisp Machine environments, that came after.","completion":"Programming Languages"}
{"prompt":"^a  Pascal has declaration blocks. See functions.\n^b Types are just regular objects, so you can just assign them.\n^c  In Perl, the \"my\" keyword scopes the variable into the block.\n^d  Technically, this does not declare name to be a mutable variable\u2014in ML, all names can only be bound once; rather, it declares name to point to a \"reference\" data structure, which is a simple mutable cell. The data structure can then be read and written to using the ! and := operators, respectively.\n^e  If no initial value is given, an invalid value is automatically assigned (which will trigger a run-time exception if it used before a valid value has been assigned). While this behaviour can be suppressed it is recommended in the interest of predictability. If no invalid value can be found for a type (for example in case of an unconstraint integer type), a valid, yet predictable value is chosen instead.\n^f  In Rust, if no initial value is given to a let or let mut variable and it is never assigned to later, there is an \"unused variable\" warning. If no value is provided for a const or static or static mut variable, there is an error. There is a \"non-upper-case globals\" error for non-uppercase const variables. After it is defined, a static mut variable can only be assigned to in an unsafe block or function.","completion":"Programming Languages"}
{"prompt":"For any strings \n  \n    \n      \n        u\n        ,\n        v\n        \u2208\n        (\n        V\n        \u222a\n        \u03a3\n        \n          )\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle u,v\\in (V\\cup \\Sigma )^{*}}\n  , we say u directly yields v, written as \n  \n    \n      \n        u\n        \u21d2\n        v\n        \n      \n    \n    {\\displaystyle u\\Rightarrow v\\,}\n  , if \n  \n    \n      \n        \u2203\n        (\n        \u03b1\n        ,\n        \u03b2\n        )\n        \u2208\n        R\n      \n    \n    {\\displaystyle \\exists (\\alpha ,\\beta )\\in R}\n   with \n  \n    \n      \n        \u03b1\n        \u2208\n        V\n      \n    \n    {\\displaystyle \\alpha \\in V}\n   and \n  \n    \n      \n        \n          u\n          \n            1\n          \n        \n        ,\n        \n          u\n          \n            2\n          \n        \n        \u2208\n        (\n        V\n        \u222a\n        \u03a3\n        \n          )\n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle u_{1},u_{2}\\in (V\\cup \\Sigma )^{*}}\n   such that \n  \n    \n      \n        u\n        \n        =\n        \n          u\n          \n            1\n          \n        \n        \u03b1\n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle u\\,=u_{1}\\alpha u_{2}}\n   and \n  \n    \n      \n        v\n        \n        =\n        \n          u\n          \n            1\n          \n        \n        \u03b2\n        \n          u\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle v\\,=u_{1}\\beta u_{2}}\n  . Thus, v is a result of applying the rule \n  \n    \n      \n        (\n        \u03b1\n        ,\n        \u03b2\n        )\n      \n    \n    {\\displaystyle (\\alpha ,\\beta )}\n   to u.","completion":"Programming Languages"}
{"prompt":"At Version 4 Unix, released in November 1973, the Unix kernel was extensively re-implemented in C. By this time, the C language had acquired some powerful features such as struct types.\nThe preprocessor was introduced around 1973 at the urging of Alan Snyder and also in recognition of the usefulness of the file-inclusion mechanisms available in BCPL and PL\/I. Its original version provided only included files and simple string replacements: #include and #define of parameterless macros. Soon after that, it was extended, mostly by Mike Lesk and then by John Reiser, to incorporate macros with arguments and conditional compilation.Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL\/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around  1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system.  Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.","completion":"Programming Languages"}
{"prompt":"The first implementation of APL using recognizable APL symbols was APL\\360 which ran on the IBM System\/360, and was completed in November 1966 though at that time remained in use only within IBM. In 1973 its implementors, Larry Breed, Dick Lathwell and Roger Moore, were awarded the Grace Murray Hopper Award from the Association for Computing Machinery (ACM). It was given \"for their work in the design and implementation of APL\\360, setting new standards in simplicity, efficiency, reliability and response time for interactive systems.\"In 1975, the IBM 5100 microcomputer offered APL\\360 as one of two built-in ROM-based interpreted languages for the computer, complete with a keyboard and display that supported all the special symbols used in the language.Significant developments to APL\\360 included CMS\/APL, which made use of the virtual storage capabilities of CMS and APLSV, which introduced shared variables, system variables and system functions. It was subsequently ported to the IBM System\/370 and VSPC platforms until its final release in 1983, after which it was replaced by APL2.","completion":"Programming Languages"}
{"prompt":"There has been much interest in domain-specific languages to improve the productivity and quality of software engineering.  Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into the development of critical software systems.\nThe Software Cost Reduction Toolkit is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.\nA newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.","completion":"Programming Languages"}
{"prompt":"Static programming languages (possibly indirectly) require developers to define the size of utilized memory before compilation (unless working around with pointer logic). Consistent with object runtime alteration, dynamic languages implicitly need to (re-)allocate memory based on program individual operations.","completion":"Programming Languages"}
{"prompt":"Logos, diagrams, and flowcharts consisting of ASCII art constructions can be inserted into source code formatted as a comment. Further, copyright notices can be embedded within source code as comments. Binary data may also be encoded in comments through a process known as binary-to-text encoding, although such practice is uncommon and typically relegated to external resource files.\nThe following code fragment is a simple ASCII diagram depicting the process flow for a system administration script contained in a Windows Script File running under Windows Script Host. Although a section marking the code appears as a comment, the diagram itself actually appears in an XML CDATA section, which is technically considered distinct from comments, but can serve similar purposes.\n\nAlthough this identical diagram could easily have been included as a comment, the example illustrates one instance where a programmer may opt not to use comments as a way of including resources in source code.","completion":"Programming Languages"}
{"prompt":"Nested lists can be written as S-expressions: ((milk juice) (honey marmalade)) is a two-element S-expression whose elements are also two-element S-expressions.  The whitespace-separated notation used in Lisp (and this article) is typical. Line breaks (newline characters) usually qualify as separators.\nThis is a simple context-free grammar for a tiny subset of English written as an S-expression (Gazdar\/Melish, Natural Language Processing in Lisp), where S=sentence, NP=Noun Phrase, VP=Verb Phrase, V=Verb:","completion":"Programming Languages"}
{"prompt":"Parenthesized S-expressions represent linked list structures. There are several ways to represent the same list as an S-expression. A cons can be written in dotted-pair notation as (a . b), where a is the car and b the cdr. A longer proper list might be written (a . (b . (c . (d . nil)))) in dotted-pair notation. This is conventionally abbreviated as (a b c d) in list notation. An improper list may be written in a combination of the two \u2013 as (a b c . d) for the list of three conses whose last cdr is d (i.e., the list (a . (b . (c . d))) in fully specified form).","completion":"Programming Languages"}
{"prompt":"Every regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, for example, is also regular.\n\nS \u2192 a\nS \u2192 aS\nS \u2192 bSThe terminals here are a and b, while the only nonterminal is S.\nThe language described is all nonempty strings of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  s and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  s that end in \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  .\nThis grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.\nEvery regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.\nUsing vertical bars, the grammar above can be described more tersely as follows:\n\nS \u2192 a | aS | bS","completion":"Programming Languages"}
{"prompt":"Assembly language uses a mnemonic to represent, e.g., each low-level machine instruction or opcode, each directive, typically also each architectural register, flag, etc. Some of the mnemonics may be built in and some user defined. Many operations require one or more operands in order to form a complete instruction. Most assemblers permit named constants, registers, and labels for program and memory locations, and can calculate expressions for operands. Thus, programmers are freed from tedious repetitive calculations and assembler programs are much more readable than machine code. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.\nSome are column oriented, with specific fields in specific columns; this was very common for machines using punched cards in the 1950s and early 1960s. Some assemblers have free-form syntax, with fields separated by delimiters, e.g., punctuation, white space. Some assemblers are hybrid, with, e.g., labels, in a specific column and other fields separated by delimiters; this became more common than column oriented syntax in the 1960s.","completion":"Programming Languages"}
{"prompt":"Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.","completion":"Programming Languages"}
{"prompt":"Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:\n\nIn object-oriented programming languages such as C++, Object Pascal, or Java, the concept of abstraction has itself become a declarative statement \u2013 using the syntax function(parameters) = 0; (in C++) or the keywords abstract and interface (in Java). After such a declaration, it is the responsibility of the programmer to implement a class to instantiate the object of the declaration.\nFunctional programming languages commonly exhibit abstractions related to functions, such as lambda abstractions (making a term into a function of some variable) and higher-order functions (parameters are functions).\nModern members of the Lisp programming language family such as Clojure, Scheme and Common Lisp support macro systems to allow syntactic abstraction. Other programming languages such as Scala also have macros, or very similar metaprogramming features (for example, Haskell has Template Haskell, and OCaml has MetaOCaml). These can allow a programmer to eliminate boilerplate code, abstract away tedious function call sequences, implement new control flow structures, and implement Domain Specific Languages (DSLs), which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in most cases) effort when compared to \"more traditional\" programming languages such as Python, C or Java.","completion":"Programming Languages"}
{"prompt":"The concept of language-oriented programming takes the approach to capture requirements in the user's terms, and then to try to create an implementation language as isomorphic as possible to the user's descriptions, so that the mapping between requirements and implementation is as direct as possible. A measure of the closeness of this isomorphism is the \"redundancy\" of the language, defined as the number of editing operations needed to implement a stand-alone change in requirements. It is not assumed a-priori what is the best language for implementing the new language. Rather, the developer can choose among options created by analysis of the information flows \u2014 what information is acquired, what its structure is, when it is acquired, from whom, and what is done with it.","completion":"Programming Languages"}
{"prompt":"In their level of abstraction, HDLs have been compared to assembly languages. There are attempts to raise the abstraction level of hardware design in order to reduce the complexity of programming in HDLs, creating a sub-field called high-level synthesis.\nCompanies such as Cadence, Synopsys and Agility Design Solutions are promoting SystemC as a way to combine high-level languages with concurrency models to allow faster design cycles for FPGAs than is possible using traditional HDLs. Approaches based on standard C or C++ (with libraries or other extensions allowing parallel programming) are found in the Catapult C tools from Mentor Graphics, and the Impulse C tools from Impulse Accelerated Technologies.\nA similar initiative from Intel is the use of Data Parallel C++, related to SYCL, as a high-level synthesis language.\nAnnapolis Micro Systems, Inc.'s CoreFire Design Suite and National Instruments LabVIEW FPGA provide a graphical dataflow approach to high-level design entry and languages such as SystemVerilog, SystemVHDL, and Handel-C seek to accomplish the same goal, but are aimed at making existing hardware engineers more productive, rather than making FPGAs more accessible to existing software engineers.\nIt is also possible to design hardware modules using MATLAB and Simulink using the MathWorks HDL Coder tool or DSP Builder for Intel FPGAs or Xilinx System Generator (XSG) from Xilinx.","completion":"Programming Languages"}
{"prompt":"Simula supports call by name:\u200a8.2.3\u200a so the Jensen's Device can easily be implemented. However, the default transmission mode for simple parameter is call by value, contrary to ALGOL which used call by name. The source code for the Jensen's Device must therefore specify call by name for the parameters when compiled by a Simula compiler.\nAnother much simpler example is the summation function \n  \n    \n      \n        \u2211\n      \n    \n    {\\displaystyle \\sum }\n   which can be implemented as follows:\n\nReal Procedure Sigma (k, m, n, u);\n   Name k, u;\n   Integer k, m, n; Real u;\nBegin\n   Real s;\n   k:= m;\n   While k <= n Do Begin s:= s + u; k:= k + 1; End;\n   Sigma:= s;\nEnd;\n\nThe above code uses call by name for the controlling variable (k) and the expression (u).\nThis allows the controlling variable to be used in the expression.\nNote that the Simula standard allows for certain restrictions on the controlling variable\nin a for loop. The above code therefore uses a while loop for maximum portability.\nThe following:\n\n  \n    \n      \n        Z\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            100\n          \n        \n        \n          \n            1\n            \n              (\n              i\n              +\n              a\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle Z=\\sum _{i=1}^{100}{1 \\over (i+a)^{2}}}\n  \ncan then be implemented as follows:\n\nZ:= Sigma (i, 1, 100, 1 \/ (i + a) ** 2);","completion":"Programming Languages"}
{"prompt":"Frameworks exist which can bridge between Prolog and other languages:\n\nThe LPA Intelligence Server allows embedding LPA Prolog for Windows in other programming languages, including: C, C++, C#, Java, Visual Basic (VB), Delphi, .NET, Lua, Python, and others. It exploits the dedicated string data type which LPA Prolog provides\nThe Logic Server application programming interface (API) allows both the extension and embedding of Prolog in C, C++, Java, VB, Delphi, .NET, and any language or environment which can call a .dll or .so. It is implemented for Amzi! Prolog Amzi! Prolog + Logic Server but the API specification can be made available for any implementation.\nJPL is a bi-directional Java Prolog bridge which ships with SWI-Prolog by default, allowing Java and Prolog to call each other (recursively). It is known to have good concurrency support and is under active development.\nInterProlog, a programming library bridge between Java and Prolog, implementing bi-directional predicate\/method calling between both languages. Java objects can be mapped into Prolog terms and vice versa. Allows the development of graphical user interfaces (GUIs) and other functions in Java while leaving logic processing in the Prolog layer. Supports XSB, with support for SWI-Prolog and YAP planned for 2013.\nProva provides native syntax integration with Java, agent messaging and reaction rules. Prova positions itself as a rule-based scripting (RBS) system for middleware. The language breaks new ground in combining imperative and declarative programming.\nPROL An embeddable Prolog engine for Java. It includes a small IDE and a few libraries.\nGNU Prolog for Java is an implementation of ISO Prolog as a Java library (gnu.prolog)\nCiao provides interfaces to C, C++, Java, and relational databases.\nC#-Prolog is a Prolog interpreter written in (managed) C#. Can easily be integrated in C# programs. Characteristics: reliable and fairly fast interpreter, command line interface, Windows-interface, builtin DCG, XML-predicates, SQL-predicates, extendible. The complete source code is available, including a parser generator that can be used for adding special purpose extensions.\nA Warren Abstract Machine for PHP A Prolog compiler and interpreter in PHP 5.3. A library that can be used standalone or within Symfony2.1 framework which was translated from Stephan Buettcher's work in Java which can be found [here stefan.buettcher.org\/cs\/wam\/]\ntuProlog is a lightweight Prolog system for distributed applications and infrastructures, intentionally designed around a minimal core, to be either statically or dynamically configured by loading\/unloading libraries of predicates. tuProlog natively supports multi-paradigm programming, providing a clean, seamless integration model between Prolog and mainstream object-oriented languages, namely Java, for tuProlog Java version, and any .NET-based language (C#, F#..), for tuProlog .NET version.","completion":"Programming Languages"}
{"prompt":"There is a spectrum of possibilities between interpreting and compiling, depending on the amount of analysis performed before the program is executed. For example, Emacs Lisp is compiled to bytecode, which is a highly compressed and optimized representation of the Lisp source, but is not machine code (and therefore not tied to any particular hardware). This \"compiled\" code is then interpreted by a bytecode interpreter (itself written in C). The compiled code in this case is machine code for a virtual machine, which is implemented not in hardware, but in the bytecode interpreter. Such compiling interpreters are sometimes also called compreters. In a bytecode interpreter each instruction starts with a byte, and therefore bytecode interpreters have up to 256 instructions, although not all may be used. Some bytecodes may take multiple bytes, and may be arbitrarily complicated.\nControl tables - that do not necessarily ever need to pass through a compiling phase - dictate appropriate algorithmic control flow via customized interpreters in similar fashion to bytecode interpreters.","completion":"Programming Languages"}
{"prompt":"QBasic, a version of Microsoft QuickBASIC without the linker to make EXE files, is present in the Windows NT and DOS-Windows 95 streams of operating systems and can be obtained for more recent releases like Windows 7 which do not have them. Prior to DOS 5, the Basic interpreter was GW-Basic. QuickBasic is part of a series of three languages issued by Microsoft for the home and office power user and small-scale professional development; QuickC and QuickPascal are the other two. For Windows 95 and 98, which do not have QBasic installed by default, they can be copied from the installation disc, which will have a set of directories for old and optional software; other missing commands like Exe2Bin and others are in these same directories.","completion":"Programming Languages"}
{"prompt":"OOP was developed to increase the reusability and maintainability of source code. Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.","completion":"Programming Languages"}
{"prompt":"Dataflow programming languages include:\n\nC\u00e9u (programming language)\nASCET\nAviSynth scripting language, for video processing\nBMDFM Binary Modular Dataflow Machine\nCAL\nCuneiform, a functional workflow language.\nCMS Pipelines\nHume\nJoule\nKeysight VEE\nKNIME is a free and open-source data analytics, reporting and integration platform\nLabVIEW, G\nLinda\nLucid\nLustre\nMax\/MSP\nMicrosoft Visual Programming Language - A component of Microsoft Robotics Studio designed for robotics programming\nNextflow: a workflow language\nOrange - An open-source, visual programming tool for data mining, statistical data analysis, and machine learning.\nOz now also distributed since 1.4.0\nPipeline Pilot\nPrograph\nPure Data\nQuartz Composer - Designed by Apple; used for graphic animations and effects\nSAC Single assignment C\nSIGNAL (a dataflow-oriented synchronous language enabling multi-clock specifications)\nSimulink\nSISAL\nSystemVerilog - A hardware description language\nVerilog - A hardware description language absorbed into the SystemVerilog standard in 2009\nVisSim - A block diagram language for simulation of dynamic systems and automatic firmware generation\nVHDL - A hardware description language\nWapice IOT-TICKET implements an unnamed visual dataflow programming language for IoT data analysis and reporting.\nXEE (Starlight) XML engineering environment\nXProc","completion":"Programming Languages"}
{"prompt":"Universally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as \n  \n    \n      \n        \u2200\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\forall x.f(x)}\n   or forall x. f x and is the intersection over all types x of the body f x, i.e. the value is of type f x for every x. Existential quantification written as \n  \n    \n      \n        \u2203\n        x\n        .\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\exists x.f(x)}\n   or exists x. f x and is the union over all types x of the body f x, i.e. the value is of type f x for some x.\nIn Haskell, universal quantification is commonly used, but existential types must be encoded by transforming exists a. f a to forall r. (forall a. f a -> r) -> r or a similar type.","completion":"Programming Languages"}
{"prompt":"Third-generation BASIC dialects such as Visual Basic, Xojo, Gambas, StarOffice Basic, BlitzMax and PureBasic introduced features to support object-oriented and event-driven programming paradigm. Most built-in procedures and functions are now represented as methods of standard objects rather than operators. Also, the operating system became increasingly accessible to the BASIC language.\nThe following example is in Visual Basic .NET:","completion":"Programming Languages"}
{"prompt":"C has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters \/* and *\/, or (since C99) following \/\/ until the end of the line. Comments delimited by \/* and *\/ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.C source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\nAs an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if ... [else] conditional execution and by do ... while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next case unless terminated by a break.\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call.  Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\nKernighan and Ritchie say in the Introduction of The C Programming Language: \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.","completion":"Programming Languages"}
{"prompt":"It is decidable whether a given grammar is a regular grammar, as well as whether it is an LL(k) grammar for a given k\u22650.:\u200a233\u200a If k is not given, the latter problem is undecidable.:\u200a252\u200aGiven a context-free language, it is neither decidable whether it is regular, nor whether it is an LL(k) language for a given k.:\u200a254","completion":"Programming Languages"}
{"prompt":"The first paper usually associated with the extensible programming language movement is M. Douglas McIlroy's 1960 paper on macros for higher-level programming languages.  Another early description of the principle of extensibility occurs in Brooker and Morris's 1960 paper on the Compiler-Compiler.  The peak of the movement was marked by two academic symposia, in 1969 and 1971.  By 1975, a survey article on the movement by Thomas A. Standish was essentially a post mortem. The Forth programming language was an exception, but it went essentially unnoticed.","completion":"Programming Languages"}
{"prompt":"In contrast to Common Lisp, all data and procedures in Scheme share a common namespace, whereas in Common Lisp functions and data have separate namespaces making it possible for a function and a variable to have the same name, and requiring special notation for referring to a function as a value. This is sometimes known as the \"Lisp-1 vs. Lisp-2\" distinction, referring to the unified namespace of Scheme and the separate namespaces of Common Lisp.In Scheme, the same primitives that are used to manipulate and bind data can be used to bind procedures. There is no equivalent of Common Lisp's defun and #' primitives.","completion":"Programming Languages"}
{"prompt":"Here is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:\n\nS \u2192 x\nS \u2192 y\nS \u2192 z\nS \u2192 S + S\nS \u2192 S \u2013 S\nS \u2192 S * S\nS \u2192 S \/ S\nS \u2192 (S)This grammar can, for example, generate the string\n\n(x + y) * x \u2013 z * y \/ (x + x)as follows:\n\nS\n\u2192 S \u2013 S (by rule 5)\n\u2192 S * S \u2013 S (by rule 6, applied to the leftmost S)\n\u2192 S * S \u2013 S \/ S (by rule 7, applied to the rightmost S)\n\u2192 (S) * S \u2013 S \/ S (by rule 8, applied to the leftmost S)\n\u2192 (S) * S \u2013 S \/ (S) (by rule 8, applied to the rightmost S)\n\u2192 (S + S) * S \u2013 S \/ (S) (by rule 4, applied to the leftmost S)\n\u2192 (S + S) * S \u2013 S * S \/ (S) (by rule 6, applied to the fourth S)\n\u2192 (S + S) * S \u2013 S * S \/ (S + S) (by rule 4, applied to the rightmost S)\n\u2192 (x + S) * S \u2013 S * S \/ (S + S) (etc.)\n\u2192 (x + y) * S \u2013 S * S \/ (S + S)\n\u2192 (x + y) * x \u2013 S * S \/ (S + S)\n\u2192 (x + y) * x \u2013 z * S \/ (S + S)\n\u2192 (x + y) * x \u2013 z * y \/ (S + S)\n\u2192 (x + y) * x \u2013 z * y \/ (x + S)\n\u2192 (x + y) * x \u2013 z * y \/ (x + x)Note that many choices were made underway as to which rewrite was going to be performed next.\nThese choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same.  For example, the second and third rewrites\n\n\u2192 S * S \u2013 S (by rule 6, applied to the leftmost S)\n\u2192 S * S \u2013 S \/ S (by rule 7, applied to the rightmost S)could be done in the opposite order:\n\n\u2192 S \u2013 S \/ S (by rule 7, applied to the rightmost S)\n\u2192 S * S \u2013 S \/ S (by rule 6, applied to the leftmost S)Also, many choices were made on which rule to apply to each selected S.\nChanging the choices made and not only the order they were made in usually affects which terminal string comes out at the end.\nLet's look at this in more detail. Consider the parse tree of this derivation:\n\nStarting at the top, step by step, an S in the tree is expanded, until no more unexpanded Ses (nonterminals) remain.\nPicking a different order of expansion will produce a different derivation, but the same parse tree.\nThe parse tree will only change if we pick a different rule to apply at some position in the tree.\nBut can a different parse tree still produce the same terminal string,\nwhich is (x + y) * x \u2013 z * y \/ (x + x) in this case?\nYes, for this particular grammar, this is possible.\nGrammars with this property are called ambiguous.\nFor example, x + y * z can be produced with these two different parse trees:\n\nHowever, the language described by this grammar is not inherently ambiguous:\nan alternative, unambiguous grammar can be given for the language, for example:\n\nT \u2192 x\nT \u2192 y\nT \u2192 z\nS \u2192 S + T\nS \u2192 S \u2013 T\nS \u2192 S * T\nS \u2192 S \/ T\nT \u2192 (S)\nS \u2192 T,once again picking S as the start symbol. This alternative grammar will produce x + y * z with a parse tree similar to the left one above, i.e. implicitly assuming the association (x + y) * z, which does not follow standard order of operations. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.","completion":"Programming Languages"}
{"prompt":"Micro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator typically exists in modern Prolog's implementations. It is typically written as not(Goal) or \\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\+ X","completion":"Programming Languages"}
{"prompt":"As typically envisioned, an extensible programming language consisted of a base language providing elementary computing facilities, and a meta-language capable of modifying the base language.  A program then consisted of meta-language modifications and code in the modified base language.\nThe most prominent language-extension technique used in the movement was macro definition.  Grammar modification was also closely associated with the movement, resulting in the eventual development of adaptive grammar formalisms.  The Lisp language community remained separate from the extensible language community, apparently because, as one researcher observed,\n\nany programming language in which programs and data are essentially interchangeable can be regarded as an extendible [sic] language.  ... this can be seen very easily from the fact that Lisp has been used as an extendible language for years.\nAt the 1969 conference, Simula was presented as an extensible programming language.\nStandish described three classes of language extension, which he called paraphrase, orthophrase, and metaphrase (otherwise paraphrase and metaphrase being translation terms).\n\nParaphrase defines a facility by showing how to exchange it for something previously defined (or to be defined).  As examples, he mentions macro definitions, ordinary procedure definitions, grammatical extensions, data definitions, operator definitions, and control structure extensions.\nOrthophrase adds features to a language that could not be achieved using the base language, such as adding an i\/o system to a base language that previously had no i\/o primitives.  Extensions must be understood as orthophrase relative to some given base language, since a feature not defined in terms of the base language must be defined in terms of some other language.  Orthophrase corresponds to the modern notion of plug-ins.\nMetaphrase modifies the interpretation rules used for pre-existing expressions.  It corresponds to the modern notion of reflection.","completion":"Programming Languages"}
{"prompt":"The need for protocol standards can be shown by looking at what happened to the Binary Synchronous Communications (BSC) protocol invented by IBM. BSC is an early link-level protocol used to connect two separate nodes. It was originally not intended to be used in a multinode network, but doing so revealed several deficiencies of the protocol. In the absence of standardization, manufacturers and organizations felt free to enhance the protocol, creating incompatible versions on their networks. In some cases, this was deliberately done to discourage users from using equipment from other manufacturers. There are more than 50 variants of the original bi-sync protocol. One can assume, that a standard would have prevented at least some of this from happening.In some cases, protocols gain market dominance without going through a standardization process. Such protocols are referred to as de facto standards. De facto standards are common in emerging markets, niche markets, or markets that are monopolized (or oligopolized). They can hold a market in a very negative grip, especially when used to scare away competition. From a historical perspective, standardization should be seen as a measure to counteract the ill-effects of de facto standards. Positive exceptions exist; a de facto standard operating system like Linux does not have this negative grip on its market, because the sources are published and maintained in an open way, thus inviting competition.","completion":"Programming Languages"}
{"prompt":"Criticisms directed at Java include the implementation of generics, speed, the handling of unsigned numbers, the implementation of floating-point arithmetic, and a history of security vulnerabilities in the primary Java VM implementation HotSpot.","completion":"Programming Languages"}
{"prompt":"Camlp4\nFelix\nNemerle\nSeed7\nRebol\nRed\nRuby (metaprogramming)\nIMP\nOpenC++\nXL\nXML\nForth\nLisp\nRacket\nScheme\nLua\nPL\/I\nSmalltalk","completion":"Programming Languages"}
{"prompt":"The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file.  Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for \"link the math library\").The most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values.  Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.","completion":"Programming Languages"}
{"prompt":"The language platform provides a self-hosted IDE named DrRacket, a continuation-based web server, a graphical user interface, and other tools. As a viable scripting tool with libraries like common scripting languages, it can be used for scripting the Unix shell. It can parse command-line arguments and execute external tools.","completion":"Programming Languages"}
{"prompt":"A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.\nIn the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.\nA compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.","completion":"Programming Languages"}
{"prompt":"Thompson wanted a programming language for developing utilities for the new platform. At first, he tried to write a Fortran compiler, but soon gave up the idea. Instead, he created a cut-down version of the recently developed BCPL systems programming language. The official description of BCPL was not available at the time and Thompson modified the syntax to be less wordy, and similar to a simplified ALGOL known as SMALGOL.  Thompson called the result B. He described B as \"BCPL semantics with a lot of SMALGOL syntax\". Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines. However, few utilities were ultimately written in B because it was too slow, and could not take advantage of PDP-11 features such as byte addressability.","completion":"Programming Languages"}
{"prompt":"Statistical modelers have developed domain-specific languages such as R (an implementation of the S language),\nBugs, Jags, and Stan. These languages provide a syntax for describing a Bayesian model and generate a method for solving it using simulation.","completion":"Programming Languages"}
{"prompt":"Strings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size.\nSince most character sets include the digits, it is possible to have a numeric string, such as \"1234\". These numeric strings are usually considered distinct from numeric values such as 1234, although some languages automatically convert between them.","completion":"Programming Languages"}
{"prompt":"This program, for Heron's formula, reads data on a tape reel containing three 5-digit integers A, B, and C as input. There are no \"type\" declarations available: variables whose name starts with I, J, K, L, M, or N are \"fixed-point\" (i.e. integers), otherwise floating-point. Since integers are to be processed in this example, the names of the variables start with the letter \"I\". The name of a variable must start with a letter and can continue with both letters and digits, up to a limit of six characters in FORTRAN II.  If A, B, and C cannot represent the sides of a triangle in plane geometry, then the program's execution will end with an error code of \"STOP 1\".  Otherwise, an output line will be printed showing the input values for A, B, and C, followed by the computed AREA of the triangle as a floating-point number occupying ten spaces along the line of output and showing 2 digits after the decimal point, the .2 in F10.2 of the FORMAT statement with label 601.","completion":"Programming Languages"}
{"prompt":"1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).\nThe logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say\n\nH :-  Body1.\n      \u2026\nH :-  Bodyk.as a definition of the predicate\n\nH iff (Body1 or \u2026 or Bodyk)where \"iff\" means \"if and only if\". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation as failure needs only the if-halves of the definitions without the axioms of equality.\nFor example, the completion of the program above is:\n\ncanfly(X) iff bird(X), not abnormal(X).\nabnormal(X) iff wounded(X).\n bird(X) iff X = john or X = mary.\n X = X.\n not john = mary.\n not mary = john.The notion of completion is closely related to McCarthy's circumscription semantics for default reasoning, and to the closed world assumption.\nAs an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in \"extended logic programming\", to formalise such phrases as \"the contrary can not be shown\", where \"contrary\" is classical negation and \"can not be shown\" is the epistemic interpretation of negation as failure.","completion":"Programming Languages"}
{"prompt":"In most dialects of Lisp including Common Lisp, by convention the value NIL evaluates to the value false in a boolean expression. In Scheme, since the IEEE standard in 1991, all values except #f, including NIL's equivalent in Scheme which is written as '(), evaluate to the value true in a boolean expression. (R5RS sec. 6.3.1)Where the constant representing the boolean value of true is T in most Lisps, in Scheme it is #t.","completion":"Programming Languages"}
{"prompt":"Interactive debugging uses debugger tools which allow an program's execution to be processed one step at a time and to be paused to inspect or alter its state. Subroutines or function calls may typically be executed at full speed and paused again upon return to their caller, or themselves single stepped, or any mixture of these options. Setpoints may be installed which permit full speed execution of code that is not suspected to be faulty, and then stop at a point that is. Putting a setpoint immediately after the end of a program loop is a convenient way to evaluate repeating code. Watchpoints are commonly available, where execution can proceed until a particular variable changes, and catchpoints which cause the debugger to stop for certain kinds of program events, such as exceptions or the loading of a shared library.\nPrint debugging or tracing is the act of watching (live or recorded) trace statements, or print statements, that indicate the flow of execution of a process and the data progression. Tracing can be done with specialized tools (like with GDB's trace) or by insertion of trace statements into the source code. The latter is sometimes called printf debugging, due to the use of the printf function in C. This kind of debugging was turned on by the command TRON in the original versions of the novice-oriented BASIC programming language. TRON stood for, \"Trace On.\" TRON caused the line numbers of each BASIC command line to print as the program ran.\nActivity tracing is like tracing (above), but rather than following program execution one instruction or function at a time, follows program activity based on the overall amount of time spent by the processor\/CPU executing particular segments of code. This is typically presented as a fraction of the program's execution time spent processing instructions within defined memory addresses (machine code programs) or certain program modules (high level language or compiled programs). If the program being debugged is shown to be spending an inordinate fraction of its execution time within traced areas, this could indicate misallocation of processor time caused by faulty program logic, or at least inefficient allocation of processor time that could benefit from optimization efforts.\nRemote debugging is the process of debugging a program running on a system different from the debugger. To start remote debugging, a debugger connects to a remote system over a communications link such as a local area network. The debugger can then control the execution of the program on the remote system and retrieve information about its state.\nPost-mortem debugging is debugging of the program after it has already crashed. Related techniques often include various tracing techniques like examining log files, outputting a call stack on the crash, and analysis of memory dump (or core dump) of the crashed process. The dump of the process could be obtained automatically by the system (for example, when the process has terminated due to an unhandled exception), or by a programmer-inserted instruction, or manually by the interactive user.\n\"Wolf fence\" algorithm: Edward Gauss described this simple but very useful and now famous algorithm in a 1982 article for Communications of the ACM as follows: \"There's one wolf in Alaska; how do you find it? First build a fence down the middle of the state, wait for the wolf to howl, determine which side of the fence it is on. Repeat process on that side only, until you get to the point where you can see the wolf.\" This is implemented e.g. in the Git version control system as the command git bisect, which uses the above algorithm to determine which commit introduced a particular bug.\nRecord and replay debugging is the technique of creating a program execution recording (e.g. using Mozilla's free rr debugging tool; enabling reversible debugging\/execution), which can be replayed and interactively debugged. Useful for remote debugging and debugging intermittent, non-determinstic, and other hard-to-reproduce defects.\nTime travel debugging is the process of stepping back in time through source code (e.g. using Undo LiveRecorder) to understand what is happening during execution of a computer program; to allow users to interact with the program; to change the history if desired and to watch how the program responds.\nDelta Debugging \u2013  a technique of automating test case simplification.:\u200ap.123\u200a\nSaff Squeeze \u2013  a technique of isolating failure within the test using progressive inlining of parts of the failing test.\nCausality tracking: There are techniques to track the cause effect chains in the computation. Those techniques can be tailored for specific bugs, such as null pointer dereferences.","completion":"Programming Languages"}
{"prompt":"There are many approaches to formal semantics; these belong to three major classes:\n\nDenotational semantics, whereby each phrase in the language is interpreted as a denotation, i.e. a conceptual meaning that can be thought of abstractly.  Such denotations are often mathematical objects inhabiting a mathematical space, but it is not a requirement that they should be so.  As a practical necessity, denotations are described using some form of mathematical notation, which can in turn be formalized as a denotational metalanguage.  For example, denotational semantics of functional languages often translate the language into domain theory. Denotational semantic descriptions can also serve as compositional translations from a programming language into the denotational metalanguage and used as a basis for designing compilers.\nOperational semantics, whereby the execution of the language is described directly (rather than by translation).  Operational semantics loosely corresponds to interpretation, although again the \"implementation language\" of the interpreter is generally a mathematical formalism.  Operational semantics may define an abstract machine (such as the SECD machine), and give meaning to phrases by describing the transitions they induce on states of the machine.  Alternatively, as with the pure lambda calculus, operational semantics can be defined via syntactic transformations on phrases of the language itself;\nAxiomatic semantics, whereby one gives meaning to phrases by describing the axioms that apply to them.  Axiomatic semantics makes no distinction between a phrase's meaning and the logical formulas that describe it; its meaning is exactly what can be proven about it in some logic.  The canonical example of axiomatic semantics is Hoare logic.Apart from the choice between denotational, operational, or axiomatic approaches, most variations in formal semantic systems arise from the choice of supporting mathematical formalism.","completion":"Programming Languages"}
{"prompt":"Ada's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.\nSpecial types provided by the language are task types and protected types.\nFor example, a date might be represented as:\n\nImportant to note: Day_type, Month_type, Year_type, Hours are incompatible types, meaning that for instance the following expression is illegal:\n\nThe predefined plus-operator can only add values of the same type, so the expression is illegal. \nTypes can be refined by declaring subtypes:\n\nTypes can have modifiers such as limited, abstract, private etc. Private types do not show their inner structure; objects of limited types cannot be copied. Ada 95 adds further features for object-oriented extension of types.","completion":"Programming Languages"}
{"prompt":"Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.Consider C assignment statement x=x * 10, this changes the value assigned to the variable x. Let us say that the initial value of x was 1, then two consecutive evaluations of the variable x yields 10 and 100 respectively. Clearly, replacing x=x * 10 with either 10 or 100 gives a program a different meaning, and so the expression is not referentially transparent. In fact, assignment statements are never referentially transparent.\nNow, consider another function such as int plusone(int x) {return x+1;} is transparent, as it does not implicitly change the input x and thus has no such side effects.\nFunctional programs exclusively use this type of function and are therefore referentially transparent.","completion":"Programming Languages"}
{"prompt":"A FRACTRAN program is an ordered list of positive fractions together with an initial positive integer input \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . The program is run by multiplying the integer \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   by the first fraction \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   in the list for which \n  \n    \n      \n        n\n        f\n      \n    \n    {\\displaystyle nf}\n   is an integer. The integer \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   is then replaced by \n  \n    \n      \n        n\n        f\n      \n    \n    {\\displaystyle nf}\n   and the rule is repeated. If no fraction in the list produces an integer when multiplied by \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , the program halts. FRACTRAN was invented by mathematician John Conway.","completion":"Programming Languages"}
{"prompt":"An interpreter is a program that reads another program, typically as text,  as seen in languages like Python. Interpreters read code, and produces the result directly. Interpreters typically read code line by line, and parses it to convert and execute the code as operations and actions.","completion":"Programming Languages"}
{"prompt":"Clojure supports anonymous functions through the \"fn\" special form:\n\nThere is also a reader syntax to define a lambda:\n\nLike Scheme, Clojure's \"named functions\" are simply syntactic sugar for lambdas bound to names:\n\nexpands to:","completion":"Programming Languages"}
{"prompt":"John G. Kemeny was the math department chairman at Dartmouth College. Based largely on his reputation as an innovator in math teaching, in 1959 the school won an Alfred P. Sloan Foundation award for $500,000 to build a new department building. Thomas E. Kurtz had joined the department in 1956, and from the 1960s Kemeny and Kurtz agreed on the need for programming literacy among students outside the traditional STEM fields. Kemeny later noted that \"Our vision was that every student on campus should have access to a computer, and any faculty member should be able to use a computer in the classroom whenever appropriate. It was as simple as that.\"Kemeny and Kurtz had made two previous experiments with simplified languages, DARSIMCO (Dartmouth Simplified Code) and DOPE (Dartmouth Oversimplified Programming Experiment). These did not progress past a single freshman class. New experiments using Fortran and ALGOL followed, but Kurtz concluded these languages were too tricky for what they desired. As Kurtz noted, Fortran had numerous oddly-formed commands, notably an \"almost impossible-to-memorize convention for specifying a loop: DO 100, I = 1, 10, 2. Is it '1, 10, 2' or '1, 2, 10', and is the comma after the line number required or not?\"Moreover, the lack of any sort of immediate feedback was a key problem; the machines of the era used batch processing and took a long time to complete a run of a program. While Kurtz was visiting MIT, John McCarthy suggested that time-sharing offered a solution; a single machine could divide up its processing time among many users, giving them the illusion of having a (slow) computer to themselves. Small programs would return results in a few seconds. This led to increasing interest in a system using time-sharing and a new language specifically for use by non-STEM students.Kemeny wrote the first version of BASIC. The acronym BASIC comes from the name of an unpublished paper by Thomas Kurtz. The new language was heavily patterned on FORTRAN II; statements were one-to-a-line, numbers were used to indicate the target of loops and branches, and many of the commands were similar or identical to Fortran. However, the syntax was changed wherever it could be improved. For instance, the difficult to remember DO loop was replaced by the much easier to remember FOR I = 1 TO 10 STEP 2, and the line number used in the DO was instead indicated by the NEXT I. Likewise, the cryptic IF statement of Fortran, whose syntax matched a particular instruction of the machine on which it was originally written, became the simpler IF I=5 THEN GOTO 100. These changes made the language much less idiosyncratic while still having an overall structure and feel similar to the original FORTRAN.The project received a $300,000 grant from the National Science Foundation, which was used to purchase a GE-225 computer for processing, and a Datanet-30 realtime processor to handle the Teletype Model 33 teleprinters used for input and output. A team of a dozen undergraduates worked on the project for about a year, writing both the DTSS system and the BASIC compiler. The first version BASIC language was released on 1 May 1964.Initially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965. Usage in the university rapidly expanded, requiring the main CPU to be replaced by a GE-235, and still later by a GE-635. By the early 1970s there were hundreds of terminals connected to the machines at Dartmouth, some of them remotely.\nWanting use of the language to become widespread, its designers made the compiler available free of charge. In the 1960s, software became a chargeable commodity; until then, it was provided without charge as a service with expensive computers, usually available only to lease. They also made it available to high schools in the Hanover, New Hampshire, area and regionally throughout New England on Teletype Model 33 and Model 35 teleprinter terminals connected to Dartmouth via dial-up phone lines, and they put considerable effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as Dartmouth BASIC.\nNew Hampshire recognized the accomplishment in 2019 when it erected a highway historical marker in Hanover describing the creation of \"the first user-friendly programming language\".","completion":"Programming Languages"}
{"prompt":"Here follows a longer example of mathematical-style pseudocode, for the Ford\u2013Fulkerson algorithm:\n\nalgorithm ford-fulkerson is\n    input: Graph G with flow capacity c, \n           source node s, \n           sink node t\n    output: Flow f such that f is maximal from s to t\n\n    (Note that f(u,v) is the flow from node u to node v, and c(u,v) is the flow capacity from node u to node v)\n\n    for each edge (u, v) in GE do\n        f(u, v) \u2190 0\n        f(v, u) \u2190 0\n\n    while there exists a path p from s to t in the residual network Gf do\n        let cf be the flow capacity of the residual network Gf\n        cf(p) \u2190 min{cf(u, v) | (u, v) in p}\n        for each edge (u, v) in p do\n            f(u, v) \u2190  f(u, v) + cf(p)\n            f(v, u) \u2190 \u2212f(u, v)\n\n    return f","completion":"Programming Languages"}
{"prompt":"Threaded code interpreters are similar to bytecode interpreters but instead of bytes they use pointers. Each \"instruction\" is a word that points to a function or an instruction sequence, possibly followed by a parameter. The threaded code interpreter either loops fetching instructions and calling the functions they point to, or fetches the first instruction and jumps to it, and every instruction sequence ends with a fetch and jump to the next instruction. Unlike bytecode there is no effective limit on the number of different instructions other than available memory and address space. The classic example of threaded code is the Forth code used in Open Firmware systems: the source language is compiled into \"F code\" (a bytecode), which is then interpreted by a virtual machine.","completion":"Programming Languages"}
{"prompt":"Grace Brewster Murray was born in New York City. She was the eldest of three children. Her parents, Walter Fletcher Murray and Mary Campbell Van Horne, were of Scottish and Dutch descent, and attended West End Collegiate Church. Her great-grandfather, Alexander Wilson Russell, an admiral in the US Navy, fought in the Battle of Mobile Bay during the Civil War.:\u200a2\u20133\u200aGrace was very curious as a child; this was a lifelong trait. At the age of seven, she decided to determine how an alarm clock worked and dismantled seven alarm clocks before her mother realized what she was doing (she was then limited to one clock). For her preparatory school education, she attended the Hartridge School in Plainfield, New Jersey. Grace was initially rejected for early admission to Vassar College at age 16 (because her test scores in Latin were too low), but she was admitted the following year. She graduated Phi Beta Kappa from Vassar in 1928 with a bachelor's degree in mathematics and physics and earned her master's degree at Yale University in 1930.\nIn 1930, Grace Murray married New York University professor Vincent Foster Hopper (1906\u20131976); they divorced in 1945. She did not marry again and retained his surname.\nIn 1934, Hopper earned a Ph.D. in mathematics from Yale under the direction of \u00d8ystein Ore. Her dissertation, \"New Types of Irreducibility Criteria\", was published that same year. She began teaching mathematics at Vassar in 1931, and was promoted to associate professor in 1941.","completion":"Programming Languages"}
{"prompt":"Python's large standard library provides tools suited to many tasks and is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals, manipulating regular expressions, and unit testing.\nSome parts of the standard library are covered by specifications\u2014for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333\u2014but most are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\nAs of 14 November 2022, the Python Package Index (PyPI), the official repository for third-party Python software, contains over 415,000 packages with a wide range of functionality, including:","completion":"Programming Languages"}
{"prompt":"Functional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.  This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions.  It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.Lazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan et al. 2008 give some practical advice for analyzing and fixing them.\nHowever, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles).","completion":"Programming Languages"}
{"prompt":"Most popular programming systems separate static program code (in the form of class definitions, functions or procedures) from dynamic, or run time, program state (such as objects or other forms of program data).  They load program code when a program starts, and any prior program state must be recreated explicitly from configuration files or other data sources. Any settings the program (and programmer) does not explicitly save must be set up again for each restart. A traditional program also loses much useful document information each time a program saves a file, quits, and reloads. This loses details such as undo history or cursor position. Image based systems don't force losing all that just because a computer is turned off, or an OS updates.\nMany Smalltalk systems, however, do not differentiate between program data (objects) and code (classes). In fact, classes are objects. Thus, most Smalltalk systems store the entire program state (including both Class and non-Class objects) in an image file.  The image can then be loaded by the Smalltalk virtual machine to restore a Smalltalk-like system to a prior state.  This was inspired by FLEX, a language created by Alan Kay and described in his M.Sc. thesis.Smalltalk images are similar to (restartable) core dumps and can provide the same functionality as core dumps, such as delayed or remote debugging with full access to the program state at the time of error.Other languages that model application code as a form of data, such as Lisp, often use image-based persistence as well (see EMACS, for example). This method of persistence is powerful for rapid development because all the development information (e.g. parse trees of the program) is saved which facilitates debugging.\nHowever, it also has serious drawbacks as a true persistence mechanism. For one thing, developers may often want to hide implementation details and not make them available in a run time environment. For reasons of legality and maintenance, allowing anyone to modify a program at run time inevitably introduces complexity and potential errors that would not be possible with a compiled system that exposes no source code in the run time environment. Also, while the persistence mechanism is easy to use, it lacks the true persistence abilities needed for most multi-user systems.  The most obvious is the ability to do transactions with multiple users accessing the same database in parallel.","completion":"Programming Languages"}
{"prompt":"When representing source code in Lisp, the first element of an S-expression is commonly an operator or function name and any remaining elements are treated as arguments. This is called \"prefix notation\" or \"Polish notation\". As an example, the Boolean expression written 4 == (2 + 2) in C, is represented as (= 4 (+ 2 2)) in Lisp's s-expr-based prefix notation.\nAs noted above, the precise definition of \"atom\" varies across LISP-like languages. A quoted string can typically contain anything but a quote, while\nan unquoted identifier atom can typically contain anything but quotes, whitespace characters, parentheses, brackets, braces, backslashes, and semicolons. In either case, a prohibited character can typically be included by escaping it with a preceding backslash. Unicode support varies.\nThe recursive case of the s-expr definition is traditionally implemented using cons cells.\nS-expressions were originally intended only for data to be manipulated by M-expressions, but the first implementation of Lisp was an interpreter of S-expression encodings of M-expressions, and Lisp programmers soon became accustomed to using S-expressions for both code and data.\nThis means that Lisp is homoiconic; that is, the primary representation of programs is also a data structure in a primitive type of the language itself.","completion":"Programming Languages"}
{"prompt":"This following immediate-mode expression generates a typical set of Pick 6 lottery numbers: six pseudo-random integers ranging from 1 to 40, guaranteed non-repeating, and displays them sorted in ascending order:\n\nThe above does a lot, concisely, although it may seem complex to a new APLer. It combines the following APL functions (also called primitives and glyphs):\n\nThe first to be executed (APL executes from rightmost to leftmost) is dyadic function ? (named deal when dyadic) that returns a vector consisting of a select number (left argument: 6 in this case) of random integers ranging from 1 to a specified maximum (right argument: 40 in this case), which, if said maximum \u2265 vector length, is guaranteed to be non-repeating; thus, generate\/create 6 random integers ranging from 1 to 40.\nThis vector is then assigned (\u2190) to the variable x, because it is needed later.\nThis vector is then sorted in ascending order by a monadic \u234b function, which has as its right argument everything to the right of it up to the next unbalanced close-bracket or close-parenthesis. The result of \u234b is the indices that will put its argument into ascending order.\nThen the output of \u234b is used to index the variable x, which we saved earlier for this purpose, thereby selecting its items in ascending sequence.Since there is no function to the left of the left-most x to tell APL what to do with the result, it simply outputs it to the display (on a single line, separated by spaces) without needing any explicit instruction to do that.\n? also has a monadic equivalent called roll, which simply returns one random integer between 1 and its sole operand [to the right of it], inclusive. Thus, a role-playing game program might use the expression ?20 to roll a twenty-sided die.","completion":"Programming Languages"}
{"prompt":"Stata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.","completion":"Programming Languages"}
{"prompt":"BCPL was first implemented by Martin Richards of the University of Cambridge in 1967.  BCPL was a response to difficulties with its predecessor, Cambridge Programming Language, later renamed Combined Programming Language (CPL), which was designed during the early 1960s. Richards created BCPL by \"removing those features of the full language which make compilation difficult\". The first compiler implementation, for the IBM 7094 under Compatible Time-Sharing System, was written while Richards was visiting Project MAC at the Massachusetts Institute of Technology in the spring of 1967. The language was first described in a paper presented to the 1969 Spring Joint Computer Conference.BCPL has been rumored to have originally stood for \"Bootstrap Cambridge Programming Language\", but CPL was never created since development stopped at BCPL, and the acronym was later reinterpreted for the BCPL book.BCPL is the language in which the original \"Hello, World!\" program was written. The first MUD was also written in BCPL (MUD1).\nSeveral operating systems were written partially or wholly in BCPL (for example, TRIPOS and the earliest versions of AmigaDOS). BCPL was also the initial language used in the Xerox PARC Alto project, the first modern personal computer; among other projects, the Bravo document preparation system was written in BCPL.\nAn early compiler, bootstrapped in 1969, by starting with a paper tape of the O-code of Richards's Atlas 2 compiler, targeted the ICT 1900 series. The two machines had different word-lengths (48 vs 24 bits), different character encodings, and different packed string representations\u2014and the successful bootstrapping increased confidence in the practicality of the method.\nBy late 1970, implementations existed for the Honeywell 635 and Honeywell 645, IBM 360, PDP-10, TX-2, CDC 6400, UNIVAC 1108, PDP-9, KDF 9 and Atlas 2. In 1974 a dialect of BCPL was implemented at BBN without using the intermediate O-code. The initial implementation was a cross-compiler hosted on BBN's TENEX PDP-10s, and directly targeted the PDP-11s used in BBN's implementation of the second generation IMPs used in the ARPANET.\nThere was also a version produced for the BBC Micro in the mid-1980s, by Richards Computer Products, a company started by John Richards, the brother of Martin Richards. The BBC Domesday Project made use of the language. Versions of BCPL for the Amstrad CPC and Amstrad PCW computers were also released in 1986 by UK software house Arnor Ltd. MacBCPL was released for the Apple Macintosh in 1985 by Topexpress Ltd, of Kensington, England.\nBoth the design and philosophy of BCPL strongly influenced B, which in turn influenced C.  Programmers at the time debated whether an eventual successor to C would be called \"D\", the next letter in the alphabet, or \"P\", the next letter in the parent language name.  The language most accepted as being C's successor is C++ (with ++ being C's increment operator), although meanwhile, a D programming language also exists.\nIn 1979, implementations of BCPL existed for at least 25 architectures; the language gradually fell out of favour as C became popular on non-Unix systems.\nMartin Richards maintains a modern version of BCPL on his website, last updated in 2018. This can be set up to run on various systems including Linux, FreeBSD, and Mac OS X. The latest distribution includes graphics and sound libraries, and there is a comprehensive manual. He continues to program in it, including for his research on musical automated score following.\nA common informal MIME type for BCPL is text\/x-bcpl.","completion":"Programming Languages"}
{"prompt":"Interpreted languages are programming languages in which programs may be executed from source code form, by an interpreter. Theoretically, any language can be compiled or interpreted, so the term interpreted language generally refers to languages that are usually interpreted rather than compiled.","completion":"Programming Languages"}
{"prompt":"There are many stylistic alternatives available when considering how comments should appear in source code. For larger projects involving a team of developers, comment styles are either agreed upon before a project starts, or evolve as a matter of convention or need as a project grows. Usually programmers prefer styles that are consistent, non-obstructive, easy to modify, and difficult to break.","completion":"Programming Languages"}
{"prompt":"PyPy is a fast, compliant interpreter of Python 2.7 and 3.8. Its just-in-time compiler often brings a significant speed improvement over CPython but some libraries written in C cannot be used with it.\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the call stack in the same way, thus allowing massively concurrent programs. PyPy also has a stackless version.\nMicroPython and CircuitPython are Python 3 variants optimized for microcontrollers, including Lego Mindstorms EV3.\nPyston is a variant of the Python runtime that uses just-in-time compilation to speed up the execution of Python programs.\nCinder is a performance-oriented fork of CPython 3.8 that contains a number of optimizations including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.","completion":"Programming Languages"}
{"prompt":"The following rules describe a formal language L over the alphabet \u03a3 = {0,\u20091,\u20092,\u20093,\u20094,\u20095,\u20096,\u20097,\u20098,\u20099,\u2009+,\u2009=}:\n\nEvery nonempty string that does not contain \"+\" or \"=\" and does not start with \"0\" is in L.\nThe string \"0\" is in L.\nA string containing \"=\" is in L if and only if there is exactly one \"=\", and it separates two valid strings of L.\nA string containing \"+\" but not \"=\" is in L if and only if every \"+\" in the string separates two valid strings of L.\nNo string is in L other than those implied by the previous rules.Under these rules, the string \"23+4=555\" is in L, but the string \"=234=+\" is not. This formal language expresses natural numbers, well-formed additions, and well-formed addition equalities, but it expresses only what they look like (their syntax), not what they mean (semantics). For instance, nowhere in these rules is there any indication that \"0\" means the number zero, \"+\" means addition, \"23+4=555\" is false, etc.","completion":"Programming Languages"}
{"prompt":"One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:\nStatic memory allocation: space for the object is provided in the binary at compile-time; these objects have an extent (or lifetime) as long as the binary which contains them is loaded into memory.\nAutomatic memory allocation: temporary objects can be stored on the stack, and this space is automatically freed and reusable after the block in which they are declared is exited.\nDynamic memory allocation: blocks of memory of arbitrary size can be requested at run-time using library functions such as malloc from a region of memory called the heap; these blocks persist until subsequently freed for reuse by calling the library function realloc or freeThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.  Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated.  (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\nHeap memory allocation has to be synchronized with its actual usage in any program to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before it is deallocated explicitly, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed, but is referenced subsequently, leading to unpredictable results. Typically, the failure symptoms appear in a portion of the program unrelated to the code that causes the error, making it difficult to diagnose the failure. Such issues are ameliorated in languages with automatic garbage collection.","completion":"Programming Languages"}
{"prompt":"Up to the 1985 ANSI COBOL standard had the ALTER statement which could be used to change the destination of an existing GO TO, which had to be in a paragraph by itself. The feature, which allowed polymorphism, was frequently condemned and seldom used.","completion":"Programming Languages"}
{"prompt":"An intersection type is a type containing those values that are members of two specified types. For example, in Java the class Boolean implements both the Serializable and the Comparable interfaces. Therefore, an object of type Boolean is a member of the type Serializable & Comparable. Considering types as sets of values, the intersection type \n  \n    \n      \n        \u03c3\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle \\sigma \\cap \\tau }\n   is the set-theoretic intersection of \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   and \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n  . It is also possible to define a dependent intersection type, denoted \n  \n    \n      \n        (\n        x\n        :\n        \u03c3\n        )\n        \u2229\n        \u03c4\n      \n    \n    {\\displaystyle (x:\\sigma )\\cap \\tau }\n  , where the type \n  \n    \n      \n        \u03c4\n      \n    \n    {\\displaystyle \\tau }\n   may depend on the term variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  .","completion":"Programming Languages"}
{"prompt":"Lisp provides many built-in procedures for accessing and controlling lists. Lists can be created directly with the list procedure, which takes any number of arguments, and returns the list of these arguments.\n\nBecause of the way that lists are constructed from cons pairs, the cons procedure can be used to add an element to the front of a list. Note that the cons procedure is asymmetric in how it handles list arguments, because of how lists are constructed.\n\nThe append procedure appends two (or more) lists to one another. Because Lisp lists are linked lists, appending two lists has asymptotic time complexity \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}","completion":"Programming Languages"}
{"prompt":"In MATLAB's programming language, the '%' character indicates a single-line comment.  Multi line comments are also available via %{ and %} brackets and can be nested, e.g.","completion":"Programming Languages"}
{"prompt":"Mauchly stayed involved in computers for the rest of his life.  He was a founding member and president of the Association for Computing Machinery (ACM) and also helped found the Society for Industrial and Applied Mathematics (SIAM), serving as its fourth president. The Eckert\u2013Mauchly Corporation was bought by Remington Rand in 1950 and for ten years Mauchly remained as Director of Univac Applications Research.  Leaving in 1959 he formed Mauchly Associates, a consulting company that later introduced the critical path method (CPM) for construction scheduling by computer. In 1967 he founded Dynatrend, a computer consulting organization. In 1973 he became a consultant to Sperry Univac.","completion":"Programming Languages"}
{"prompt":"In the Curl web-content language, multiple inheritance is used as classes with no instances may implement methods. Common mixins include all skinnable ControlUIs inheriting from SkinnableControlUI, user interface delegate objects that require dropdown menus inheriting from StandardBaseDropdownUI and such explicitly named mixin classes as FontGraphicMixin, FontVisualMixin and NumericAxisMixin-of class. Version 7.0 added library access so that mixins do not need to be in the same package or be public abstract. Curl constructors are factories that facilitates using multiple-inheritance without explicit declaration of either interfaces or mixins.","completion":"Programming Languages"}
{"prompt":"A number of languages implement a form of switch statement in exception handling, where if an exception is raised in a block, a separate branch is chosen, depending on the exception. In some cases a default branch, if no exception is raised, is also present. An early example is Modula-3, which use the TRY...EXCEPT syntax, where each EXCEPT defines a case. This is also found in Delphi, Scala, and Visual Basic .NET.","completion":"Programming Languages"}
{"prompt":"Macros are not the same as \"section names\" in standard documentation. Literate programming macros hide the real code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as \"if\", \"while\" or \"case\". This can be seen in the following wc literate program.\n\nThe macros stand for any chunk of code or other macros, and are more general than top-down or bottom-up \"chunking\", or than subsectioning. Donald Knuth said that when he realized this, he began to think of a program as a web of various parts.","completion":"Programming Languages"}
{"prompt":"Scheme specifies a comparatively full set of numerical datatypes including complex and rational types, which is known in Scheme as the numerical tower (R5RS sec. 6.2). The standard treats these as abstractions, and does not commit the implementor to any particular internal representations.\nNumbers may have the quality of exactness. An exact number can only be produced by a sequence of exact operations involving other exact numbers\u2014inexactness is thus contagious. The standard specifies that any two implementations must produce equivalent results for all operations resulting in exact numbers.\nThe R5RS standard specifies procedures exact->inexact and inexact->exact which can be used to change the exactness of a number. inexact->exact produces \"the exact number that is numerically closest to the argument\". exact->inexact produces  \"the inexact number that is numerically closest to the argument\". The R6RS standard omits these procedures from the main report, but specifies them as R5RS compatibility procedures in the standard library (rnrs r5rs (6)).\nIn the R5RS standard, Scheme implementations are not required to implement the whole numerical tower, but they must implement \"a coherent subset consistent with both the purposes of the implementation and the spirit of the Scheme language\" (R5RS sec. 6.2.3).  The new R6RS standard does require implementation of the whole tower, and \"exact integer objects and exact rational number objects of practically unlimited size and precision, and to implement certain procedures...so they always return exact results when given exact arguments\" (R6RS sec. 3.4, sec. 11.7.1).Example 1: exact arithmetic in an implementation that supports exact \nrational complex numbers.\n\nExample 2: Same arithmetic in an implementation that supports neither exact \nrational numbers nor complex numbers but does accept real numbers in rational notation.\n\nBoth implementations conform to the R5RS standard but the second does not conform to R6RS because it does not implement the full numerical tower.","completion":"Programming Languages"}
{"prompt":"Lua programs are not interpreted directly from the textual Lua file, but are compiled into bytecode, which is then run on the Lua virtual machine. The compilation process is typically invisible to the user and is performed during run-time, especially when a JIT compiler is used, but it can be done offline in order to increase loading performance or reduce the memory footprint of the host environment by leaving out the compiler. Lua bytecode can also be produced and executed from within Lua, using the dump function from the string library and the load\/loadstring\/loadfile functions. Lua version 5.3.4 is implemented in approximately 24,000 lines of C code.Like most CPUs, and unlike most virtual machines (which are stack-based), the Lua VM is register-based, and therefore more closely resembles an actual hardware design. The register architecture both avoids excessive copying of values and reduces the total number of instructions per function. The virtual machine of Lua 5 is one of the first register-based pure VMs to have a wide use. Parrot and Android's Dalvik are two other well-known register-based VMs.  PCScheme's VM was also register-based.This example is the bytecode listing of the factorial function defined above (as shown by the luac 5.1 compiler):\nfunction <factorial.lua:1,7> (9 instructions, 36 bytes at 0x8063c60)\n1 param, 6 slots, 0 upvalues, 6 locals, 2 constants, 0 functions\n\t1\t[2]\tLOADK    \t1 -1\t; 1\n\t2\t[3]\tLOADK    \t2 -2\t; 2\n\t3\t[3]\tMOVE     \t3 0\n\t4\t[3]\tLOADK    \t4 -1\t; 1\n\t5\t[3]\tFORPREP  \t2 1\t; to 7\n\t6\t[4]\tMUL      \t1 1 5\n\t7\t[3]\tFORLOOP  \t2 -2\t; to 6\n\t8\t[6]\tRETURN   \t1 2\n\t9\t[7]\tRETURN   \t0 1","completion":"Programming Languages"}
{"prompt":"Localization is the core feature of the Citrine Programming Language. Citrine is designed to be translatable to every written human language. For instance the West Frisian language version is called Citrine\/FY. Citrine features localized keywords, localized numbers and localized punctuation. Users can translate code files from one language into another using a string-based approach. At the time of writing, Citrine supports 111 human languages. Support is not limited to well-known languages; all natural human languages are being accepted for inclusion, up to EGIDS-6.","completion":"Programming Languages"}
{"prompt":"The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.\nWhile the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.\nThe main phases of the front end include the following:\n\nLine reconstruction converts the input character sequence to a canonical form ready for the parser. Languages which strop their keywords or allow arbitrary spaces within identifiers require this phase. The top-down, recursive-descent, table-driven parsers used in the 1960s typically read the source one character at a time and did not require a separate tokenizing phase. Atlas Autocode and Imp (and some implementations of ALGOL and Coral 66) are examples of stropped languages whose compilers would have a Line Reconstruction phase.\nPreprocessing supports macro substitution and conditional compilation. Typically the preprocessing phase occurs before syntactic or semantic analysis; e.g. in the case of C, the preprocessor manipulates lexical tokens rather than syntactic forms. However, some languages such as Scheme support macro substitutions based on syntactic forms.\nLexical analysis (also known as lexing or tokenization) breaks the source code text into a sequence of small pieces called lexical tokens. This phase can be divided into two stages: the scanning, which segments the input text into syntactic units called lexemes and assigns them a category; and the evaluating, which converts lexemes into a processed value. A token is a pair consisting of a token name and an optional token value. Common token categories may include identifiers, keywords, separators, operators, literals and comments, although the set of token categories varies in different programming languages. The lexeme syntax is typically a regular language, so a finite state automaton constructed from a regular expression can be used to recognize it. The software doing lexical analysis is called a lexical analyzer. This may not be a separate step\u2014it can be combined with the parsing step in scannerless parsing, in which case parsing is done at the character level, not the token level.\nSyntax analysis (also known as parsing) involves parsing the token sequence to identify the syntactic structure of the program. This phase typically builds a parse tree, which replaces the linear sequence of tokens with a tree structure built according to the rules of a formal grammar which define the language's syntax. The parse tree is often analyzed, augmented, and transformed by later phases in the compiler.\nSemantic analysis adds semantic information to the parse tree and builds the symbol table. This phase performs semantic checks such as type checking (checking for type errors), or object binding (associating variable and function references with their definitions), or definite assignment (requiring all local variables to be initialized before use), rejecting incorrect programs or issuing warnings. Semantic analysis usually requires a complete parse tree, meaning that this phase logically follows the parsing phase, and logically precedes the code generation phase, though it is often possible to fold multiple phases into one pass over the code in a compiler implementation.","completion":"Programming Languages"}
{"prompt":"Program code can be written in S-expressions, usually using prefix notation.\nExample in Common Lisp:\n\nS-expressions can be read in Lisp using the function READ. READ reads the textual representation of an S-expression and returns Lisp data. The function PRINT can be used to output an S-expression. The output then can be read with the function READ, when all printed data objects have a readable representation.  Lisp has readable representations for numbers, strings, symbols, lists and many other data types. Program code can be formatted as pretty printed S-expressions using the function PPRINT (note: with two Ps, short for pretty-print).\nLisp programs are valid S-expressions, but not all S-expressions are valid Lisp programs. (1.0 + 3.1) is a valid S-expression, but not a valid Lisp program, since Lisp uses prefix notation and a floating point number (here 1.0) is not valid as an operation (the first element of the expression).\nAn S-expression preceded by a single quotation mark, as in 'x, is syntactic sugar for a quoted S-expression, in this case (quote x).","completion":"Programming Languages"}
{"prompt":"Context-free languages are closed under the various operations, that is, if the languages K and L are \ncontext-free, so is the result of the following operations:\n\nunion K \u222a L; concatenation K \u2218 L; Kleene star L*\nsubstitution (in particular homomorphism)\ninverse homomorphism\nintersection with a regular languageThey are not closed under general intersection (hence neither under complementation) and set difference.","completion":"Programming Languages"}
{"prompt":"A variable is assigned a value via the ':=' syntax. So:\n\nAssigns the string 'aeiou' to the formerly declared vowels variable. The string is an object (a sequence of characters between single quotes is the syntax for literal strings), created by the compiler at compile time.\nIn the original Parc Place image, the glyph of the underscore character \u27e8_\u27e9 appeared as a left-facing arrow \u27e8\u2190\u27e9 (like in the 1963 version of the ASCII code).  Smalltalk originally accepted this left-arrow as the only assignment operator.  Some modern code still contains what appear to be underscores acting as assignments, hearkening back to this original usage.  Most modern Smalltalk implementations accept either the underscore or the colon-equals syntax.","completion":"Programming Languages"}
{"prompt":"Information theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.","completion":"Programming Languages"}
{"prompt":"Functional programming languages define programs and subroutines as mathematical functions and treat them as first-class. Many so-called functional languages are \"impure\", containing imperative features. Many functional languages are tied to mathematical calculation tools. Functional languages include:","completion":"Programming Languages"}
{"prompt":"Instructions (statements) in assembly language are generally very simple, unlike those in high-level languages. Generally, a mnemonic is a symbolic name for a single executable machine language instruction (an opcode), and there is at least one opcode mnemonic defined for each machine language instruction. Each instruction typically consists of an operation or opcode plus zero or more operands. Most instructions refer to a single value or a pair of values.  Operands can be immediate (value coded in the instruction itself), registers specified in the instruction or implied, or the addresses of data located elsewhere in storage. This is determined by the underlying processor architecture: the assembler merely reflects how this architecture works. Extended mnemonics are often used to specify a combination of an opcode with a specific operand, e.g., the System\/360 assemblers use B as an extended mnemonic for BC with a mask of 15 and NOP (\"NO OPeration\" \u2013 do nothing for one step) for BC with a mask of 0.\nExtended mnemonics are often used to support specialized uses of instructions, often for purposes not obvious from the instruction name. For example, many CPU's do not have an explicit NOP instruction, but do have instructions that can be used for the purpose. In 8086 CPUs the instruction xchg ax,ax is used for nop, with nop being a pseudo-opcode to encode the instruction xchg ax,ax. Some disassemblers recognize this and will decode the xchg ax,ax instruction as nop. Similarly, IBM assemblers for System\/360 and System\/370 use the extended mnemonics NOP and NOPR for BC and BCR with zero masks.  For the SPARC architecture, these are known as synthetic instructions.Some assemblers also support simple built-in macro-instructions that generate two or more machine instructions. For instance, with some Z80 assemblers the instruction ld hl,bc is recognized to generate ld l,c followed by ld h,b. These are sometimes known as pseudo-opcodes.\nMnemonics are arbitrary symbols; in 1985 the IEEE published Standard 694 for a uniform set of mnemonics to be used by all assemblers. The standard has since been withdrawn.","completion":"Programming Languages"}
{"prompt":"Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.\n\nEach relatively abstract, \"higher\" level builds on a relatively concrete, \"lower\" level, which tends to provide an increasingly \"granular\" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.","completion":"Programming Languages"}
{"prompt":"Regular expressions can often be created (\"induced\" or \"learned\") based on a set of example strings. This is known as the induction of regular languages and is part of the general problem of grammar induction in computational learning theory. Formally, given examples of strings in a regular language, and perhaps also given examples of strings not in that regular language, it is possible to induce a grammar for the language, i.e., a regular expression that generates that language. Not all regular languages can be induced in this way (see language identification in the limit), but many can. For example, the set of examples {1, 10, 100}, and negative set (of counterexamples) {11, 1001, 101, 0} can be used to induce the regular expression 1\u22c50* (1 followed by zero or more 0s).","completion":"Programming Languages"}
{"prompt":"In Scheme the primitive datatypes are disjoint. Only one of the following predicates can be true of any Scheme object: boolean?, pair?, symbol?, number?, char?, string?, vector?, port?, procedure?. (R5RS sec 3.2)Within the numerical datatype, by contrast, the numerical values overlap. For example, an integer value satisfies all of the integer?, rational?, real?, complex? and number? predicates at the same time. (R5RS sec 6.2)","completion":"Programming Languages"}
{"prompt":"Dyalog APL was first released by British company Dyalog Ltd. in 1983 and, as of 2018, is available for AIX, Linux (including on the Raspberry Pi), macOS and Microsoft Windows platforms. It is based on APL2, with extensions to support object-oriented programming, functional programming, and tacit programming. Licences are free for personal\/non-commercial use.In 1995, two of the development team \u2013 John Scholes and Peter Donnelly \u2013 were awarded the Iverson Award for their work on the interpreter. Gitte Christensen and Morten Kromberg were joint recipients of the Iverson Award in 2016.","completion":"Programming Languages"}
{"prompt":"A pragma is a compiler directive that conveys information to the compiler to allow specific manipulating of compiled output.  Certain pragmas are built into the language, while others are implementation-specific.\nExamples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code instead of a function call (as C\/C++ does with inline functions).","completion":"Programming Languages"}
{"prompt":"Daniel P. Friedman is the author or co-author of the following books:\n\nThe Little Lisper ISBN 0-262-56038-0  (MIT Press, 1987)\nThe Little Schemer ISBN 0-262-56099-2 (MIT Press, 4th Ed., 1996)\nThe Little MLer ISBN 0-262-56114-X (MIT Press, 1998)\nA Little Java, A Few Patterns ISBN 0-262-56115-8 (MIT Press, 1998)\nThe Seasoned Schemer ISBN 0-262-56100-X (MIT Press, 1996)\nThe Reasoned Schemer ISBN 0-262-56214-6 (MIT Press, 2018)\nThe Little Prover ISBN 0-262-52795-2\nEssentials of Programming Languages ISBN 0-262-06217-8 (MIT Press, 3rd Ed. 2008)\nScheme and the Art of Programming ISBN 0-262-19288-8\nCoordinated Computing: Tools and Techniques for Distributed Software ISBN 0-07-022439-0\nThe Little Typer ISBN 9780262536431\nThe Little Learner ISBN 9780262546379 (MIT Press, 2023)","completion":"Programming Languages"}
{"prompt":"Python is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal.","completion":"Programming Languages"}
{"prompt":"For communication to occur, protocols have to be selected. The rules can be expressed by algorithms and data structures. Hardware and operating system independence is enhanced by expressing the algorithms in a portable programming language. Source independence of the specification provides wider interoperability.\nProtocol standards are commonly created by obtaining the approval or support of a standards organization, which initiates the standardization process. The members of the standards organization agree to adhere to the work result on a voluntary basis. Often the members are in control of large market shares relevant to the protocol and in many cases, standards are enforced by law or the government because they are thought to serve an important public interest, so getting approval can be very important for the protocol.","completion":"Programming Languages"}
{"prompt":"There are many concepts and theories in continuous mathematics which have discrete versions, such as discrete calculus, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, discrete optimization, discrete probability theory, discrete probability distribution, difference equations, discrete dynamical systems, and discrete vector measures.","completion":"Programming Languages"}
{"prompt":"C is an imperative, procedural language in the ALGOL tradition.  It has a static type system. In C, all executable code is contained within subroutines (also called \"functions\", though not in the sense of functional programming). Function parameters are passed by value, although arrays are passed as pointers, i.e. the address of the first item in the array. Pass-by-reference is simulated in C by explicitly passing pointers to the thing being referenced.\nC program source text is free-form code.  Semicolons terminate statements, while curly braces are used to group statements into blocks.\nThe C language also exhibits the following characteristics:\n\nThe language has a small, fixed number of keywords, including a full set of control flow primitives: if\/else, for, do\/while, while, and switch. User-defined names are not distinguished from keywords by any kind of sigil.\nIt has a large number of arithmetic, bitwise, and logic operators: +,+=,++,&,||, etc.\nMore than one assignment may be performed in a single statement.\nFunctions:\nFunction return values can be ignored, when not needed.\nFunction and data pointers permit ad hoc run-time polymorphism.\nFunctions may not be defined within the lexical scope of other functions.\nVariables may be defined within a function, with scope.\nA function may call itself, so recursion is supported.\nData typing is static, but weakly enforced; all data has a type, but implicit conversions are possible.\nUser-defined (typedef) and compound types are possible.\nHeterogeneous aggregate data types (struct) allow related data elements to be accessed and assigned as a unit. The contents of whole structs cannot be compared using a single built-in operator (the elements must be compared individually).\nUnion is a structure with overlapping members; it allows multiple data types to share the same memory location.\nArray indexing is a secondary notation, defined in terms of pointer arithmetic. Whole arrays cannot be assigned or compared using a single built-in operator. There is no \"array\" keyword in use or definition; instead, square brackets indicate arrays syntactically, for example month[11].\nEnumerated types are possible with the enum keyword. They are freely interconvertible with integers.\nStrings are not a distinct data type, but are conventionally implemented as null-terminated character arrays.\nLow-level access to computer memory is possible by converting machine addresses to pointers.\nProcedures (subroutines not returning values) are a special case of function, with an empty return type void.\nMemory can be allocated to a program with calls to library routines.\nA preprocessor performs macro definition, source code file inclusion, and conditional compilation.\nThere is a basic form of modularity: files can be compiled separately and linked together, with control over which functions and data objects are visible to other files via static and extern attributes.\nComplex functionality such as I\/O, string manipulation, and mathematical functions are consistently delegated to library routines.\nThe generated code after compilation has relatively straightforward needs on the underlying platform, which makes it suitable for creating operating systems and for use in embedded systems.While C does not include certain features found in other languages (such as object orientation and garbage collection), these can be implemented or emulated, often through the use of external libraries (e.g., the GLib Object System or the Boehm garbage collector).","completion":"Programming Languages"}
{"prompt":"\"High-level language\" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses, and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input\/output, may also be present. One thing to note about high-level programming languages is that these languages allow the programmer to be detached and separated from the machine. That is, unlike low-level languages like assembly or machine language, high-level programming can amplify the programmer's instructions and trigger a lot of data movements in the background without their knowledge. The responsibility and power of executing instructions have been handed over to the machine from the programmer.","completion":"Programming Languages"}
{"prompt":"Cambridge Modula-2 by Cambridge Microprocessor Systems is based on a subset of PIM4 with language extensions for embedded development. The compiler runs on DOS and it generates code for Motorola 68000 series (M68k) based embedded microcontrollers running a MINOS operating system.","completion":"Programming Languages"}
{"prompt":"No written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. There has been only one implementation of the interpreter, and the language has evolved along with it. That interpreter, together with its functional tests, stands as a de facto specification of the language. Perl 6, however, started with a specification, and several projects aim to implement some or all of the specification.Perl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C. As of 2010, the interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. Alternatively, the interpreter can be compiled to a link library and embedded in other programs. There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).The interpreter has an object-oriented architecture. All of the elements of the Perl language\u2014scalars, arrays, hashes, coderefs, file handles\u2014are represented in the interpreter by C structs. Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.The life of a Perl interpreter divides broadly into a compile phase and a run phase.  In Perl, the phases are the major stages in the interpreter's life-cycle. Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.Most of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time.  Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. The compile time for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run time during the compile phase and spends most of the run phase in run time.  Code in BEGIN blocks executes at run time but in the compile phase.\nAt compile time, the interpreter parses Perl code into a syntax tree. At run time, it executes the program by walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.Perl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase. Therefore, Perl cannot be parsed by a straight Lex\/Yacc lexer\/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.It is often said that \"Only perl can parse Perl\", meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true. Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. It is a longstanding result that the halting problem is undecidable, and therefore not even Perl can always parse Perl. Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.Other programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that \"parsing Perl suffers from the 'halting problem',\" which was later proved.Perl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. These run as part of the normal build process and extensively exercise the interpreter and its core modules. Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.","completion":"Programming Languages"}
{"prompt":"After moving to New York City he trained initially as a radio technician and became interested in mathematics. He graduated from Columbia University with a bachelor's degree in 1949 and a master's degree in 1950, both in mathematics, and joined IBM in 1950. During his first three years, he worked on the Selective Sequence Electronic Calculator (SSEC); his first major project was to write a program to calculate positions of the Moon. In 1953 Backus developed the language Speedcoding, the first high-level language created for an IBM computer, to aid in software development for the IBM 701 computer.Programming was very difficult at this time, and in 1954 Backus assembled a team to define and develop Fortran for the IBM 704 computer. Fortran was the first high-level programming language to be put to broad use. This widely used language made computers practical and accessible machines for scientists and others without requiring them to have deep knowledge of the machinery.","completion":"Programming Languages"}
{"prompt":"PL\/I Programming with PLUM (1976)\nSoftware Specifications: A comparison of formal methods (1979)\nProgramming Languages: Design and Implementation (Third Edition) (1996)\nFoundations of Empirical Software Engineering: The Legacy of Victor R .Basili (2005)\nThe Golden Age of Computer Technology: Through the Eyes of an Aging Geek (2020)","completion":"Programming Languages"}
{"prompt":"Assembly language is typically used in a system's boot code, the low-level code that initializes and tests the system hardware prior to booting the operating system and is often stored in ROM. (BIOS on IBM-compatible PC systems and CP\/M is an example.)\nAssembly language is often used for low-level code, for instance for operating system kernels, which cannot rely on the availability of pre-existing system calls and must indeed implement them for the particular processor architecture on which the system will be running.\nSome compilers translate high-level languages into assembly first before fully compiling, allowing the assembly code to be viewed for debugging and optimization purposes.\nSome compilers for relatively low-level languages, such as Pascal or C, allow the programmer to embed assembly language directly in the source code (so called inline assembly). Programs using such facilities can then construct abstractions using different assembly language on each hardware platform. The system's portable code can then use these processor-specific components through a uniform interface.\nAssembly language is useful in reverse engineering. Many programs are distributed only in machine code form which is straightforward to translate into assembly language by a disassembler, but more difficult to translate into a higher-level language through a decompiler. Tools such as the Interactive Disassembler make extensive use of disassembly for such a purpose. This technique is used by hackers to crack commercial software, and competitors to produce software with similar results from competing companies.\nAssembly language is used to enhance speed of execution, especially in early personal computers with limited processing power and RAM.\nAssemblers can be used to generate blocks of data, with no high-level language overhead, from formatted and commented source code, to be used by other code.","completion":"Programming Languages"}
{"prompt":"The Wolfram Language is the programming language of Mathematica. Anonymous functions are important in programming the latter. There are several ways to create them. Below are a few anonymous functions that increment a number. The first is the most common. #1 refers to the first argument and & marks the end of the anonymous function.\n\nSo, for instance:\n\nAlso, Mathematica has an added construct to make recursive anonymous functions. The symbol '#0' refers to the entire function. The following function calculates the factorial of its input:\n\nFor example, 6 factorial would be:","completion":"Programming Languages"}
{"prompt":"It provides a mechanism for multiple inheritance by allowing one class to use common functionality from multiple classes, but without the complex semantics of multiple inheritance.\nCode reusability: Mixins are useful when a programmer wants to share functionality between different classes. Instead of repeating the same code over and over again, the common functionality can simply be grouped into a mixin and then included into each class that requires it.\nMixins allow inheritance and use of only the desired features from the parent class, not necessarily all of the features from the parent class.","completion":"Programming Languages"}
{"prompt":"Source embeddable languages embed small pieces of executable code inside a piece of free-form text, often a web page.\nClient-side embedded languages are limited by the abilities of the browser or intended client. They aim to provide dynamism to web pages without the need to recontact the server.\nServer-side embedded languages are much more flexible, since almost any language can be built into a server. The aim of having fragments of server-side code embedded in a web page is to generate additional markup dynamically; the code itself disappears when the page is served, to be replaced by its output.","completion":"Programming Languages"}
{"prompt":"ALGOL 60 as officially defined had no I\/O facilities; implementations defined their own in ways that were rarely compatible with each other. In contrast, ALGOL 68 offered an extensive library of transput (ALGOL 68 parlance for input\/output) facilities.\nALGOL 60 provided two evaluation strategies for parameter passing: the common call-by-value, and call-by-name. The procedure declaration specified, for each formal parameter, which was to be used: value specified for call-by-value, and omitted for call-by-name. Call-by-name has certain effects in contrast to call-by-reference. For example, without specifying the parameters as value or reference, it is impossible to develop a procedure that will swap the values of two parameters if the actual parameters that are passed in are an integer variable and an array that is indexed by that same integer variable. Think of passing a pointer to swap(i, A[i]) in to a function. Now that every time swap is referenced, it's reevaluated. Say i := 1 and A[i] := 2, so every time swap is referenced it'll return the other combination of the values ([1,2], [2,1], [1,2] and so on). A similar situation occurs with a random function passed as actual argument.\nCall-by-name is known by many compiler designers for the interesting \"thunks\" that are used to implement it. Donald Knuth devised the \"man or boy test\" to separate compilers that correctly implemented \"recursion and non-local references.\" This test contains an example of call-by-name.","completion":"Programming Languages"}
{"prompt":"Shakespeare is designed to make programs look like Shakespearean plays. For example, the following statement declares a point in the program which can be reached via a GOTO-type statement:\n Act I: Hamlet's insults and flattery.","completion":"Programming Languages"}
{"prompt":"The Java programming language requires the presence of a software platform in order for compiled programs to be executed.\nOracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system.","completion":"Programming Languages"}
{"prompt":"Procedural programming languages are also imperative languages, because they make explicit references to the state of the execution environment. This could be anything from variables (which may correspond to processor registers) to something like the position of the \"turtle\" in the Logo programming language.\nOften, the terms \"procedural programming\" and \"imperative programming\" are used synonymously. However, procedural programming relies heavily on blocks and scope, whereas imperative programming as a whole may or may not have such features. As such, procedural languages generally use reserved words that act on blocks, such as if, while, and for, to implement control flow, whereas non-structured imperative languages use goto statements and branch tables for the same purpose.","completion":"Programming Languages"}
{"prompt":"Mixins first appeared in Symbolics's object-oriented Flavors system (developed by Howard Cannon), which was an approach to object-orientation used in Lisp Machine Lisp. The name was inspired by Steve's Ice Cream Parlor in Somerville, Massachusetts: The owner of the ice cream shop offered a basic flavor of ice cream (vanilla, chocolate, etc.) and blended in a combination of extra items (nuts, cookies, fudge, etc.) and called the item a \"mix-in\", his own trademarked term at the time.","completion":"Programming Languages"}
{"prompt":"A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are more graphical in nature, using visual relationships between symbols to specify a program.\nThe syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.\nThe programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus\u2013Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:\n\nThis grammar specifies the following:\n\nan expression is either an atom or a list;\nan atom is either a number or a symbol;\na number is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign;\na symbol is a letter followed by zero or more of any characters (excluding whitespace); and\na list is a matched pair of parentheses, with zero or more expressions inside it.The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).\nNot all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language's rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.\nUsing natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:\n\n\"Colorless green ideas sleep furiously.\" is grammatically well-formed but has no generally accepted meaning.\n\"John is a married bachelor.\" is grammatically well-formed but expresses a meaning that cannot be true.The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):\n\nIf the type declaration on the first line were omitted, the program would trigger an error on the undefined variable p during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information.\nThe grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.","completion":"Programming Languages"}
{"prompt":"Object-oriented programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP. It performs operations on operands. The features listed below are common among languages considered to be strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.","completion":"Programming Languages"}
{"prompt":"(The way the bold text has to be written depends on the implementation, e.g. 'INTEGER'\u2014quotation marks included\u2014for integer. This is known as stropping.)\n\nprocedure Absmax(a) Size:(n, m) Result:(y) Subscripts:(i, k);\n    value n, m; array a; integer n, m, i, k; real y;\ncomment The absolute greatest element of the matrix a, of size n by m,\n    is copied to y, and the subscripts of this element to i and k;\nbegin\n    integer p, q;\n    y := 0; i := k := 1;\n    for p := 1 step 1 until n do\n        for q := 1 step 1 until m do\n            if abs(a[p, q]) > y then\n                begin y := abs(a[p, q]);\n                    i := p; k := q\n                end\nend Absmax\n\nHere is an example of how to produce a table using Elliott 803 ALGOL.\n FLOATING POINT ALGOL TEST'\n BEGIN REAL A,B,C,D'\n READ D'\n FOR A:= 0.0 STEP D UNTIL 6.3 DO\n BEGIN\n   PRINT PUNCH(3),\u00a3\u00a3L??'\n   B := SIN(A)'\n   C := COS(A)'\n   PRINT PUNCH(3),SAMELINE,ALIGNED(1,6),A,B,C'\n END\n END'\n\nPUNCH(3) sends output to the teleprinter rather than the tape punch.\nSAMELINE suppresses the carriage return + line feed normally printed between arguments.\nALIGNED(1,6) controls the format of the output with one digit before and six after the decimal point.","completion":"Programming Languages"}
{"prompt":"The following code samples are ALGOL 68 versions of the above ALGOL 60 code samples.\nALGOL 68 implementations used ALGOL 60's approaches to stropping. In ALGOL 68's case tokens with the bold typeface are reserved words, types (modes) or operators.\n\nproc abs max = ([,]real a, ref real y, ref int i, k)real:\ncomment The absolute greatest element of the matrix a, of size \u2308a by 2\u2308a\nis transferred to y, and the subscripts of this element to i and k; comment\nbegin\n   real y := 0; i := \u230aa; k := 2\u230aa;\n   for p from \u230aa to \u2308a do\n     for q from 2\u230aa to 2\u2308a do\n       if abs a[p, q] > y then\n           y := abs a[p, q];\n           i := p; k := q\n       fi\n     od\n   od;\n   y\nend # abs max #\n\nNote: lower (\u230a) and upper (\u2308) bounds of an array, and array slicing, are directly available to the programmer.\n\nfloating point algol68 test:\n(\n  real a,b,c,d;\n   \n  # printf \u2013 sends output to the file stand out. #\n  # printf($p$); \u2013 selects a new page #\n  printf(($pg$,\"Enter d:\"));  \n  read(d);\n   \n  for step from 0 while a:=step*d; a <= 2*pi do\n    printf($l$);  # $l$ - selects a new line. #\n    b := sin(a);\n    c := cos(a);\n    printf(($z-d.6d$,a,b,c))  # formats output with 1 digit before and 6 after the decimal point. #\n  od\n)","completion":"Programming Languages"}
{"prompt":"In the mid-eighties, a number of papers introduced the notion of hygienic macro expansion (syntax-rules), a pattern-based system where the syntactic environments of the macro definition and the macro use are distinct, allowing macro definers and users not to worry about inadvertent variable capture (cf. referential transparency). Hygienic macros have been standardized for Scheme in the R5RS, R6RS, and R7RS standards. A number of competing implementations of hygienic macros exist such as syntax-rules, syntax-case, explicit renaming, and syntactic closures. Both syntax-rules and syntax-case have been standardized in the Scheme standards.\nRecently, Racket has combined the notions of hygienic macros with a \"tower of evaluators\", so that the syntactic expansion time of one macro system is the ordinary runtime of another block of code, and showed how to apply interleaved expansion and parsing in a non-parenthesized language.A number of languages other than Scheme either implement hygienic macros or implement partially hygienic systems. Examples include Scala, Rust, Elixir, Julia, Dylan, Nim, and Nemerle.","completion":"Programming Languages"}
{"prompt":"^a  The standard constants real shorts and real lengths can be used to determine how many shorts and longs can be usefully prefixed to short real and long real. The actual sizes of short real, real, and long real are available as the constants short max real, max real and long max real etc. With the constants short small real, small real and long small real available for each type's machine epsilon.\n^b  declarations of single precision often are not honored\n^c  The value of n is provided by the SELECTED_REAL_KIND intrinsic function.\n^d  ALGOL 68G's runtime option --precision \"number\" can set precision for long long reals to the required \"number\" significant digits. The standard constants long long real width and long long max real can be used to determine actual precision.\n^e  These IEEE floating-point types will be introduced in the next COBOL standard.\n^f  Same size as double on many implementations.\n^g  Swift supports 80-bit extended precision floating point type, equivalent to long double in C languages.","completion":"Programming Languages"}
{"prompt":"See reflection for calling and declaring functions by strings.\n\n^a  Pascal requires \"forward;\" for forward declarations.\n^b  Eiffel allows the specification of an application's root class and feature.\n^c  In Fortran, function\/subroutine parameters are called arguments (since PARAMETER is a language keyword); the CALL keyword is required for subroutines.\n^d  Instead of using \"foo\", a string variable may be used instead containing the same value.","completion":"Programming Languages"}
{"prompt":"ALGOL was developed jointly by a committee of European and American computer scientists in a meeting in 1958 at the Swiss Federal Institute of Technology in Zurich (cf. ALGOL 58). It specified three different syntaxes: a reference syntax, a publication syntax, and an implementation syntax. The different syntaxes permitted it to use different keyword names and conventions for decimal points (commas vs periods) for different languages.\nALGOL was used mostly by research computer scientists in the United States and in Europe. Its use in commercial applications was hindered by the absence of standard input\/output facilities in its description and the lack of interest in the language by large computer vendors other than Burroughs Corporation. ALGOL 60 did however become the standard for the publication of algorithms and had a profound effect on future language development.\n\nJohn Backus developed the Backus normal form method of describing programming languages specifically for ALGOL 58. It was revised and expanded by Peter Naur for ALGOL 60, and at Donald Knuth's suggestion renamed Backus\u2013Naur form.Peter Naur: \"As editor of the ALGOL Bulletin I was drawn into the international discussions of the language and was selected to be member of the European language design group in November 1959. In this capacity I was the editor of the ALGOL 60 report, produced as the result of the ALGOL 60 meeting in Paris in January 1960.\"The following people attended the meeting in Paris (from 1 to 16 January):\n\nFriedrich Ludwig Bauer, Peter Naur, Heinz Rutishauser, Klaus Samelson, Bernard Vauquois, Adriaan van Wijngaarden, and Michael Woodger (from Europe)\nJohn Warner Backus, Julien Green, Charles Katz, John McCarthy, Alan Jay Perlis, and Joseph Henry Wegstein (from the USA).Alan Perlis gave a vivid description of the meeting: \"The meetings were exhausting, interminable, and exhilarating. One became aggravated when one's good ideas were discarded along with the bad ones of others. Nevertheless, diligence persisted during the entire period. The chemistry of the 13 was excellent.\"\nALGOL 60 inspired many languages that followed it. Tony Hoare remarked: \"Here is a language so far ahead of its time that it was not only an improvement on its predecessors but also on nearly all its successors.\" The Scheme programming language, a variant of Lisp that adopted the block structure and lexical scope of ALGOL, also adopted the wording \"Revised Report on the Algorithmic Language Scheme\" for its standards documents in homage to ALGOL.","completion":"Programming Languages"}
{"prompt":"C++11 supports anonymous functions (technically function objects), called lambda expressions, which have the form:\n\nwhere \"specs\" is of the form \"specifiers exception attr trailing-return-type in that order; each of these components is optional\". If it is absent, the return type is deduced from return statements as if for a function with declared return type auto.\nThis is an example lambda expression:\n\nC++11 also supports closures, here called captures. Captures are defined between square brackets [and ] in the declaration of lambda expression. The mechanism allows these variables to be captured by value or by reference. The following table demonstrates this:\n\nVariables captured by value are constant by default. Adding mutable after the parameter list makes them non-constant.\nC++14 and newer versions support init-capture, for example:\n\nThe following two examples demonstrate use of a lambda expression:\n\nThis computes the total of all elements in the list. The variable total is stored as a part of the lambda function's closure. Since it is a reference to the stack variable total, it can change its value.\n\nThis will cause total to be stored as a reference, but value will be stored as a copy.\nThe capture of this is special. It can only be captured by value, not by reference. However in C++17, the current object can be captured by value (denoted by *this), or can be captured by reference (denoted by this). this can only be captured if the closest enclosing function is a non-static member function. The lambda will have the same access as the member that created it, in terms of protected\/private members.\nIf this is captured, either explicitly or implicitly, then the scope of the enclosed class members is also tested. Accessing members of this does not need explicit use of this-> syntax.\nThe specific internal implementation can vary, but the expectation is that a lambda function that captures everything by reference will store the actual stack pointer of the function it is created in, rather than individual references to stack variables. However, because most lambda functions are small and local in scope, they are likely candidates for inlining, and thus need no added storage for references.\nIf a closure object containing references to local variables is invoked after the innermost block scope of its creation, the behaviour is undefined.\nLambda functions are function objects of an implementation-dependent type; this type's name is only available to the compiler. If the user wishes to take a lambda function as a parameter, the parameter type must be a template type, or they must create a std::function or a similar object to capture the lambda value. The use of the auto keyword can help store the lambda function,\n\nHere is an example of storing anonymous functions in variables, vectors, and arrays; and passing them as named parameters:\n\nA lambda expression with an empty capture specification ([]) can be implicitly converted into a function pointer with the same type as the lambda was declared with. So this is legal:\n\nSince C++17, a lambda can be declared constexpr, and since C++20, consteval with the usual semantics. These specifiers go after the parameter list, like mutable. Starting from C++23, the lambda can also be static if it has no captures. The static and mutable specifiers are not allowed to be combined.\nAlso since C++23 a lambda expression can be recursive through explicit this as first parameter:\n\nIn addition to that, C++23 modified the syntax so that the parentheses can be omitted in the case of a lambda that takes no arguments even if the lambda has a specifier. It also made it so that an attribute specifier sequence that appears before the parameter list, lambda specifiers, or noexcept specifier (there must be one of them) applies to the function call operator or operator template of the closure type. Otherwise, it applies to the type of the function call operator or operator template. Previously, such a sequence always applied to the type of the function call operator or operator template of the closure type making e.g the [[noreturn]] attribute impossible to use with lambdas.\nThe Boost library provides its own syntax for lambda functions as well, using the following syntax:\n\nSince C++14, the function parameters of a lambda can be declared with auto. The resulting lambda is called a generic lambda and is essentially an anonymous function template since the rules for type deduction of the auto parameters are the rules of template argument deduction. As of C++20, template parameters can also be declared explicitly with the following syntax:","completion":"Programming Languages"}
{"prompt":"It is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.\nJavaScript, Lua, Python and Go had first class functions from their inception. Python had support for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2, though Python 3 relegated  \"reduce\" to the functools standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, C++11, and Kotlin.In PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.\nIn Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes.In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array, and Object.freeze() in JavaScript.","completion":"Programming Languages"}
{"prompt":"In the mid-1950s, when assembly language programming was commonly used to write programs for digital computers, the use of macro instructions was initiated for two main purposes: to reduce the amount of program coding that had to be written by generating several assembly language statements from one macro instruction and to enforce program writing standards, e.g. specifying input\/output commands in standard ways. Macro instructions were effectively a middle step between assembly language programming and the high-level programming languages that followed, such as FORTRAN and COBOL. Two of the earliest programming installations to develop \"macro languages\" for the IBM 705 computer were at Dow Chemical Corp. in Delaware and the Air Material Command, Ballistics Missile Logistics Office in California. A macro instruction written in the format of the target assembly language would be processed by a macro compiler, which was a pre-processor to the assembler, to generate one or more assembly language instructions to be processed next by the assembler program that would translate the assembly language instructions into machine language instructions.By the late 1950s the macro language was followed by the Macro Assemblers. This was a combination of both where one program served both functions, that of a macro pre-processor and an assembler in the same package.In 1959, Douglas E. Eastwood and Douglas McIlroy of Bell Labs introduced conditional and recursive macros into the popular SAP assembler, creating what is known as Macro SAP. McIlroy's 1960 paper was seminal in the area of extending any (including high-level) programming languages through macro processors.Macro Assemblers allowed assembly language programmers to implement their own macro-language and allowed limited portability of code between two machines running the same CPU but different operating systems, for example, early versions of MS-DOS and CP\/M-86. The macro library would need to be written for each target machine but not the overall assembly language program. Note that more powerful macro assemblers allowed use of conditional assembly constructs in macro instructions that could generate different code on different machines or different operating systems, reducing the need for multiple libraries.In the 1980s and early 1990s, desktop PCs were only running at a few MHz and assembly language routines were commonly used to speed up programs written in C, Fortran, Pascal and others. These languages, at the time, used different calling conventions. Macros could be used to interface routines written in assembly language to the front end of applications written in almost any language. Again, the basic assembly language code remained the same, only the macro libraries needed to be written for each target language.In modern operating systems such as Unix and its derivatives, operating system access is provided through subroutines, usually provided by dynamic libraries. High-level languages such as C offer comprehensive access to operating system functions, obviating the need for assembler language programs for such functionality.","completion":"Programming Languages"}
{"prompt":"APL has been standardized by the American National Standards Institute (ANSI) working group X3J10 and International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC), ISO\/IEC Joint Technical Committee 1 Subcommittee 22 Working Group 3. The Core APL language is specified in ISO 8485:1989, and the Extended APL language is specified in ISO\/IEC 13751:2001.","completion":"Programming Languages"}
{"prompt":"Following her retirement from the Navy, she was hired as a senior consultant to Digital Equipment Corporation (DEC). Hopper was initially offered a position by Rita Yavinsky, but she insisted on going through the typical formal interview process. She then proposed in jest that she would be willing to accept a position which made her available on alternating Thursdays, exhibited at their museum of computing as a pioneer, in exchange for a generous salary and unlimited expense account. Instead, she was hired as a full-time Principal Corporate Consulting Engineer, a tech-track SVP-equivalent. In this position, Hopper represented the company at industry forums, serving on various industry committees, along with other obligations. She retained that position until her death at age 85 in 1992.\nAt DEC Hopper served primarily as a goodwill ambassador. She lectured widely about the early days of computing, her career, and on efforts that computer vendors could take to make life easier for their users. She visited most of Digital's engineering facilities, where she generally received a standing ovation at the conclusion of her remarks. Although no longer a serving officer, she always wore her Navy full dress uniform to these lectures contrary to U.S. Department of Defense policy. In 2016 Hopper received the Presidential Medal of Freedom, the nation\u2019s highest civilian honor, in recognition of her remarkable contributions to the field of computer science.\n\"The most important thing I've accomplished, other than building the compiler,\" she said, \"is training young people. They come to me, you know, and say, 'Do you think we can do this?' I say, 'Try it.' And I back 'em up. They need that. I keep track of them as they get older and I stir 'em up at intervals so they don't forget to take chances.\"","completion":"Programming Languages"}
{"prompt":"The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language that places no constraints on the order in which operations are performed.Logic programming, with its current syntax of facts and rules, can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA4, Popler, Conniver, QLISP, and the concurrent language Ether.Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\nProlog also contributed to the development of the programming languages ALF, Fril, G\u00f6del, Mercury, Oz, Ciao, Visual Prolog, XSB, and \u03bbProlog. \nAlthough Prolog was developed to combine declarative and procedural representations of knowledge, the purely declarative interpretation of logic programs returned to prominence around 1977 when Herv\u00e9 Gallaire and Jack Minker organized a workshop on logic and databases, which later developed into the field of deductive databases and Datalog.\nThis focus on the logical, declarative reading of logic programs was given further impetus by the development of constraint logic programming in the 1980s and Answer Set Programming in the 1990s. It is also receiving renewed emphasis in recent applications of PrologThe Association for Logic Programming was founded in 1986 to promote Logic Programming.","completion":"Programming Languages"}
{"prompt":"Keyboard and mouse macros that are created using an application's built-in macro features are sometimes called application macros. They are created by carrying out the sequence once and letting the application record the actions. An underlying macro programming language, most commonly a scripting language, with direct access to the features of the application may also exist.\nThe programmers' text editor, Emacs, (short for \"editing macros\") follows this idea to a conclusion. In effect, most of the editor is made of macros. Emacs was originally devised as a set of macros in the editing language TECO; it was later ported to dialects of Lisp.\nAnother programmers' text editor, Vim (a descendant of vi), also has an implementation of keyboard macros. It can record into a register (macro) what a person types on the keyboard and it can be replayed or edited just like VBA macros for Microsoft Office. Vim also has a scripting language called Vimscript to create macros.\nVisual Basic for Applications (VBA) is a programming language included in Microsoft Office from Office 97 through Office 2019 (although it was available in some components of Office prior to Office 97). However, its function has evolved from and replaced the macro languages that were originally included in some of these applications.\nXEDIT, running on the Conversational Monitor System (CMS) component of VM, supports macros written in EXEC, EXEC2 and REXX, and some CMS commands were actually wrappers around XEDIT macros. The Hessling Editor (THE), a partial clone of XEDIT, supports Rexx macros using Regina and Open Object REXX (oorexx). Many common applications, and some on PCs, use Rexx as a scripting language.","completion":"Programming Languages"}
{"prompt":"Short R code calculating Mandelbrot set through the first 20 iterations of equation z = z2 + c plotted for different complex constants c.  This example demonstrates:\n\nuse of community-developed external libraries (called packages), like the caTools package\nhandling of complex numbers\nmultidimensional arrays of numbers used as basic data type, see variables C, Z, and X.","completion":"Programming Languages"}
{"prompt":"In recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript\/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.","completion":"Programming Languages"}
{"prompt":"The term \"near real-time\" or \"nearly real-time\" (NRT), in telecommunications and computing, refers to the time delay introduced, by automated data processing or network transmission, between the occurrence of an event and the use of the processed data, such as for display or feedback and control purposes. For example, a near-real-time display depicts an event or situation as it existed at the current time minus the processing time, as nearly the time of the live event.The distinction between the terms \"near real time\" and \"real time\" is somewhat nebulous and must be defined for the situation at hand. The term implies that there are no significant delays. In many cases, processing described as \"real-time\" would be more accurately described as \"near real-time\".\nNear real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export\/import to common flat files that the other database can import\/export on a scheduled basis so that they can sync\/share common data in \"near real-time\" with each other.\nThe distinction between \"near real-time\" and \"real-time\" varies, and the delay is dependent on the type and speed of the transmission. The delay in near real-time is typically in a range of 1\u201310 seconds.","completion":"Programming Languages"}
{"prompt":"When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.\nAbstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if one wishes to know what the result of the evaluation of a mathematical expression involving only integers +, -, \u00d7, is worth modulo n, then one needs only perform all operations modulo n (a familiar form of this abstraction is casting out nines).\nAbstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from them\u2014even though the abstraction may simply yield a result of undecidability. For instance, students in a class may be abstracted by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer \"I don't know\".\nThe level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.\nAbstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer \"I don't know\" to some questions).\nAbstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.","completion":"Programming Languages"}
{"prompt":"The first generation of PLT Scheme revisions introduced features for programming in the large with both modules and classes. Version 42 introduced units \u2013 a first-class module system \u2013 to complement classes for large scale development. The class system gained features (e.g. Java-style interfaces) and also lost several features (e.g. multiple inheritance) throughout these versions.  The language evolved throughout a number of successive versions, and gaining milestone popularity in Version 53, leading to extensive work and the following Version 100, which would be equivalent to a \"1.0\" release in current popular version systems.\nThe next major revision was named Version 200, which introduced a new default module system that cooperates with macros. In particular, the module system ensures that run-time and compile-time computation are separated to support a \"tower of languages\". Unlike units, these modules are not first-class objects.\nVersion 300 introduced Unicode support, foreign library support, and refinements to the class system.  Later on, the 300 series improved the performance of the language runtime with an addition of a JIT compiler and a switch to a default generational garbage collection.\nBy the next major release, the project had switched to a more conventional sequence-based version numbering.  Version 4.0 introduced the #lang shorthand to specify the language that a module is written in.  Further, the revision introduced immutable pairs and lists, support for fine-grained parallelism, and a statically-typed dialect.On 7 June 2010, PLT Scheme was renamed Racket. The renaming coincided with the release of Version 5.0. Subsequently, the graphical user interface (GUI) backend was rewritten in Racket from C++ in Version 5.1 using native UI toolkits on all platforms. Version 5.2 included a background syntax checking tool, a new plotting library, a database library, and a new extended REPL. Version 5.3 included a new submodule feature for optionally loaded modules, new optimization tools, a JSON library, and other features. Version 5.3.1 introduced major improvements to DrRacket: the background syntax checker was turned on by default and a new documentation preview tool was added.In version 6.0, Racket released its second-generation package management system. As part of this development, the principal DrRacket and Racket repository was reorganized and split into a large set of small packages, making it possible to install a minimal racket and to install only those packages needed.Version 7 of Racket was released with a new macro expander written in Racket as part the preparations for supporting moving to the Chez Scheme runtime system and supporting multiple runtime systems. On 19 November 2019, Racket 7.5 was released. The license of Racket 7.5 was less restrictive. They use now either the Apache 2.0 license or the MIT license.On 2021 February 13, Racket 8.0 was released. Racket 8.0 marks the first release where Racket with the Chez Scheme runtime system, known as Racket CS, is the default implementation. Racket CS is faster, easier to maintain and develop, backward-compatible with existing Racket programs, and has better parallel garbage collection.","completion":"Programming Languages"}
{"prompt":"To date there have been at least 70 augmentations, extensions, derivations and sublanguages of Algol 60.\nThe Burroughs dialects included special Bootstrapping dialects such as ESPOL and NEWP.  The latter is still used for Unisys MCP system software.","completion":"Programming Languages"}
{"prompt":"1974 \u2013 Comparative Notes on Algol 68 and PL\/I \u2013 S. H. Valentine \u2013 November 1974\n1976 \u2013 Evaluation of ALGOL 68, JOVIAL J3B, Pascal, Simula 67, and TACPOL Versus TINMAN \u2013 Requirements for a Common High Order Programming Language.\n1977 \u2013 A comparison of PASCAL and ALGOL 68 \u2013 Andrew S. Tanenbaum \u2013 June 1977.\n1993 \u2013 Five Little Languages and How They Grew \u2013 BLISS, Pascal, ALGOL 68, BCPL & C \u2013 Dennis M. Ritchie \u2013 April 1993.\n2009 \u2013 On Go \u2013 oh, go on \u2013 How well will Google's Go stand up against Brand X programming language? \u2013 David Given \u2013 November 2009","completion":"Programming Languages"}
{"prompt":"In the next example, the class person gets a new superclass. The print method gets redefined such that it assembles several methods into the effective method. The effective method gets assembled based on the class of the argument and the at runtime available and applicable methods.","completion":"Programming Languages"}
{"prompt":"A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.\nDepending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are \"escaped\", i.e. preceded by an escape sequence, in this case, the backslash \\. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid \"backslash-osis\" or leaning toothpick syndrome it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters ( ) and { } be primarily literal, and \"escape\" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are  {}[]()^$.|*+? and \\. The usual characters that become metacharacters when escaped are dswDSW and N.","completion":"Programming Languages"}
{"prompt":"IBM began development of FORTRAN IV starting in 1961, as a result of customer demands. FORTRAN IV removed the machine-dependent features of FORTRAN II (such as READ INPUT TAPE), while adding new features such as a LOGICAL data type, logical Boolean expressions and the logical IF statement as an alternative to the arithmetic IF statement.  FORTRAN IV was eventually released in 1962, first for the IBM 7030 (\"Stretch\") computer, followed by versions for the IBM 7090, IBM 7094, and later for the IBM 1401 in 1966.By 1965, FORTRAN IV was supposed to be compliant with the standard being developed by the American Standards Association X3.4.3 FORTRAN Working Group.Between 1966 and 1968, IBM offered several FORTRAN IV compilers for its System\/360, each named by letters that indicated the minimum amount of memory the compiler needed to run.\n\nThe letters (F, G, H) matched the codes used with System\/360 model numbers to indicate memory size, each letter increment being a factor of two larger::\u200ap. 5\u200a\n1966 : FORTRAN IV F for DOS\/360 (64K bytes)\n1966 : FORTRAN IV G for OS\/360 (128K bytes)\n1968 : FORTRAN IV H for OS\/360 (256K bytes)Digital Equipment Corporation maintained DECSYSTEM-10 Fortran IV (F40) for PDP-10 from 1967 to 1975At about this time FORTRAN IV had started to become an important educational tool and implementations such as the University of Waterloo's WATFOR and WATFIV were created to simplify the complex compile and link processes of earlier compilers.","completion":"Programming Languages"}
{"prompt":"Anonymous classes of lambda-compatible interfaces are similar, but not exactly equivalent, to lambda expressions.\nTo illustrate, in the following example, anonymousClass and lambdaExpression are both instances of IntegerMath that add their two parameters:\n\nThe main difference here is that the lambda expression does not necessarily need to allocate a new instance for the IntegerMath, and can return the same instance every time this code is run.\nAdditionally, in the OpenJDK implementation at least, lambdas are compiled to invokedynamic instructions, with the lambda body inserted as a static method into the surrounding class, rather than generating a new class file entirely.","completion":"Programming Languages"}
{"prompt":"SETL\nABC\nPython (also under C)\nSwift (also under Ruby, Objective-C, and Haskell)\nBoo\nCobra (syntax and features)\nNim (also under Oberon)","completion":"Programming Languages"}
{"prompt":"Compilers are programs that read programs, also usually as some form of text, and converts the code into lower level machine code or operations. Compiled formats generated by compilers store the lower level actions as a file.  Compiled languages converted to machine code, tend to be a lot faster, as lower level operations are easier to run, and outcomes can be predicted and compiled ahead of time.","completion":"Programming Languages"}
{"prompt":"Lisp lists, being simple linked lists, can share structure with one another. That is to say, two lists can have the same tail, or final sequence of conses. For instance, after the execution of the following Common Lisp code:\n\nthe lists foo and bar are (a b c) and (x b c) respectively. However, the tail (b c) is the same structure in both lists. It is not a copy; the cons cells pointing to b and c are in the same memory locations for both lists.\nSharing structure rather than copying can give a dramatic performance improvement. However, this technique can interact in undesired ways with functions that alter lists passed to them as arguments. Altering one list, such as by replacing the c with a goose, will affect the other:\n\nThis changes foo to (a b goose), but thereby also changes bar to (x b goose) \u2013 a possibly unexpected result. This can be a source of bugs, and functions which alter their arguments are documented as destructive for this very reason.\nAficionados of functional programming avoid destructive functions. In the Scheme dialect, which favors the functional style, the names of destructive functions are marked with a cautionary exclamation point, or \"bang\"\u2014such as set-car! (read set car bang), which replaces the car of a cons. In the Common Lisp dialect, destructive functions are commonplace; the equivalent of set-car! is named rplaca for \"replace car\". This function is rarely seen, however, as Common Lisp includes a special facility, setf, to make it easier to define and use destructive functions. A frequent style in Common Lisp is to write code functionally (without destructive calls) when prototyping, then to add destructive calls as an optimization where it is safe to do so.","completion":"Programming Languages"}
{"prompt":"The term computer language is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. Similarly, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.\nOne way of classifying computer languages is by the computations they are capable of expressing, as described by the theory of computation. The majority of practical programming languages are Turing complete, and all Turing complete languages can implement the same set of algorithms. ANSI\/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet are often called programming languages. However, some authors restrict the term \"programming language\" to Turing complete languages.Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.","completion":"Programming Languages"}
{"prompt":"Other than Flavors and CLOS (a part of Common Lisp), some languages that use mixins are:\n\nAda (by extending an existing tagged record with arbitrary operations in a generic)\nC# (since C# 8.0, by means of default methods of interfaces)\nCobra\nColdFusion (Class based using includes and Object based by assigning methods from one object to another at runtime)\nCurl (with Curl RTE)\nD (called \"template mixins\"; D also includes a \"mixin\" statement that compiles strings as code.)\nDart\nFactor\nGroovy\nJava (since Java 8, by means of default methods of interfaces)\nJavaScript Delegation - Functions as Roles (Traits and Mixins)\nKotlin\nLess\nMagik\nMATLAB\nOCaml\nPerl (through roles in the Moose extension of the Perl 5 object system)\nPHP's \"traits\"\nPython\nRacket (mixins documentation)\nRaku\nRuby\nRust\nSass (A stylesheet language)\nScala\nSmalltalk\nSwift\nSystemVerilog\nXOTcl\/TclOO (object systems builtin to Tcl)\nTypeScript (mixins documentation)\nValaSome languages do not support mixins on the language level, but can easily mimic them by copying methods from one object to another at runtime, thereby \"borrowing\" the mixin's methods. This is also possible with statically typed languages, but it requires constructing a new object with the extended set of methods.\nOther languages that do not support mixins can support them in a round-about way via other language constructs. For example, Visual Basic .NET and C# support the addition of extension methods on interfaces, meaning any class implementing an interface with extension methods defined will have the extension methods available as pseudo-members.","completion":"Programming Languages"}
{"prompt":"Depending on the intended audience of the code and other considerations, the level of detail and description may vary considerably.\nFor example, the following Java comment would be suitable in an introductory text designed to teach beginning programming:\n\nThis level of detail, however, would not be appropriate in the context of production code, or other situations involving experienced developers. Such rudimentary descriptions are inconsistent with the guideline: \"Good comments ... clarify intent.\" Further, for professional coding environments, the level of detail is ordinarily well defined to meet a specific performance requirement defined by business operations.","completion":"Programming Languages"}
{"prompt":"Chomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.Such rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations does not meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\nChomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved. Gerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.","completion":"Programming Languages"}
{"prompt":"Due to the exploding complexity of digital electronic circuits since the 1970s (see Moore's law), circuit designers needed digital logic descriptions to be performed at a high level without being tied to a specific electronic technology, such as ECL, TTL or CMOS. HDLs were created to implement register-transfer level abstraction, a model of the data flow and timing of a circuit.There are two major hardware description languages: VHDL and Verilog. There are different types of description in them: \"dataflow, behavioral and structural\".\nExample of dataflow of VHDL:","completion":"Programming Languages"}
{"prompt":"Dataflow programs are represented in different ways. A traditional program is usually represented as a series of text instructions, which is reasonable for describing a serial system which pipes data between small, single-purpose tools that receive, process, and return. Dataflow programs start with an input, perhaps the command line parameters, and illustrate how that data is used and modified. The flow of data is explicit, often visually illustrated as a line or pipe.\nIn terms of encoding, a dataflow program might be implemented as a hash table, with uniquely identified inputs as the keys, used to look up pointers to the instructions. When any operation completes, the program scans down the list of operations until it finds the first operation where all inputs are currently valid, and runs it. When that operation finishes, it will typically output data, thereby making another operation become valid.\nFor parallel operation, only the list needs to be shared; it is the state of the entire program. Thus the task of maintaining state is removed from the programmer and given to the language's runtime. On machines with a single processor core where an implementation designed for parallel operation would simply introduce overhead, this overhead can be removed completely by using a different runtime.","completion":"Programming Languages"}
{"prompt":"Programming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.\nMany programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one \"universal\" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:\n\nPrograms range from tiny scripts written by individual hobbyists to huge systems written by hundreds of programmers.\nProgrammers range in expertise from novices who need simplicity above all else to experts who may be comfortable with considerable complexity.\nPrograms must balance speed, size, and simplicity on systems ranging from microcontrollers to supercomputers.\nPrograms may be written once and not change for generations, or they may undergo continual modification.\nProgrammers may simply differ in their tastes: they may be accustomed to discussing problems and expressing them in a particular language.One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.\nNatural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural-language programming as \"foolish\". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.\nA language's designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.","completion":"Programming Languages"}
{"prompt":"Almost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as short and long and their corresponding unsigned variants in C\/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal\/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\nFloating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form a \u00d7 2b (where a and b are integers), but displayed in familiar decimal form.\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\nFor independence from architecture details, a Bignum or arbitrary precision numeric type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.","completion":"Programming Languages"}
{"prompt":"An algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.\nIf there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type.\nOne common ADT is the option type, defined in Haskell as data Maybe a = Nothing | Just a.","completion":"Programming Languages"}
{"prompt":"Lisp and Scheme support anonymous functions using the \"lambda\" construct, which is a reference to lambda calculus. Clojure supports anonymous functions with the \"fn\" special form and #() reader syntax.","completion":"Programming Languages"}
{"prompt":"The 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating the so-called \"fifth-generation\" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.\nOne important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of \"Write once, run anywhere\" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel; rather, they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).\nProgramming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft's LINQ.\nFourth-generation programming languages (4GL) are computer programming languages that aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth-generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.","completion":"Programming Languages"}
{"prompt":"Literate programming is very often misunderstood to refer only to formatted documentation produced from a common file with both source code and comments \u2013 which is properly called documentation generation \u2013 or to voluminous commentaries included with code. This is the converse of  literate programming: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; while in literate programming, code is embedded in documentation, with the code following the structure of the documentation.\nThis misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are \"literate programming tools\". However, because these tools do not implement the \"web of abstract concepts\" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.","completion":"Programming Languages"}
{"prompt":"In the 1970s, Hopper advocated for the Defense Department to replace large, centralized systems with networks of small, distributed computers. Any user on any computer node could access common databases located on the network.:\u200a119\u200a She developed the implementation of standards for testing computer systems and components, most significantly for early programming languages such as FORTRAN and COBOL. The Navy tests for conformance to these standards led to significant convergence among the programming language dialects of the major computer vendors. In the 1980s, these tests (and their official administration) were assumed by the National Bureau of Standards (NBS), known today as the National Institute of Standards and Technology (NIST).","completion":"Programming Languages"}
{"prompt":"Common Lisp provides mixins in CLOS (Common Lisp Object System) similar to Flavors.\nobject-width is a generic function with one argument that uses the + method combination. This combination determines that all applicable methods for a generic function will be called and the results will be added.\n\nbutton is a class with one slot for the button text.\n\nThere is a method for objects of class button that computes the width based on the length of the button text. + is the method qualifier for the method combination of the same name.\n\nA border-mixin class. The naming is just a convention. There are no superclasses, and no slots.\n\nThere is a method computing the width of the border. Here it is just 4.\n\nbordered-button is a class inheriting from both border-mixin and button.\n\nWe can now compute the width of a button. Calling object-width computes 80. The result is the result of the single applicable method: the method object-width for the class button.\n\nWe can also compute the width of a bordered-button. Calling object-width computes 84. The result is the sum of the results of the two applicable methods: the method object-width for the class button and the method object-width for the class border-mixin.","completion":"Programming Languages"}
{"prompt":"Although Prolog is widely used in research and education, Prolog and other logic programming languages have not had a significant impact on the computer industry in general. Most applications are small by industrial standards, with few exceeding 100,000 lines of code. Programming in the large is considered to be complex because not all Prolog compilers support modules, and there are compatibility problems between the module systems of the major Prolog compilers. Portability of Prolog code across implementations has also been a problem, but developments since 2007 have meant: \"the portability within the family of Edinburgh\/Quintus derived Prolog implementations is good enough to allow for maintaining portable real-world applications.\"Software developed in Prolog has been criticised for having a high performance penalty compared to conventional programming languages. In particular, Prolog's non-deterministic evaluation strategy can be problematic when programming deterministic computations, or when even using \"don't care non-determinism\" (where a single choice is made instead of backtracking over all possibilities). Cuts and other language constructs may have to be used to achieve desirable performance, destroying one of Prolog's main attractions, the ability to run programs \"backwards and forwards\".Prolog is not purely declarative: because of constructs like the cut operator, a procedural reading of a Prolog program is needed to understand it. The order of clauses in a Prolog program is significant, as the execution strategy of the language depends on it. Other logic programming languages, such as Datalog, are truly declarative but restrict the language. As a result, many practical Prolog programs are written to conform to Prolog's depth-first search order, rather than as purely declarative logic programs.","completion":"Programming Languages"}
{"prompt":"^a  In Rust, std::env::args and std::env::args_os return iterators, std::env::Args and std::env::ArgsOs respectively. Args converts each argument to a String and it panics if it reaches an argument that cannot be converted to UTF-8. ArgsOs returns a non-lossy representation of the raw strings from the operating system (std::ffi::OsString), which can be invalid UTF-8.\n^b  In Visual Basic, command-line arguments are not separated. Separating them requires a split function Split(string).\n^c  The COBOL standard includes no means to access command-line arguments, but common compiler extensions to access them include defining parameters for the main program or using ACCEPT statements.","completion":"Programming Languages"}
{"prompt":"Line comments generally use an arbitrary delimiter or sequence of tokens to indicate the beginning of a comment, and a newline character to indicate the end of a comment.\nIn this example, all the text from the ASCII characters \/\/ to the end of the line is ignored.\n\nOften such a comment has to begin at far left and extend to the whole line. However, in many languages, it is also possible to put a comment inline with a command line, to add a comment to it \u2013 as in this Perl example:\n\nIf a language allows both line comments and block comments, programming teams may decide upon a convention of using them differently: e.g. line comments only for minor comments, and  block comments to describe higher-level abstractions.","completion":"Programming Languages"}
{"prompt":"This Fortran IV code fragment demonstrates how comments are used in that language, which is very column-oriented. A letter \"C\" in column 1 causes the entire line to be treated as a comment.\n\nNote that the columns of a line are otherwise treated as four fields: 1 to 5 is the label field, 6 causes the line to be taken as a continuation of the previous statement; and declarations and statements go in 7 to 72.","completion":"Programming Languages"}
{"prompt":"There are many variants and extensions of BNF, generally either for the sake of simplicity and succinctness, or to adapt it to a specific application. One common feature of many variants is the use of regular expression repetition operators such as * and +. The extended Backus\u2013Naur form (EBNF) is a common one.\nAnother common extension is the use of square brackets around optional items. Although not present in the original ALGOL 60 report (instead introduced a few years later in IBM's PL\/I definition), the notation is now universally recognised.","completion":"Programming Languages"}
{"prompt":"The satellites of the Russian radionavigation-satellite service framework GLONASS, similar to the United States Global Positioning System (GPS), are programmed in Modula-2.","completion":"Programming Languages"}
{"prompt":"The various Microsoft, Lotus, and Corel office suites and related products are programmable with Visual Basic in one form or another, including LotusScript, which is very similar to VBA 6. The Host Explorer terminal emulator uses WWB as a macro language; or more recently the programme and the suite in which it is contained is programmable in an in-house Basic variant known as Hummingbird Basic. The VBScript variant is used for programming web content, Outlook 97, Internet Explorer, and the Windows Script Host. WSH also has a Visual Basic for Applications (VBA) engine installed as the third of the default engines along with VBScript, JScript, and the numerous proprietary or open source engines which can be installed like PerlScript, a couple of Rexx-based engines, Python, Ruby, Tcl, Delphi, XLNT, PHP, and others; meaning that the two versions of Basic can be used along with the other mentioned languages, as well as LotusScript, in a WSF file, through the component object model, and other WSH and VBA constructions. VBScript is one of the languages that can be accessed by the 4Dos, 4NT, and Take Command enhanced shells. SaxBasic and WWB are also very similar to the Visual Basic line of Basic implementations. The pre-Office 97 macro language for Microsoft Word is known as WordBASIC. Excel 4 and 5 use Visual Basic itself as a macro language. Chipmunk Basic, an old-school interpreter similar to BASICs of the 1970s, is available for Linux, Microsoft Windows and macOS.","completion":"Programming Languages"}
{"prompt":"Constraint logic programming extends Prolog to include concepts from constraint satisfaction. A constraint logic program allows constraints in the body of clauses, such as: A(X,Y) :- X+Y>0. It is suited to large-scale combinatorial optimisation problems and is thus useful for applications in industrial settings, such as automated time-tabling and production scheduling. Most Prolog systems ship with at least one constraint solver for finite domains, and often also with solvers for other domains like rational numbers.","completion":"Programming Languages"}
{"prompt":"The following examples illustrate the basic syntax of the language and use of the command-line interface. (An expanded list of standard language features can be found in the R manual, \"An Introduction to R\".)\nIn R, the generally preferred assignment operator is an arrow made from two characters <-, although = can be used in some cases.","completion":"Programming Languages"}
{"prompt":"Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester.\nMercury Autocode had a limited repertoire of variables a-z and a'-z' and, in some ways resembled early versions of the later Dartmouth BASIC language.  It pre-dated ALGOL, having no concept of stacks and hence no recursion or dynamically-allocated arrays.  In order to overcome the relatively small store size available on Mercury, large programs were written as distinct \"chapters\", each of which constituted an overlay.  Some skill was required to minimise time-consuming transfers of control between chapters.  This concept of overlays from drum under user control became common until virtual memory became available in later machines.  Slightly different dialects of Mercury Autocode were implemented for the Ferranti Atlas (distinct from the later Atlas Autocode) and the ICT 1300 and 1900 range.\nThe version for the EDSAC 2 was devised by David Hartley of  University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances, and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A version was developed for the successor Titan (the prototype Atlas 2 computer) as a temporary stop-gap while a more substantially advanced language known as CPL was being developed. CPL was never completed but did give rise to BCPL (developed by M. Richards), which in turn led to B and ultimately C. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.","completion":"Programming Languages"}
{"prompt":"A new language standardization process began at the 2003 Scheme workshop, with the goal of producing an R6RS standard in 2006. This process broke with the earlier RnRS approach of unanimity.\nR6RS features a standard module system, allowing a split between the core language and libraries. Several drafts of the R6RS specification were released, the final version being R5.97RS. A successful vote resulted in ratifying the new standard, announced on August 28, 2007.Currently the newest releases of various Scheme implementations support the R6RS standard. There is a portable reference implementation of the proposed implicitly phased libraries for R6RS, called psyntax, which loads and bootstraps itself properly on various older Scheme implementations.A feature of R6RS is the record-type descriptor (RTD). When an RTD is created and used, the record type representation can show the memory layout. It also calculated object field bit mask and mutable Scheme object field bit masks, and helped the garbage collector know what to do with the fields without traversing the whole fields list that are saved in the RTD. RTD allows users to expand the basic RTD to create a new record system.R6RS introduces numerous significant changes to the language.  The source code is now specified in Unicode, and a large subset of Unicode characters may now appear in Scheme symbols and identifiers, and there are other minor changes to the lexical rules. Character data is also now specified in Unicode. Many standard procedures have been moved to the new standard libraries, which themselves form a large expansion of the standard, containing procedures and syntactic forms that were formerly not part of the standard. A new module system has been introduced, and systems for exception handling are now standardized. Syntax-rules has been replaced with a more expressive syntactic abstraction facility (syntax-case) which allows the use of all of Scheme at macro expansion time. Compliant implementations are now required to support Scheme's full numeric tower, and the semantics of numbers have been expanded, mainly in the direction of support for the IEEE 754 standard for floating point numerical representation.","completion":"Programming Languages"}
{"prompt":"Functional programming has been employed in a wide range of industrial applications. For example, Erlang, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems, but has since become popular for building a range of applications at companies such as Nortel, Facebook, \u00c9lectricit\u00e9 de France and WhatsApp. Scheme, a dialect of Lisp, was used as the basis for several applications on early Apple Macintosh computers and has been applied to problems such as training-simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming and static analysis of embedded software. Haskell, though initially intended as a research language, has also been applied in areas such as aerospace systems, hardware design and web programming.Other functional programming languages that have seen use in industry include Scala, F#, Wolfram Language, Lisp, Standard ML and Clojure.Functional \"platforms\" have been popular in finance for risk analytics (particularly with large investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts, similar in manner to Gr\u00f6bner basis optimizations but also for regulatory frameworks such as Comprehensive Capital Analysis and Review. Given the use of OCaml and Caml variations in finance, these systems are sometimes considered related to a categorical abstract machine. Functional programming is heavily influenced by category theory.","completion":"Programming Languages"}
{"prompt":"a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.Python provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses round to even: round(1.5) and round(2.5) both produce 2. Versions before 3 used round-away-from-zero: round(0.5) is 1.0, round(-0.5) is \u22121.0.Python allows Boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.Python uses arbitrary-precision arithmetic for all integer operations. The Decimal type\/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes. The Fraction class in the fractions module provides arbitrary precision for rational numbers.Due to Python's extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.","completion":"Programming Languages"}
{"prompt":"One of the aims of esoteric programming languages is to parody or spoof existing languages and trends in the field of programming. For instance, the first esoteric language INTERCAL began as a spoof of languages used in the 1960's, such as APL, Fortran, and COBOL. INTERCAL's rules appear to be the inverse of rules in these other languages. However, the subject of parody is not always another established programming language. Shakespeare can be viewed as spoofing the structure of Shakespearean plays, for instance. The language Ook! is a parody of Brainfuck, where Brainfuck's eight commands are replaced by various orangutang sounds like \"Ook. Ook?\"","completion":"Programming Languages"}
{"prompt":"LISP 1 \u2013 First implementation.\nLISP 1.5 \u2013 First widely distributed version, developed by McCarthy and others at MIT. So named because it contained several improvements on the original \"LISP 1\" interpreter, but was not a major restructuring as the planned LISP 2 would be.\nStanford LISP 1.6 \u2013 This was a successor to LISP 1.5 developed at the Stanford AI Lab, and widely distributed to PDP-10 systems running the TOPS-10 operating system. It was rendered obsolete by Maclisp and InterLisp.\nMACLISP \u2013 developed for MIT's Project MAC, MACLISP is a direct descendant of LISP 1.5. It ran on the PDP-10 and Multics systems. MACLISP would later come to be called Maclisp, and is often referred to as MacLisp. The \"MAC\" in MACLISP is related neither to Apple's Macintosh nor to McCarthy.\nInterlisp \u2013 developed at BBN Technologies for PDP-10 systems running the TENEX operating system, later adopted as a \"West coast\" Lisp for the Xerox Lisp machines as InterLisp-D. A small version called \"InterLISP 65\" was published for the 6502-based Atari 8-bit family computer line. For quite some time, Maclisp and InterLisp were strong competitors.\nFranz Lisp \u2013 originally a University of California, Berkeley project; later developed by Franz Inc. The name is a humorous deformation of the name \"Franz Liszt\", and does not refer to Allegro Common Lisp, the dialect of Common Lisp sold by Franz Inc., in more recent years.\nXLISP, which AutoLISP was based on.\nStandard Lisp and Portable Standard Lisp were widely used and ported, especially with the Computer Algebra System REDUCE.\nZetaLisp, also termed Lisp Machine Lisp \u2013 used on the Lisp machines, direct descendant of Maclisp. ZetaLisp had a big influence on Common Lisp.\nLeLisp is a French Lisp dialect. One of the first Interface Builders (called SOS Interface) was written in LeLisp.\nScheme (1975).\nCommon Lisp (1984), as described by Common Lisp the Language \u2013 a consolidation of several divergent attempts (ZetaLisp, Spice Lisp, NIL, and S-1 Lisp) to create successor dialects to Maclisp, with substantive influences from the Scheme dialect as well. This version of Common Lisp was available for wide-ranging platforms and was accepted by many as a de facto standard until the publication of ANSI Common Lisp (ANSI X3.226-1994). Among the most widespread sub-dialects of Common Lisp are Steel Bank Common Lisp (SBCL), CMU Common Lisp (CMU-CL), Clozure OpenMCL (not to be confused with Clojure!), GNU CLisp, and later versions of Franz Lisp; all of them adhere to the later ANSI CL standard (see below).\nDylan was in its first version a mix of Scheme with the Common Lisp Object System.\nEuLisp \u2013 attempt to develop a new efficient and cleaned-up Lisp.\nISLISP \u2013 attempt to develop a new efficient and cleaned-up Lisp. Standardized as ISO\/IEC 13816:1997 and later revised as ISO\/IEC 13816:2007: Information technology \u2013 Programming languages, their environments and system software interfaces \u2013 Programming language ISLISP.\nIEEE Scheme \u2013 IEEE standard, 1178\u20131990 (R1995).\nANSI Common Lisp \u2013 an American National Standards Institute (ANSI) standard for Common Lisp, created by subcommittee X3J13, chartered to begin with Common Lisp: The Language as a base document and to work through a public consensus process to find solutions to shared issues of portability of programs and compatibility of Common Lisp implementations. Although formally an ANSI standard, the implementation, sale, use, and influence of ANSI Common Lisp has been and continues to be seen worldwide.\nACL2 or \"A Computational Logic for Applicative Common Lisp\", an applicative (side-effect free) variant of Common LISP. ACL2 is both a programming language which can model computer systems, and a tool to help proving properties of those models.\nClojure, a recent dialect of Lisp which compiles to the Java virtual machine and has a particular focus on concurrency.\nGame Oriented Assembly Lisp (or GOAL) is a video game programming language developed by Andy Gavin at Naughty Dog. It was written using Allegro Common Lisp and used in the development of the entire Jak and Daxter series of games developed by Naughty Dog.","completion":"Programming Languages"}
{"prompt":"Design Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1994 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them.\nThe book describes the following patterns:\n\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern","completion":"Programming Languages"}
{"prompt":"An anaphoric macro is a type of programming macro that deliberately captures some form supplied to the macro which may be referred to by an anaphor (an expression referring to another). Anaphoric macros first appeared in Paul Graham's On Lisp and their name is a reference to linguistic anaphora\u2014the use of words as a substitute for preceding words.","completion":"Programming Languages"}
{"prompt":"Traditionally, a program is modelled as a series of operations happening in a specific order; this may be referred to as sequential,:\u200ap.3\u200a\nprocedural,control flow (indicating that the program chooses a specific path), or imperative programming. The program focuses on commands, in line with the von Neumann:\u200ap.3\u200a vision of sequential programming, where data is normally \"at rest\".:\u200ap.7\u200aIn contrast, dataflow programming emphasizes the movement of data and models programs as a series of connections. Explicitly defined inputs and outputs connect operations, which function like black boxes.:\u200ap.2\u200a An operation runs as soon as all of its inputs become valid. Thus, dataflow languages are inherently parallel and can work well in large, decentralized systems.:\u200ap.3","completion":"Programming Languages"}
{"prompt":"A derivation of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.\nA derivation proves that the string belongs to the grammar's language.\nA derivation is fully determined by giving, for each step:\n\nthe rule applied in that step\nthe occurrence of its left-hand side to which it is appliedFor clarity, the intermediate string is usually given as well.\nFor instance, with the grammar:\n\nS \u2192 S + S\nS \u2192 1\nS \u2192 athe string\n\n1 + 1 + acan be derived from the start symbol S with the following derivation:\n\nS\n\u2192 S + S (by rule 1. on S)\n\u2192 S + S + S (by rule 1. on the second S)\n\u2192 1 + S + S (by rule 2. on the first S)\n\u2192 1 + 1 + S (by rule 2. on the second S)\n\u2192 1 + 1 + a (by rule 3. on the third S)Often, a strategy is followed that deterministically chooses the next nonterminal to rewrite:\n\nin a leftmost derivation, it is always the leftmost nonterminal;\nin a rightmost derivation, it is always the rightmost nonterminal.Given such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, one leftmost derivation of the same string is\n\nS\n\u2192 S + S (by rule 1 on the leftmost S)\n\u2192 1 + S (by rule 2 on the leftmost S)\n\u2192 1 + S + S (by rule 1 on the leftmost S)\n\u2192 1 + 1 + S (by rule 2 on the leftmost S)\n\u2192 1 + 1 + a (by rule 3 on the leftmost S),which can be summarized as\n\nrule 1\nrule 2\nrule 1\nrule 2\nrule 3.One rightmost derivation is:\n\nS\n\u2192 S + S (by rule 1 on the rightmost S)\n\u2192 S + S + S (by rule 1 on the rightmost S)\n\u2192 S + S + a (by rule 3 on the rightmost S)\n\u2192 S + 1 + a (by rule 2 on the rightmost S)\n\u2192 1 + 1 + a (by rule 2 on the rightmost S),which can be summarized as\n\nrule 1\nrule 1\nrule 3\nrule 2\nrule 2.The distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.\nA derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string \"1 + 1 + a\" is derived according to the leftmost derivation outlined above, the structure of the string would be:\n\n{{1}S + {{1}S + {a}S}S}Swhere {...}S indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:\n\nThis tree is called a parse tree or \"concrete syntax tree\" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another rightmost derivation of the same string\n\nS\n\u2192 S + S (by rule 1 on the rightmost S)\n\u2192 S + a (by rule 3 on the rightmost S)\n\u2192 S + S + a (by rule 1 on the rightmost S)\n\u2192 S + 1 + a (by rule 2 on the rightmost S)\n\u2192 1 + 1 + a (by rule 2 on the rightmost S),which defines a string with a different structure\n\n{{{1}S + {1}S}S + {a}S}Sand a different parse tree:\n\nNote however that both parse trees can be obtained by both leftmost and rightmost derivations.  For example, the last tree can be obtained with the leftmost derivation as follows:\n\nS\n\u2192 S + S (by rule 1 on the leftmost S)\n\u2192 S + S + S (by rule 1 on the leftmost S)\n\u2192 1 + S + S (by rule 2 on the leftmost S)\n\u2192 1 + 1 + S (by rule 2 on the leftmost S)\n\u2192 1 + 1 + a (by rule 3 on the leftmost S),If a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an ambiguous grammar. Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called inherently ambiguous languages.","completion":"Programming Languages"}
{"prompt":"The Lua programming language uses double-hyphens, --, for single line comments in a similar way to Ada, Eiffel, Haskell, SQL and VHDL languages. Lua also has block comments, which start with --[[ and run until a closing ]]\nFor example:\n\nA common technique to comment out a piece of code, is to enclose the code between --[[ and\n--]], as below:\n\nIn this case, it's possible to reactivate the code by adding a single hyphen to the first line:\n\nIn the first example, the --[[ in the first line starts a long comment, and the two hyphens in the last line\nare still inside that comment. In the second example, the sequence ---[[ starts an ordinary, single-line\ncomment, so that the first and the last lines become independent comments. In this case, the print is\noutside comments. In this case, the last line becomes an independent comment, as it starts with --.\nLong comments in Lua can be more complex than these, as you can read in the section called \"Long strings\" c.f. Programming in Lua.","completion":"Programming Languages"}
{"prompt":"Although Lua does not have a built-in concept of classes, object-oriented programming can be emulated using functions and tables. An object is formed by putting methods and fields in a table. Inheritance (both single and multiple) can be implemented with metatables, delegating nonexistent methods and fields to a parent object.\nThere is no such concept as \"class\" with these techniques; rather, prototypes are used, similar to Self or JavaScript. New objects are created either with a factory method (that constructs new objects from scratch) or by cloning an existing object.\nCreating a basic vector object:\n\nHere, setmetatable tells Lua to look for an element in the Vector table if it is not present in the vec table. vec.magnitude, which is equivalent to vec[\"magnitude\"], first looks in the vec table for the magnitude element. The vec table does not have a magnitude element, but its metatable delegates to the Vector table for the magnitude element when it's not found in the vec table.\nLua provides some syntactic sugar to facilitate object orientation. To declare member functions inside a prototype table, one can use function table:func(args), which is equivalent to function table.func(self, args). Calling class methods also makes use of the colon: object:func(args) is equivalent to object.func(object, args).\nThat in mind, here is a corresponding class with : syntactic sugar:","completion":"Programming Languages"}
{"prompt":"Purely functional data structures are often represented in a different way to their imperative counterparts. For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap,  are based on arrays. Arrays can be replaced by maps or random access lists, which admit purely functional implementation, but have logarithmic access and update times. Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.","completion":"Programming Languages"}
{"prompt":"ALGOL 60 \u2013 with COBOL \u2013 were the first languages to seek standardization.\n\nISO 1538:1984 Programming languages \u2013 ALGOL 60 (stabilized)\nISO\/TR 1672:1977 Hardware representation of ALGOL basic symbols ... (now withdrawn)","completion":"Programming Languages"}
{"prompt":"procedure Absmax(a) Size:(n, m) Result:(y) Subscripts:(i, k);\n    value n, m; array a; integer n, m, i, k; real y;\ncomment The absolute greatest element of the matrix a, of size n by m,\n    is copied to y, and the subscripts of this element to i and k;\nbegin\n    integer p, q;\n    y := 0; i := k := 1;\n    for p := 1 step 1 until n do\n        for q := 1 step 1 until m do\n            if abs(a[p, q]) > y then\n                begin y := abs(a[p, q]);\n                    i := p; k := q\n                end\nend Absmax\n\nImplementations differ in how the text in bold must be written. The word 'INTEGER', including the quotation marks, must be used in some implementations in place of integer, above, thereby designating it as a special keyword.\nFollowing is an example of how to produce a table using Elliott 803 ALGOL:\n FLOATING POINT ALGOL TEST'\n BEGIN REAL A,B,C,D'\n\n READ D'\n\n FOR A:= 0.0 STEP D UNTIL 6.3 DO\n BEGIN\n   PRINT PUNCH(3),\u00a3\u00a3L??'\n   B := SIN(A)'\n   C := COS(A)'\n   PRINT PUNCH(3),SAMELINE,ALIGNED(1,6),A,B,C'\n END'\n END'","completion":"Programming Languages"}
{"prompt":"Further blurring the distinction between interpreters, bytecode interpreters and compilation is just-in-time (JIT) compilation, a technique in which the intermediate representation is compiled to native machine code at runtime. This confers the efficiency of running native code, at the cost of startup time and increased memory use when the bytecode or AST is first compiled. The earliest published JIT compiler is generally attributed to work on LISP by John McCarthy in 1960. Adaptive optimization is a complementary technique in which the interpreter profiles the running program and compiles its most frequently executed parts into native code. The latter technique is a few decades old, appearing in languages such as Smalltalk in the 1980s.Just-in-time compilation has gained mainstream attention amongst language implementers in recent years, with Java, the .NET Framework, most modern JavaScript implementations, and Matlab now including JIT compilers.","completion":"Programming Languages"}
{"prompt":"A major class of scripting languages has grown out of the automation of job control, which relates to starting and controlling the behavior of system programs (in this sense, one might think of shells as being descendants of IBM's JCL, or Job Control Language, which was used for exactly this purpose). Many of these languages' interpreters double as command-line interpreters such as the Unix shell or the MS-DOS COMMAND.COM. Others, such as AppleScript offer the use of English-like commands to build scripts.","completion":"Programming Languages"}
{"prompt":"There are a large number of Smalltalk variants. The unqualified word Smalltalk is often used to indicate the Smalltalk-80 language and compatible VM, the first version to be made publicly available and created in 1980. The first hardware-environments which ran the Smalltalk VMs were Xerox Alto computers.\nSmalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, Adele Goldberg wrote most of the documentation, and Dan Ingalls implemented most of the early versions. The first version, termed Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in \"a page of code\". A later variant used for research work is now termed Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.\nAfter significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created.  This system had a development environment featuring most of the now familiar tools, including a class library code browser\/editor. Smalltalk-80 added metaclasses, to help maintain the \"everything is an object\" (except variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways to create instances).\nSmalltalk-80 was the first language variant made available outside of PARC. In 1981 it was shared with Tektronix, Hewlett-Packard, Apple Computer, and DEC for review and debugging on their platforms. The August 1981 issue of Byte Magazine was devoted to Smalltalk-80 and brought its ideas to a large audience. Several books on Smalltalk-80 were also published. Smalltalk-80 became the basis for all future commercial versions of Smalltalk. The final release of Smalltalk-80 Version 1 was in November 1981. Xerox only distributed Version 1 to Apple, DEC, HP, and Tektronix, but these companies were allowed unrestricted\nredistribution via any system they built. This encouraged the wide spread of Smalltalk. Later, in 1983, Xerox released Smalltalk-80 Version 2. This version was generally available to the public, although under a restrictive license. Versions 1 and 2 were fairly similar, although Version 2 did have some added features such as a spelling corrector. Each release consisted of a virtual image (platform-independent file with object definitions) and a virtual machine specification.ANSI Smalltalk has been the standard language reference since 1998. Two currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks. (Dan Ingalls later ported Hobbes to Squeak.)\nDuring the late 1980s to mid-1990s, Smalltalk environments\u2014including support, training and add-ons\u2014were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix\/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS\/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers.  While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge\/Smalltalk.  Easel introduced Enfin at this time on Windows and OS\/2.  Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge).  Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.\nIn 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk\/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom. VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.\nCincom, GemTalk, and Instantiations, continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java instead and it is, as of 2005, supported by Instantiations, Inc. who renamed the product VA Smalltalk (VAST Platform) and continue to release new versions yearly. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented toward research and use in commercial environments.\nA significant development, that has spread across all Smalltalk environments as of 2016, is the increasing usage of two web frameworks, Seaside and AIDA\/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone, and Instantiations incorporating and extending it.","completion":"Programming Languages"}
{"prompt":"A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition. Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.\n\nFactor\nForth\njq (function application is also supported)\nJoy\nPostScript","completion":"Programming Languages"}
{"prompt":"He was born in Montecatini Terme, Italy. He attended the University of Pisa before receiving his PhD from the University of Edinburgh in 1982 for research supervised by Gordon Plotkin.","completion":"Programming Languages"}
{"prompt":"JavaScript\/ECMAScript supports anonymous functions.\n\nES6 supports \"arrow function\" syntax, where a => symbol separates the anonymous function's parameter list from the body:\n\nThis construct is often used in Bookmarklets. For example, to change the title of the current document (visible in its window's title bar) to its URL, the following bookmarklet may seem to work.\n\nHowever, as the assignment statement returns a value (the URL itself), many browsers actually create a new page to display this value.\nInstead, an anonymous function, that does not return a value, can be used:\n\nThe function statement in the first (outer) pair of parentheses declares an anonymous function, which is then executed when used with the last pair of parentheses. This is almost equivalent to the following, which populates the environment with f unlike an anonymous function.\n\nUse void() to avoid new pages for arbitrary anonymous functions:\n\nor just:\n\nJavaScript has syntactic subtleties for the semantics of defining, invoking and evaluating anonymous functions. These subliminal nuances are a direct consequence of the evaluation of parenthetical expressions. The following constructs which are called immediately-invoked function expression illustrate this:\n\n and \n\nRepresenting \"function(){ ... }\" by f, the form of the constructs are \na parenthetical within a parenthetical (f()) and a parenthetical applied to a parenthetical (f)().\nNote the general syntactic ambiguity of a parenthetical expression, parenthesized arguments to a function and the parentheses around the formal parameters in a function definition. In particular, JavaScript defines a , (comma) operator in the context of a parenthetical expression. It is no mere coincidence that the syntactic forms coincide for an expression and a function's arguments (ignoring the function formal parameter syntax)! If f is not identified in the constructs above, they become (()) and ()(). The first provides no syntactic hint of any resident function but the second MUST evaluate the first parenthetical as a function to be legal JavaScript. (Aside: for instance, the ()'s could be ([],{},42,\"abc\",function(){}) as long as the expression evaluates to a function.)\nAlso, a function is an Object instance (likewise objects are Function instances) and the object literal notation brackets, {} for braced code, are used when defining a function this way (as opposed to using new Function(...)). In a very broad non-rigorous sense (especially since global bindings are compromised), an arbitrary sequence of braced JavaScript statements, {stuff}, can be considered to be a fixed point of\n\nMore correctly but with caveats, \n\nNote the implications of the anonymous function in the JavaScript fragments that follow:\n\nfunction(){ ... }() without surrounding ()'s is generally not legal\n(f=function(){ ... }) does not \"forget\" f globally unlike (function f(){ ... })Performance metrics to analyze the space and time complexities of function calls, call stack, etc. in a JavaScript interpreter engine implement easily with these last anonymous function constructs. From the implications of the results, it is possible to deduce some of an engine's recursive versus iterative implementation details, especially tail-recursion.","completion":"Programming Languages"}
{"prompt":"A pioneer dataflow language was BLODI (BLOck DIagram), published in 1961 by John Larry Kelly, Jr., Carol Lochbaum and Victor A. Vyssotsky for specifying sampled data systems. A BLODI specification of functional units (amplifiers, adders, delay lines, etc.) and their interconnections was compiled into a single loop that updated the entire system for one clock tick.\nIn a 1966 Ph.D. thesis, The On-line Graphical Specification of Computer Procedures, Bert Sutherland created one of the first graphical dataflow programming frameworks in order to make parallel programming easier. Subsequent dataflow languages were often developed at the large supercomputer labs. POGOL, an otherwise conventional data-processing language developed at NSA, compiled large-scale applications composed of multiple file-to-file operations, e.g. merge, select, summarize, or transform, into efficient code that eliminated the creation of or writing to intermediate files to the greatest extent possible. SISAL, a popular dataflow language developed at Lawrence Livermore National Laboratory, looks like most statement-driven languages, but variables should be assigned once. This allows the compiler to easily identify the inputs and outputs. A number of offshoots of SISAL have been developed, including SAC, Single Assignment C, which tries to remain as close to the popular C programming language as possible.\nThe United States Navy funded development of ACOS and SPGN (signal processing graph notation) starting in the early 1980s. This is in use on a number of platforms in the field today.A more radical concept is Prograph, in which programs are constructed as graphs onscreen, and variables are replaced entirely with lines linking inputs to outputs. Incidentally, Prograph was originally written on the Macintosh, which remained single-processor until the introduction of the DayStar Genesis MP in 1996.There are many hardware architectures oriented toward the efficient implementation of dataflow programming models. MIT's tagged token dataflow architecture was designed by Greg Papadopoulos.Data flow has been proposed as an abstraction for specifying the global behavior of distributed system components: in the live distributed objects programming model, distributed data flows are used to store and communicate state, and as such, they play the role analogous to variables, fields, and parameters in Java-like programming languages.","completion":"Programming Languages"}
{"prompt":"The programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural-language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.\nIt was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative\/procedural interpretation later became formalised in the Prolog notation\n\nH :- B1, \u2026, Bn.which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, ..., Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski's procedural interpretation and LUSH were described in a 1973 memo, published in 1974.Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.","completion":"Programming Languages"}
{"prompt":"^a  \"step n\" is used to change the loop interval. If \"step\" is omitted, then the loop interval is 1.\n^b  This implements the universal quantifier (\"for all\" or \"\n  \n    \n      \n        \u2200\n      \n    \n    {\\displaystyle \\forall }\n  \") as well as the existential quantifier (\"there exists\" or \"\n  \n    \n      \n        \u2203\n      \n    \n    {\\displaystyle \\exists }\n  \").\n^c  THRU may be used instead of THROUGH.\n^d  \u00abIS\u00bb GREATER \u00abTHAN\u00bb may be used instead of >.\n^e  Type of set expression must implement trait std::iter::IntoIterator.","completion":"Programming Languages"}
{"prompt":"A Lisp list is implemented as a singly linked list. Each cell of this list is called a cons (in Scheme, a pair) and is composed of two pointers, called the car and cdr. These are respectively equivalent to the data and next fields discussed in the article linked list.\nOf the many data structures that can be built out of cons cells, one of the most basic is called a proper list. A proper list is either the special nil (empty list) symbol, or a cons in which the car points to a datum (which may be another cons structure, such as a list), and the cdr points to another proper list.\nIf a given cons is taken to be the head of a linked list, then its car points to the first element of the list, and its cdr points to the rest of the list. For this reason, the car and cdr functions are also called first and rest when referring to conses which are part of a linked list (rather than, say, a tree).\nThus, a Lisp list is not an atomic object, as an instance of a container class in C++ or Java would be. A list is nothing more than an aggregate of linked conses. A variable that refers to a given list is simply a pointer to the first cons in the list. Traversal of a list can be done by cdring down the list; that is, taking successive cdrs to visit each cons of the list; or by using any of several higher-order functions to map a function over a list.\nBecause conses and lists are so universal in Lisp systems, it is a common misconception that they are Lisp's only data structures. In fact, all but the most simplistic Lisps have other data structures, such as vectors (arrays), hash tables, structures, and so forth.","completion":"Programming Languages"}
{"prompt":"Data-oriented languages provide powerful ways of searching and manipulating the relations that have been described as entity relationship tables which map one set of things into other sets. Examples of data-oriented languages include:","completion":"Programming Languages"}
{"prompt":"Macros in the PL\/I language are written in a subset of PL\/I itself: the compiler executes \"preprocessor statements\" at compilation time, and the output of this execution forms part of the code that is compiled. The ability to use a familiar procedural language as the macro language gives power much greater than that of text substitution macros, at the expense of a larger and slower compiler.\nFrame technology's frame macros have their own command syntax but can also contain text in any language. Each frame is both a generic component in a hierarchy of nested subassemblies, and a procedure for integrating itself with its subassembly frames (a recursive process that resolves integration conflicts in favor of higher level subassemblies). The outputs are custom documents, typically compilable source modules. Frame technology can avoid the proliferation of similar but subtly different components, an issue that has plagued software development since the invention of macros and subroutines.\nMost assembly languages have less powerful procedural macro facilities, for example allowing a block of code to be repeated N times for loop unrolling; but these have a completely different syntax from the actual assembly language.","completion":"Programming Languages"}
{"prompt":"The lambda calculus, developed in the 1930s by Alonzo Church, is a formal system of computation built from function application. In 1937 Alan Turing proved that the lambda calculus and Turing machines are equivalent models of computation, showing that the lambda calculus is Turing complete. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation, combinatory logic, was developed by Moses Sch\u00f6nfinkel and Haskell Curry in the 1920s and 1930s.Church later developed a weaker system, the simply-typed lambda calculus, which extended the lambda calculus by assigning a data type to all terms. This forms the basis for statically typed functional programming.\nThe first high-level functional programming language, Lisp, was developed in the late 1950s for the IBM 700\/7000 series of scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). Lisp functions were defined using Church's lambda notation, extended with a label construct to allow recursive functions. Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.Information Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of generator, which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on the mutating list structure and similar imperative features.\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\nIn the mid-1960s, Peter Landin invented SECD machine, the first abstract machine for a functional programming language, described a correspondence between ALGOL 60 and the lambda calculus, and proposed the ISWIM programming language.John Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\". He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.\nThe 1973 language ML was created by Robin Milner at the University of Edinburgh, and David Turner developed the language SASL at the University of St Andrews. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML.\nIn the 1970s, Guy L. Steele and Gerald Jay Sussman developed Scheme, as described in the Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs. Scheme was the first dialect of lisp to use lexical scoping and to require tail-call optimization, features that encourage functional programming.\nIn the 1980s, Per Martin-L\u00f6f developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.The lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing as of 1990.\nMore recently it has found use in niches such as parametric CAD in the OpenSCAD language built on the CGAL framework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.Functional programming continues to be used in commercial settings.","completion":"Programming Languages"}
{"prompt":"In the 17th century, Gottfried Leibniz imagined and described the characteristica universalis, a universal and formal language which utilised pictographs. During this period, Carl Friedrich Gauss also investigated the problem of Gauss codes.Gottlob Frege attempted to realize Leibniz's ideas, through a notational system first outlined in Begriffsschrift (1879) and more fully developed in his 2-volume Grundgesetze der Arithmetik (1893\/1903). This described a \"formal language of pure language.\"In the first half of the 20th century, several developments were made with relevance to formal languages. Axel Thue published four papers relating to words and language between 1906 and 1914. The last of these introduced what Emil Post later termed 'Thue Systems', and gave an early example of an undecidable problem. Post would later use this paper as the basis for a 1947 proof \"that the word problem for semigroups was recursively insoluble\", and later devised the canonical system for the creation of formal languages. \nIn 1907, Leonardo Torres Quevedo introduced a formal language for the description of mechanical drawings (mechanical devices), in Vienna. He published \"Sobre un sistema de notaciones y s\u00edmbolos destinados a facilitar la descripci\u00f3n de las m\u00e1quinas\"  (\"On a system of notations and symbols intended to facilitate the description of machines\"). Heinz Zemanek rated it as an equivalent to a programming language for the numerical control of machine tools.Noam Chomsky devised an abstract representation of formal and natural languages, known as the Chomsky hierarchy. In 1959 John Backus developed the Backus-Naur form to describe the syntax of a high level programming language, following his work in the creation of FORTRAN. Peter Naur was the secretary\/editor for the ALGOL60 Report in which he used Backus\u2013Naur form to describe the Formal part of ALGOL60.","completion":"Programming Languages"}
{"prompt":"Since ALGOL 60 had no I\/O facilities, there is no portable hello world program in ALGOL.\nThe next three examples are in Burroughs Extended Algol. The first two direct output at the interactive terminal they are run on. The first uses a character array, similar to C. The language allows the array identifier to be used as a pointer to the array, and hence in a REPLACE statement.\n\nA simpler program using an inline format:\n\nAn even simpler program using the Display statement. Note that its output would end up at the system console ('SPO'):\n\nAn alternative example, using Elliott Algol I\/O is as follows.  Elliott Algol used different characters for \"open-string-quote\" and \"close-string-quote\", represented here by  \u2018  and  \u2019 .\n\nBelow is a version from Elliott 803 Algol (A104). The standard Elliott 803 used five-hole paper tape and thus only had upper case.  The code lacked any quote characters so \u00a3 (UK Pound Sign) was used for open quote and ? (Question Mark) for close quote.  Special sequences were placed in double quotes (e.g. \u00a3\u00a3L?? produced a new line on the teleprinter).\n\n  HIFOLKS'\n  BEGIN\n     PRINT \u00a3HELLO WORLD\u00a3L??'\n  END'\n\nThe ICT 1900 series Algol I\/O version allowed input from paper tape or punched card.  Paper tape 'full' mode allowed lower case.  Output was to a line printer. The open and close quote characters were represented using '(' and ')' and spaces by %.\n  'BEGIN'\n     WRITE TEXT('('HELLO%WORLD')');\n  'END'","completion":"Programming Languages"}
{"prompt":"Portability was a problem in the early days because there was no agreed upon standard\u2014not even IBM's reference manual\u2014and computer companies vied to differentiate their offerings from others by providing incompatible features.  Standards have improved portability.  The 1966 standard provided a reference syntax and semantics, but vendors continued to provide incompatible extensions.  Although careful programmers were coming to realize that use of incompatible extensions caused expensive portability problems, and were therefore using programs such as The PFORT Verifier, it was not until after the 1977 standard, when the National Bureau of Standards (now NIST) published FIPS PUB 69, that processors purchased by the U.S. Government were required to diagnose extensions of the standard.  Rather than offer two processors, essentially every compiler eventually had at least an option to diagnose extensions.Incompatible extensions were not the only portability problem.  For numerical calculations, it is important to take account of the characteristics of the arithmetic.  This was addressed by Fox et al. in the context of the 1966 standard by the PORT library. The ideas therein became widely used, and were eventually incorporated into the 1990 standard by way of intrinsic inquiry functions.  The widespread (now almost universal) adoption of the IEEE 754 standard for binary floating-point arithmetic has essentially removed this problem.\nAccess to the computing environment (e.g., the program's command line, environment variables, textual explanation of error conditions) remained a problem until it was addressed by the 2003 standard.\nLarge collections of library software that could be described as being loosely related to engineering and scientific calculations, such as graphics libraries, have been written in C, and therefore access to them presented a portability problem.  This has been addressed by incorporation of C interoperability into the 2003 standard.\nIt is now possible (and relatively easy) to write an entirely portable program in Fortran, even without recourse to a preprocessor.","completion":"Programming Languages"}
{"prompt":"Perhaps the most significant development in the early history of FORTRAN was the decision by the American Standards Association (now American National Standards Institute (ANSI)) to form a committee sponsored by the Business Equipment Manufacturers Association (BEMA) to develop an American Standard Fortran.  The resulting two standards, approved in March 1966, defined two languages, FORTRAN (based on FORTRAN IV, which had served as a de facto standard), and Basic FORTRAN (based on FORTRAN II, but stripped of its machine-dependent features).  The FORTRAN defined by the first standard, officially denoted X3.9-1966, became known as FORTRAN 66 (although many continued to term it FORTRAN IV, the language on which the standard was largely based).  FORTRAN 66 effectively became the first industry-standard version of FORTRAN. FORTRAN 66 included:\n\nMain program, SUBROUTINE, FUNCTION, and BLOCK DATA program units\nINTEGER, REAL, DOUBLE PRECISION, COMPLEX, and LOGICAL data types\nCOMMON, DIMENSION, and EQUIVALENCE statements\nDATA statement for specifying initial values\nIntrinsic and EXTERNAL (e.g., library) functions\nAssignment statement\nGO TO, computed GO TO, assigned GO TO, and ASSIGN statements\nLogical IF and arithmetic (three-way) IF statements\nDO loop statement\nREAD, WRITE, BACKSPACE, REWIND, and ENDFILE statements for sequential I\/O\nFORMAT statement and assigned format\nCALL, RETURN, PAUSE, and STOP statements\nHollerith constants in DATA and FORMAT statements, and as arguments to procedures\nIdentifiers of up to six characters in length\nComment lines\nEND line","completion":"Programming Languages"}
{"prompt":"The language is formally defined in the standards R5RS (1998) and R6RS (2007).  They describe standard \"forms\": keywords and accompanying syntax, which provide the control structure of the language, and standard procedures which perform common tasks.","completion":"Programming Languages"}
{"prompt":"There are a number of important subclasses of the context-free grammars:\n\nLR(k) grammars (also known as deterministic context-free grammars) allow parsing (string recognition) with deterministic pushdown automata (PDA), but they can only describe deterministic context-free languages.\nSimple LR, Look-Ahead LR grammars are subclasses that allow further simplification of parsing. SLR and LALR are recognized using the same PDA as LR, but with simpler tables, in most cases.\nLL(k) and LL(*) grammars allow parsing by direct construction of a leftmost derivation as described above, and describe even fewer languages.\nSimple grammars are a subclass of the LL(1) grammars mostly interesting for its theoretical property that language equality of simple grammars is decidable, while language inclusion is not.\nBracketed grammars have the property that the terminal symbols are divided into left and right bracket pairs that always match up in rules.\nLinear grammars have no rules with more than one nonterminal on the right-hand side.\nRegular grammars are a subclass of the linear grammars and describe the regular languages, i.e. they correspond to finite automata and regular expressions.LR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars.  On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected.  Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.","completion":"Programming Languages"}
{"prompt":"Very early in the history of EMCC, John Mauchly assumed responsibility for programming, coding, and applications for the planned computer systems. His early interaction with representatives of the Census Bureau in 1944 and 1945, and discussion with people interested in statistics, weather prediction, and various business problems in 1945 and 1946 focused his attention on the need to provide new users with the software to accomplish their objectives. He knew it would be difficult to sell computers without application materials, and without training in how to use the systems. And so, EMCC began to assemble a staff of mathematicians interested in coding in early 1947. (from Norberg)\nMauchly's interest lay in the application of computers, as well as to their architecture and organization.  His experience with programming the ENIAC and its successors led him to create Short Code (see \"The UNIVAC SHORT CODE\"), the first programming language actually used on a computer (predated by Zuse's conceptual Plankalkul). It was a pseudocode interpreter for mathematical problems proposed in 1949 and ran on the UNIVAC I and II. Mauchly's belief in the importance of languages led him to hire Grace Murray Hopper to develop a compiler for the UNIVAC.\nJohn Mauchly has also been credited for being the first one using the verb \"to program\" in his 1942 paper on electronic computing, although in the context of ENIAC, not in its current meaning.","completion":"Programming Languages"}
{"prompt":"John W. Mauchly was born on August 30, 1907, to Sebastian and Rachel (Scheidemantel) Mauchly in Cincinnati, Ohio. He moved with his parents and sister, Helen Elizabeth (Betty), at an early age to Chevy Chase, Maryland, when Sebastian Mauchly obtained a position at the Carnegie Institution of Washington as head of its Section of Terrestrial Electricity. As a youth, Mauchly was interested in science, and in particular with electricity, and as a young teenager was known to fix neighbors' electric systems. Mauchly attended E.V. Brown Elementary School in Chevy Chase and McKinley Technical High School in Washington, DC. At McKinley, Mauchly was extremely active in the debate team, was a member of the national honor society, and became editor-in-chief of the school's newspaper, Tech Life. After graduating from high school in 1925, he earned a scholarship to study engineering at Johns Hopkins University. He subsequently transferred to the physics department, and without completing his undergraduate degree, instead earned a Ph.D. in physics in 1932.From 1932 to 1933, Mauchly served as a research assistant at Johns Hopkins University where he concentrated on calculating energy levels of the formaldehyde spectrum. Mauchly's teaching career truly began in 1933 at Ursinus College where he was appointed head of the physics department, where he was, in fact, the only staff member.In the summer of 1941, Mauchly took a Defense Training Course for Electronics at the University of Pennsylvania Moore School of Electrical Engineering. There he met the lab instructor, J. Presper Eckert (1919\u20131995), with whom he would form a long-standing working partnership. Following the course, Mauchly was hired as an instructor of electrical engineering and in 1943, he was promoted to assistant professor of electrical engineering. Following the outbreak of World War II, the United States Army Ordnance Department contracted the Moore School to build an electronic computer which, as proposed by Mauchly and Eckert, would accelerate the recomputation of artillery firing tables.In 1959, Mauchly left Sperry Rand and started Mauchly Associates, Inc. One of Mauchly Associates' notable achievements was the development of the Critical Path Method (CPM) which provided for automated construction scheduling. Mauchly also set up a consulting organization, Dynatrend, in 1967 and worked as a consultant to Sperry UNIVAC from 1973 until his death in 1980.John Mauchly died on January 8, 1980, in Ambler, Pennsylvania, during heart surgery and following a long illness. His first wife, Mary Augusta Walzl, a mathematician, whom he married on December 30, 1930, drowned in 1946. John and Mary Mauchly had two children, James (Jimmy) and Sidney. In 1948, Mauchly married Kathleen Kay McNulty (1921\u20132006), one of the six original ENIAC programmers; they had five children Sara (Sallie), Kathleen (Kathy), John, Virginia (Gini), and Eva.","completion":"Programming Languages"}
{"prompt":"Principles of Programming Languages (POPL)\nProgramming Language Design and Implementation (PLDI)\nInternational Symposium on Memory Management (ISMM)\nLanguages, Compilers, and Tools for Embedded Systems (LCTES)\nSymposium on Principles and Practice of Parallel Programming (PPoPP)\nInternational Conference on Functional Programming (ICFP)\nSystems, Programming, Languages, and Applications: Software for Humanity (SPLASH)\nObject-Oriented Programming, Systems, Languages, and Applications (OOPSLA)\nHistory of Programming Languages (HOPL)\nDynamic Languages Symposium (DLS)","completion":"Programming Languages"}
{"prompt":"In modern protocol design, protocols are layered to form a protocol stack. Layering is a design principle that divides the protocol design task into smaller steps, each of which accomplishes a specific part, interacting with the other parts of the protocol only in a small number of well-defined ways. Layering allows the parts of a protocol to be designed and tested without a combinatorial explosion of cases, keeping each design relatively simple.\nThe communication protocols in use on the Internet are designed to function in diverse and complex settings. Internet protocols are designed for simplicity and modularity and fit into a coarse hierarchy of functional layers defined in the Internet Protocol Suite. The first two cooperating protocols, the Transmission Control Protocol (TCP) and the Internet Protocol (IP) resulted from the decomposition of the original Transmission Control Program, a monolithic communication protocol, into this layered communication suite.\nThe OSI model was developed internationally based on experience with networks that predated the internet as a reference model for general communication with much stricter rules of protocol interaction and rigorous layering.\nTypically, application software is built upon a robust data transport layer. Underlying this transport layer is a datagram delivery and routing mechanism that is typically connectionless in the Internet. Packet relaying across networks happens over another layer that involves only network link technologies, which are often specific to certain physical layer technologies, such as Ethernet. Layering provides opportunities to exchange technologies when needed, for example, protocols are often stacked in a tunneling arrangement to accommodate the connection of dissimilar networks. For example, IP may be tunneled across an Asynchronous Transfer Mode (ATM) network.","completion":"Programming Languages"}
{"prompt":"ISO\/IEC 1539-1:2010, informally known as Fortran 2008, was approved in September 2010. As with Fortran 95, this is a minor upgrade, incorporating clarifications and corrections to Fortran 2003, as well as introducing some new capabilities.  The new capabilities include:\n\nSub-modules \u2013 additional structuring facilities for modules; supersedes ISO\/IEC TR 19767:2005\nCoarray Fortran \u2013 a parallel execution model\nThe DO CONCURRENT construct \u2013 for loop iterations with no interdependencies\nThe CONTIGUOUS attribute \u2013 to specify storage layout restrictions\nThe BLOCK construct \u2013 can contain declarations of objects with construct scope\nRecursive allocatable components \u2013 as an alternative to recursive pointers in derived typesThe Final Draft international Standard (FDIS) is available as document N1830.A supplement to Fortran 2008 is the International Organization for Standardization (ISO) Technical Specification (TS) 29113 on Further Interoperability of Fortran with C, which has been submitted to ISO in May 2012 for approval. The specification adds support for accessing the array descriptor from C and allows ignoring the type and rank of arguments.","completion":"Programming Languages"}
{"prompt":"Up to the R5RS standard, the standard comment in Scheme was a semicolon, which makes the rest of the line invisible to Scheme. Numerous implementations have supported alternative conventions permitting comments to extend for more than a single line, and the R6RS standard permits two of them: an entire s-expression may be turned into a comment (or \"commented out\") by preceding it with #; (introduced in SRFI 62) and a multiline comment or \"block comment\" may be produced by surrounding text with #| and |#.","completion":"Programming Languages"}
{"prompt":"Scheme is primarily a functional programming language. It shares many characteristics with other members of the Lisp programming language family. Scheme's very simple syntax is based on s-expressions, parenthesized lists in which a prefix operator is followed by its arguments. Scheme programs thus consist of sequences of nested lists. Lists are also the main data structure in Scheme, leading to a close equivalence between source code and data formats (homoiconicity). Scheme programs can easily create and evaluate pieces of Scheme code dynamically.\nThe reliance on lists as data structures is shared by all Lisp dialects. Scheme inherits a rich set of list-processing primitives such as cons, car and cdr from its Lisp progenitors. Scheme uses strictly but dynamically typed variables and supports first class procedures. Thus, procedures can be assigned as values to variables or passed as arguments to procedures.\nThis section concentrates mainly on innovative features of the language, including those features that distinguish Scheme from other Lisps. Unless stated otherwise, descriptions of features relate to the R5RS standard. In examples provided in this section, the notation \"===> result\" is used to indicate the result of evaluating the expression on the immediately preceding line. This is the same convention used in R5RS.","completion":"Programming Languages"}
{"prompt":"After having declined somewhat in the 1990s, Lisp has experienced a resurgence of interest after 2000. Most new activity has been focused around implementations of Common Lisp, Scheme, Emacs Lisp, Clojure, and Racket, and includes development of new portable libraries and applications.\nMany new Lisp programmers were inspired by writers such as Paul Graham and Eric S. Raymond to pursue a language others considered antiquated. New Lisp programmers often describe the language as an eye-opening experience and claim to be substantially more productive than in other languages. This increase in awareness may be contrasted to the \"AI winter\" and Lisp's brief gain in the mid-1990s.As of 2010, there were eleven actively maintained Common Lisp implementations.The open source community has created new supporting infrastructure: CLiki is a wiki that collects Common Lisp related information, the Common Lisp directory lists resources, #lisp is a popular IRC channel and allows the sharing and commenting of code snippets (with support by lisppaste, an IRC bot written in Lisp), Planet Lisp collects the contents of various Lisp-related blogs, on LispForum users discuss Lisp topics, Lispjobs is a service for announcing job offers and there is a weekly news service, Weekly Lisp News. Common-lisp.net is a hosting site for open source Common Lisp projects. Quicklisp is a library manager for Common Lisp.\nFifty years of Lisp (1958\u20132008) was celebrated at LISP50@OOPSLA. There are regular local user meetings in Boston, Vancouver, and Hamburg. Other events include the European Common Lisp Meeting, the European Lisp Symposium and an International Lisp Conference.\nThe Scheme community actively maintains over twenty implementations. Several significant new implementations (Chicken, Gambit, Gauche, Ikarus, Larceny, Ypsilon) have been developed in the 2000s (decade). The Revised5 Report on the Algorithmic Language Scheme standard of Scheme was widely accepted in the Scheme community. The Scheme Requests for Implementation process has created a lot of quasi standard libraries and extensions for Scheme. User communities of individual Scheme implementations continue to grow. A new language standardization process was started in 2003 and led to the R6RS Scheme standard in 2007. Academic use of Scheme for teaching computer science seems to have declined somewhat. Some universities are no longer using Scheme in their computer science introductory courses; MIT now uses Python instead of Scheme for its undergraduate computer science program and MITx massive open online course.There are several new dialects of Lisp: Arc, Hy, Nu, Liskell, and LFE (Lisp Flavored Erlang). The parser for Julia is implemented in Femtolisp, a dialect of Scheme (Julia is inspired by Scheme, which in turn is a Lisp dialect).\nIn October 2019, Paul Graham released a specification for Bel, \"a new dialect of Lisp.\"","completion":"Programming Languages"}
{"prompt":"Backus served on the international committees that developed ALGOL 58 and the very influential ALGOL 60, which quickly became the de facto worldwide standard for publishing algorithms. Backus developed the Backus\u2013Naur form (BNF), published in the UNESCO report on ALGOL 58. It was a formal notation able to describe any context-free programming language, and was important in the development of compilers. A few deviations from this approach were tried\u2014notably in Lisp and APL\u2014but by the 1970s, following the development of automated compiler generators such as yacc, Backus\u2013Naur context-free specifications for computer languages had become quite standard. This contribution helped Backus win the Turing Award in 1977.","completion":"Programming Languages"}
{"prompt":"Many other BASIC dialects have also sprung up since 1990, including the open source QB64 and FreeBASIC, inspired by QBasic, and the Visual Basic-styled RapidQ, HBasic, Basic For Qt and Gambas. Modern commercial incarnations include PureBasic, PowerBASIC, Xojo, Monkey X and True BASIC (the direct successor to Dartmouth BASIC from a company controlled by Kurtz).\nSeveral web-based simple BASIC interpreters also now exist, including Microsoft's Small Basic and Google's wwwBASIC. A number of compilers also exist that convert BASIC into JavaScript, such as JSBasic which re-implements Applesoft BASIC, Spider BASIC, and NS Basic.\nBuilding from earlier efforts such as Mobile Basic and CellularBASIC, many dialects are now available for smartphones and tablets. Through the Apple App Store for iOS options include Hand BASIC, Learn BASIC, Smart Basic based on Minimal BASIC, Basic! by\nmiSoft, and BASIC by Anastasia Kovba. The Google Play store for Android meanwhile has the touchscreen focused Touch Basic, B4A, the RFO BASIC! interpreter based on Dartmouth Basic, and adaptations of SmallBasic, BBC Basic, Tiny Basic, X11-Basic, and NS Basic.\nOn game consoles, an application for the Nintendo 3DS and Nintendo DSi called Petit Computer allows for programming in a slightly modified version of BASIC with DS button support. A version has also been released for Nintendo Switch, which has also been supplied a version of the Fuze Code System, a BASIC variant first implemented as a custom Raspberry Pi machine. Previously BASIC was made available on consoles as Family BASIC (for the Nintendo Famicom) and PSX Chipmunk Basic (for the original PlayStation), while yabasic was ported to the PlayStation 2 and FreeBASIC to the original Xbox, with Dragon BASIC created for homebrew on the Game Boy Advance and Nintendo DS.","completion":"Programming Languages"}
{"prompt":"The classic \"Hello, World!\" program can be written as follows:\n\nor as:\n\nA comment in Lua starts with a double-hyphen and runs to the end of the line, similar to Ada, Eiffel, Haskell, SQL and VHDL. Multi-line strings and comments are adorned with double square brackets.\n\nSingle line comment:Multi-line comment:The factorial function is implemented as a function in this example:","completion":"Programming Languages"}
{"prompt":"One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate run time support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End-users commonly use a Java Runtime Environment (JRE) installed on their device for standalone Java applications or a web browser for Java applets.\nStandard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.\nThe use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions made interpreted programs almost always run more slowly than native executables. Just-in-time (JIT) compilers that compile byte-codes to machine code during runtime were introduced from an early stage. Java's Hotspot compiler is actually two compilers in one; and with GraalVM (included in e.g. Java 11, but removed as of Java 16) allowing tiered compilation. Java itself is platform-independent and is adapted to the particular platform it is to run on by a Java virtual machine (JVM), which translates the Java bytecode into the platform's machine language.","completion":"Programming Languages"}
{"prompt":"Prolog is an untyped language. Attempts to introduce and extend Prolog with types began in the 1980s, and continue as of 2008. Type information is useful not only for type safety but also for reasoning about Prolog programs.","completion":"Programming Languages"}
{"prompt":"Swing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+, and Motif are supplied by Sun. Apple also provides an Aqua look and feel for macOS. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.","completion":"Programming Languages"}
{"prompt":"Curly-bracket or curly-brace programming languages have a syntax that defines statement blocks using the curly bracket or brace characters { and }. This syntax originated with BCPL (1966), and was popularized by C. Many curly-bracket languages descend from or are strongly influenced by C. Examples of curly-bracket languages include:","completion":"Programming Languages"}
{"prompt":"The use of Java-related technology in Android led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices. District Judge William Alsup ruled on May 31, 2012, that APIs cannot be copyrighted, but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014. On May 26, 2016, the district court decided in favor of Google, ruling the copyright infringement of the Java API in Android constitutes fair use. In March 2018, this ruling was overturned by the Appeals Court, which sent down the case of determining the damages to federal court in San Francisco.\nGoogle filed a petition for writ of certiorari with the Supreme Court of the United States in January 2019 to challenge the two rulings that were made by the Appeals Court in Oracle's favor. On April 5, 2021, the Court ruled 6-2 in Google's favor, that its use of Java APIs should be considered fair use. However, the court refused to rule on the copyrightability of APIs, choosing instead to determine their ruling by considering Java's API copyrightable \"purely for argument\u2019s sake.\"","completion":"Programming Languages"}
{"prompt":"Sometimes source code contains a novel or noteworthy solution to a specific problem. In such cases, comments may contain an explanation of the methodology. Such explanations may include diagrams and formal mathematical proofs. This may constitute explanation of the code, rather than a clarification of its intent; but others tasked with maintaining the code base may find such explanation crucial. This might especially be true in the case of highly specialized problem domains; or rarely used optimizations, constructs or function-calls.For example, a programmer may add a comment to explain why an insertion sort was chosen instead of a quicksort, as the former is, in theory, slower than the latter. This could be written as follows:","completion":"Programming Languages"}
{"prompt":"Various implementations of APL by APLX, Dyalog, et al., include extensions for object-oriented programming, support for .NET, XML-array conversion primitives, graphing, operating system interfaces, and lambda calculus expressions. Freeware versions include GNU APL for Linux and NARS2000 for Windows (which runs on Linux under Wine). Both of these are fairly complete versions of APL2 with various language extensions.","completion":"Programming Languages"}
{"prompt":"C has both directly and indirectly influenced many later languages such as C++ and Java. The most pervasive influence has been syntactical; all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models or large-scale program structures that differ from those of C, sometimes radically.\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\nWhen object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.The C++ programming language (originally named \"C with Classes\") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic\/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\nIn addition to C++ and Objective-C, Ch, Cilk, and Unified Parallel C are nearly supersets of C.","completion":"Programming Languages"}
{"prompt":"Many languages evaluate expressions inside switch blocks at runtime, allowing a number of less obvious uses for the construction. This prohibits certain compiler optimizations, so is more common in dynamic and scripting languages where the enhanced flexibility is more important than the performance overhead.","completion":"Programming Languages"}
{"prompt":"Python's statements include:\n\nThe assignment statement, using a single equals sign =\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if)\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block\nThe while statement, which executes a block of code as long as its condition is true\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups); it also ensures that clean-up code in a finally block is always run regardless of how the block exits\nThe raise statement, used to raise a specified exception or re-raise a caught exception\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming\nThe def statement, which defines a function or method\nThe with statement, which encloses a code block within a context manager (for example, acquiring a lock before it is run, then releasing the lock; or opening and closing a file), allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try\/finally idiom\nThe break statement, which exits a loop\nThe continue statement, which skips the rest of the current iteration and continues with the next\nThe del statement, which removes a variable\u2014deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined\nThe pass statement, serving as a NOP, syntactically needed to create an empty code block\nThe assert statement, used in debugging to check for conditions that should apply\nThe yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines\nThe return statement, used to return a value from a function\nThe import and from statements, used to import modules whose functions or variables can be used in the current programThe assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing\u2014in contrast to statically-typed languages, where each variable may contain only a value of a certain type.\nPython does not support tail call optimization or first-class continuations, and, according to Van Rossum, it never will. However, better support for coroutine-like functionality is provided by extending Python's generators. Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python 2.5 on, it is possible to pass data back into a generator function; and from version 3.3, it can be passed through multiple stack levels.","completion":"Programming Languages"}
{"prompt":"Both object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases do not store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides. One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.","completion":"Programming Languages"}
{"prompt":"During the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\".  This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO\/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO\/IEC JTC1\/SC22\/WG14.  National adoption of an update to the international standard typically occurs within a year of ISO publication.\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\nC89 is supported by current C compilers, and most modern C code is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits.  Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\nAfter the ANSI\/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO\/IEC 9899\/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.","completion":"Programming Languages"}
{"prompt":"Binary combinatory logic, otherwise known as binary lambda calculus, is designed from an algorithmic information theory perspective to allow for the densest possible code with the most minimal means, featuring a 29-byte self interpreter, a 21-byte prime number sieve, and a 112-byte Brainfuck interpreter.","completion":"Programming Languages"}
{"prompt":"R is an interpreted language; users can access it through a command-line interpreter. If a user types 2+2 at the R command prompt and presses enter, the computer replies with 4.\nR supports procedural programming with functions and, for some functions, object-oriented programming with generic functions. Due to its S heritage, R has stronger object-oriented programming facilities than most statistical computing languages. Extending it is facilitated by its lexical scoping rules, which are derived from Scheme. R uses S syntax (not to be confused with S-expressions) to represent both data and code. R's extensible object system includes objects for (among others): regression models, time-series and geo-spatial coordinates. Advanced users can write C, C++, Java, .NET or Python code to manipulate R objects directly.Functions are first-class objects and can be manipulated in the same way as data objects, facilitating meta-programming that allows multiple dispatch. Function arguments are passed by value, and are lazy\u2014that is to say, they are only evaluated when they are used, not when the function is called. A generic function acts differently depending on the classes of the arguments passed to it. In other words, the generic function dispatches the method implementation specific to that object's class. For example, R has a generic print function that can print almost every class of object in R with print(objectname). R is highly extensible through the use of packages for specific functions and specific applications.","completion":"Programming Languages"}
{"prompt":"Literate programming was first introduced in 1984 by Donald Knuth, who intended it to create programs that were suitable literature for human beings. He implemented it at Stanford University as a part of his research on algorithms and digital typography. The implementation was called \"WEB\" since he believed that it was one of the few three-letter words of English that had not yet been applied to computing. However, it resembles the complicated nature of software delicately pieced together from simple materials. The practice of literate programming has seen an important resurgence in the 2010s with the use of computational notebooks, especially in data science.","completion":"Programming Languages"}
{"prompt":"The ability to provide a design of different levels of abstraction can\n\nsimplify the design considerably\nenable different role players to effectively work at various levels of abstraction\nsupport the portability of software artifacts (model-based ideally)Systems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.\nLayered architecture partitions the concerns of the application into stacked groups (layers).\nIt is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.","completion":"Programming Languages"}
{"prompt":"The term real-time derives from its use in early simulation, in which a real-world process is simulated at a rate that matched that of the real process (now called real-time simulation to avoid ambiguity). Analog computers, most often, were capable of simulating at a much faster pace than real-time, a situation that could be just as dangerous as a slow simulation if it were not also recognized and accounted for. \nMinicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG (Digital on-screen graphic) scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data and so operating systems such as Data General's RDOS (Real-Time Disk Operating System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads\/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOS and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.\n\nOnce when the MOS Technology 6502 (used in the Commodore 64 and Apple II), and later when the Motorola 68000 (used in the Macintosh, Atari ST, and Amiga) were popular, anybody could use their home computer as a real-time system. The possibility to deactivate other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus surpassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. The Motorola 68000 and subsequent family members (68010, 68020 etc.) also became popular with manufacturers of industrial control systems. This application area is one in which real-time control offers genuine advantages in terms of process performance and safety.","completion":"Programming Languages"}
{"prompt":"JIT compilation can be applied to some programs, or can be used for certain capacities, particularly dynamic capacities such as regular expressions. For example, a text editor may compile a regular expression provided at runtime to machine code to allow faster matching: this cannot be done ahead of time, as the pattern is only provided at runtime. Several modern runtime environments rely on JIT compilation for high-speed code execution, including most implementations of Java, together with Microsoft's .NET. Similarly, many regular-expression libraries feature JIT compilation of regular expressions, either to bytecode or to machine code. JIT compilation is also used in some emulators, in order to translate machine code from one CPU architecture to another.\nA common implementation of JIT compilation is to first have AOT compilation to bytecode (virtual machine code), known as bytecode compilation, and then have JIT compilation to machine code (dynamic compilation), rather than interpretation of the bytecode. This improves the runtime performance compared to interpretation, at the cost of lag due to compilation. JIT compilers translate continuously, as with interpreters, but caching of compiled code minimizes lag on future execution of the same code during a given run. Since only part of the program is compiled, there is significantly less lag than if the entire program were compiled prior to execution.","completion":"Programming Languages"}
{"prompt":"The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:\nSpeakeasy, developed in 1964 at Argonne National Laboratory (ANL) by Stanley Cohen, is an object-oriented programming system (OOPS), much like the later MATLAB, IDL and Mathematica numerical package. Speakeasy has a clear Fortran foundation syntax. It first addressed efficient physics computing internally at ANL, was modified for research use (as \"Modeleasy\") for the Federal Reserve Board in the early 1970s and then was made available commercially; Speakeasy and Modeleasy are still in use.\nSimula, invented in the late 1960s by Nygaard and Dahl as a superset of ALGOL 60, was the first language designed to support object-oriented programming.\nFORTH, the earliest concatenative programming language was designed by Charles Moore in 1969 as a personal development system while at the National Radio Astronomy Observatory (NRAO).\nC, an early systems programming language, was developed by Dennis Ritchie and Ken Thompson at Bell Labs between 1969 and 1973.\nSmalltalk (mid-1970s) provided a complete ground-up design of an object-oriented language.\nProlog, designed in 1972 by Alain Colmerauer, Phillipe Roussel, and Robert Kowalski, was the first logic programming language.\nML built a polymorphic type system (invented by Robin Milner in 1973) on Lisp, pioneering statically typed functional programming languages. Each of these languages spawned an entire family of descendants, and most modern languages count at least one of them in their ancestry.The 1960s and 1970s also saw considerable debate over the merits of \"structured programming\", which essentially meant programming without the use of goto. A significant fraction of programmers believed that, even in languages that provide goto, it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages had no goto, which forced the use of structured programming.\nTo provide even faster compile times, some languages were structured for \"one-pass compilers\" which expect subordinate routines to be defined first, as with Pascal, where the main routine, or driver function, is the final section of the program listing.\nSome notable languages that were developed in this period include:","completion":"Programming Languages"}
{"prompt":"One of the first uses of the term protocol in a data-commutation context occurs in a memorandum entitled A Protocol for Use in the NPL Data Communications Network written by Roger Scantlebury and Keith Bartlett in April 1967.On the ARPANET, the starting point for host-to-host communication in 1969 was the 1822 protocol, which defined the transmission of messages to an IMP. The Network Control Protocol (NCP) for the ARPANET was first implemented in 1970. The NCP interface allowed application software to connect across the ARPANET by implementing higher-level communication protocols, an early example of the protocol layering concept.Networking research in the early 1970s by Robert E. Kahn and Vint Cerf led to the formulation of the Transmission Control Program (TCP). Its RFC 675 specification was written by Cerf with Yogen Dalal and Carl Sunshine in December 1974, still a monolithic design at this time.\nThe International Networking Working Group agreed on a connectionless datagram standard which was presented to the CCITT in 1975 but was not adopted by the CCITT or by the ARPANET. International research, particularly the work of R\u00e9mi Despr\u00e9s, contributed to the development of the X.25 standard, based on virtual circuits by the CCITT in 1976. Computer manufacturers developed proprietary protocols such as IBM's Systems Network Architecture (SNA), Digital Equipment Corporation's DECnet and Xerox Network Systems.TCP software was redesigned as a modular protocol stack. Originally referred to as IP\/TCP, it was installed on SATNET in 1982 and on the ARPANET in January 1983. The development of a complete protocol suite by 1989, as outlined in RFC 1122 and RFC 1123, laid the foundation for the growth of TCP\/IP as a comprehensive protocol suite as the core component of the emerging Internet.International work on a reference model for communication standards led to the OSI model, published in 1984. For a period in the late 1980s and early 1990s, engineers, organizations and nations became polarized over the issue of which standard, the OSI model or the Internet protocol suite, would result in the best and most robust computer networks.","completion":"Programming Languages"}
{"prompt":"The ALGOLs were conceived at a time when character sets were diverse and evolving rapidly; also, the ALGOLs were defined so that only uppercase letters were required.\n1960: IFIP \u2013 The Algol 60 language and report included several mathematical symbols which are available on modern computers and operating systems, but, unfortunately, were unsupported on most computing systems at the time. For instance: \u00d7, \u00f7, \u2264, \u2265, \u2260, \u00ac, \u2228, \u2227, \u2282, \u2261, \u2423 and \u23e8.\n1961 September: ASCII \u2013 The ASCII character set, then in an early stage of development, had the \\ (Back slash) character added to it in order to support ALGOL's boolean operators \/\\ and \\\/.1962: ALCOR \u2013 This character set included the unusual \"\u16ed\" runic cross character for multiplication and the \"\u23e8\" Decimal Exponent Symbol for floating point notation.1964: GOST \u2013 The 1964 Soviet standard GOST 10859 allowed the encoding of 4-bit, 5-bit, 6-bit and 7-bit characters in ALGOL.1968: The \"Algol 68 Report\" \u2013 used extant ALGOL characters, and further adopted \u2192, \u2193, \u2191, \u25a1, \u230a, \u2308, \u23a9, \u23a7, \u25cb, \u22a5, and \u00a2 characters which can be found on the IBM 2741 keyboard with typeball (or golf ball) print heads inserted (such as the APL golf ball). These became available in the mid-1960s while ALGOL 68 was being drafted.  The report was translated into Russian, German, French, and Bulgarian, and allowed programming in languages with larger character sets, e.g., Cyrillic alphabet of the Soviet BESM-4. All ALGOL's characters are also part of the Unicode standard and most of them are available in several popular fonts.\n2009 October: Unicode \u2013 The \u23e8 (Decimal Exponent Symbol) for floating point notation was added to Unicode 5.2 for backward compatibility with historic Buran programme ALGOL software.","completion":"Programming Languages"}
{"prompt":"Before joining the University of Oxford in 2014, and Microsoft Research in Cambridge, UK in 1997, he worked for Bell Labs and Digital Equipment Corporation, and contributed to Unix software including vismon.","completion":"Programming Languages"}
{"prompt":"The canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols \"(\" and \")\" and one nonterminal symbol S. The production rules are\n\nS \u2192 SS,\nS \u2192 (S),\nS \u2192 ()The first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.","completion":"Programming Languages"}
{"prompt":"Gherkin is a language designed to define test cases to check the behavior of software, without specifying how that behavior is implemented. It is meant to be read and used by non-technical users using a natural language syntax and a line-oriented design. The tests defined with Gherkin must then be implemented in a general programming language. Then, the steps in a Gherkin program acts as a syntax for method invocation accessible to non-developers.","completion":"Programming Languages"}
{"prompt":"During the late 1960s, high-level languages such as PL\/S, BLISS, BCPL, extended ALGOL (for Burroughs large systems) and C included some degree of access to low-level programming functions. One method for this is inline assembly, in which assembly code is embedded in a high-level language that supports this feature. Some of these languages also allow architecture-dependent compiler optimization directives to adjust the way a compiler uses the target processor architecture.","completion":"Programming Languages"}
{"prompt":"A self-interpreter is a programming language interpreter written in a programming language which can interpret itself; an example is a BASIC interpreter written in BASIC. Self-interpreters are related to self-hosting compilers.\nIf no compiler exists for the language to be interpreted, creating a self-interpreter requires the implementation of the language in a host language (which may be another programming language or assembler). By having a first interpreter such as this, the system is bootstrapped and new versions of the interpreter can be developed in the language itself. It was in this way that Donald Knuth developed the TANGLE interpreter for the language WEB of the industrial standard TeX typesetting system.\nDefining a computer language is usually done in relation to an abstract machine (so-called operational semantics) or as a mathematical function (denotational semantics). A language may also be defined by an interpreter in which the semantics of the host language is given. The definition of a language by a self-interpreter is not well-founded (it cannot define a language), but a self-interpreter tells a reader about the expressiveness and elegance of a language. It also enables the interpreter to interpret its source code, the first step towards reflective interpreting.\nAn important design dimension in the implementation of a self-interpreter is whether a feature of the interpreted language is implemented with the same feature in the interpreter's host language. An example is whether a closure in a Lisp-like language is implemented using closures in the interpreter language or implemented \"manually\" with a data structure explicitly storing the environment. The more features implemented by the same feature in the host language, the less control the programmer of the interpreter has; a different behavior for dealing with number overflows cannot be realized if the arithmetic operations are delegated to corresponding operations in the host language.\nSome languages such as Lisp and Prolog have elegant self-interpreters. Much research on self-interpreters (particularly reflective interpreters) has been conducted in the Scheme programming language, a dialect of Lisp. In general, however, any Turing-complete language allows writing of its own interpreter. Lisp is such a language, because Lisp programs are lists of symbols and other lists. XSLT is such a language, because XSLT programs are written in XML. A sub-domain of metaprogramming is the writing of domain-specific languages (DSLs).\nClive Gifford introduced a measure quality of self-interpreter (the eigenratio), the limit of the ratio between computer time spent running a stack of N self-interpreters and time spent to run a stack of N \u2212 1 self-interpreters as N goes to infinity. This value does not depend on the program being run.\nThe book Structure and Interpretation of Computer Programs presents examples of meta-circular interpretation for Scheme and its dialects. Other examples of languages with a self-interpreter are Forth and Pascal.","completion":"Programming Languages"}
{"prompt":"Macro languages transform one source code file into another. A \"macro\" is essentially a short piece of text that expands into a longer one (not to be confused with hygienic macros), possibly with parameter substitution. They are often used to preprocess source code. Preprocessors can also supply facilities like file inclusion.\nMacro languages may be restricted to acting on specially labeled code regions (pre-fixed with a # in the case of the C preprocessor). Alternatively, they may not, but in this case it is still often undesirable to (for instance) expand a macro embedded in a string literal, so they still need a rudimentary awareness of syntax. That being the case, they are often still applicable to more than one language. Contrast with source-embeddable languages like PHP, which are fully featured.\n\ncpp (the C preprocessor)\nm4 (originally from AT&T, bundled with Unix)\nML\/I (general-purpose macro processor)","completion":"Programming Languages"}
{"prompt":"In the 1970s the US Department of Defense (DoD) became concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence's requirements. After many iterations beginning with an original straw-man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.\nHOLWG crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.\n\nRequests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (Honeywell, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at Honeywell, was chosen and given the name Ada\u2014after Augusta Ada, Countess of Lovelace. This proposal was influenced by the language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook.Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not only defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain: Ada and Lisp.  Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive.   Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required Ada Compiler Validation Capability (ACVC) validation suite that was required in another novel feature of the Ada language effort. The Jargon File, a dictionary of computer hacker slang originating in 1975\u20131983, notes in an entry on Ada that \"it is precisely what one might expect given that kind of endorsement by fiat; designed by committee...difficult to use, and overall a disastrous, multi-billion-dollar boondoggle...Ada Lovelace...would almost certainly blanch at the use her name has been latterly put to; the kindest thing that has been said about it is that there is probably a good small language screaming to get out from inside its vast, elephantine bulk.\"The first validated Ada implementation was the NYU Ada\/Ed translator, certified on April 11, 1983. NYU Ada\/Ed is implemented in the high-level set language SETL. Several commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, Irvine Compiler, TLD Systems, and Verdix. Computer manufacturers who had a significant business in the defense, aerospace, or related industries, also offered Ada compilers and tools on their platforms; these included Concurrent Computer Corporation, Cray Research, Inc., Digital Equipment Corporation, Harris Computer Systems, and Siemens Nixdorf Informationssysteme AG.In 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software, though exceptions to this rule were often granted.  The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace commercial off-the-shelf (COTS) technology. Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada.By the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to fully exploiting Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.\nFor example, the Airplane Information Management System, the fly-by-wire system software in the Boeing 777, was written in Ada.  Developed by Honeywell Air Transport Systems in collaboration with consultants from DDC-I, it became arguably the best-known of any Ada project, civilian or military. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK's next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.\nIt is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.","completion":"Programming Languages"}
{"prompt":"Javadoc is a comprehensive documentation system, created by Sun Microsystems. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are \/** and *\/, whereas the normal multi-line comments in Java are delimited by \/* and *\/, and single-line comments start with \/\/.","completion":"Programming Languages"}
{"prompt":"BNF's syntax itself may be represented with a BNF like the following:\n\nNote that \"\" is the empty string.\nThe original BNF did not use quotes as shown in <literal> rule. This assumes that no whitespace is necessary for proper interpretation of the rule.\n<EOL> represents the appropriate line-end specifier (in ASCII, carriage-return, line-feed or both depending on the operating system). <rule-name> and <text> are to be substituted with a declared rule's name\/label or literal text, respectively.\nIn the U.S. postal address example above, the entire block-quote is a syntax.  Each line or unbroken grouping of lines is a rule; for example one rule begins with <name-part> ::=. The other part of that rule (aside from a line-end) is an expression, which consists of two lists separated by a pipe |.  These two lists consists of some terms (three terms and two terms, respectively).  Each term in this particular rule is a rule-name.","completion":"Programming Languages"}
{"prompt":"Historically, C was sometimes used for web development using the Common Gateway Interface (CGI) as a \"gateway\" for information between the web application, the server, and the browser. C may have been chosen over interpreted languages because of its speed, stability, and near-universal availability.  It is no longer common practice for web development to be done in C, and many other web development tools exist.","completion":"Programming Languages"}
{"prompt":"An example of a Hello world program in Simula:\n\nBegin\n   OutText (\"Hello, World!\");\n   Outimage;\nEnd;\n\nSimula is case-insensitive.","completion":"Programming Languages"}
{"prompt":"The C standard was further revised in the late 1990s, leading to the publication of ISO\/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.C99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with \/\/, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.In addition, the C99 standard requires support for Unicode identifiers in the form of escaped characters (e.g. \\u0040 or \\U0001f431) and suggests support for raw Unicode names.","completion":"Programming Languages"}
{"prompt":"The character class is the most basic regex concept after a literal match. It makes one small sequence of characters match a larger set of characters. For example, [A-Z] could stand for any uppercase letter in the English alphabet, and \\d could mean any digit. Character classes apply to both POSIX levels.\nWhen specifying a range of characters, such as [a-Z] (i.e. lowercase a to uppercase Z), the computer's locale settings determine the contents by the numeric ordering of the character encoding. They could store digits in that sequence, or the ordering could be abc\u2026zABC\u2026Z, or aAbBcC\u2026zZ. So the POSIX standard defines a character class, which will be known by the regex processor installed. Those definitions are in the following table:\n\nPOSIX character classes can only be used within bracket expressions. For example, [[:upper:]ab] matches the uppercase letters and lowercase \"a\" and \"b\".\nAn additional non-POSIX class understood by some tools is [:word:], which is usually defined as [:alnum:] plus underscore. This reflects the fact that in many programming languages these are the characters that may be used in identifiers. The editor Vim further distinguishes word and word-head classes (using the notation \\w and \\h) since in many programming languages the characters that can begin an identifier are not the same as those that can occur in other positions: numbers are generally excluded, so an identifier would look like \\h\\w* or [[:alpha:]_][[:alnum:]_]* in POSIX notation.\nNote that what the POSIX regex standards call character classes are commonly referred to as POSIX character classes in other regex flavors which support them. With most other regex flavors, the term character class is used to describe what POSIX calls bracket expressions.","completion":"Programming Languages"}
{"prompt":"A parse tree is made up of nodes and branches. In the picture the parse tree is the entire structure, starting from S and ending in each of the leaf nodes (John, ball, the, hit). In a parse tree, each node is either a root node, a branch node, or a leaf node. In the above example, S is a root node, NP and VP are branch nodes, while John, ball, the, and hit are all leaf nodes.\nNodes can also be referred to as parent nodes and child nodes. A parent node is one which has at least one other node linked by a branch under it. In the example, S is a parent of both NP and VP. A child node is one which has at least one node directly above it to which it is linked by a branch of the tree. Again from our example, hit is a child node of V.\nA nonterminal function is a function (node) which is either a root or a branch in that tree whereas a terminal function is a function (node) in a parse tree which is a leaf.\nFor binary trees (where each parent node has two immediate child nodes), the number of \npossible parse trees for a sentence with n words is given by the Catalan number \n  \n    \n      \n        \n          C\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle C_{n}}\n  .","completion":"Programming Languages"}
{"prompt":"Because of its expressive power and (relative) ease of reading, many other utilities and programming languages have adopted syntax similar to Perl's\u2014for example, Java, JavaScript, Julia, Python, Ruby, Qt, Microsoft's .NET Framework, and XML Schema. Some languages and tools such as Boost and PHP support multiple regex flavors. Perl-derivative regex implementations are not identical and usually implement a subset of features found in Perl 5.0, released in 1994. Perl sometimes does incorporate features initially found in other languages. For example, Perl 5.10 implements syntactic extensions originally developed in PCRE and Python.","completion":"Programming Languages"}
{"prompt":"Using Greibach's theorem, it can be shown that the two following problems are undecidable:\n\nGiven a context-sensitive grammar, does it describe a context-free language?\nGiven a context-free grammar, does it describe a regular language?","completion":"Programming Languages"}
{"prompt":"Dataflow programming languages rely on a (usually visual) representation of the flow of data to specify the program.  Frequently used for reacting to discrete events or for processing streams of data.  Examples of dataflow languages include:","completion":"Programming Languages"}
{"prompt":"ALGOL 60 as officially defined had no I\/O facilities; implementations defined their own in ways that were rarely compatible with each other. In contrast, ALGOL 68 offered an extensive library of transput (input\/output) facilities.\nALGOL 60 allowed for two evaluation strategies for parameter passing: the common call-by-value, and call-by-name. Call-by-name has certain effects in contrast to call-by-reference. For example, without specifying the parameters as value or reference, it is impossible to develop a procedure that will swap the values of two parameters if the actual parameters that are passed in are an integer variable and an array that is indexed by that same integer variable. Think of passing a pointer to swap(i, A[i]) in to a function. Now that every time swap is referenced, it is reevaluated. Say i := 1 and A[i] := 2, so every time swap is referenced it will return the other combination of the values ([1,2], [2,1], [1,2] and so on). A similar situation occurs with a random function passed as actual argument.\nCall-by-name is known by many compiler designers for the interesting \"thunks\" that are used to implement it. Donald Knuth devised the \"man or boy test\" to separate compilers that correctly implemented \"recursion and non-local references.\" This test contains an example of call-by-name.\nALGOL 68 was defined using a two-level grammar formalism invented by Adriaan van Wijngaarden and which bears his name. Van Wijngaarden grammars use a context-free grammar to generate an infinite set of productions that will recognize a particular ALGOL 68 program; notably, they are able to express the kind of requirements that in many other programming language standards are labelled \"semantics\" and have to be expressed in ambiguity-prone natural language prose, and then implemented in compilers as ad hoc code attached to the formal language parser.","completion":"Programming Languages"}
{"prompt":"Amsterdam Compiler Kit (ACK) Modula-2 \u2013 for MINIX; freeware\nADW Modula-2 \u2013 for Windows, ISO compliant, ISO\/IEC 10514-1, ISO\/IEC 10514-2 (OO extension), ISO\/IEC 10514-3 (Generic extension); freeware\nAglet Modula-2 \u2013 for AmigaOS 4.0 for PowerPC; freeware\nFitted Software Tools (FST) Modula-2 \u2013 for DOS; freeware\nGardens Point Modula-2 (GPM) \u2013 for BSD, Linux, OS\/2, Solaris; ISO compliant; freeware, as of 30 July 2014\nGardens Point Modula-2 (GPM\/CLR) \u2013 for .NET Framework; freeware\nGNU Modula-2 \u2013 for GCC platforms, version 1.0 released 11 December 2010; compliance: PIM2, PIM3, PIM4, ISO; free software, GNU General Public License (GPL)\nLogitech SA - they also had a \"Real Time Kernel\" for embedded usage (1987)\nM2Amiga \u2013 for Amiga; free software\nM2M \u2013 by N. Wirth and collaborators from ETH Zurich, cross-platform, generates M-code for virtual machine; freeware\nMacMETH \u2013 by N. Wirth and collaborators from ETH Zurich for Macintosh, Classic only; freeware\nMod51 \u2013 for the Intel 80x51 microcontroller family, ISO compliant, IEC1132 extensions; proprietary software\nMegamax Modula-2 \u2013 for Atari ST with documentation in German only; freeware\nModula-2 R10 \u2013 reference compiler for this Modula; open-source, peer review\nModulaWare \u2013 for OpenVMS (VAX and Alpha), ISO compliant; proprietary software\nORCA\/Modula-2 \u2013 for Apple IIGS by The Byte Works for the Apple Programmer's Workshop\np1 Modula-2 \u2013 for Macintosh, Classic and macOS (PowerPC and Carbon (API) only), ISO compliant; proprietary software\nMOCKA \u2013 for various platforms, PIM compliant; commercial, freeware Linux\/BSD versions\nTDI Modula-2 \u2013 for Atari ST, by TDI Software\nTerra M2VMS \u2013 for OpenVMS (VAX and Alpha), PIM compliant; proprietary software\nm2c, Ulm Modula-2 System \u2013 for Solaris (Sun SPARC and Motorola 68k); free software, GNU General Public License (GPL)\nXDS \u2013 ISO compliant, TopSpeed compatible library: Native XDS-x86 for x86 (Windows and Linux); XDS-C for Windows and Linux (16- and 32-bit versions), targets C (K&R & ANSI); freeware","completion":"Programming Languages"}
{"prompt":"Some programming language researchers criticise the notion of paradigms as a classification of programming languages, e.g. Harper, and Krishnamurthi.  They argue that many programming languages cannot be strictly classified into one paradigm, but rather include features from several paradigms. See Comparison of multi-paradigm programming languages.","completion":"Programming Languages"}
{"prompt":"One classification of compilers is by the platform on which their generated code executes. This is known as the target platform.\nA native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.\nThe output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers.\nThe lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.\nWhile a common compiler type outputs machine code, there are many other types:\n\nSource-to-source compilers are a type of compiler that takes a high-level language as its input and outputs a high-level language. For example, an automatic parallelizing compiler will frequently take in a high-level language program as an input and then transform the code and annotate it with parallel code annotations (e.g. OpenMP) or language constructs (e.g. Fortran's DOALL statements). Other terms for a source-to-source compiler are transcompiler or transpiler.\nBytecode compilers compile to assembly language of a theoretical machine, like some Prolog implementations\nThis Prolog machine is also known as the Warren Abstract Machine (or WAM).\nBytecode compilers for Java, Python are also examples of this category.\nJust-in-time compilers (JIT compiler) defer compilation until runtime. JIT compilers exist for many modern languages including Python, JavaScript, Smalltalk, Java, Microsoft .NET's Common Intermediate Language (CIL) and others.  A JIT compiler generally runs inside an interpreter.  When the interpreter detects that a code path is \"hot\", meaning it is executed frequently, the JIT compiler will be invoked and compile the \"hot\" code for increased performance.\nFor some languages, such as Java, applications are first compiled using a bytecode compiler and delivered in a machine-independent intermediate representation.  A bytecode interpreter executes the bytecode, but the JIT compiler will translate the bytecode to machine code when increased performance is necessary.\nHardware compilers (also known as synthesis tools) are compilers whose input is a hardware description language and whose output is a description, in the form of a netlist or otherwise, of a hardware configuration.\nThe output of these compilers target computer hardware at a very low level, for example a field-programmable gate array (FPGA) or structured application-specific integrated circuit (ASIC). Such compilers are said to be hardware compilers, because the source code they compile effectively controls the final configuration of the hardware and how it operates. The output of the compilation is only an interconnection of transistors or lookup tables.\nAn example of hardware compiler is XST, the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other hardware vendors.\nAn assembler is a program that compiles human readable assembly language to machine code, the actual instructions executed by hardware.  The inverse program that translates machine code to assembly language is called a disassembler.\nA program that translates from a low-level language to a higher level one is a decompiler.\nA program that translates into an object code format that is not supported on the compilation machine is called a cross compiler and is commonly used to prepare code for execution on embedded software applications.\nA program that rewrites object code back into the same type of object code while applying optimisations and transformations is a binary recompiler.","completion":"Programming Languages"}
{"prompt":"The map function performs a function call on each element of a list. The following example squares every element in an array with an anonymous function.\n\nThe anonymous function accepts an argument and multiplies it by itself (squares it). The above form is discouraged by the creators of the language, who maintain that the form presented below has the same meaning and is more aligned with the philosophy of the language:","completion":"Programming Languages"}
{"prompt":"Like most modern programming languages and unlike earlier Lisps such as Maclisp, Scheme is lexically scoped: all possible variable bindings in a program unit can be analyzed by reading the text of the program unit without consideration of the contexts in which it may be called. This contrasts with dynamic scoping which was characteristic of early Lisp dialects, because of the processing costs associated with the primitive textual substitution methods used to implement lexical scoping algorithms in compilers and interpreters of the day. In those Lisps, it was perfectly possible for a reference to a free variable inside a procedure to refer to quite distinct bindings external to the procedure, depending on the context of the call.\nThe impetus to incorporate lexical scoping, which was an unusual scoping model in the early 1970s, into their new version of Lisp, came from Sussman's studies of ALGOL. He suggested that ALGOL-like lexical scoping mechanisms would help to realize their initial goal of implementing Hewitt's Actor model in Lisp.The key insights on how to introduce lexical scoping into a Lisp dialect were popularized in Sussman and Steele's 1975 Lambda Paper, \"Scheme: An Interpreter for Extended Lambda Calculus\", where they adopted the concept of the lexical closure (on page 21), which had been described in an AI Memo in 1970 by Joel Moses, who attributed the idea to Peter J. Landin.","completion":"Programming Languages"}
{"prompt":"Using the function keyword:\n\nOr using an arrow function:\n\nCFML supports any statements within the function's definition, not simply expressions.\nCFML supports recursive anonymous functions:\n\nCFML anonymous functions implement closure.","completion":"Programming Languages"}
{"prompt":"For a variety of reasons, one might wish to describe the relationships between different formal semantics.  For example:\n\nTo prove that a particular operational semantics for a language satisfies the logical formulas of an axiomatic semantics for that language.  Such a proof demonstrates that it is \"sound\" to reason about a particular (operational) interpretation strategy using a particular (axiomatic) proof system.\nTo prove that operational semantics over a high-level machine is related by a simulation with the semantics over a low-level machine, whereby the low-level abstract machine contains more primitive operations than the high-level abstract machine definition of a given language. Such a proof demonstrates that the low-level machine \"faithfully implements\" the high-level machine.It is also possible to relate multiple semantics through abstractions via the theory of abstract interpretation.","completion":"Programming Languages"}
{"prompt":"R was started by professors Ross Ihaka and Robert Gentleman as a programming language to teach introductory statistics at the University of Auckland. The language took heavy inspiration from the S programming language with most S programs able to run unaltered in R as well as from Scheme's lexical scoping allowing for local variables. The name of the language comes from being an S language successor and the shared first letter of the authors, Ross and Robert. Ihaka and Gentleman first shared binaries of R on the data archive StatLib and the s-news mailing list in August 1993. In June 1995, statistician Martin M\u00e4chler convinced Ihaka and Gentleman to make R free and open-source under the GNU General Public License. Mailing lists for the R project began on 1 April 1997 preceding the release of version 0.50. R officially became a GNU project on 5 December 1997 when version 0.60 released. The first official 1.0 version was released on 29 February 2000.The Comprehensive R Archive Network (CRAN) was founded in 1997 by Kurt Hornik and Fritz Leisch to host R's source code, executable files, documentation, and user-created packages. Its name and scope mimics the Comprehensive TeX Archive Network and the Comprehensive Perl Archive Network. CRAN originally had three mirrors and 12 contributed packages. As of December 2022, it has 103 mirrors and 18,976 contributed packages.The R Core Team was formed in 1997 to further develop the language. As of January 2022, it consists of Chambers, Gentleman, Ihaka, and M\u00e4chler, plus statisticians Douglas Bates, Peter Dalgaard, Kurt Hornik, Michael Lawrence, Friedrich Leisch, Uwe Ligges, Thomas Lumley, Sebastian Meyer, Paul Murrell, Martyn Plummer, Brian Ripley, Deepayan Sarkar, Duncan Temple Lang, Luke Tierney, and Simon Urbanek, as well as computer scientist Tomas Kalibera. Stefano Iacus, Guido Masarotto, Heiner Schwarte, Seth Falcon, Martin Morgan, and Duncan Murdoch were members. In April 2003, the R Foundation was founded as a non-profit organization to provide further support for the R project.","completion":"Programming Languages"}
{"prompt":"Tables are the most important data structures (and, by design, the only built-in composite data type) in Lua and are the foundation of all user-created types. They are associative arrays with addition of automatic numeric key and special syntax.\nA table is a collection of key and data pairs, where the data is referenced by key; in other words, it is a hashed heterogeneous associative array.\nTables are created using the {} constructor syntax.\n\nTables are always passed by reference (see Call by sharing).\nA key (index) can be any value except nil and NaN, including functions.\n\nA table is often used as structure (or record) by using strings as keys. Because such use is very common, Lua features a special syntax for accessing such fields.\n\nBy using a table to store related functions, it can act as a namespace.\n\nTables are automatically assigned a numerical key, enabling them to be used as an array data type. The first automatic index is 1 rather than 0 as it is for many other programming languages (though an explicit index of 0 is allowed).\nA numeric key 1 is distinct from a string key \"1\".\n\nThe length of a table t is defined to be any integer index n such that t[n] is not nil and t[n+1] is nil; moreover, if t[1] is nil, n can be zero. For a regular array, with non-nil values from 1 to a given n, its length is exactly that n, the index of its last value. If the array has \"holes\" (that is, nil values between other non-nil values), then #t can be any of the indices that directly precedes a nil value (that is, it may consider any such nil value as the end of the array).\n\nA table can be an array of objects.\n\nUsing a hash map to emulate an array is normally slower than using an actual array; however, Lua tables are optimized for use as arrays to help avoid this issue.","completion":"Programming Languages"}
{"prompt":"The R Journal is an open access, refereed journal of the R project. It features short to medium-length articles on the use and development of R, including packages, programming tips, CRAN news, and foundation news.","completion":"Programming Languages"}
{"prompt":"Scratch is a block-based educational language. The text of the blocks is translated into many languages, and users can select different translations. Unicode characters are supported in variable and list names. (Scratch lists are not stored inside variables the way arrays or lists are handled in most languages. Variables only store strings, numbers, and, with workarounds, boolean values, while lists are a separate data type that store sequences of these values.) Projects can be \"translated\" by simply changing the language of the editor, although this does not translate the variable names.","completion":"Programming Languages"}
{"prompt":"Standards for some Lisp-derived programming languages include a specification for their S-expression syntax.  These include Common Lisp (ANSI standard document ANSI INCITS 226-1994 (R2004)), Scheme (R5RS and R6RS), and ISLISP.","completion":"Programming Languages"}
{"prompt":"Transaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.","completion":"Programming Languages"}
{"prompt":"To date there have been at least 70 augmentations, extensions, derivations and sublanguages of ALGOL 60.\nThe Burroughs dialects included special system programming dialects such as ESPOL and NEWP.","completion":"Programming Languages"}
{"prompt":"Computing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others. The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.\nA central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.\nWithin a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.\nSome abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky \u2013 that they can never completely hide the details below; however, this does not negate the usefulness of abstraction.\nSome abstractions are designed to inter-operate with other abstractions \u2013 for example, a programming language may contain a foreign function interface for making calls to the lower-level language.","completion":"Programming Languages"}
{"prompt":"LOLCODE is designed to resemble the speech of lolcats. The following is the \"Hello World\" example:\n\nHAI\nCAN HAS STDIO?\nVISIBLE \"HAI WORLD!\"\nKTHXBYE\n\nWhile the semantics of LOLCODE is not unusual, its syntax has been described as a linguistic phenomenon, representing an unusual example of informal speech and internet slang in programming.","completion":"Programming Languages"}
{"prompt":"It is certainly possible to represent hardware semantics using traditional programming languages such as C++, which operate on control flow semantics as opposed to data flow, although to function as such, programs must be augmented with extensive and unwieldy class libraries. Generally, however, software programming languages do not include any capability for explicitly expressing time, and thus cannot function as hardware description languages. Before the introduction of System Verilog in 2002, C++ integration with a logic simulator was one of the few ways to use object-oriented programming in hardware verification. System Verilog is the first major HDL to offer object orientation and garbage collection.\nUsing the proper subset of hardware description language, a program called a synthesizer, or logic synthesis tool, can infer hardware logic operations from the language statements and produce an equivalent netlist of generic hardware primitives to implement the specified behaviour. Synthesizers generally ignore the expression of any timing constructs in the text. Digital logic synthesizers, for example, generally use clock edges as the way to time the circuit, ignoring any timing constructs. The ability to have a synthesizable subset of the language does not itself make a hardware description language.","completion":"Programming Languages"}
{"prompt":"Scheme supports delayed evaluation through the delay form and the procedure force.\n\nThe lexical context of the original definition of the promise is preserved, and its value is also preserved after the first use of force. The promise is only ever evaluated once.\nThese primitives, which produce or handle values known as promises, can be used to implement advanced lazy evaluation constructs such as streams.In the R6RS standard, these are no longer primitives, but instead, are provided as part of the R5RS compatibility library (rnrs r5rs (6)).\nIn R5RS, a suggested implementation of delay and force is given, implementing the promise as a procedure with no arguments (a thunk) and using memoization to ensure that it is only ever evaluated once, irrespective of the number of times force is called (R5RS sec. 6.4).SRFI 41 enables the expression of both finite and infinite sequences with extraordinary economy. For example, this is a definition of the Fibonacci sequence using the functions defined in SRFI 41:","completion":"Programming Languages"}
{"prompt":"A more realistic example with use of classes,:\u200a1.3.3,\u200a2\u200a subclasses:\u200a2.2.1\u200a and virtual procedures::\u200a2.2.3\u200a\nBegin\n   Class Glyph;\n      Virtual: Procedure print Is Procedure print;;\n   Begin\n   End;\n   \n   Glyph Class Char (c);\n      Character c;\n   Begin\n      Procedure print;\n        OutChar(c);\n   End;\n   \n   Glyph Class Line (elements);\n      Ref (Glyph) Array elements;\n   Begin\n      Procedure print;\n      Begin\n         Integer i;\n         For i:= 1 Step 1 Until UpperBound (elements, 1) Do\n            elements (i).print;\n         OutImage;\n      End;\n   End;\n   \n   Ref (Glyph) rg;\n   Ref (Glyph) Array rgs (1 : 4);\n   \n   ! Main program;\n   rgs (1):- New Char ('A');\n   rgs (2):- New Char ('b');\n   rgs (3):- New Char ('b');\n   rgs (4):- New Char ('a');\n   rg:- New Line (rgs);\n   rg.print;\nEnd;\n\nThe above example has one super class (Glyph) with two subclasses (Char and Line). There is one virtual procedure with two implementations. The execution starts by executing the main program. Simula lacks the concept of abstract classes, since classes with pure virtual procedures can be instantiated. This means that in the above example, all classes can be instantiated. Calling a pure virtual procedure will however produce a run-time error.","completion":"Programming Languages"}
{"prompt":"The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.\nA programming language specification can take several forms, including the following:\n\nAn explicit definition of the syntax, static semantics, and execution semantics of the language. While syntax is commonly specified using a formal grammar, semantic definitions may be written in natural language (e.g., as in the C language), or a formal semantics (e.g., as in Standard ML and Scheme specifications).\nA description of the behavior of a translator for the language (e.g., the C++ and Fortran specifications). The syntax and semantics of the language have to be inferred from this description, which may be written in natural or formal language.\nA reference or model implementation, sometimes written in the language being specified (e.g., Prolog or ANSI REXX). The syntax and semantics of the language are explicit in the behavior of the reference implementation.","completion":"Programming Languages"}
{"prompt":"Aspect-oriented programming enables developers to add new functionality to code, known as \"advice\", without modifying that code itself; rather, it uses a pointcut to implement the advice into code blocks.\n\nAda\nAspectJ\nGroovy\nNemerle","completion":"Programming Languages"}
{"prompt":"In non-procedural paradigms, goto is less relevant or completely absent. One of the main alternatives is message passing, which is of particular importance in concurrent computing, interprocess communication, and object oriented programming. In these cases, the individual components do not have arbitrary transfer of control, but the overall control may be scheduled in complex ways, such as via preemption. The influential languages Simula and Smalltalk were among the first to introduce the concepts of messages and objects.  By encapsulating state data, object-oriented programming reduced software complexity to interactions (messages) between objects.","completion":"Programming Languages"}
{"prompt":"The R language supports array paradigm by default. The following example illustrates a process of multiplication of two matrices followed by an addition of a scalar (which is, in fact, a one-element vector) and a vector:","completion":"Programming Languages"}
{"prompt":"Various implementations have been developed from Prolog to extend logic programming abilities in many directions. These include types, modes, constraint logic programming (CLP), object-oriented logic programming (OOLP), concurrency, linear logic (LLP), functional and higher-order logic programming abilities, plus interoperability with knowledge bases:","completion":"Programming Languages"}
{"prompt":"In Pascal comments are delimited by '{ ... }'. As an alternative, for computers that do not support these characters, '(* ... *)' are allowed. In Niklaus Wirth's more modern family of languages (including Modula-2 and Oberon), comments are delimited by '(* ... *)'.for example:","completion":"Programming Languages"}
{"prompt":"In the original LISP there were two fundamental data types: atoms and lists. A list was a finite ordered sequence of elements, where each element is either an atom or a list, and an atom was a number or a symbol. A symbol was essentially a unique named item, written as an alphanumeric string in source code, and used either as a variable name or as a data item in symbolic processing. For example, the list (FOO (BAR 1) 2) contains three elements: the symbol FOO, the list (BAR 1), and the number 2.\nThe essential difference between atoms and lists was that atoms were immutable and unique. Two atoms that appeared in different places in source code but were written in exactly the same way represented the same object, whereas each list was a separate object that could be altered independently of other lists and could be distinguished from other lists by comparison operators.\nAs more data types were introduced in later Lisp dialects, and programming styles evolved, the concept of an atom lost importance. Many dialects still retained the predicate atom for legacy compatibility, defining it true for any object which is not a cons.","completion":"Programming Languages"}
{"prompt":"In extensible programming, a compiler is not a monolithic program that converts source code input into binary executable output. The compiler itself must be extensible to the point that it is really a collection of plugins that assist with the translation of source language input into anything. For example, an extensible compiler will support the generation of object code, code documentation, re-formatted source code, or any other desired output. The architecture of the compiler must permit its users to \"get inside\" the compilation process and provide alternative processing tasks at every reasonable step in the compilation process.\nFor just the task of translating source code into something that can be executed on a computer, an extensible compiler should:\n\nuse a plug-in or component architecture for nearly every aspect of its function\ndetermine which language or language variant is being compiled and locate the appropriate plug-in to recognize and validate that language\nuse formal language specifications to syntactically and structurally validate arbitrary source languages\nassist with the semantic validation of arbitrary source languages by invoking an appropriate validation plug-in\nallow users to select from different kinds of code generators so that the resulting executable can be targeted for different processors, operating systems, virtual machines, or other execution environment.\nprovide facilities for error generation and extensions to it\nallow new kinds of nodes in the abstract syntax tree (AST),\nallow new values in nodes of the AST,\nallow new kinds of edges between nodes,\nsupport the transformation of the input AST, or portions thereof, by some external \"pass\"\nsupport the translation of the input AST, or portions thereof, into another form by some external \"pass\"\nassist with the flow of information between internal and external passes as they both transform and translate the AST into new ASTs or other representations","completion":"Programming Languages"}
{"prompt":"Consider the following example written in the C programming language:\n\nThe variables s, x, and y were declared as a character array, floating point number, and an integer, respectively. The type system rejects, at compile-time, such fallacies as trying to add s and x. Since C23, type inference can be used in C with the keyword auto. Using that feature, the preceding example could become:\n\nSimilarly to the second example, in Standard ML, the types do not need to be explicitly declared. Instead, the type is determined by the type of the assigned expression.\n\nThere are no manifest types in this program, but the compiler still infers the types string, real and int for them, and would reject the expression s+x as a compile-time error.","completion":"Programming Languages"}
{"prompt":"The OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity, and for overemphasizing one aspect of software design and modeling (data\/objects) at the expense of other important aspects (computation\/algorithms).Luca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex. The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.Christopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP; however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.In an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.Alexander Stepanov compares object orientation unfavourably to generic programming:\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras \u2014 families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting \u2014 saying that everything is an object is saying nothing at all.\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".Leo Brodie has suggested a connection between the standalone nature of objects and a tendency to duplicate code in violation of the don't repeat yourself principle of software development.\nSteve Yegge noted that, as opposed to functional programming:\nObject Oriented Programming puts the nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It's not as if OOP has suddenly made verbs less important in the way we actually think. It's a strangely skewed perspective.\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.Eric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency. Raymond compares this unfavourably to the approach taken with Unix and the C programming language.Rob Pike, a programmer involved in the creation of UTF-8 and Go, has called object-oriented programming \"the Roman numerals of computing\" and has said that OOP languages frequently shift the focus from data structures and algorithms to types. Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.Regarding inheritance, Bob Martin states that because they are software, related classes do not necessarily share the relationships of the things they represent.","completion":"Programming Languages"}
{"prompt":"Prolog has been used in Watson. Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured Information Management Architecture) framework. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using Apache Hadoop framework to provide distributed computing. Prolog is used for pattern matching over natural language parse trees. The developers have stated: \"We required a language in which we could conveniently express pattern matching rules over the parse trees and other annotations (such as named entity recognition results), and a technology that could execute these rules very efficiently. We found that Prolog was the ideal choice for the language due to its simplicity and expressiveness.\" Prolog is being used in the Low-Code Development Platform GeneXus, which is focused around AI. Open source graph database TerminusDB is implemented in prolog. TerminusDB is designed for collaboratively building and curating knowledge graphs.","completion":"Programming Languages"}
{"prompt":"Procedural programming languages are based on the concept of the unit and scope (the data viewing range) of an executable code statement. A procedural program is composed of one or more units or modules, either user coded or provided in a code library; each module is composed of one or more procedures, also called a function, routine, subroutine, or method, depending on the language. Examples of procedural languages include:","completion":"Programming Languages"}
{"prompt":"The first published literate programming environment was WEB, introduced by Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe. The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and  C++, runs on most operating systems and can produce TeX and PDF documentation.\nThere are various other implementations of the literate programming concept as given below. Many of the newer ones among these don't have macros and hence violate the order of human logic principle, which makes them more of semi-literate tools. These however, allow cellular execution of code which makes them more on the likes of exploratory programming tools. \n\nOther useful tools include:\n\nThe Leo text editor is an outlining editor which supports optional noweb and CWEB markup. The author of Leo mixes two different approaches: first, Leo is an outlining editor, which helps with management of large texts; second, Leo incorporates some of the ideas of literate programming, which in its pure form (i.e., the way it is used by Knuth Web tool or tools like \"noweb\") is possible only with some degree of inventiveness and the use of the editor in a way not exactly envisioned by its author (in modified @root nodes). However, this and other extensions (@file nodes) make outline programming and text management successful and easy and in some ways similar to literate programming.\nThe Haskell programming language has native support for semi-literate programming. The compiler\/interpreter supports two file name extensions: .hs and .lhs; the latter stands for literate Haskell.The literate scripts can be full LaTeX source text, at the same time it can be compiled, with no changes, because the interpreter only compiles the text in a code environment, for example:\n\nThe code can be also marked in the Richard Bird style, starting each line with a greater than symbol and a space, preceding and ending the piece of code with blank lines.\nThe LaTeX listings package provides a lstlisting environment which can be used to embellish the source code. It can be used to define a code environment to use within Haskell to print the symbols in the following manner:\n\nwhich can be configured to yield:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                c\n                o\n                m\n                p\n                ::\n                (\n                \u03b2\n                \u2192\n                \u03b3\n                )\n                \u2192\n                (\n                \u03b1\n                \u2192\n                \u03b2\n                )\n                \u2192\n                (\n                \u03b1\n                \u2192\n                \u03b3\n                )\n              \n            \n            \n              \n              \n                \n                (\n                g\n                comp\n                \u2061\n                f\n                )\n                x\n                =\n                g\n                (\n                f\n                x\n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&comp::(\\beta \\to \\gamma )\\to (\\alpha \\to \\beta )\\to (\\alpha \\to \\gamma )\\\\&(g\\operatorname {comp} f)x=g(fx)\\end{aligned}}}\n  \nAlthough the package does not provide means to organize chunks of code, one can split the LaTeX source code in different files. See listings manual for an overview.The Web 68 Literate Programming system used Algol 68 as the underlying programming language, although there was nothing in the pre-processor 'tang' to force the use of that language.\nThe customization mechanism of the Text Encoding Initiative which enables the constraining, modification, or extension of the TEI scheme enables users to mix prose documentation with fragments of schema specification in their One Document Does-it-all format. From this prose documentation, schemas, and processing model pipelines can be generated and Knuth's Literate Programming paradigm is cited as the inspiration for this way of working.","completion":"Programming Languages"}
{"prompt":"Literate programming, as a form of imperative programming, structures programs as a human-centered web, as in a hypertext essay: documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.\nIndependent of the imperative branch, declarative programming paradigms were developed. In these languages, the computer is told what the problem is, not how to solve the problem \u2013  the program is structured as a set of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. An archetype of a declarative language is the fourth generation language SQL, and the family of functional languages and logic programming.\nFunctional programming is a subset of declarative programming. Programs written using this paradigm use functions, blocks of code intended to behave like mathematical functions. Functional languages discourage changes in the value of variables through assignment, making a great deal of use of recursion instead.\nThe logic programming paradigm views computation as automated reasoning over a body of knowledge. Facts about the problem domain are expressed as logic formulas, and programs are executed by applying inference rules over them until an answer to the problem is found, or the set of formulas is proved inconsistent.\nSymbolic programming is a paradigm that describes programs able to manipulate formulas and program components as data.  Programs can thus effectively modify themselves, and appear to \"learn\", making them suited for applications such as artificial intelligence, expert systems, natural-language processing and computer games.  Languages that support this paradigm include Lisp and Prolog.Differentiable programming structures programs so that they can be differentiated throughout, usually via automatic differentiation.","completion":"Programming Languages"}
{"prompt":"In a scientific programming language, we can compute function optima with a syntax close to mathematical language. For instance, the following Julia code finds the minimum of the polynomial \n  \n    \n      \n        P\n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          x\n          \n            2\n          \n        \n        \u2212\n        3\n        x\n        y\n        +\n        5\n        \n          y\n          \n            2\n          \n        \n        \u2212\n        7\n        y\n        +\n        3\n      \n    \n    {\\displaystyle P(x,y)=x^{2}-3xy+5y^{2}-7y+3}\n  .\n\nIn this example, Newton's method for minimizing is used. Modern scientific programming languages will use automatic differentiation to compute the gradients and Hessians of the function given as input; cf. differentiable programming. Here, automatic forward differentiation has been chosen for that task.\nOlder scientific programming languages such as the venerable Fortran would require the programmer to pass, next to the function to be optimized, a function that computes the gradient, and a function that computes the Hessian.\nWith more knowledge of the function to be minimized, more efficient algorithms can be used.  For instance, convex optimization provides faster computations when the function is convex, quadratic programming provides faster computations when the function is at most quadratic in its variables, and linear programming when the function is at most linear.","completion":"Programming Languages"}
{"prompt":"The field of formal semantics encompasses all of the following:\n\nThe definition of semantic models\nThe relations between different semantic models\nThe relations between different approaches to meaning\nThe relation between computation and the underlying mathematical structures from fields such as logic, set theory, model theory, category theory, etc.It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.","completion":"Programming Languages"}
{"prompt":"The two kinds of variables commonly used in Smalltalk are instance variables and temporary variables. Other variables and related terminology depend on the particular implementation. For example, VisualWorks has class shared variables and namespace shared variables, while Squeak and many other implementations have class variables, pool variables and global variables.\nTemporary variable declarations in Smalltalk are variables declared inside a method (see below). They are declared at the top of the method as names separated by spaces and enclosed by vertical bars. For example:\n\ndeclares a temporary variable named index which contains initially the value nil.\nMultiple variables may be declared within one set of bars:\n\ndeclares two variables: index and vowels.  All variables are initialized.  Variables are initialized to nil except the indexed variables of Strings, which are initialized to the null character or ByteArrays which are initialized to 0.","completion":"Programming Languages"}
{"prompt":"The syntax of Prolog does not specify which arguments of a predicate are inputs and which are outputs. However, this information is significant and it is recommended that it be included in the comments. Modes provide valuable information when reasoning about Prolog programs and can also be used to accelerate execution.","completion":"Programming Languages"}
{"prompt":"Interrupt Driven Programming (1971)\nReversible Execution (1973)\nOptimization of Structured Programs (1974)\nPerspectives on software engineering (1978)\nA case study in rapid prototyping (1980)\nImplementation of language enhancements (1981)\nSoftware engineering practices in the United States and Japan (1984)\nA functional correctness model of program verification (1990)\nThe role for executable specifications in system maintenance (1991)\nSEL's software process-improvement program (1995)\nSoftware Engineering technology infusion within NASA (1996)\nExperimental models for validating computer technology (1998)\nA web-based tool for data analysis and presentation (1998)\nSoftware process improvement in small organizations: A case study (2005)\nUnderstanding the high-performance computing community: A software engineer\u2019s perspective (2008)","completion":"Programming Languages"}
{"prompt":"Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language \u2013 for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.\nInterpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a set of directly executed machine instructions is needed somewhere at the bottom of the execution stack (see machine language).\nFurthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.\nSome language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.","completion":"Programming Languages"}
{"prompt":"An assembler program creates object code by translating combinations of mnemonics and syntax for operations and addressing modes into their numerical equivalents. This representation typically includes an operation code (\"opcode\") as well as other control bits and data. The assembler also calculates constant expressions and resolves symbolic names for memory locations and other entities. The use of symbolic references is a key feature of assemblers, saving tedious calculations and manual address updates after program modifications. Most assemblers also include macro facilities for performing textual substitution \u2013 e.g., to generate common short sequences of instructions as inline, instead of called subroutines.\nSome assemblers may also be able to perform some simple types of instruction set-specific optimizations. One concrete example of this may be the ubiquitous x86 assemblers from various vendors. Called jump-sizing, most of them are able to perform jump-instruction replacements (long jumps replaced by short or relative jumps) in any number of passes, on request. Others may even do simple rearrangement or insertion of instructions, such as some assemblers for RISC architectures that can help optimize a sensible instruction scheduling to exploit the CPU pipeline as efficiently as possible.Assemblers have been available since the 1950s, as the first step above machine language and before high-level programming languages such as Fortran, Algol, COBOL and Lisp. There have also been several classes of translators and semi-automatic code generators with properties similar to both assembly and high-level languages, with Speedcode as perhaps one of the better-known examples.\nThere may be several assemblers with different syntax for a particular CPU or instruction set architecture. For instance, an instruction to add memory data to a register in a x86-family processor might be add eax,[ebx], in original Intel syntax, whereas this would be written addl (%ebx),%eax in the AT&T syntax used by the GNU Assembler. Despite different appearances, different syntactic forms generally generate the same numeric machine code. A single assembler may also have different modes in order to support variations in syntactic forms as well as their exact semantic interpretations (such as FASM-syntax, TASM-syntax, ideal mode, etc., in the special case of x86 assembly programming).","completion":"Programming Languages"}
{"prompt":"On New Year's Day 1992, Hopper died in her sleep of natural causes at her home in Arlington County, Virginia; she was 85 years of age. She was interred with full military honors in Arlington National Cemetery.","completion":"Programming Languages"}
{"prompt":"Everything in Smalltalk-80, unless customised to avoid the possibility, is available for modification from within a running program. This means that, for example, the IDE can be changed in a running system without restarting it.  In some implementations, the syntax of the language or the garbage collection implementation can also be changed on the fly. Even the statement true become: false is valid in Smalltalk, although executing it is not recommended except for demonstration purposes (see virtual machine, image-based persistence, and backups).","completion":"Programming Languages"}
{"prompt":"This Java code fragment shows a block comment used to describe the setToolTipText method. The formatting is consistent with Sun Microsystems Javadoc standards. The comment is designed to be read by the Javadoc processor.","completion":"Programming Languages"}
{"prompt":"When attempting to sort in a non-standard way, it may be easier to contain the sorting logic as an anonymous function instead of creating a named function.\nMost languages provide a generic sort function that implements a sort algorithm that will sort arbitrary objects.\nThis function usually accepts an arbitrary function that determines how to compare whether two elements are equal or if one is greater or less than the other.\nConsider this Python code sorting a list of strings by length of the string:\n\nThe anonymous function in this example is the lambda expression:\n\nThe anonymous function accepts one argument, x, and returns the length of its argument, which is then used by the sort() method as the criteria for sorting.\nBasic syntax of a lambda function in Python is \n\nThe expression returned by the lambda function can be assigned to a variable and used in the code at multiple places.\n\nAnother example would be sorting items in a list by the name of their class (in Python, everything has a class):\n\nNote that 11.2 has class name \"float\", 10 has class name \"int\", and 'number' has class name \"str\". The sorted order is \"float\", \"int\", then \"str\".","completion":"Programming Languages"}
{"prompt":"James Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991. Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time. The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee, a type of coffee from Indonesia. Gosling designed Java with a C\/C++-style syntax that system and application programmers would find familiar.Sun Microsystems released the first public implementation as Java 1.0 in 1996. It promised write once, run anywhere (WORA) functionality, providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular. The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification. With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 \u2013  1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.\nIn 1997, Sun Microsystems approached the ISO\/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process. Java remains a de facto standard, controlled through the Java Community Process. At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.\nOn November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software (FOSS), under the terms of the GPL-2.0-only license. On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software\/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.Sun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an evangelist. Following Oracle Corporation's acquisition of Sun Microsystems in 2009\u201310, Oracle has described itself as the steward of Java technology with a relentless commitment to fostering a community of participation and transparency. This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see the Android section).\nOn April 2, 2010, James Gosling resigned from Oracle.In January 2016, Oracle announced that Java run-time environments based on JDK 9 will discontinue the browser plugin.Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.Oracle (and others) highly recommend uninstalling outdated and unsupported versions of Java, due to unresolved security issues in older versions.","completion":"Programming Languages"}
{"prompt":"Python's expressions include:\n\nThe +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of divisions in Python: floor division (or integer division) \/\/ and floating-point\/division. Python uses the ** operator for exponentiation.\nPython uses the + operator for string concatenation. Python uses the * operator for duplicating a string a specified number of times.\nThe @ infix operator. It is intended to be used by libraries such as NumPy for matrix multiplication.\nThe syntax :=, called the \"walrus operator\", was introduced in Python 3.8. It assigns values to variables as part of a larger expression.\nIn Python, == compares by value. Python's is operator may be used to compare object identities (comparison by reference), and comparisons may be chained\u2014for example, a <= b <= c.\nPython uses and, or, and not as Boolean operators.\nPython has a type of expression called a list comprehension, as well as a more general expression called a generator expression.\nAnonymous functions are implemented using lambda expressions; however, there may be only one expression in each body.\nConditional expressions are written as x if c else y (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as keys of dictionaries, provided all of the tuple's elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. Thus, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5), which is then assigned back to t\u2014thereby effectively \"modifying the contents\" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.\nPython features sequence unpacking where multiple expressions, each evaluating to anything that can be assigned (to a variable, writable property, etc.) are associated in an identical manner to that forming tuple literals\u2014and, as a whole, are put on the left-hand side of the equal sign in an assignment statement. The statement expects an iterable object on the right-hand side of the equal sign that produces the same number of values as the provided writable expressions; when iterated through them, it assigns each of the produced values to the corresponding expression on the left.\nPython has a \"string format\" operator % that functions analogously to printf format strings in C\u2014e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 2.6+ and 3+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2). Python 3.6 added \"f-strings\": spam = \"blah\"; eggs = 2; f'spam={spam} eggs={eggs}'.\nStrings in Python can be concatenated by \"adding\" them (with the same operator as for adding integers and floats), e.g. \"spam\" + \"eggs\" returns \"spameggs\". If strings contain numbers, they are added as strings rather than integers, e.g. \"2\" + \"2\" returns \"22\".\nPython has various string literals:\nDelimited by single or double quote marks; unlike in Unix shells, Perl, and Perl-influenced languages, single and double quote marks work the same. Both use the backslash (\\) as an escape character. String interpolation became available in Python 3.6 as \"formatted string literals\".\nTriple-quoted (beginning and ending with three single or double quote marks), which may span multiple lines and function like here documents in shells, Perl, and Ruby.\nRaw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. (Compare \"@-quoting\" in C#.)\nPython has array index and array slicing expressions in lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted\u2014for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.In Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\n\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statementsStatements cannot be a part of an expression\u2014so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is syntactically valid (but probably unintended) C code, but if c = 1: ... causes a syntax error in Python.","completion":"Programming Languages"}
{"prompt":"A program written in assembly language consists of a series of mnemonic processor instructions and meta-statements (known variously as declarative operations, directives, pseudo-instructions, pseudo-operations and pseudo-ops), comments and data. Assembly language instructions usually consist of an opcode mnemonic followed by an operand, which might be a list of data, arguments or parameters.  Some instructions may be \"implied,\" which means the data upon which the instruction operates is implicitly defined by the instruction itself\u2014such an instruction does not take an operand.  The resulting statement is translated by an assembler into machine language instructions that can be loaded into memory and executed.\nFor example, the instruction below tells an x86\/IA-32 processor to move an immediate 8-bit value into a register. The binary code for this instruction is 10110 followed by a 3-bit identifier for which register to use. The identifier for the AL register is 000, so the following machine code loads the AL register with the data 01100001.\n10110000 01100001\n\nThis binary computer code can be made more human-readable by expressing it in hexadecimal as follows.\n\nB0 61\n\nHere, B0 means 'Move a copy of the following value into AL, and 61 is a hexadecimal representation of the value 01100001, which is 97 in decimal. Assembly language for the 8086 family provides the mnemonic MOV (an abbreviation of move) for instructions such as this, so the machine code above can be written as follows in assembly language, complete with an explanatory comment if required, after the semicolon. This is much easier to read and to remember.\n\nIn some assembly languages (including this one) the same mnemonic, such as MOV, may be used for a family of related instructions for loading, copying and moving data, whether these are immediate values, values in registers, or memory locations pointed to by values in registers or by immediate (a.k.a. direct) addresses.  Other assemblers may use separate opcode mnemonics such as L for \"move memory to register\", ST for \"move register to memory\", LR for \"move register to register\", MVI for \"move immediate operand to memory\", etc.\nIf the same mnemonic is used for different instructions, that means that the mnemonic corresponds to several different binary instruction codes, excluding data (e.g. the 61h in this example), depending on the operands that follow the mnemonic.  For example, for the x86\/IA-32 CPUs, the Intel assembly language syntax MOV AL, AH represents an instruction that moves the contents of register AH into register AL. The hexadecimal form of this instruction is:\n\n88 E0\n\nThe first byte, 88h, identifies a move between a byte-sized register and either another register or memory, and the second byte, E0h, is encoded (with three bit-fields) to specify that both operands are registers, the source is AH, and the destination is AL.\nIn a case like this where the same mnemonic can represent more than one binary instruction, the assembler determines which instruction to generate by examining the operands.  In the first example, the operand 61h is a valid hexadecimal numeric constant and is not a valid register name, so only the B0 instruction can be applicable.  In the second example, the operand AH is a valid register name and not a valid numeric constant (hexadecimal, decimal, octal, or binary), so only the 88 instruction can be applicable.\nAssembly languages are always designed so that this sort of lack of ambiguity is universally enforced by their syntax.  For example, in the Intel x86 assembly language, a hexadecimal constant must start with a numeral digit, so that the hexadecimal number 'A' (equal to decimal ten) would be written as 0Ah or 0AH, not AH, specifically so that it cannot appear to be the name of register AH.  (The same rule also prevents ambiguity with the names of registers BH, CH, and DH, as well as with any user-defined symbol that ends with the letter H and otherwise contains only characters that are hexadecimal digits, such as the word \"BEACH\".)\nReturning to the original example, while the x86 opcode 10110000 (B0) copies an 8-bit value into the AL register, 10110001 (B1) moves it into CL and 10110010 (B2) does so into DL. Assembly language examples for these follow.\n\nThe syntax of MOV can also be more complex as the following examples show.\n\nIn each case, the MOV mnemonic is translated directly into one of the opcodes 88-8C, 8E, A0-A3, B0-BF, C6 or C7 by an assembler, and the programmer normally does not have to know or remember which.Transforming assembly language into machine code is the job of an assembler, and the reverse can at least partially be achieved by a disassembler. Unlike high-level languages, there is a one-to-one correspondence between many simple assembly statements and machine language instructions. However, in some cases, an assembler may provide pseudoinstructions (essentially macros) which expand into several machine language instructions to provide commonly needed functionality. For example, for a machine that lacks a \"branch if greater or equal\" instruction, an assembler may provide a pseudoinstruction that expands to the machine's \"set if less than\" and \"branch if zero (on the result of the set instruction)\". Most full-featured assemblers also provide a rich macro language (discussed below) which is used by vendors and programmers to generate more complex code and data sequences. Since the information about pseudoinstructions and macros defined in the assembler environment is not present in the object program, a disassembler cannot reconstruct the macro and pseudoinstruction invocations but can only disassemble the actual machine instructions that the assembler generated from those abstract assembly-language entities. Likewise, since comments in the assembly language source file are ignored by the assembler and have no effect on the object code it generates, a disassembler is always completely unable to recover source comments.\nEach computer architecture has its own machine language.  Computers differ in the number and type of operations they support, in the different sizes and numbers of registers, and in the representations of data in storage. While most general-purpose computers are able to carry out essentially the same functionality, the ways they do so differ; the corresponding assembly languages reflect these differences.\nMultiple sets of mnemonics or assembly-language syntax may exist for a single instruction set, typically instantiated in different assembler programs. In these cases, the most popular one is usually that supplied by the CPU manufacturer and used in its documentation.\nTwo examples of CPUs that have two different sets of mnemonics are the Intel 8080 family and the Intel 8086\/8088.  Because Intel claimed copyright on its assembly language mnemonics (on each page of their documentation published in the 1970s and early 1980s, at least), some companies that independently produced CPUs compatible with Intel instruction sets invented their own mnemonics.  The Zilog Z80 CPU, an enhancement of the Intel 8080A, supports all the 8080A instructions plus many more; Zilog invented an entirely new assembly language, not only for the new instructions but also for all of the 8080A instructions.  For example, where Intel uses the mnemonics MOV, MVI, LDA, STA, LXI, LDAX, STAX, LHLD, and SHLD for various data transfer instructions, the Z80 assembly language uses the mnemonic LD for all of them.  A similar case is the NEC V20 and V30 CPUs, enhanced copies of the Intel 8086 and 8088, respectively.  Like Zilog with the Z80, NEC invented new mnemonics for all of the 8086 and 8088 instructions, to avoid accusations of infringement of Intel's copyright.  (It is questionable whether such copyrights can be valid, and later CPU companies such as AMD and Cyrix republished Intel's x86\/IA-32 instruction mnemonics exactly with neither permission nor legal penalty.)  It is doubtful whether in practice many people who programmed the V20 and V30 actually wrote in NEC's assembly language rather than Intel's; since any two assembly languages for the same instruction set architecture are isomorphic (somewhat like English and Pig Latin), there is no requirement to use a manufacturer's own published assembly language with that manufacturer's products.","completion":"Programming Languages"}
{"prompt":"In 2004 he was inducted as a Fellow of the Association for Computing Machinery. He was elected a Fellow of the Royal Society (FRS) in 2005. In 2007, Cardelli was awarded the Senior AITO Dahl\u2013Nygaard Prize named for Ole-Johan Dahl and Kristen Nygaard.","completion":"Programming Languages"}
{"prompt":"The following code fragments in C demonstrate just a tiny example of how comments can vary stylistically, while still conveying the same basic information:\n\nFactors such as personal preference, flexibility of programming tools, and other considerations tend to influence the stylistic variants used in source code. For example, Variation Two might be disfavored among programmers who do not have source code editors that can automate the alignment and visual appearance of text in comments.\nSoftware consultant and technology commentator Allen Holub is one expert who advocates aligning the left edges of comments:\n \n \nThe use of \/* and *\/ as block comment delimiters was inherited from PL\/I into the B programming language, the immediate predecessor of the C programming language.","completion":"Programming Languages"}
{"prompt":"In Scheme, procedures are bound to variables. At R5RS the language standard formally mandated that programs may change the variable bindings of built-in procedures, effectively redefining them. (R5RS \"Language changes\")  For example, + can be extended to accept strings as well as numbers by redefining it:\n\nIn R6RS every binding, including the standard ones, belongs to some library, and all exported bindings are immutable. (R6RS sec 7.1)  Because of this, redefinition of standard procedures by mutation is forbidden. Instead, it is possible to import a different procedure under the name of a standard one, which in effect is similar to redefinition.","completion":"Programming Languages"}
{"prompt":"Parnas, Shore & Weiss (1976) identified five definitions of a \"type\" that were used\u2014sometimes implicitly\u2014in the literature:\n\nSyntactic\nA type is a purely syntactic label associated with a variable when it is declared. Although useful for advanced type systems such as substructural type systems, such definitions provide no intuitive meaning of the types.\nRepresentation\nA type is defined in terms of a composition of more primitive types\u2014often machine types.\nRepresentation and behaviour\nA type is defined as its representation and a set of operators manipulating these representations.\nValue space\nA type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.\nValue space and behaviour\nA type is a set of values which a variable can possess and a set of functions that one can apply to these values.The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.","completion":"Programming Languages"}
{"prompt":"Inline comments in Python use the hash (#) character, as in the two examples in this code:\n\nBlock comments, as defined in this article, do not technically exist in Python. A bare string literal represented by a triple-quoted string can be used, but is not ignored by the interpreter in the same way that \"#\" comment is. In the examples below, the triple double-quoted strings act in this way as comments, but are also treated as docstrings:","completion":"Programming Languages"}
{"prompt":"In late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer.:\u200a69\u200a Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Harold Stern, Lois Haibt, and David Sayre. Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in the Laning and Zierler system of 1952.\nA draft specification for The IBM Mathematical Formula Translating System was completed by November 1954.:\u200a71\u200a The first manual for FORTRAN appeared in October 1956,:\u200a72\u200a with the first FORTRAN compiler delivered in April 1957.:\u200a75\u200a This was the first optimizing compiler, because customers were reluctant to use a high-level programming language unless its compiler could generate code with performance approaching that of hand-coded assembly language.While the community was skeptical that this new method could possibly outperform hand-coding, it reduced the number of programming statements necessary to operate a machine by a factor of 20, and quickly gained acceptance.  John Backus said during a 1979 interview with Think, the IBM employee magazine, \"Much of my work has come from being lazy. I didn't like writing programs, and so, when I was working on the IBM 701, writing programs for computing missile trajectories, I started work on a programming system to make it easier to write programs.\"The language was widely adopted by scientists for writing numerically intensive programs, which encouraged compiler writers to produce compilers that could generate faster and more efficient code.  The inclusion of a complex number data type in the language made Fortran especially suited to technical applications such as electrical engineering.By 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers.  Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed.  For these reasons, FORTRAN is considered to be the first widely used cross-platform programming language.\nFORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters.  The compiler could be run from tape, or from a 2200-card deck; it used no further tape or disk storage.  It kept the program in memory and loaded overlays that gradually transformed it, in place, into executable form, as described by Haines.\nThis article was reprinted, edited, in both editions of Anatomy of a Compiler and in the IBM manual \"Fortran Specifications and Operating Procedures, IBM 1401\".  The executable form was not entirely machine language; rather, floating-point arithmetic, sub-scripting, input\/output, and function references were interpreted, preceding UCSD Pascal P-code by two decades. GOTRAN, a simplified, interpreted version of FORTRAN I (with only 12 statements not 32) for \"load and go\" operation was available (at least for the early IBM 1620 computer). Modern Fortran, and almost all later versions, are fully compiled, as done for other high-performance languages.\nThe development of Fortran paralleled the early evolution of compiler technology, and many advances in the theory and design of compilers were specifically motivated by the need to generate efficient code for Fortran programs.","completion":"Programming Languages"}
{"prompt":"Axiom, which is evolved from scratchpad, a computer algebra system developed by IBM. It is now being developed by Tim Daly, one of the developers of scratchpad, Axiom is totally written as a literate program.","completion":"Programming Languages"}
{"prompt":"ALGOL 68's standard document was published in numerous natural languages. The standard allowed the internationalization of the programming language. On December 20, 1968, the \"Final Report\" (MR 101) was adopted by the Working Group, then subsequently approved by the General Assembly of UNESCO's IFIP for publication. Translations of the standard were made for Russian, German, French, Bulgarian, and then later Japanese. The standard was also available in Braille. ALGOL 68 went on to become the GOST\/\u0413\u041e\u0421\u0422-27974-88 standard in the Soviet Union.\n\nGOST 27974-88 Programming language ALGOL 68 \u2013 \u042f\u0437\u044b\u043a \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0410\u041b\u0413\u041e\u041b 68\nGOST 27975-88 Programming language ALGOL 68 extended \u2013 \u042f\u0437\u044b\u043a \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0410\u041b\u0413\u041e\u041b 68 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u043d\u044b\u0439In English, Algol68's case statement reads case ~ in ~ out ~ esac. In Russian, this reads \u0432\u044b\u0431 ~ \u0432 ~ \u043b\u0438\u0431\u043e ~ \u0431\u044b\u0432.","completion":"Programming Languages"}
{"prompt":"Experts have varying viewpoints on whether, and when, comments are appropriate in source code. Some assert that source code should be written with few comments, on the basis that the source code should be self-explanatory or self-documenting. Others suggest code should be extensively commented (it is not uncommon for over 50% of the non-whitespace characters in source code to be contained within comments).In between these views is the assertion that comments are neither beneficial nor harmful by themselves, and what matters is that they are correct and kept in sync with the source code, and omitted if they are superfluous, excessive, difficult to maintain or otherwise unhelpful.Comments are sometimes used to document contracts in the design by contract approach to programming.","completion":"Programming Languages"}
{"prompt":"In array languages, operations are generalized to apply to both scalars and arrays. Thus, a+b expresses the sum of two scalars if a and b are scalars, or the sum of two arrays if they are arrays.\nAn array language simplifies programming but possibly at a cost known as the abstraction penalty. Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).","completion":"Programming Languages"}
{"prompt":"In Lua (much as in Scheme) all functions are anonymous. A named function in Lua is simply a variable holding a reference to a function object.Thus, in Lua\n\nis just syntactical sugar for\n\nAn example of using anonymous functions for reverse-order sorting:","completion":"Programming Languages"}
{"prompt":"Subtyping \u2013 a form of polymorphism \u2013 is when calling code can be independent of which class in the supported hierarchy it is operating on \u2013 the parent class or one of its descendants. Meanwhile, the same operation name among objects in an inheritance hierarchy may behave differently.\nFor example, objects of type Circle and Square are derived from a common class called Shape. The Draw function for each type of Shape implements what is necessary to draw itself while calling code can remain indifferent to the particular type of Shape being drawn.\nThis is another type of abstraction that simplifies code external to the class hierarchy and enables strong separation of concerns.","completion":"Programming Languages"}
{"prompt":"In this classic early BASIC code fragment the REM (\"Remark\") keyword is used to add comments.\n\nIn later Microsoft BASICs, including Quick Basic, Q Basic, Visual Basic, Visual Basic .NET, and VB Script; and in descendants such as FreeBASIC and Gambas any text on a line after an ' (apostrophe) character is also treated as a comment.\nAn example in Visual Basic .NET:","completion":"Programming Languages"}
{"prompt":"PharoJS an open-source transpiler from Smalltalk to Javascript, extending the Pharo environment\nSqueakJS an OpenSmalltalk-compatible VM for the web, also runs older Squeak apps like Etoys or Scratch","completion":"Programming Languages"}
{"prompt":"Textbooks and scientific publications related to computer science and numerical computation often use pseudocode in description of algorithms, so that all programmers can understand them, even if they do not all know the same programming languages. In textbooks, there is usually an accompanying introduction explaining the particular conventions in use. The level of detail of the pseudocode may in some cases approach that of formalized general-purpose languages.\nA programmer who needs to implement a specific algorithm, especially an unfamiliar one, will often start with a pseudocode description, and then  \"translate\" that description into the target programming language and modify it to interact correctly with the rest of the program. Programmers may also start a project by sketching out the code in pseudocode on paper before writing it in its actual language, as a top-down structuring approach, with a process of steps to be followed as a refinement.","completion":"Programming Languages"}
{"prompt":"Formal languages are used as tools in multiple disciplines. However, formal language theory rarely concerns itself with particular languages (except as examples), but is mainly concerned with the study of various types of formalisms to describe languages. For instance, a language can be given as\n\nthose strings generated by some formal grammar;\nthose strings described or matched by a particular regular expression;\nthose strings accepted by some automaton, such as a Turing machine or finite-state automaton;\nthose strings for which some decision procedure (an algorithm that asks a sequence of related YES\/NO questions) produces the answer YES.Typical questions asked about such formalisms include:\n\nWhat is their expressive power? (Can formalism X describe every language that formalism Y can describe? Can it describe other languages?)\nWhat is their recognizability? (How difficult is it to decide whether a given word belongs to a language described by formalism X?)\nWhat is their comparability? (How difficult is it to decide whether two languages, one described in formalism X and one in formalism Y, or in X again, are actually the same language?).Surprisingly often, the answer to these decision problems is \"it cannot be done at all\", or \"it is extremely expensive\" (with a characterization of how expensive). Therefore, formal language theory is a major application area of computability theory and complexity theory. Formal languages may be classified in the Chomsky hierarchy based on the expressive power of their generative grammar as well as the complexity of their recognizing automaton. Context-free grammars and regular grammars provide a good compromise between expressivity and ease of parsing, and are widely used in practical applications.","completion":"Programming Languages"}
{"prompt":"The parsing problem, checking whether a given word belongs to the language given by a context-free grammar, is decidable, using one of the general-purpose parsing algorithms:\n\nCYK algorithm (for grammars in Chomsky normal form)\nEarley parser\nGLR parser\nLL parser (only for the proper subclass of for LL(k) grammars)Context-free parsing for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to boolean matrix multiplication, thus inheriting its complexity upper bound of O(n2.3728639). Conversely, Lillian Lee has shown O(n3\u2212\u03b5) boolean matrix multiplication to be reducible to O(n3\u22123\u03b5) CFG parsing, thus establishing some kind of lower bound for the latter.","completion":"Programming Languages"}
{"prompt":"The traditional Hello world program can be written in Java as:\n\nAll source files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file with the .class suffix (Main.class, in this case). Only then can it be executed or launched. The Java source file may only contain one public class, but it can contain multiple classes with a non-public access modifier and any number of public inner classes. When the source file contains multiple classes, it is necessary to make one class (introduced by the class keyword) public (preceded by the public keyword) and name the source file with that public class name.\nA class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.\nThe keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private (a method that can only be accessed in the same class) and protected (which allows code from the same package to access). If a piece of code attempts to access private methods or protected methods, the JVM will throw a SecurityException.\nThe keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods and require a specific instance of a class to operate.\nThe keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.\nThe method name main is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.\nThe main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (to the args parameter which is still an array of String objects), but it allows an alternative syntax for creating and passing the array.\nThe Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.\nPrinting is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.\nThe string \"Hello World!\" is automatically converted to a String object by the compiler.","completion":"Programming Languages"}
{"prompt":"The Template feature of MediaWiki is an embedded domain-specific language whose fundamental purpose is to support the creation of page templates and the transclusion (inclusion by reference) of MediaWiki pages into other MediaWiki pages.","completion":"Programming Languages"}
{"prompt":"Scheme is a very simple language, much easier to implement than many other languages of comparable expressive power.  This ease is attributable to the use of lambda calculus to derive much of the syntax of the language from more primitive forms. For instance of the 23 s-expression-based syntactic constructs defined in the R5RS Scheme standard, 14 are classed as derived or library forms, which can be written as macros involving more fundamental forms, principally lambda. As R5RS (\u00a73.1) says: \"The most fundamental of the variable binding constructs is the lambda expression, because all other variable binding constructs can be explained in terms of lambda expressions.\"\nFundamental forms: define, lambda, quote, if, define-syntax, let-syntax, letrec-syntax, syntax-rules, set!\nDerived forms: do, let, let*, letrec, cond, case, and, or, begin, named let, delay, unquote, unquote-splicing, quasiquoteExample: a macro to implement let as an expression using lambda to perform the variable bindings.\n\nThus using let as defined above a Scheme implementation would rewrite \"(let ((a 1)(b 2)) (+ b a))\" as \"((lambda (a b) (+ b a)) 1 2)\", which reduces implementation's task to that of coding procedure instantiations.\nIn 1998, Sussman and Steele remarked that the minimalism of Scheme was not a conscious design goal, but rather the unintended outcome of the design process. \"We were actually trying to build something complicated and discovered, serendipitously, that we had accidentally designed something that met all our goals but was much simpler than we had intended....we realized that the lambda calculus\u2014a small, simple formalism\u2014could serve as the core of a powerful and expressive programming language.\"","completion":"Programming Languages"}
{"prompt":"The implementation in MATLAB allows the same economy allowed by using the Fortran language.\n\nA variant of the MATLAB language is the GNU Octave language, which extends the original language with augmented assignments:\n\nBoth MATLAB and GNU Octave natively support linear algebra operations such as matrix multiplication, matrix inversion, and the numerical solution of system of linear equations, even using the Moore\u2013Penrose pseudoinverse.The Nial example of the inner product of two arrays can be implemented using the native matrix multiplication operator. If a is a row vector of size [1 n] and b is a corresponding column vector of size [n 1].\n\na * b;\n\nBy contrast, the entrywise product is implemented as:\n\na .* b;\n\nThe inner product between two matrices having the same number of elements can be implemented with the auxiliary operator (:), which reshapes a given matrix into a column vector, and the transpose operator ':\n\nA(:)' * B(:);","completion":"Programming Languages"}
{"prompt":"He is the author of one book on type systems, Types and Programming Languages ISBN 0-262-16209-1.  He has also edited a collection of articles to create a second volume Advanced Topics in Types and Programming Languages ISBN 0-262-16228-8. Based on the notes he collected while learning category theory during his PhD, he also published an introductory book on this topic\u2014Basic Category Theory for Computer Scientists, ISBN 0-262-66071-7. He is one of the authors of the freely available book Software Foundations.","completion":"Programming Languages"}
{"prompt":"The Erlang Open Telecom Platform was originally designed for use inside Ericsson as a domain-specific language. The language itself offers a platform of libraries to create finite state machines, generic servers and event managers that quickly allow an engineer to deploy applications, or support libraries, that have been shown in industry benchmarks to outperform other languages intended for a mixed set of domains, such as C and C++. The language is now officially open source and can be downloaded from their website.","completion":"Programming Languages"}
{"prompt":"Higher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator \n  \n    \n      \n        d\n        \n          \/\n        \n        d\n        x\n      \n    \n    {\\displaystyle d\/dx}\n  , which returns the derivative of a function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  .\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\nHigher-order functions enable partial application or currying, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.","completion":"Programming Languages"}
{"prompt":"In Python and some other implementations (e.g. Java), the three common quantifiers (*, + and ?) are greedy by default because they match as many characters as possible. The regex \".+\" (including the double-quotes) applied to the string\n\n\"Ganymede,\" he continued, \"is the largest moon in the Solar System.\"\n\nmatches the entire line (because the entire line begins and ends with a double-quote) instead of matching only the first part, \"Ganymede,\". The aforementioned quantifiers may, however, be made lazy or minimal or reluctant, matching as few characters as possible, by appending a question mark: \".+?\" matches only \"Ganymede,\".","completion":"Programming Languages"}
{"prompt":"In mathematical logic, a formal theory is a set of sentences expressed in a formal language.\nA formal system (also called a logical calculus, or a logical system) consists of a formal language together with a deductive apparatus (also called a deductive system). The deductive apparatus may consist of a set of transformation rules, which may be interpreted as valid rules of inference, or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Although a formal language can be identified with its formulas, a formal system cannot be likewise identified by its theorems. Two formal systems \n  \n    \n      \n        \n          \n            F\n            S\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {FS}}}\n   and \n  \n    \n      \n        \n          \n            F\n            \n              S\n              \u2032\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {FS'}}}\n   may have all the same theorems and yet differ in some significant proof-theoretic way (a formula A may be a syntactic consequence of a formula B in one but not another for instance).\nA formal proof or derivation is a finite sequence of well-formed formulas (which may be interpreted as sentences, or propositions) each of which is an axiom or follows from the preceding formulas in the sequence by a rule of inference. The last sentence in the sequence is a theorem of a formal system. Formal proofs are useful because their theorems can be interpreted as true propositions.","completion":"Programming Languages"}
{"prompt":"Assembly directives, also called pseudo-opcodes, pseudo-operations or pseudo-ops, are commands given to an assembler \"directing it to perform operations other than assembling instructions\". Directives affect how the assembler operates and \"may affect the object code, the symbol table, the listing file, and the values of internal assembler parameters\". Sometimes the term pseudo-opcode is reserved for directives that generate object code, such as those that generate data.The names of pseudo-ops often start with a dot to distinguish them from machine instructions.  Pseudo-ops can make the assembly of the program dependent on parameters input by a programmer, so that one program can be assembled in different ways, perhaps for different applications. Or, a pseudo-op can be used to manipulate presentation of a program to make it easier to read and maintain. Another common use of pseudo-ops is to reserve storage areas for run-time data and optionally initialize their contents to known values.\nSymbolic assemblers let programmers associate arbitrary names (labels or symbols) with memory locations and various constants. Usually, every constant and variable is given a name so instructions can reference those locations by name, thus promoting self-documenting code. In executable code, the name of each subroutine is associated with its entry point, so any calls to a subroutine can use its name. Inside subroutines, GOTO destinations are given labels. Some assemblers support local symbols which are often lexically distinct from normal symbols (e.g., the use of \"10$\" as a GOTO destination).\nSome assemblers, such as NASM, provide flexible symbol management, letting programmers manage different namespaces, automatically calculate offsets within data structures, and assign labels that refer to literal values or the result of simple computations performed by the assembler. Labels can also be used to initialize constants and variables with relocatable addresses.\nAssembly languages, like most other computer languages, allow comments to be added to program source code that will be ignored during assembly. Judicious commenting is essential in assembly language programs, as the meaning and purpose of a sequence of binary machine instructions can be difficult to determine. The \"raw\" (uncommented) assembly language generated by compilers or disassemblers is quite difficult to read when changes must be made.","completion":"Programming Languages"}
{"prompt":"The Racket programming language and RascalMPL were designed to support language-oriented programming from the ground up. Other language workbench tools such as JetBrains MPS, Kermeta, or Xtext provide the tools to design and implement DSLs and language-oriented programming.Erdweg, Sebastian (2013). \"The State of the Art in Language Workbenches\". Software Language Engineering. Lecture Notes in Computer Science. Vol. 8225. pp. 197\u2013217. doi:10.1007\/978-3-319-02654-1_11. ISBN 978-3-319-02653-4. S2CID 5234848. Retrieved 4 July 2023.","completion":"Programming Languages"}
{"prompt":"Just as software engineering (as a process) is defined by differing methodologies, so the programming languages (as models of computation) are defined by differing paradigms. Some languages are designed to support one paradigm (Smalltalk supports object-oriented programming, Haskell supports functional programming), while other programming languages support multiple paradigms (such as Object Pascal, C++, Java, JavaScript, C#, Scala, Visual Basic, Common Lisp, Scheme, Perl, PHP, Python, Ruby, Oz, and F#). For example, programs written in C++, Object Pascal or PHP can be purely procedural, purely object-oriented, or can contain elements of both or other paradigms. Software designers and programmers decide how to use those paradigm elements.\nIn object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes that act on a logical shared data structures.\nMany programming paradigms are as well known for the techniques they forbid as for those they enable. For instance, pure functional programming disallows use of side-effects, while structured programming disallows use of the goto statement. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles. Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.\nProgramming paradigms can also be compared with programming models, which allows invoking an execution model by using only an API. Programming models can also be classified into paradigms based on features of the execution model.\nFor parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  Thus, it is more convenient to use a base sequential language and insert API calls to parallel execution models via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.","completion":"Programming Languages"}
{"prompt":"A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.","completion":"Programming Languages"}
{"prompt":"There are at least three different algorithms that decide whether and how a given regex matches a string.\nThe oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton (NFA) to be transformed into a deterministic finite automaton (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size m has the time and memory cost of O(2m), but it can be run on a string of size n in time O(n).  Note that the size of the expression is the size after abbreviations, such as numeric quantifiers, have been expanded.\nAn alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to O(mn). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the \"lazy DFA\" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky. Modern implementations include the re1-re2-sregex family based on Cox's code.\nThe third algorithm is to match the pattern against the input string by backtracking. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like (a|aa)*b that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called Regular expression Denial of Service (ReDoS).\nAlthough backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match. GNU grep (and the underlying gnulib DFA) uses such a strategy.Sublinear runtime algorithms have been achieved using Boyer-Moore (BM) based algorithms and related DFA optimization techniques such as the reverse scan. GNU grep, which supports a wide variety of POSIX syntaxes and extensions, uses BM for a first-pass prefiltering, and then uses an implicit DFA. Wu agrep, which implements approximate matching, combines the prefiltering into the DFA in BDM (backward DAWG matching). NR-grep's BNDM extends the BDM technique with Shift-Or bit-level parallelism.A few theoretical alternatives to backtracking for backreferences exist, and their \"exponents\" are tamer in that they are only related to the number of backreferences, a fixed property of some regexp languages such as POSIX. One naive method that duplicates a non-backtracking NFA for each backreference note has a complexity of \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n            k\n            +\n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathrm {O} }(n^{2k+2})}\n   time and \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          n\n          \n            2\n            k\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathrm {O} }(n^{2k+1})}\n   space for a haystack of length n and k backreferences in the RegExp. A very recent theoretical work based on memory automata gives a tighter bound based on \"active\" variable nodes used, and a polynomial possibility for some backreferenced regexps.","completion":"Programming Languages"}
{"prompt":"Set theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.\nIn discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.","completion":"Programming Languages"}
{"prompt":"IBM's FORTRAN II appeared in 1958.  The main enhancement was to support procedural programming by allowing user-written subroutines and functions which returned values with parameters passed by reference.  The COMMON statement provided a way for subroutines to access common (or global) variables. Six new statements were introduced:\nSUBROUTINE, FUNCTION, and END\nCALL and RETURN\nCOMMONOver the next few years, FORTRAN II added support for the DOUBLE PRECISION and COMPLEX data types.\nEarly FORTRAN compilers supported no recursion in subroutines. Early computer architectures supported no concept of a stack, and when they did directly support subroutine calls, the return location was often stored in one fixed location adjacent to the subroutine code (e.g. the IBM 1130) or a specific machine register (IBM 360 et seq), which only allows recursion if a stack is maintained by software and the return address is stored on the stack before the call is made and restored after the call returns. Although not specified in FORTRAN 77, many F77 compilers supported recursion as an option, and the Burroughs mainframes, designed with recursion built-in, did so by default. It became a standard in Fortran 90 via the new keyword RECURSIVE.","completion":"Programming Languages"}
{"prompt":"2017 (for 2007): Statistically Rigorous Java Performance Evaluation, Andy Georges, Dries Buytaert, Lieven Eeckhout\n2016 (for 2006): The DaCapo benchmarks: Java benchmarking development and analysis, Stephen M. Blackburn, Robin Garner, Chris Hoffmann, Asjad M. Khan, Kathryn S. McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton, Samuel Z. Guyer, Martin Hirzel, Antony Hosking, Maria Jump, Han Lee, J. Eliot B. Moss, Aashish Phansalkar, Darko Stefanovi\u0107, Thomas VanDrunen, Daniel von Dincklage, Ben Wiedermann\n2015 (for 2005): X10: An Object-Oriented Approach to Non-Uniform Cluster Computing, Philippe Charles, Christian Grothoff, Vijay Saraswat, Christopher Donawa, Allan Kielstra, Kemal Ebcioglu, Christoph von Praun, and Vivek Sarkar\n2014 (for 2004): Mirrors: Design Principles for Meta-level Facilities of Object-Oriented Programming Languages, Gilad Bracha and David Ungar\n2013 (for 2003): Language Support for Lightweight Transactions, Tim Harris and Keir Fraser\n2012 (for 2002): Reconsidering Custom Memory Allocation, Emery D. Berger, Benjamin G. Zorn, and Kathryn S. McKinley\n2010 (for 2000): Adaptive Optimization in the Jalape\u00f1o JVM, Matthew Arnold, Stephen Fink, David Grove, Michael Hind, and Peter F. Sweeney\n2009 (for 1999): Implementing Jalape\u00f1o in Java, Bowen Alpern, C. R. Attanasio, John J. Barton, Anthony Cocchi, Susan Flynn Hummel, Derek Lieber, Ton Ngo, Mark Mergen, Janice C. Shepherd, and Stephen Smith\n2008 (for 1998): Ownership Types for Flexible Alias Protection, David G. Clarke, John M. Potter, and James Noble\n2007 (for 1997): Call Graph Construction in Object-Oriented Languages, David Grove, Greg DeFouw, Jeffrey Dean, and Craig Chambers\n2006 (for 1986\u20131996):\nSubject Oriented Programming: A Critique of Pure Objects, William Harrison and Harold Ossher\nConcepts and Experiments in Computational Reflection, Pattie Maes\nSelf: The Power of Simplicity, David Ungar and Randall B. Smith","completion":"Programming Languages"}
{"prompt":"The debugging process normally begins with identifying the steps to reproduce the problem. This can be a non-trivial task, particularly with parallel processes and some Heisenbugs for example. The specific user environment and usage history can also make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it crash when parsing a large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Simplification may be done manually using a divide-and-conquer approach, in which the programmer attempts to remove some parts of original test case then checks if the problem still occurs. When debugging in a GUI, the programmer can try skipping some user interaction from the original problem description to check if the remaining actions are sufficient for causing the bug to occur.\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, tracing can be used. In simple cases, tracing is just a few print statements which output the values of variables at particular points during the execution of the program.","completion":"Programming Languages"}
{"prompt":"An HDL is grossly similar to a software programming language, but there are major differences. Most programming languages are inherently procedural (single-threaded), with limited syntactical and semantic support to handle concurrency. HDLs, on the other hand, resemble concurrent programming languages in their ability to model multiple parallel processes (such as flip-flops and adders) that automatically execute independently of one another. Any change to the process's input automatically triggers an update in the simulator's process stack.\nBoth programming languages and HDLs are processed by a compiler (often called a synthesizer in the HDL case), but with different goals. For HDLs, \"compiling\" refers to logic synthesis; the process of transforming the HDL code listing into a physically realizable gate netlist. The netlist output can take any of many forms: a \"simulation\" netlist with gate-delay information, a \"handoff\" netlist for post-synthesis placement and routing on a semiconductor die, or a generic industry-standard Electronic Design Interchange Format (EDIF) (for subsequent conversion to a JEDEC-format file).\nOn the other hand, a software compiler converts the source-code listing into a microprocessor-specific object code for execution on the target microprocessor. As HDLs and programming languages borrow concepts and features from each other, the boundary between them is becoming less distinct. However, pure HDLs are unsuitable for general purpose application software development, just as general-purpose programming languages are undesirable for modeling hardware.\nYet as electronic systems grow increasingly complex, and reconfigurable systems become increasingly common, there is growing desire in the industry for a single language that can perform some tasks of both hardware design and software programming. SystemC is an example of such\u2014embedded system hardware can be modeled as non-detailed architectural blocks (black boxes with modeled signal inputs and output drivers). The target application is written in C or C++ and natively compiled for the host-development system; as opposed to targeting the embedded CPU, which requires host-simulation of the embedded CPU or an emulated CPU.\nThe high level of abstraction of SystemC models is well suited to early architecture exploration, as architectural modifications can be easily evaluated with little concern for signal-level implementation issues. However, the threading model used in SystemC relies on shared memory, causing the language not to handle parallel execution or low-level models well.","completion":"Programming Languages"}
{"prompt":"ABS\nAbsolute value\nATN\nArctangent (result in radians)\nCOS\nCosine (argument in radians)\nEXP\nExponential function\nINT\nInteger part (typically floor function)\nLOG\nNatural logarithm\nRND\nRandom number generation\nSIN\nSine (argument in radians)\nSQR\nSquare root\nTAN\nTangent (argument in radians)","completion":"Programming Languages"}
{"prompt":"As an example, consider this possible BNF for a U.S. postal address:\n\n \nThis translates into English as: \n\nA postal address consists of a name-part, followed by a street-address part, followed by a zip-code part.\nA name-part consists of either: a personal-part followed by a last name followed by an optional suffix (Jr., Sr., or dynastic number) and end-of-line, or a personal part followed by a name part (this rule illustrates the use of recursion in BNFs, covering the case of people who use multiple first and middle names and initials).\nA personal-part consists of either a first name or an initial followed by a dot.\nA street address consists of a house number, followed by a street name, followed by an optional apartment specifier, followed by an end-of-line.\nA zip-part consists of a town-name, followed by a comma, followed by a state code, followed by a ZIP-code followed by an end-of-line.\nAn opt-suffix-part consists of a suffix, such as \"Sr.\", \"Jr.\" or a roman-numeral, or an empty string (i.e. nothing).\nAn opt-apt-num consists of an apartment number or an empty string (i.e. nothing).Note that many things (such as the format of a first-name, apartment number, ZIP-code, and Roman numeral) are left unspecified here. If necessary, they may be described using additional BNF rules.","completion":"Programming Languages"}
{"prompt":"Perl's culture and community has developed alongside the language itself. Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. The community that surrounds Perl was, in fact, the topic of Wall's first \"State of the Onion\" talk.State of the Onion is the name for Wall's yearly keynote-style summaries on the progress of Perl and its community.  They are characterized by his hallmark humor, employing references to Perl's culture, the wider hacker culture, Wall's linguistic background, sometimes his family life, and occasionally even his Christian background. Each talk is first given at various Perl conferences and is eventually also published online.\nIn email, Usenet, and message board postings, \"Just another Perl hacker\" (JAPH) programs are a common trend, originated by Randal L. Schwartz, one of the earliest professional Perl trainers. In the parlance of Perl culture, Perl programmers are known as Perl hackers, and from this derives the practice of writing short programs to print out the phrase \"Just another Perl hacker\". In the spirit of the original concept, these programs are moderately obfuscated and short enough to fit into the signature of an email or Usenet message. The \"canonical\" JAPH as developed by Schwartz includes the comma at the end, although this is often omitted.\nPerl \"golf\" is the pastime of reducing the number of characters (key \"strokes\") used in a Perl program to the bare minimum, much in the same way that golf players seek to take as few shots as possible in a round. The phrase's first use emphasized the difference between pedestrian code meant to teach a newcomer and terse hacks likely to amuse experienced Perl programmers, an example of the latter being JAPHs that were already used in signatures in Usenet postings and elsewhere. Similar stunts had been an unnamed pastime in the language APL in previous decades. The use of Perl to write a program that performed RSA encryption prompted a widespread and practical interest in this pastime. In subsequent years, the term \"code golf\" has been applied to the pastime in other languages. A Perl Golf Apocalypse was held at Perl Conference 4.0 in Monterey, California in July 2000.\nAs with C, obfuscated code competitions were a well known pastime in the late 1990s. The Obfuscated Perl Contest was a competition held by The Perl Journal from 1996 to 2000 that made an arch virtue of Perl's syntactic flexibility. Awards were given for categories such as \"most powerful\"\u2014programs that made efficient use of space\u2014and \"best four-line signature\" for programs that fit into four lines of 76 characters in the style of a Usenet signature block.Perl poetry is the practice of writing poems that can be compiled as legal Perl code, for example the piece known as Black Perl. Perl poetry is made possible by the large number of English words that are used in the Perl language. New poems are regularly submitted to the community at PerlMonks.","completion":"Programming Languages"}
{"prompt":"Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming \u2014 advanced computing technologies and sophisticated new languages and programming tools \u2014 have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.Programmers work in many settings, including corporate information technology (IT) departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is considered a profession.Programmers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours. More complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer's supervision.","completion":"Programming Languages"}
{"prompt":"Starting in the early 1980s, IBM APL development, under the leadership of Jim Brown, implemented a new version of the APL language that contained as its primary enhancement the concept of nested arrays, where an array can contain other arrays, and new language features which facilitated integrating nested arrays into program workflow. Ken Iverson, no longer in control of the development of the APL language, left IBM and joined I. P. Sharp Associates, where one of his major contributions was directing the evolution of Sharp APL to be more in accord with his vision. APL2 was first released for CMS and TSO in 1984. The APL2 Workstation edition (Windows, OS\/2, AIX, Linux, and Solaris) followed later.As other vendors were busy developing APL interpreters for new hardware, notably Unix-based microcomputers, APL2 was almost always the standard chosen for new APL interpreter developments. Even today, most APL vendors or their users cite APL2 compatibility as a selling point for those products. IBM cites its use for problem solving, system design, prototyping, engineering and scientific computations, expert systems, for teaching mathematics and other subjects, visualization and database access.","completion":"Programming Languages"}
{"prompt":"Class-based object-oriented programming languages support objects defined by their class. Class definitions include member data. Message passing is a key concept, if not the main concept, in object-oriented languages.\nPolymorphic functions parameterized by the class of some of their arguments are typically called methods. In languages with single dispatch, classes typically also include method definitions. In languages with multiple dispatch, methods are defined by generic functions. There are exceptions where single dispatch methods are generic functions (e.g. Bigloo's object system).","completion":"Programming Languages"}
{"prompt":"Assembly languages were not available at the time when the stored-program computer was introduced. Kathleen Booth \"is credited with inventing assembly language\" based on theoretical work she began in 1947, while working on the ARC2 at Birkbeck, University of London following consultation by Andrew Booth (later her husband) with mathematician John von Neumann and physicist Herman Goldstine at the Institute for Advanced Study.In late 1948, the Electronic Delay Storage Automatic Calculator (EDSAC) had an assembler (named \"initial orders\") integrated into its bootstrap program. It used one-letter mnemonics developed by David Wheeler, who is credited by the IEEE Computer Society as the creator of the first \"assembler\". Reports on the EDSAC introduced the term \"assembly\" for the process of combining fields into an instruction word. SOAP (Symbolic Optimal Assembly Program) was an assembly language for the IBM 650 computer written by Stan Poley in 1955.Assembly languages eliminate much of the error-prone, tedious, and time-consuming first-generation programming needed with the earliest computers, freeing programmers from tedium such as remembering numeric codes and calculating addresses. They were once widely used for all sorts of programming. However, by the late 1950s, their use had largely been supplanted by higher-level languages, in the search for improved programming productivity. Today, assembly language is still used for direct hardware manipulation, access to specialized processor instructions, or to address critical performance issues. Typical uses are device drivers, low-level embedded systems, and real-time systems (see \u00a7 Current usage).\nNumerous programs have been written entirely in assembly language. The Burroughs MCP (1961) was the first computer for which an operating system was not developed entirely in assembly language; it was written in Executive Systems Problem Oriented Language (ESPOL), an Algol dialect. Many commercial applications were written in assembly language as well, including a large amount of the IBM mainframe software written by large corporations. COBOL, FORTRAN and some PL\/I eventually displaced much of this work, although a number of large organizations retained assembly-language application infrastructures well into the 1990s.\nAssembly language has long been the primary development language for 8-bit home computers such Atari 8-bit family, Apple II, MSX, ZX Spectrum, and Commodore 64. Interpreted BASIC dialects on these systems offer insufficient execution speed and insufficient facilities to take full advantage of the available hardware. These systems have severe resource constraints,  idiosyncratic memory and display architectures, and provide limited system services. There are also few high-level language compilers suitable for microcomputer use. Similarly, assembly language is the default choice for 8-bit consoles such as the Atari 2600 and Nintendo Entertainment System.\nKey software for IBM PC compatibles was written in assembly language, such as MS-DOS, Turbo Pascal, and the Lotus 1-2-3 spreadsheet. As computer speed grew exponentially, assembly language became a tool for speeding up parts of programs, such as the rendering of Doom, rather than a dominant development language. In the 1990s, assembly language was used to get performance out of systems such as the Sega Saturn  and as the primary language for arcade hardware based on the TMS34010 integrated CPU\/GPU such as Mortal Kombat and NBA Jam.","completion":"Programming Languages"}
{"prompt":"Essential to HDL design is the ability to simulate HDL programs. Simulation allows an HDL description of a design (called a model) to pass design verification, an important milestone that validates the design's intended function (specification) against the code implementation in the HDL description. It also permits architectural exploration. The engineer can experiment with design choices by writing multiple variations of a base design, then comparing their behavior in simulation. Thus, simulation is critical for successful HDL design.\nTo simulate an HDL model, an engineer writes a top-level simulation environment (called a test bench). At minimum, a testbench contains an instantiation of the model (called the device under test or DUT), pin\/signal declarations for the model's I\/O, and a clock waveform. The testbench code is event driven: the engineer writes HDL statements to implement the (testbench-generated) reset-signal, to model interface transactions (such as a host\u2013bus read\/write), and to monitor the DUT's output. An HDL simulator \u2014 the program that executes the testbench \u2014 maintains the simulator clock, which is the master reference for all events in the testbench simulation. Events occur only at the instants dictated by the testbench HDL (such as a reset-toggle coded into the testbench), or in reaction (by the model) to stimulus and triggering events. Modern HDL simulators have full-featured graphical user interfaces, complete with a suite of debug tools. These allow the user to stop and restart the simulation at any time, insert simulator breakpoints (independent of the HDL code), and monitor or modify any element in the HDL model hierarchy. Modern simulators can also link the HDL environment to user-compiled libraries, through a defined PLI\/VHPI interface. Linking is system-dependent (x86, SPARC etc. running Windows\/Linux\/Solaris), as the HDL simulator and user libraries are compiled and linked outside the HDL environment.\nDesign verification is often the most time-consuming portion of the design process, due to the disconnect between a device's functional specification, the designer's interpretation of the specification, and the imprecision of the HDL language. The majority of the initial test\/debug cycle is conducted in the HDL simulator environment, as the early stage of the design is subject to frequent and major circuit changes. An HDL description can also be prototyped and tested in hardware \u2014 programmable logic devices are often used for this purpose. Hardware prototyping is comparatively more expensive than HDL simulation, but offers a real-world view of the design. Prototyping is the best way to check interfacing against other hardware devices and hardware prototypes. Even those running on slow FPGAs offer much shorter simulation times than pure HDL simulation.","completion":"Programming Languages"}
{"prompt":"Mod51 by Mandeno Granville Electronics is based on ISO Modula-2 with language extensions for embedded development following IEC 1131, an industry standard for programmable logic controllers (PLC) closely related to Modula-2. The Mod51 compiler generates standalone code for 80C51 based microcontrollers.","completion":"Programming Languages"}
{"prompt":"Interpreters are frequently used to execute command languages, and glue languages since each operator executed in command language is usually an invocation of a complex routine such as an editor or compiler.\nSelf-modifying code can easily be implemented in an interpreted language. This relates to the origins of interpretation in Lisp and artificial intelligence research.\nVirtualization. Machine code intended for a hardware architecture can be run using a virtual machine. This is often used when the intended architecture is unavailable, or among other uses, for running multiple copies.\nSandboxing: While some types of sandboxes rely on operating system protections, an interpreter or virtual machine is often used. The actual hardware architecture and the originally intended hardware architecture may or may not be the same. This may seem pointless, except that sandboxes are not compelled to actually execute all the instructions the source code it is processing. In particular, it can refuse to execute code that violates any security constraints it is operating under.\nEmulators for running computer software written for obsolete and unavailable hardware on more modern equipment.","completion":"Programming Languages"}
{"prompt":"The introduction of the first microcomputers in the mid-1970s was the start of explosive growth for BASIC. It had the advantage that it was fairly well known to the young designers and computer hobbyists who took an interest in microcomputers, many of whom had seen BASIC on minis or mainframes. Despite Dijkstra's famous judgement in 1975, \"It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration\", BASIC was one of the few languages that was both high-level enough to be usable by those without training and small enough to fit into the microcomputers of the day, making it the de facto standard programming language on early microcomputers.\nThe first microcomputer version of BASIC was co-written by Bill Gates, Paul Allen and Monte Davidoff for their newly formed company, Micro-Soft. This was released by MITS in punch tape format for the Altair 8800 shortly after the machine itself, immediately cementing BASIC as the primary language of early microcomputers. Members of the Homebrew Computer Club began circulating copies of the program, causing Gates to write his Open Letter to Hobbyists, complaining about this early example of software piracy.\nPartially in response to Gates's letter, and partially to make an even smaller BASIC that would run usefully on 4 KB machines, Bob Albrecht urged Dennis Allison to write their own variation of the language. How to design and implement a stripped-down version of an interpreter for the BASIC language was covered in articles by Allison in the first three quarterly issues of the People's Computer Company newsletter published in 1975 and implementations with source code published in Dr. Dobb's Journal of Tiny BASIC Calisthenics & Orthodontia: Running Light Without Overbyte. This led to a wide variety of Tiny BASICs with added features or other improvements, with versions from Tom Pittman and Li-Chen Wang becoming particularly well known.Micro-Soft, by this time Microsoft, ported their interpreter for the MOS 6502, which quickly become one of the most popular microprocessors of the 8-bit era. When new microcomputers began to appear, notably the \"1977 trinity\" of the TRS-80, Commodore PET and Apple II, they either included a version of the MS code, or quickly introduced new models with it. Ohio Scientific's personal computers also joined this trend at that time. By 1978, MS BASIC was a de facto standard and practically every home computer of the 1980s included it in ROM. Upon boot, a BASIC interpreter in direct mode was presented.\nCommodore Business Machines included Commodore BASIC, based on Microsoft BASIC. The Apple II and TRS-80 each had two versions of BASIC, a smaller introductory version introduced with the initial releases of the machines and an MS-based version introduced as interest in the platforms increased. As new companies entered the field, additional versions were added that subtly changed the BASIC family. The Atari 8-bit family had its own Atari BASIC that was modified in order to fit on an 8 KB ROM cartridge. Sinclair BASIC was introduced in 1980 with the Sinclair ZX80, and was later extended for the Sinclair ZX81 and the Sinclair ZX Spectrum. The BBC published BBC BASIC, developed by Acorn Computers Ltd, incorporating many extra structured programming keywords and advanced floating-point operation features.\nAs the popularity of BASIC grew in this period, computer magazines published complete source code in BASIC for video games, utilities, and other programs. Given BASIC's straightforward nature, it was a simple matter to type in the code from the magazine and execute the program. Different magazines were published featuring programs for specific computers, though some BASIC programs were considered universal and could be used in machines running any variant of BASIC (sometimes with minor adaptations). Many books of type-in programs were also available, and in particular, Ahl published versions of the original 101 BASIC games converted into the Microsoft dialect and published it from Creative Computing as BASIC Computer Games. This book, and its sequels, provided hundreds of ready-to-go programs that could be easily converted to practically any BASIC-running platform. The book reached the stores in 1978, just as the home computer market was starting off, and it became the first million-selling computer book. Later packages, such as Learn to Program BASIC would also have gaming as an introductory focus. On the business-focused CP\/M computers which soon became widespread in small business environments, Microsoft BASIC (MBASIC) was one of the leading applications.In 1978, David Lien published the first edition of The BASIC Handbook: An Encyclopedia of the BASIC Computer Language, documenting keywords across over 78 different computers. By 1981, the second edition documented keywords from over 250 different computers, showcasing the explosive growth of the microcomputer era.","completion":"Programming Languages"}
{"prompt":"Debugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates. The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as debuggers. Debuggers are software tools which enable the programmer to monitor the execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term debugger can also refer to the person who is doing the debugging.\nGenerally, high-level programming languages, such as Java, make debugging easier, because they have features such as exception handling and type checking that make real sources of erratic behaviour easier to spot. In programming languages such as C or assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugger tools may be needed.\nIn certain situations, general purpose software tools that are language specific in nature can be very useful. These take the form of static code analysis tools. These tools look for a very specific set of known problems, some common and some rare, within the source code, concentrating more on the semantics (e.g. data flow) rather than the syntax, as compilers and interpreters do.\nBoth commercial and free tools exist for various languages; some claim to be able to detect hundreds of different problems. These tools can be extremely useful when checking very large source trees, where it is impractical to do code walk-throughs. A typical example of a problem detected would be a variable dereference that occurs before the variable is assigned a value. As another example, some such tools perform strong type checking when the language does not require it. Thus, they are better at locating likely errors in code that is syntactically correct. But these tools have a reputation of false positives, where correct code is flagged as dubious. The old Unix lint program is an early example.\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers, or in-circuit emulators (ICEs) are often used, alone or in combination. An ICE may perform many of the typical software debugger's tasks on low-level software and firmware.","completion":"Programming Languages"}
{"prompt":"Edsger W. Dijkstra was born in Rotterdam.  His father was a chemist who was president of the Dutch Chemical Society; he taught chemistry at a secondary school and was later its superintendent. His mother was a mathematician, but never had a formal job.Dijkstra had considered a career in law and had hoped to represent the Netherlands in the United Nations. However, after graduating from school in 1948, at his parents' suggestion he studied mathematics and physics and then theoretical physics at the University of Leiden.In the early 1950s, electronic computers were a novelty. Dijkstra stumbled on his career by accident, and through his supervisor, Professor Johannes Haantjes, he met Adriaan van Wijngaarden, the director of the Computation Department at the Mathematical Center in Amsterdam, who offered Dijkstra a job; he officially became the Netherlands' first \"programmer\" in March 1952.For some time Dijkstra remained committed to physics, working on it in Leiden three days out of each week. With increasing exposure to computing, however, his focus began to shift. As he recalled:\nAfter having programmed for some three years, I had a discussion with A. van Wijngaarden, who was then my boss at the Mathematical Center in Amsterdam, a discussion for which I shall remain grateful to him as long as I live. The point was that I was supposed to study theoretical physics at the University of Leiden simultaneously, and as I found the two activities harder and harder to combine, I had to make up my mind, either to stop programming and become a real, respectable theoretical physicist, or to carry my study of physics to a formal completion only, with a minimum of effort, and to become....., yes what? A programmer? But was that a respectable profession? For after all, what was programming? Where was the sound body of knowledge that could support it as an intellectually respectable discipline? I remember quite vividly how I envied my hardware colleagues, who, when asked about their professional competence, could at least point out that they knew everything about vacuum tubes, amplifiers and the rest, whereas I felt that, when faced with that question, I would stand empty-handed. Full of misgivings I knocked on Van Wijngaarden's office door, asking him whether I could \"speak to him for a moment\"; when I left his office a number of hours later, I was another person. For after having listened to my problems patiently, he agreed that up till that moment there was not much of a programming discipline, but then he went on to explain quietly that automatic computers were here to stay, that we were just at the beginning and could not I be one of the persons called to make programming a respectable discipline in the years to come? This was a turning point in my life and I completed my study of physics formally as quickly as I could. \nWhen Dijkstra married Maria (Ria) C. Debets in 1957, he was required as a part of the marriage rites to state his profession. He stated that he was a programmer, which was unacceptable to the authorities, there being no such profession then in The Netherlands.In 1959, he received his PhD from the University of Amsterdam for a thesis entitled 'Communication with an Automatic Computer', devoted to a description of the assembly language designed for the first commercial computer developed in the Netherlands, the Electrologica X1. His thesis supervisor was Van Wijngaarden.","completion":"Programming Languages"}
{"prompt":"Many esoteric programming languages are designed to produce code that is deeply obfuscated, making it difficult to read and to write. The purpose of this may be to provide an interesting puzzle or challenge for program writers: Malbolge for instance was explicitly designed to be challenging, and so it has features like self-modifying code and highly counterintuitive operations. On the other hand, some esoteric languages become difficult to write due to their other design choices. Brainfuck is committed to the idea of a minimalist instruction set, so even though its instructions are straightforward in principle, the code that arises is difficult for a human to read. INTERCAL's difficulty arises as a result of the choice to avoid operations used in any other programming language, which stems from its origin as a parody of other languages.","completion":"Programming Languages"}
{"prompt":"The fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\nKenneth E. Iverson described the rationale behind array programming (actually referring to APL) as follows:\nmost programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician.\nThe thesis is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.\nIndeed, the very suggestiveness of a notation may make it seem harder to learn because of the many properties it suggests for explorations.\n[...]\n\nUsers of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted since a clear statement of an algorithm can usually be used as a basis from which one may easily derive a more efficient algorithm.\nThe basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\nFunction rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.","completion":"Programming Languages"}
{"prompt":"The Hello world program is used by virtually all texts to new programming languages as the first program learned to show the most basic syntax and environment of the language. For Smalltalk, the program is extremely simple to write.  The following code, the message \"show:\" is sent to the object \"Transcript\" with the String literal 'Hello, world!' as its argument. Invocation of the \"show:\" method causes the characters of its argument (the String literal 'Hello, world!') to be displayed in the transcript (\"terminal\") window.\n\nNote that a Transcript window would need to be open in order to see the results of this example.","completion":"Programming Languages"}
{"prompt":"Interpreters were used as early as 1952 to ease programming within the limitations of computers at the time (e.g. a shortage of program storage space, or no native support for floating point numbers). Interpreters were also used to translate between low-level machine languages, allowing code to be written for machines that were still under construction and tested on computers that already existed. The first interpreted high-level language was Lisp. Lisp was first implemented by Steve Russell on an IBM 704 computer. Russell had read John McCarthy's paper, \"Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I\", and realized (to McCarthy's surprise) that the Lisp eval function could be implemented in machine code. The result was a working Lisp interpreter which could be used to run Lisp programs, or more properly, \"evaluate Lisp expressions\".","completion":"Programming Languages"}
{"prompt":"The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages. The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.Large Perl programs start more slowly than similar programs in compiled languages because Perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article \"A Timely Start\", Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path. Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling. Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.A number of tools have been introduced to improve this situation. The first such tool was Apache's mod_perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.Once Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.The most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex, but flexible, XS mechanism.","completion":"Programming Languages"}
{"prompt":"The dependency-based parse trees of dependency grammars see all nodes as terminal, which means they do not acknowledge the distinction between terminal and non-terminal categories. They are simpler on average than constituency-based parse trees because they contain fewer nodes. The dependency-based parse tree for the example sentence above is as follows:\n\nThis parse tree lacks the phrasal categories (S, VP, and NP) seen in the constituency-based counterpart above. Like the constituency-based tree, constituent structure is acknowledged. Any complete sub-tree of the tree is a constituent. Thus this dependency-based parse tree acknowledges the subject noun John and the object noun phrase the ball as constituents just like the constituency-based parse tree does.\nThe constituency vs. dependency distinction is far-reaching. Whether the additional syntactic structure associated with constituency-based parse trees is necessary or beneficial is a matter of debate.","completion":"Programming Languages"}
{"prompt":"Weak typing allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.\nStrong typing prevents these program faults. An attempt to perform an operation on the wrong type of value raises an error. Strongly-typed languages are often termed type-safe or safe.\nAn alternative definition for \"weakly typed\" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors. Strong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.It may seem odd to some professional programmers that C could be \"weakly, statically typed\". However, the use of the generic pointer, the void* pointer, does allow casting pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).","completion":"Programming Languages"}
{"prompt":"Chef is a stack-oriented programming language created by David Morgan-Mar, designed to make programs look like cooking recipes. Programs consist of a title, a list of variables and their data values, and a list of stack manipulation instructions. A joking design principle states that \"program recipes should not only generate valid output, but be easy to prepare and delicious\", and Morgan-Mar notes that an example Hello World program with \"101 eggs\" and \"111 cups oil\" would produce \"a lot of food for one person.\"","completion":"Programming Languages"}
{"prompt":"2023: OCaml\n2022: CompCert\n2021: WebAssembly\n2020: Pin (computer program)\n2019: Scala (programming language)\n2018: Racket (programming language)\n2016: V8 (JavaScript engine)\n2015: Z3 Theorem Prover\n2014: GNU Compiler Collection (GCC)\n2013: Coq proof assistant\n2012: Jikes Research Virtual Machine (RVM)\n2011: Simon Peyton Jones and Simon Marlow (Glasgow Haskell Compiler)\n2010: Chris Lattner (LLVM)","completion":"Programming Languages"}
{"prompt":"Prolog is a homoiconic language and provides many facilities for reflection. Its implicit execution strategy makes it possible to write a concise meta-circular evaluator (also called meta-interpreter) for pure Prolog code:\n\nwhere true represents an empty conjunction, and clause(Head, Body) unifies with clauses in the database of the form Head :- Body.\nSince Prolog programs are themselves sequences of Prolog terms (:-\/2 is an infix operator) that are easily read and inspected using built-in mechanisms (like read\/1), it is possible to write customized interpreters that augment Prolog with domain-specific features. For example, Sterling and Shapiro present a meta-interpreter that performs reasoning with uncertainty, reproduced here with slight modifications::\u200a330\u200a\n\nThis interpreter uses a table of built-in Prolog predicates of the form:\u200a327\u200a\n\nand clauses represented as clause_cf(Head, Body, Certainty). Given those, it can be called as solve(Goal, Certainty) to execute Goal and obtain a measure of certainty about the result.","completion":"Programming Languages"}
{"prompt":"A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.","completion":"Software Engineering"}
{"prompt":"Social computing is an area that is concerned with the intersection of social behavior and computational systems. Human\u2013computer interaction research develops theories, principles, and guidelines for user interface designers.","completion":"Software Engineering"}
{"prompt":"When fixing a bug, it is a good practice to push a test case that reproduces the bug. This avoids the fix to be reverted, and the bug to reappear, which is known as a regression.","completion":"Software Engineering"}
{"prompt":"Mills was an IBM Fellow and Member of the Corporate Technical Committee at IBM, a Technical Staff Member at GE\nand RCA, and President of Mathematica and Software Engineering Technology. At GE, he developed a three-month curriculum in management science attended by hundreds of GE executives. At IBM, he was the primary architect of the IBM Software Engineering Institute where thousands of IBM software personnel were trained in the mathematical foundations of software. He later embodied the mathematical and statistical principles for software in the Cleanroom software engineering process. As founder of Software Engineering Technology, he created an enterprise for Cleanroom technology transfer.","completion":"Software Engineering"}
{"prompt":"Increase business revenue by ensuring the system can process transactions within the requisite timeframe\nEliminate system failure requiring scrapping and writing off the system development effort due to performance objective failure\nEliminate late system deployment due to performance issues\nEliminate avoidable system rework due to performance issues\nEliminate avoidable system tuning efforts\nAvoid additional and unnecessary hardware acquisition costs\nReduce increased software maintenance costs due to performance problems in production\nReduce increased software maintenance costs due to software impacted by ad hoc performance fixes\nReduce additional operational overhead for handling system issues due to performance problems\nIdentify future bottlenecks by simulation over prototype\nIncrease server capability","completion":"Software Engineering"}
{"prompt":"Data models represent information areas of interest. While there are many ways to create data models, according to Len Silverston (1997) only two modeling methodologies stand out, top-down and bottom-up:  \n\nBottom-up models or View Integration models are often the result of a reengineering effort. They usually start with existing data structures forms, fields on application screens, or reports. These models are usually physical, application-specific, and incomplete from an enterprise perspective. They may not promote data sharing, especially if they are built without reference to other parts of the organization.\nTop-down logical data models, on the other hand, are created in an abstract way by getting information from people who know the subject area. A system may not implement all the entities in a logical model, but the model serves as a reference point or template.Sometimes models are created in a mixture of the two methods: by considering the data needs and structure of an application and by consistently referencing a subject-area model. In many environments the distinction between a logical data model and a physical data model is blurred. In addition, some CASE tools don't make a distinction between logical and physical data models.","completion":"Software Engineering"}
{"prompt":"There are several prizes in the field of software engineering:\nThe Codie awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.\nJolt Awards are awards in the software industry.\nStevens Award is a software engineering award given in memory of Wayne Stevens.\nHarlan Mills Award for \"contributions to the theory and practice of the information sciences, focused on software engineering\".","completion":"Software Engineering"}
{"prompt":"Coding standard is an agreed upon set of rules that the entire development team agree to adhere to throughout the project.  The standard specifies a consistent style and format for source code, within the chosen programming language, as well as various programming constructs and patterns that should be avoided in order to reduce the probability of defects.  The coding standard may be a standard conventions specified by the language vendor (e.g. The Code Conventions for the Java Programming Language, recommended by Sun), or custom defined by the development team.\nExtreme Programming backers advocate code that is self-documenting to the furthest degree possible.  This reduces the need for code comments, which can get out of sync with the code itself.","completion":"Software Engineering"}
{"prompt":"DNA-based computing and quantum computing are areas of active research for both computing hardware and software, such as the development of quantum algorithms. Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits, including those based on Josephson junctions and rapid single flux quantum technology, are becoming more nearly realizable with the discovery of nanoscale superconductors.Fiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.Another field of research is spintronics. Spintronics can provide computing power and storage, without heat buildup. Some research is being done on hybrid chips, which combine photonics and spintronics. There is also research ongoing on combining plasmonics, photonics, and electronics.","completion":"Software Engineering"}
{"prompt":"Testing is central to extreme programming. Extreme programming's approach is that if a little testing can eliminate a few flaws, a lot of testing can eliminate many more flaws.\n\nUnit tests determine whether a given feature works as intended. Programmers write as many automated tests as they can think of that might \"break\" the code; if all tests run successfully, then the coding is complete. Every piece of code that is written is tested before moving on to the next feature.\nAcceptance tests verify that the requirements as understood by the programmers satisfy the customer's actual requirements.System-wide integration testing was encouraged, initially, as a daily end-of-day activity, for early detection of incompatible interfaces, to reconnect before the separate sections diverged widely from coherent functionality. However, system-wide integration testing has been reduced, to weekly, or less often, depending on the stability of the overall interfaces in the system.","completion":"Software Engineering"}
{"prompt":"To satisfy the data modeling enhancement requirements that were identified in the IISS-6202 project, a sub-contractor, DACOM, obtained a license to the logical database design technique (LDDT) and its supporting software (ADAM). LDDT had been developed in 1982 by Robert G. Brown of The Database Design Group entirely outside the IDEF program and with no knowledge of IDEF1. LDDT combined elements of the relational data model, the E\u2013R model, and generalization in a way specifically intended to support data modeling and the transformation of the data models into database designs. The graphic syntax of LDDT differed from that of IDEF1 and, more importantly, LDDT contained interrelated modeling concepts not present in IDEF1. Mary E. Loomis wrote a concise summary of the syntax and semantics of a substantial subset of LDDT, using terminology compatible with IDEF1 wherever possible. DACOM labeled the result IDEF1X and supplied it to the ICAM program.Because the IDEF program was funded by the government, the techniques are in the public domain. In addition to the ADAM software, sold by DACOM under the name Leverage, a number of CASE tools use IDEF1X as their representation technique for data modeling.\nThe IISS projects actually produced working prototypes of an information processing environment that would run in heterogeneous computing environments. Current advancements in such techniques as Java and JDBC are now achieving the goals of ubiquity and versatility across computing environments which was first demonstrated by IISS.","completion":"Software Engineering"}
{"prompt":"Since 2010, the Ministry of Health (MoH) has been working on the Malaysian Health Data Warehouse (MyHDW) project. MyHDW aims to meet the diverse needs of timely health information provision and management, and acts as a platform for the standardization and integration of health data from a variety of sources (Health Informatics Centre, 2013). The Ministry of Health has embarked on introducing the electronic Hospital Information Systems (HIS) in several public hospitals including Putrajaya Hospital, Serdang Hospital and Selayang Hospital. Similarly, under Ministry of Higher Education, hospitals such as University of Malaya Medical Centre (UMMC) and University Kebangsaan Malaysia Medical Centre (UKMMC) are also using HIS for healthcare delivery.\nA hospital information system (HIS) is a comprehensive, integrated information system designed to manage the administrative, financial and clinical aspects of a hospital. As an area of medical informatics, the aim of hospital information system is to achieve the best possible support of patient care and administration by electronic data processing. HIS plays a vital role in planning, initiating, organizing and controlling the operations of the subsystems of the hospital and thus provides a synergistic organization in the process.","completion":"Software Engineering"}
{"prompt":"Leadership\nCoaching\nCommunication\nListening\nMotivation\nVision, SEs are good at this\nExample, everyone follows a good example best\nHuman resource management\nHiring, getting people into an organization\nTraining\nEvaluation\nProject management\nGoal setting\nCustomer interaction (Rethink)\nEstimation\nRisk management\nChange management\nProcess management\nSoftware development processes\nMetrics","completion":"Software Engineering"}
{"prompt":"In Nepal, Bachelor of Science in Computer Science and Information Technology (B.Sc.CSIT ) is a four-year course of study. The Bachelor of Computer Science and Information Technology is provided by Tribhuvan University and the degree awarded is referred to as BScCSIT.","completion":"Software Engineering"}
{"prompt":"Health Informatics projects in Canada are implemented provincially, with different provinces creating different systems. A national, federally funded, not-for-profit organisation called Canada Health Infoway was created in 2001 to foster the development and adoption of electronic health records across Canada. As of December 31, 2008, there were 276 EHR projects under way in Canadian hospitals, other health-care facilities, pharmacies and laboratories, with an investment value of $1.5-billion from Canada Health Infoway.Provincial and territorial programmes include the following:\n\neHealth Ontario was created as an Ontario provincial government agency in September 2008. It has been plagued by delays and its CEO was fired over a multimillion-dollar contracts scandal in 2009.\nAlberta Netcare was created in 2003 by the Government of Alberta. Today the netCARE portal is used daily by thousands of clinicians. It provides access to demographic data, prescribed\/dispensed drugs, known allergies\/intolerances, immunizations, laboratory test results, diagnostic imaging reports, the diabetes registry and other medical reports. netCARE interface capabilities are being included in electronic medical record products that are being funded by the provincial government.","completion":"Software Engineering"}
{"prompt":"An ER model is primarily conceptual, an ontology that expresses predicates in a domain of knowledge.\nER models are readily used to represent relational database structures (after Codd and Date) but not so often to represent other kinds of data structure (data warehouses, document stores etc.)\nSome ER model notations include symbols to show super-sub-type relationships and mutual exclusion between relationships; some don't.\nAn ER model does not show an entity's life history (how its attributes and\/or relationships change over time in response to events). For many systems, such state changes are nontrivial and important enough to warrant explicit specification.\nSome have extended ER modeling with constructs to represent state changes, an approach supported by the original author; an example is Anchor Modeling.\nOthers model state changes separately, using state transition diagrams or some other process modeling technique.\nMany other kinds of diagram are drawn to model other aspects of systems, including the 14 diagram types offered by UML.\nToday, even where ER modeling could be useful, it is uncommon because many use tools that support similar kinds of model, notably class diagrams for OO programming and data models for relational database management systems. Some of these tools can generate code from diagrams and reverse-engineer diagrams from code.\nIn a survey, Brodie and Liu could not find a single instance of entity\u2013relationship modeling inside a sample of ten Fortune 100 companies.  Badia and Lemire blame this lack of use on the lack of guidance but also on the lack of benefits, such as lack of support for data integration.\nThe enhanced entity\u2013relationship model (EER modeling) introduces several concepts not in ER modeling, but are closely related to object-oriented design, like is-a relationships.\nFor modelling temporal databases, numerous ER extensions have been considered. Similarly, the ER model was found unsuitable for multidimensional databases (used in OLAP applications); no dominant conceptual model has emerged in this field yet, although they generally revolve around the concept of OLAP cube (also known as data cube within the field).","completion":"Software Engineering"}
{"prompt":"Computer engineers in this area develop improvements in human-computer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.","completion":"Software Engineering"}
{"prompt":"Experimental software engineering is a branch of software engineering interested in devising experiments on software, in collecting data from the experiments, and in devising laws and theories from this data. Proponents of this method advocate that the nature of software is such that we can advance the knowledge on software through experiments only.","completion":"Software Engineering"}
{"prompt":"Compiler theory is the theory of writing compilers (or more generally, translators); programs that translate a program written in one language into another form. The actions of a compiler are traditionally broken up into syntax analysis (scanning and parsing), semantic analysis (determining what a program should do), optimization (improving the performance of a program as indicated by some metric; typically execution speed) and code generation (generation and output of an equivalent program in some target language; often the instruction set of a CPU).","completion":"Software Engineering"}
{"prompt":"Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.","completion":"Software Engineering"}
{"prompt":"IDEF9, or integrated definition for business constraint discovery, is designed to assist in the discovery and analysis of constraints in a business system. A primary motivation driving the development of IDEF9 was an acknowledgment that the collection of constraints that forge an enterprise system is generally poorly defined. The knowledge of what constraints exist and how those constraints interact is incomplete, disjoint, distributed, and often completely unknown. Just as living organisms do not need to be aware of the genetic or autonomous constraints that govern certain behaviors, organizations can (and most do) perform well without explicit knowledge of the glue that structures the system. In order to modify business in a predictable manner, however, the knowledge of these constraints is as critical as knowledge of genetics is to the genetic engineer.","completion":"Software Engineering"}
{"prompt":"In Asia and Australia-New Zealand, the regional group called the Asia Pacific Association for Medical Informatics (APAMI) was established in 1994 and now consists of more than 15 member regions in the Asia Pacific Region.","completion":"Software Engineering"}
{"prompt":"A build server compiles the code periodically. The build server may automatically run tests and\/or implement other continuous quality control processes. Such processes aim to improve software quality and delivery time by periodically running additional static analyses, measuring performance, extracting documentation from the source code, and facilitating manual QA processes.","completion":"Software Engineering"}
{"prompt":"The cognitive dimensions framework provides a vocabulary to evaluate and modify design solutions. Cognitive dimensions offer a lightweight approach to analysis of a design quality, rather than an in-depth, detailed description. They provide a common vocabulary for discussing notation, user interface or programming language design.\nDimensions provide high-level descriptions of the interface and how the user interacts with it: examples include consistency, error-proneness, hard mental operations, viscosity and premature commitment. These concepts aid the creation of new designs from existing ones through design maneuvers that alter the design within a particular dimension.","completion":"Software Engineering"}
{"prompt":"FTS's largest strength, spreading the development over multiple time zones, is simultaneously its largest weakness. Its distributed workflow is more complex to implement due to cultural and technical differences as well as the differences in time making coordination and communication challenging.\nThe main reason why FTS is difficult to implement is because the handoffs are an essential element that is hard to get right. The largest factor causing this difficulty is poor communication.There are few documented cases of companies successfully applying FTS. Some companies have claimed to successfully implement FTS but these companies did not practice the daily handoffs. However, a limited amount of successful applications of FTS that did include daily handoffs of artefacts, using a distributed-concurrent model, were found by Cameron.Recent studies on FTS have moved to mathematical modeling of FTS. The research is focused on the issue of speed and the issues around the handoffs.","completion":"Software Engineering"}
{"prompt":"The Three Amigos, also referred to as a \"Specification Workshop\", is a meeting where the Product Owner discusses the requirement in the form of Specification by Example with different stakeholders like the QA and development team. The key goal for this discussion is to trigger conversation and identify any missing specifications. The discussion also gives a platform for QA, development team and Product owner to converge and hear out each other's perspective to enrich the requirement and also make sure if they are building the right product.The three Amigos are \n\nBusiness - Role of the Business user is to define the problem only (and not venture into suggesting any solution)\nDevelopment - Role of the Developers involve to suggest ways to fix the problem\nTesting - Role of testers is to question the solution, bring up as many as different possibilities for brain storming through What-If scenarios and help make the solution more precise to fix the problem.","completion":"Software Engineering"}
{"prompt":"Some other key events in the history of programming language theory since then:\n\n1950sNoam Chomsky developed the Chomsky hierarchy in the field of linguistics, a discovery which has directly impacted programming language theory and other branches of computer science.1960sThe Simula language was developed by Ole-Johan Dahl and Kristen Nygaard; it is widely considered to be the first example of an object-oriented programming language; Simula also introduced the concept of coroutines.\nIn 1964, Peter Landin is the first to realize Church's lambda calculus can be used to model programming languages. He introduces the SECD machine which \"interprets\" lambda expressions.\nIn 1965, Landin introduces the J operator, essentially a form of continuation.\nIn 1966, Landin introduces ISWIM, an abstract computer programming language in his article The Next 700 Programming Languages. It is influential in the design of languages leading to the Haskell programming language.\nIn 1966, Corrado B\u00f6hm introduced the programming language CUCH (Curry-Church).\nIn 1967, Christopher Strachey publishes his influential set of lecture notes Fundamental Concepts in Programming Languages, introducing the terminology R-values, L-values, parametric polymorphism, and ad hoc polymorphism.\nIn 1969, J. Roger Hindley publishes The Principal Type-Scheme of an Object in Combinatory Logic, later generalized into the Hindley\u2013Milner type inference algorithm.\nIn 1969, Tony Hoare introduces the Hoare logic, a form of axiomatic semantics.\nIn 1969, William Alvin Howard observed that a \"high-level\" proof system, referred to as natural deduction, can be directly interpreted in its intuitionistic version as a typed variant of the model of computation known as lambda calculus. This became known as the Curry\u2013Howard correspondence.1970sIn 1970, Dana Scott first publishes his work on denotational semantics.\nIn 1972, logic programming and Prolog were developed thus allowing computer programs to be expressed as mathematical logic.\nA team of scientists at Xerox PARC led by Alan Kay develop Smalltalk, an object-oriented language widely known for its innovative development environment.\nIn 1974, John C. Reynolds discovers System F. It had already been discovered in 1971 by the mathematical logician Jean-Yves Girard.\nFrom 1975, Gerald Jay Sussman and Guy Steele develop the Scheme programming language, a Lisp dialect incorporating lexical scoping, a unified namespace, and elements from the actor model including first-class continuations.\nBackus, at the 1977 Turing Award lecture, assailed the current state of industrial languages and proposed a new class of programming languages now known as function-level programming languages.\nIn 1977, Gordon Plotkin introduces Programming Computable Functions, an abstract typed functional language.\nIn 1978, Robin Milner introduces the Hindley\u2013Milner type inference algorithm for ML. Type theory became applied as a discipline to programming languages, this application has led to tremendous advances in type theory over the years.1980sIn 1981, Gordon Plotkin publishes his paper on structured operational semantics.\nIn 1988, Gilles Kahn published his paper on natural semantics.\nThere emerged process calculi, such as the Calculus of Communicating Systems of Robin Milner, and the Communicating sequential processes model of C. A. R. Hoare, as well as similar models of concurrency such as the actor model of Carl Hewitt.\nIn 1985, the release of Miranda sparks an academic interest in lazy-evaluated pure functional programming languages. A committee was formed to define an open standard resulting in the release of the Haskell 1.0 standard in 1990.\nBertrand Meyer created the methodology Design by contract and incorporated it into the Eiffel programming language.1990sGregor Kiczales, Jim Des Rivieres and Daniel G. Bobrow published the book The Art of the Metaobject Protocol.\nEugenio Moggi and Philip Wadler introduced the use of monads for structuring programs written in functional programming languages.","completion":"Software Engineering"}
{"prompt":"Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.","completion":"Software Engineering"}
{"prompt":"Mechatronics students take courses in various fields:\nEngineering Mathematics\nMechanical engineering\nElectronics engineering\nElectrical engineering\nMaterials science and engineering\nComputer engineering\nComputer aided and integrated manufacturing systems\nComputer-aided design\nElectronic design automation\nComputer Science\nSystems engineering\nControl engineering\nRobotics","completion":"Software Engineering"}
{"prompt":"Machine vision\nAutomation and robotics\nServo-mechanics\nSensing and control systems\nAutomotive engineering, automotive equipment in the design of subsystems such as anti-lock braking systems\nBuilding automation \/ Home automation\nComputer-machine controls, such as computer driven machines like CNC milling machines, CNC waterjets, and CNC plasma cutters\nExpert systems\nIndustrial goods\nConsumer products\nMechatronics systems\nMedical mechatronics, medical imaging systems\nStructural dynamic systems\nTransportation and vehicular systems\nMechatronics as the new language of the automobile\nEngineering and manufacturing systems\nPackaging\nElectronics\nComputers\nMicrocontrollers \/ PLCs\nMicroprocessors\nBiomechatronics","completion":"Software Engineering"}
{"prompt":"Seemingly, every new technology and practice from the 1970s through the 1990s was trumpeted as a silver bullet to solve the software crisis. Tools, discipline, formal methods, process, and professionalism were touted as silver bullets:\nTools: Especially emphasized were tools: structured programming, object-oriented programming, CASE tools such as ICL's CADES CASE system, Ada, documentation, and standards were touted as silver bullets.\nDiscipline: Some pundits argued that the software crisis was due to the lack of discipline of programmers.\nFormal methods: Some believed that if formal engineering methodologies would be applied to software development, then production of software would become as predictable an industry as other branches of engineering. They advocated proving all programs correct.\nProcess: Many advocated the use of defined processes and methodologies like the Capability Maturity Model.\nProfessionalism: This led to work on a code of ethics, licenses, and professionalism.In 1986, Fred Brooks published his No Silver Bullet article, arguing that no individual technology or practice would ever make a 10-fold improvement in productivity within 10 years.Debate about silver bullets raged over the following decade. Advocates for Ada, components, and processes continued arguing for years that their favorite technology would be a silver bullet. Skeptics disagreed. Eventually, almost everyone accepted that no silver bullet would ever be found. Yet, claims about silver bullets pop up now and again, even today.Some interpret no silver bullet to mean that software engineering failed. However, with further reading, Brooks goes on to say: \"We will surely make substantial progress over the next 40 years; an order of magnitude over 40 years is hardly magical ...\"The search for a single key to success never worked. All known technologies and practices have only made incremental improvements to productivity and quality. Yet, there are no silver bullets for any other profession, either. Others interpret no silver bullet as proof that software engineering has finally matured and recognized that projects succeed due to hard work.However, it could also be said that there are, in fact, a range of silver bullets today, including lightweight methodologies (see \"Project management\"), spreadsheet calculators, customized browsers, in-site search engines, database report generators, integrated design-test coding-editors with memory\/differences\/undo, and specialty shops that generate niche software, such as information web sites, at a fraction of the cost of totally customized web site development.  Nevertheless, the field of software engineering appears too complex and diverse for a single \"silver bullet\" to improve most issues, and each issue accounts for only a small portion of all software problems.","completion":"Software Engineering"}
{"prompt":"In following years a software engineering student will often have a much stronger focus on Software systems and data management. The inclusion of human factors in a software engineering degree has been heavily debated, arguing that products of software engineers often are too difficult to use by consumers.Sample B.S. in Software Engineering Degree Information from the University of Virginia - WiseCore SWE Requirements:\n\nIntroduction to Software Engineering\nSoftware Requirements & Modeling\nSoftware Design & Construction\nSoftware Testing, Verification, and Validation\nSoftware Quality Assurance\nSoftware Project Management\nSoftware Configuration ManagementCS Requirements:\n\nFundamentals of Programming\nData Structures\nIntroduction to Algorithms\nOperating Systems\nComputer Architecture\nProgramming Languages\nHuman - Computer Interaction\nDiscrete Mathematics\nDatabase DesigningMath Requirements:\n\nProbability & Statistics\nCalculus I\nCalculus II\nCalculus III\nLinear Algebra\nBoolean Algebra","completion":"Software Engineering"}
{"prompt":"The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the  Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC\/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff\u2013Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.","completion":"Software Engineering"}
{"prompt":"Deliverables must be developed for many SE projects. Software engineers rarely make all of these deliverables themselves. They usually cooperate with the writers, trainers, installers, marketers, technical support people, and others who make many of these deliverables.\n\nApplication software \u2014 the software\nDatabase \u2014 schemas and data.\nDocumentation, online and\/or print, FAQ, Readme, release notes, Help, for each role\nUser\nAdministrator\nManager\nBuyer\nAdministration and Maintenance policy, what should be backed-up, checked, configured, ...\nInstallers\nMigration\nUpgrade from previous installations\nUpgrade from competitor's installations\nTraining materials, for each role\nUser\nAdministrator\nManager\nBuyer\nSupport info for computer support groups.\nMarketing and sales materials\nWhite papers, explain the technologies used in the applications","completion":"Software Engineering"}
{"prompt":"In Boston, Hamilton initially intended to enroll in graduate study in abstract mathematics at Brandeis University. However, in mid-1959, Hamilton began working for Edward Norton Lorenz, in the meteorology department at MIT. She developed software for predicting weather, programming on the LGP-30 and the PDP-1 computers at Marvin Minsky's Project MAC. Her work contributed to Lorenz's publications on chaos theory. At the time, computer science and software engineering were not yet established disciplines; instead, programmers learned on the job with hands-on experience. She moved on to another project in the summer of 1961, and hired and trained Ellen Fetter as her replacement.","completion":"Software Engineering"}
{"prompt":"Personas are archetypes that describe the various goals and observed behaviour patterns among users.A persona encapsulates critical behavioural data in a way that both designers and stakeholders can understand, remember, and relate to. Personas use storytelling to engage users' social and emotional aspects, which helps designers to either visualize the best product behaviour or see why the recommended design is successful.","completion":"Software Engineering"}
{"prompt":"Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.","completion":"Software Engineering"}
{"prompt":"BCS conducts its own BCS Higher Education Qualifications in many countries. It was formerly known as BCS Professional Examinations which consisted of Parts 1 and 2 of which passing of Part 2 with the professional project was equivalent to a British honours degree. These programs had an early history of success, with participants coming from all parts of the world, including Asia. Many private computing schools outside the UK have hosted students in preparation for BCS Part 1 and 2 examinations. The level of current qualifications are:\n\nCertificate in IT (equivalent to the first year of an honours university degree)\nDiploma in IT (equivalent to the second year of an honours university degree)\nProfessional Graduate Diploma in IT (equivalent to a British honours university degree)e-typee-type is a qualification that allows individuals to improve and certify their typing skills. The average user can save up to 21 days a year by improving their typing speed as well as preventing repetitive strain injury (RSI). e-type comes with full support materials and computer-based courseware before allowing the user to assess their skills using a simple online test.\nDigital CreatorDigital Creator is a set of engaging qualifications that teach digital media skills through creative projects. They are designed for all types and ages of learners \u2013 in schools from Key Stage 2 to Key Stage 4 and in all areas of adult learning.\n\nITQ \u2013 The Flexible IT qualificationThe BCS ITQ is a range of IT user qualifications made up of a combination of units available on the ITQ framework.\nThe framework consists of a wide range of units covering all aspects of IT user applications, including word processing, spreadsheets, the internet, multimedia software and design software.","completion":"Software Engineering"}
{"prompt":"Within the steering phase the programmers and business people can \"steer\" the process. That is to say, they can make changes. Individual user stories, or relative priorities of different user stories, might change; estimates might prove wrong. This is the chance to adjust the plan accordingly.","completion":"Software Engineering"}
{"prompt":"DevOps is a software engineering approach that centers around cultural change, specifically the collaboration of the various teams involved in software delivery (developers, operations, quality assurance, management, etc.), as well as automating the processes in software delivery.","completion":"Software Engineering"}
{"prompt":"Born in Liberty Center, Iowa. As a young man, Mills studied art with Grant Wood. During World War II, Mills became a bomber pilot in the U.S. Army Air Corps. His skills in flying and teaching were such that rather than having him fly missions, the Army assigned him to train other pilots.","completion":"Software Engineering"}
{"prompt":"Another variant is Motion control for Advanced Mechatronics, presently recognized as a key technology in mechatronics. The robustness of motion control will be represented as a function of stiffness and a basis for practical realization. Target of motion is parameterized by control stiffness which could be variable according to the task reference. The system robustness of motion always requires very high stiffness in the controller.","completion":"Software Engineering"}
{"prompt":"Computers and Biomedical Research, published in 1967, was one of the first dedicated journals to health informatics. Other early journals included Computers and Medicine, published by the American Medical Association; Journal of Clinical Computing, published by Gallagher Printing; Journal of Medical Systems, published by Plenum Press; and MD Computing, published by Springer-Verlag. In 1984, Lippincott published the first nursing-specific journal, titled Journal Computers in Nursing, which is now known as Computers Informatics Nursing (CIN).As of September 7, 2016, there are roughly 235 informatics journals listed in the National Library of Medicine (NLM) catalog of journals. The Journal Citation Reports for 2018 gives the top three journals in medical informatics as the Journal of Medical Internet Research (impact factor of 4.945), JMIR mHealth and uHealth (4.301) and the Journal of the American Medical Informatics Association (4.292).","completion":"Software Engineering"}
{"prompt":"The Internet of things (IoT) is the inter-networking of physical devices, embedded with electronics, software, sensors, actuators, and network connectivity which enable these objects to collect and exchange data. IoT and mechatronics are complementary. Many of the smart components associated with the Internet of Things will be essentially mechatronic. The development of the IoT is forcing mechatronics engineers, designers, practitioners and educators to research the ways in which mechatronic systems and components are perceived, designed and manufactured. This allows them to face up to new issues such as data security, machine ethics and the human-machine interface.Knowledge of programming is very important. A mechatronics engineer has to do programming in different levels example.\u2014PLC programming, drone programming, hardware programming, CNC programming etc. Due to combination of electronics engineering, soft skills from computer side is important. Important programming languages for mechatronics engineer to learn is Java, Python, C++ and C programming language.","completion":"Software Engineering"}
{"prompt":"Time zone differences reduce opportunities for real-time collaboration. Team members have to be flexible to achieve overlap with remote colleagues. The limited overlap and the delay in responses have a negative impact on the coordination.\nDaily handoff cycles or handing off work-in-progress are a requirement of FTS because without it the time to market cannot be decreased.\nGeographical dispersion\nCost estimation\nLoss of teamness\nNumber of sites\nCoordination breakdown\nManagerial difficulties\nTechnical platforms","completion":"Software Engineering"}
{"prompt":"A medical robot is a robot used in the medical sciences. They include surgical robots. These are in most telemanipulators, which use the surgeon's activators on one side to control the \"effector\" on the other side. There are the following types of medical robots:\n\nSurgical robots: either allow surgical operations to be carried out with better precision than an unaided human surgeon or allow remote surgery where a human surgeon is not physically present with the patient.\nRehabilitation robots: facilitate and support the lives of infirm, elderly people, or those with dysfunction of body parts affecting movement. These robots are also used for rehabilitation and related procedures, such as training and therapy.\nBiorobots: a group of robots designed to imitate the cognition of humans and animals.\nTelepresence robots: allow off-site medical professionals to move, look around, communicate, and participate from remote locations.\nPharmacy automation: robotic systems to dispense oral solids in a retail pharmacy setting or preparing sterile IV admixtures in a hospital pharmacy setting.\nCompanion robot: has the capability to engage emotionally with users keeping them company and alerting if there is a problem with their health.\nDisinfection robot: has the capability to disinfect a whole room in mere minutes, generally using pulsed ultraviolet light. They are being used to fight Ebola virus disease.","completion":"Software Engineering"}
{"prompt":"The branch of industrial engineer includes the design of machinery, assembly and process lines of various manufacturing industries. This branch can be said somewhat similar to automation and robotics. Mechatronics engineers who works as industrial engineers design and develop infrastructure of a manufacturing plant. Also it can be said that they are architect of machines. One can work as an industrial designer to design the industrial layout and plan for setting up of a manufacturing industry or as an industrial technician to lookover the technical requirements and repairing of the particular factory.","completion":"Software Engineering"}
{"prompt":"Most dynamic analysis techniques are based on some kind of code instrumentation or transformation.\n\nDynInst is a runtime code-patching library that is useful in developing dynamic program analysis probes and applying them to compiled executables. Dyninst does not require source code or recompilation in general, however, non-stripped executables and executables with debugging symbols are easier to instrument.\nIroh.js is a runtime code analysis library for JavaScript. It keeps track of the code execution path, provides runtime listeners to listen for specific executed code patterns and allows the interception and manipulatation of the program's execution behavior.","completion":"Software Engineering"}
{"prompt":"As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board\u2014which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)\u2014identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human\u2013computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\nComputer science is no more about computers than astronomy is about telescopes.","completion":"Software Engineering"}
{"prompt":"Because of the high complexity of software, it is not possible to understand most programs at a single glance even for the most experienced software developer. The abstractions provided by high-level programming languages also make it harder to understand the connection between the source code written by a programmer and the actual program's behaviour. In order to find bugs in programs and to prevent creating new bugs when extending a program, a software developer uses some programming tools to visualize all kinds of information about programs.\nFor example, a debugger allows a programmer to extract information about a running program in terms of the source language used to program it. The debugger can compute the value of a variable in the source program from the state of the concrete machine by using information stored by the compiler. Memory debuggers can directly point out questionable or outright wrong memory accesses of running programs which may otherwise remain undetected and are a common source of program failures.","completion":"Software Engineering"}
{"prompt":"In systems engineering and software engineering a function model is created with a functional modeling perspective. The functional perspective is one of the perspectives possible in business process modelling, other perspectives are for example behavioural, organisational or informational.A functional modeling perspective concentrates on describing the dynamic process. The main concept in this modeling perspective is the process, this could be a function, transformation, activity, action, task etc. A well-known example of a modeling language employing this perspective is data flow diagrams.\nThe perspective uses four symbols to describe a process, these being:\n\nProcess: Illustrates transformation from input to output.\nStore:\t   Data-collection or some sort of material.\nFlow:\t   Movement of data or material in the process.\nExternal Entity: External to the modeled system, but interacts with it.Now, with these symbols, a process can be represented as a network of these symbols. This decomposed process is a DFD, data flow diagram.\n\nIn Dynamic Enterprise Modeling a division is made in the Control model, Function Model, Process model and Organizational model.","completion":"Software Engineering"}
{"prompt":"Program analysis is the general problem of examining a program and determining key characteristics (such as the absence of classes of program errors). Program transformation is the process of transforming a program in one form (language) to another form.","completion":"Software Engineering"}
{"prompt":"Some profilers operate by sampling. A sampling profiler probes the target program's call stack at regular intervals using operating system interrupts. Sampling profiles are typically less numerically accurate and specific, but allow the target program to run at near full speed.\nThe resulting data are not exact, but a statistical approximation. \"The actual amount of error is usually more than one sampling period. In fact, if a value is n times the sampling period, the expected error in it is the square-root of n sampling periods.\"In practice, sampling profilers can often provide a more accurate picture of the target program's execution than other approaches, as they are not as intrusive to the target program, and thus don't have as many side effects (such as on memory caches or instruction decoding pipelines). Also since they don't affect the execution speed as much, they can detect issues that would otherwise be hidden. They are also relatively immune to over-evaluating the cost of small, frequently called routines or 'tight' loops. They can show the relative amount of time spent in user mode versus interruptible kernel mode such as system call processing.\nStill, kernel code to handle the interrupts entails a minor loss of CPU cycles, diverted cache usage, and is unable to distinguish the various tasks occurring in uninterruptible kernel code (microsecond-range activity).\nDedicated hardware can go beyond this: ARM Cortex-M3 and some recent MIPS processors JTAG interface have a PCSAMPLE register, which samples the program counter in a truly undetectable manner, allowing non-intrusive collection of a flat profile.\nSome commonly used statistical profilers for Java\/managed code are SmartBear Software's AQtime and Microsoft's CLR Profiler. Those profilers also support native code profiling, along with Apple Inc.'s Shark (OSX), OProfile (Linux), Intel VTune and Parallel Amplifier (part of Intel Parallel Studio), and Oracle Performance Analyzer, among others.","completion":"Software Engineering"}
{"prompt":"Programmers should take a \"simple is best\" approach to software design.  Whenever a new piece of code is written, the author should ask themselves 'is there a simpler way to introduce the same functionality?'. If the answer is yes, the simpler course should be chosen.  Refactoring should also be used to make complex code simpler.","completion":"Software Engineering"}
{"prompt":"Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.","completion":"Software Engineering"}
{"prompt":"An ER model is usually the result of systematic analysis to define and describe what data is created and needed by processes in an area of a business. Typically, it represents records of entities and events monitored and directed by business processes, rather than the processes themselves. It is usually drawn in a graphical form as boxes (entities) that are connected by lines (relationships) which express the associations and dependencies between entities. It can also be expressed in a verbal form, for example: one building may be divided into zero or more apartments, but one apartment can only be located in one building.\nEntities may be characterized not only by relationships, but also by additional properties (attributes), which include identifiers called \"primary keys\". Diagrams created to represent attributes as well as entities and relationships may be called entity-attribute-relationship diagrams, rather than entity\u2013relationship models.\nAn ER model is typically implemented as a database. In a simple relational database implementation, each row of a table represents one instance of an entity type, and each field in a table represents an attribute type. In a relational database a relationship between entities is implemented by storing the primary key of one entity as a pointer or \"foreign key\" in the table of another entity.\nThere is a tradition for ER\/data models to be built at two or three levels of abstraction. Note that the conceptual-logical-physical hierarchy below is used in other kinds of specification, and is different from the three schema approach to software engineering.\n\nConceptual data model\nThis is the highest level ER model in that it contains the least granular detail but establishes the overall scope of what is to be included within the model set. The conceptual ER model normally defines master reference data entities that are commonly used by the organization.  Developing an enterprise-wide conceptual ER model is useful to support documenting the data architecture for an organization.\nA conceptual ER model may be used as the foundation for one or more logical data models (see below). The purpose of the conceptual ER model is then to establish structural metadata commonality for the master data entities between the set of logical ER models. The conceptual data model may be used to form commonality relationships between ER models as a basis for data model integration.\nLogical data model\nA logical ER model does not require a conceptual ER model, especially if the scope of the logical ER model includes only the development of a distinct information system. The logical ER model contains more detail than the conceptual ER model. In addition to master data entities, operational and transactional data entities are now defined. The details of each data entity are developed and the relationships between these data entities are established. The logical ER model is however developed independently of the specific database management system into which it can be implemented.\nPhysical data model\nOne or more physical ER models may be developed from each logical ER model. The physical ER model is normally developed to be instantiated as a database. Therefore, each physical ER model must contain enough detail to produce a database and each physical ER model is technology dependent since each database management system is somewhat different.\nThe physical model is normally instantiated in the structural metadata of a database management system as relational database objects such as database tables, database indexes such as unique key indexes, and database constraints such as a foreign key constraint or a commonality constraint.  The ER model is also normally used to design modifications to the relational database objects and to maintain the structural metadata of the database.The first stage of information system design uses these models during the requirements analysis to describe information needs or the type of information that is to be stored in a database. The data modeling technique can be used to describe any ontology (i.e. an overview and classifications of used terms and their relationships) for a certain area of interest. In the case of the design of an information system that is based on a database, the conceptual data model is, at a later stage (usually called logical design), mapped to a logical data model, such as the relational model; this in turn is mapped to a physical model during physical design. Note that sometimes, both of these phases are referred to as \"physical design.\"","completion":"Software Engineering"}
{"prompt":"The Interaction Design Association was created in 2003 to serve the community. The organization has over 80,000 members and more than 173 local groups. IxDA hosts Interaction the annual interaction design conference, and the Interaction Awards.","completion":"Software Engineering"}
{"prompt":"The word mechatronics was registered as trademark by the company in Japan with the registration number of \"46-32714\" in 1971. The company later released the right to use the word to the public, and the word began being used globally.\nWith the advent of information technology in the 1980s, microprocessors were introduced into mechanical systems, improving performance significantly. By the 1990s, advances in computational intelligence were applied to mechatronics in ways that revolutionized the field.","completion":"Software Engineering"}
{"prompt":"Behavior-driven development, an extension of test-driven development, is a development process that makes use of a simple DSL. These DSLs convert structured natural language statements into executable tests.  The result is a closer relationship to acceptance criteria for a given function and the tests used to validate that functionality.  As such it is a natural extension of TDD testing in general.\nBDD focuses on:\n\nWhere to start in the process\nWhat to test and what not to test\nHow much to test in one go\nWhat to call the tests\nHow to understand why a test failsAt its heart, BDD is about rethinking the approach to unit testing and acceptance testing in order to avoid issues that naturally arise. For example, BDD suggests that unit test names be whole sentences starting with a conditional verb (\"should\" in English for example) and should be written in order of business value. Acceptance tests should be written using the standard agile framework of a user story: \"Being a [role\/actor\/stakeholder] I want a [feature\/capability] yielding a [benefit]\". Acceptance criteria should be written in terms of scenarios and implemented in classes: Given [initial context], when [event occurs], then [ensure some outcomes] .\nStarting from this point, many people developed BDD frameworks over a period of years, finally framing it in terms of a communication and collaboration framework for developers, QA and non-technical or business participants in a software project. During the \"Agile specifications, BDD and Testing eXchange\" in November 2009 in London, Dan North gave the following description of BDD:\n\nBDD is a second-generation, outside-in, pull-based, multiple-stakeholder, multiple-scale, high-automation, agile methodology. It describes a cycle of interactions with well-defined outputs, resulting in the delivery of working, tested software that matters.\nDuring an interview with Dan North at GOTO Conference in 2013, Liz Keogh defined BDD as:\n\nIt's using examples to talk through how an application behaves... And having conversations about those examples.\n\nDan North created a BDD framework, JBehave, followed by a story-level BDD framework for Ruby called RBehave which was later integrated into the RSpec project. He also worked with David Chelimsky, Aslak Helles\u00f8y and others to develop RSpec and also to write \"The RSpec Book: Behaviour Driven Development with RSpec, Cucumber, and Friends\". The first story-based framework in RSpec was later replaced by Cucumber mainly developed by Aslak Helles\u00f8y. Capybara, which is a part of the Cucumber testing framework is one such web-based test automation software.","completion":"Software Engineering"}
{"prompt":"Software maintenance refers to the activities required to provide cost-effective support after shipping the software product. Software maintenance is modifying and updating software applications after distribution to correct faults and to improve its performance. Software has a lot to do with the real world and when the real world changes, software maintenance is required. Software maintenance includes: error correction, optimization, deletion of unused and discarded features, and enhancement of features that already exist. Usually, maintenance takes up about 40% to 80% of the project cost therefore, focusing on maintenance keeps the costs down.","completion":"Software Engineering"}
{"prompt":"A typical functional requirement will contain a unique name and number, a brief summary, and a rationale. This information is used to help the reader understand why the requirement is needed, and to track the requirement through the development of the system. The crux of the requirement is the description of the required behavior, which must be clear and readable. The described behavior may come from organizational or business rules, or it may be discovered through elicitation sessions with users, stakeholders, and other experts within the organization. Many requirements may be uncovered during the use case development. When this happens, the requirements analyst may create a placeholder requirement with a name and summary, and research the details later, to be filled in when they are better known.","completion":"Software Engineering"}
{"prompt":"The first applications of computers to medicine and health care in Brazil started around 1968, with the installation of the first mainframes in public university hospitals, and the use of programmable calculators in scientific research applications. Minicomputers, such as the IBM 1130 were installed in several universities, and the first applications were developed for them, such as the hospital census in the School of Medicine of Ribeir\u00e3o Preto and patient master files, in the Hospital das Cl\u00ednicas da Universidade de S\u00e3o Paulo, respectively at the cities of Ribeir\u00e3o Preto and S\u00e3o Paulo campuses of the University of S\u00e3o Paulo. In the 1970s, several Digital Corporation and Hewlett-Packard minicomputers were acquired for public and Armed Forces hospitals, and more intensively used for intensive-care unit, cardiology diagnostics, patient monitoring and other applications. In the early 1980s, with the arrival of cheaper microcomputers, a great upsurge of computer applications in health ensued, and in 1986 the Brazilian Society of Health Informatics was founded, the first Brazilian Congress of Health Informatics was held, and the first Brazilian Journal of Health Informatics was published. In Brazil, two universities are pioneers in teaching and research in medical informatics, both the University of Sao Paulo and the Federal University of Sao Paulo offer undergraduate programs highly qualified in the area as well as extensive graduate programs (MSc and PhD). In 2015 the Universidade Federal de Ci\u00eancias da Sa\u00fade de Porto Alegre, Rio Grande do Sul, also started to offer undergraduate program.","completion":"Software Engineering"}
{"prompt":"Follow the Sun can be traced back to the mid-1990s where IBM had the first global software team which was specifically set up to take advantages of FTS. The team was spread out across five sites around the globe. Unfortunately, in this case FTS was unsuccessful because it was uncommon to hand off the software artifacts daily.\nTwo other cases of FTS at IBM have been documented by Treinen and Miller-Frost. The first team was spread out across a site in the United States and a site in Australia. FTS was successful for this team. The second team was spread out across a site in the United States and a site in India. In this case FTS was unsuccessful because of miscommunication, time zone issues and cultural differences.","completion":"Software Engineering"}
{"prompt":"Much like test-driven design practice, behavior-driven development assumes the use of specialized support tooling in a project. BDD can be seen as a more specific version of TDD, as it requires to supply not only test code but a separate document in addition to describe the behavior in a more human-readable language. This requires a two-step process for executing the tests, reading and parsing the descriptions, and reading the test code and finding the corresponding test implementation to execute. This process makes BDD slightly more laborious to work with as a developer, but due to its human-readable nature the value of those documents extends to an even less technical audience, and can hence serve as a communication means for describing requirements (\"features\").","completion":"Software Engineering"}
{"prompt":"A library of similar metamodels has been called a Zoo of metamodels.\nThere are several types of meta-model zoos. Some are expressed in ECore. Others are written in MOF 1.4 \u2013 XMI 1.2. The metamodels expressed in UML-XMI1.2 may be uploaded in Poseidon for UML, a UML CASE tool.","completion":"Software Engineering"}
{"prompt":"Thoughtworks has claimed reasonable success on distributed XP projects with up to sixty people.In 2004, industrial extreme programming (IXP) was introduced as an evolution of XP. It is intended to bring the ability to work in large and distributed teams. It now has 23 practices and flexible values.","completion":"Software Engineering"}
{"prompt":"The Distributed Management Task Force (DMTF) provides a standard set of information models for various enterprise domains under the general title of the Common Information Model (CIM). Specific information models are derived from CIM for particular management domains.\nThe TeleManagement Forum (TMF) has defined an advanced model for the Telecommunication domain (the Shared Information\/Data model, or SID) as another. This includes views from the business, service and resource domains within the Telecommunication industry. The TMF has established a set of principles that an OSS integration should adopt, along with a set of models that provide standardized approaches.\nThe models interact with the information model (the Shared Information\/Data Model, or SID), via a process model (the Business Process Framework (eTOM), or eTOM) and a life cycle model.","completion":"Software Engineering"}
{"prompt":"Two major influences shaped software development in the 1990s: \n\nInternally, object-oriented programming replaced procedural programming as the programming paradigm favored by some developers.\nExternally, the rise of the Internet and the dot-com boom emphasized speed-to-market and company growth as competitive business factors.Rapidly changing requirements demanded shorter product life-cycles, and often clashed with traditional methods of software development.\nThe Chrysler Comprehensive Compensation System (C3) started in order to determine the best way to use object technologies, using the payroll systems at Chrysler as the object of research, with Smalltalk as the language and  GemStone as the data access layer. Chrysler brought in Kent Beck, a prominent Smalltalk practitioner, to do performance tuning on the system, but his role expanded as he noted several problems with the development process. He took this opportunity to propose and implement some changes in development practices - based on his work with his frequent collaborator, Ward Cunningham. Beck describes the early conception of the methods:\nThe first time I was asked to lead a team, I asked them to do a little bit of the things I thought were sensible, like testing and reviews. The second time there was a lot more on the line. I thought, \"Damn the torpedoes, at least this will make a good article,\" [and] asked the team to crank up all the knobs to 10 on the things I thought were essential and leave out everything else.\nBeck invited Ron Jeffries to the project to help develop and refine these methods. Jeffries thereafter acted as a coach to instill the practices as habits in the C3 team.\nInformation about the principles and practices behind XP disseminated to the wider world through discussions on the original wiki, Cunningham's WikiWikiWeb. Various contributors discussed and expanded upon the ideas, and some spin-off methodologies resulted (see agile software development). Also, XP concepts have been explained, for several years, using a hypertext system map on the XP website at http:\/\/www.extremeprogramming.org circa 1999.\nBeck edited a series of books on XP, beginning with his own Extreme Programming Explained (1999, ISBN 0-201-61641-6), spreading his ideas to a much larger audience. Authors in the series went through various aspects attending XP and its practices. The series included a book critical of the practices.","completion":"Software Engineering"}
{"prompt":"Integrated development environments combine the features of many tools into one package. They for example make it easier to do specific tasks, such as searching for content only in files in a particular project. IDEs may for example be used for development of enterprise-level applications.\nDifferent aspects of IDEs for specific programming languages can be found in this comparison of integrated development environments.","completion":"Software Engineering"}
{"prompt":"Telehealth is the distribution of health-related services and information via electronic information and telecommunication technologies. It allows long-distance patient and clinician contact, care, advice, reminders, education, intervention, monitoring, and remote admissions. Telemedicine is sometimes used as a synonym, or is used in a more limited sense to describe remote clinical services, such as diagnosis and monitoring. Remote monitoring, also known as self-monitoring or testing, enables medical professionals to monitor a patient remotely using various technological devices. This method is primarily used for managing chronic diseases or specific conditions, such as heart disease, diabetes mellitus, or asthma. These services can provide comparable health outcomes to traditional in-person patient encounters, supply greater satisfaction to patients, and may be cost-effective. Telerehabilitation (or e-rehabilitation[40][41]) is the delivery of rehabilitation services over telecommunication networks and the Internet. Most types of services fall into two categories: clinical assessment (the patient's functional abilities in his or her environment), and clinical therapy. Some fields of rehabilitation practice that have explored telerehabilitation are: neuropsychology, speech-language pathology, audiology, occupational therapy, and physical therapy. Telerehabilitation can deliver therapy to people who cannot travel to a clinic because the patient has a disability or because of travel time. Telerehabilitation also allows experts in rehabilitation to engage in a clinical consultation at a distance.","completion":"Software Engineering"}
{"prompt":"One important move in model-driven engineering is the systematic use of model transformation languages. The OMG has proposed a standard for this called QVT for Queries\/Views\/Transformations. QVT is based on the meta-object facility (MOF). Among many other model transformation languages (MTLs), some examples of implementations of this standard are AndroMDA, VIATRA, Tefkat, MT, ManyDesigns Portofino.","completion":"Software Engineering"}
{"prompt":"Profilers, which are also programs themselves, analyze target programs by collecting information on their execution. Based on their data granularity, on how profilers collect information, they are classified into event based or statistical profilers. Profilers interrupt program execution to collect information, which may result in a limited resolution in the time measurements, which should be taken with a grain of salt. Basic block profilers report a number of machine clock cycles devoted to executing each line of code, or a timing based on adding these together; the timings reported per basic block may not reflect a difference between cache hits and misses.","completion":"Software Engineering"}
{"prompt":"Computer Science\nInformation engineering\nInformation technology\nTraditional engineering\nComputer engineering\nElectrical engineering\nSoftware engineering\nDomain engineering\nInformation technology engineering\nKnowledge engineering\nUser interface engineering\nWeb engineering\nArts and Sciences\nMathematics\nComputer science\nInformation science\nApplication software\nInformation systems\nProgramming\nSystems Engineering","completion":"Software Engineering"}
{"prompt":"Logging time, defect, and size data is an essential part of planning and tracking PSP projects, as historical data is used to improve estimating accuracy.\nThe PSP uses the PROxy-Based Estimation (PROBE) method to improve a developer's estimating skills for more accurate project planning. For project tracking, the PSP uses the earned value method.\nThe PSP also uses statistical techniques, such as correlation, linear regression, and standard deviation, to translate data into useful information for improving estimating, planning and quality. These statistical formulas are calculated by the PSP tool.","completion":"Software Engineering"}
{"prompt":"Avionics is also considered a variant of mechatronics as it combines several fields such as electronics and telecom with Aerospace engineering. It is the subdiscipline of mechatronics engineering and aerospace engineering which is engineering branch focusing on electronics systems of aircraft. The word avionics is a blend of aviation and electronics. The electronics system of aircraft includes aircraft communication addressing and reporting system, air navigation, aircraft flight control system, aircraft collision avoidance systems, flight recorder, weather radar and lightning detector. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform.","completion":"Software Engineering"}
{"prompt":"According to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.","completion":"Software Engineering"}
{"prompt":"Extreme Programming Explained describes extreme programming as a software-development discipline that organizes people to produce higher-quality software more productively.\nXP attempts to reduce the cost of changes in requirements by having multiple short development cycles, rather than a long one.\nIn this doctrine, changes are a natural, inescapable and desirable aspect of software-development projects, and should be planned for, instead of attempting to define a stable set of requirements.\nExtreme programming also introduces a number of basic values, principles and practices on top of the agile methodology.","completion":"Software Engineering"}
{"prompt":"This is an iterative process of gathering requirements and estimating the work impact of each of those requirements.\n\nWrite a Story: Business has come with a problem; during a meeting, development will try to define this problem and get requirements. Based on the business problem, a story (user story) has to be written. This is done by business, where they point out what they want a part of the system to do. It is important that development has no influence on this story. The story is written on a user story card.\nEstimate a Story: Development estimates how long it will take to implement the work implied by the story card. Development can also create spike solutions to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. Again, this may not influence the business requirements.\nSplit a Story: Every design critical complexity has to be addressed before starting the iteration planning. If development isn't able to estimate the story, it needs to be split up and written again.When business cannot come up with any more requirements, one proceeds to the commitment phase.","completion":"Software Engineering"}
{"prompt":"Half of all practitioners today have degrees in computer science, information systems, or information technology. A small, but growing, number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering Bachelor's degree in the UK and the world; in the following year, the University of Sheffield established a similar program.  In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs. In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004, in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master's degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities.  Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.\nIn 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS (\u00c9cole de technologie sup\u00e9rieure) University and UQAM (Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.","completion":"Software Engineering"}
{"prompt":"A Business reference model is a reference model, concentrating on the functional and organizational aspects of the core business of an enterprise, service organization or government agency. In enterprise engineering a business reference model is part of an Enterprise Architecture Framework or Architecture Framework, which defines how to organize the structure and views associated with an Enterprise Architecture.\nA reference model in general is a model of something that embodies the basic goal or idea of something and can then be looked at as a reference for various purposes. A business reference model is a means to describe the business operations of an organization, independent of the organizational structure that perform them. Other types of business reference model can also depict the relationship between the business processes, business functions, and the business area's business reference model. These reference model can be constructed in layers, and offer a foundation for the analysis of service components, technology, data, and performance.","completion":"Software Engineering"}
{"prompt":"Parasoft Jtest uses runtime error detection to expose defects such as race conditions, exceptions, resource and memory leaks, and security attack vulnerabilities.\nIntel Inspector performs run-time threading and memory error analysis in Windows.\nParasoft Insure++ is a runtime memory analysis and error detection tool. Its Inuse component provides a graphical view of memory allocations over time, with specific visibility of overall heap usage, block allocations, possible outstanding leaks, etc.\nGoogle's Thread Sanitizer is a data race detection tool. It instruments LLVM IR to capture racy memory accesses.","completion":"Software Engineering"}
{"prompt":"Several benefits of continuous delivery have been reported.\nAccelerated time to market: Continuous delivery lets an organization deliver the business value inherent in new software releases to customers more quickly. This capability helps the company stay a step ahead of the competition.\nBuilding the right product: Frequent releases let the application development teams obtain user feedback more quickly. This lets them work on only the useful features. If they find that a feature isn't useful, they spend no further effort on it. This helps them build the right product.\nImproved productivity and efficiency: Significant time savings for developers, testers, operations engineers, etc. through automation.\nReliable releases: The risks associated with a release have significantly decreased, and the release process has become more reliable. With continuous delivery, the deployment process and scripts are tested repeatedly before deployment to production. So, most errors in the deployment process and scripts have already been discovered. With more frequent releases, the number of code changes in each release decreases. This makes finding and fixing any problems that do occur easier, reducing the time in which they have an impact.\nImproved product quality: The number of open bugs and production incidents has decreased significantly.\nImproved customer satisfaction: A higher level of customer satisfaction is achieved.Obstacles have also been investigated.\nCustomer preferences: Some customers do not want continuous updates to their systems. This is especially true at the critical stages of their operations.\nDomain restrictions: In some domains, such as telecom and medical, regulations require extensive testing before new versions are allowed to enter the operations phase.\nLack of test automation: Lack of test automation leads to a lack of developer confidence and can prevent using continuous delivery.\nDifferences in environments: Different environments used in the development, testing and production can result in undetected issues slipping to the production environment.\nTests needing a human oracle: Not all quality attributes can be verified with automation. These attributes require humans in the loop, slowing down the delivery pipeline.Eight further adoption challenges were raised and elaborated on by Chen. These challenges are in the areas of organizational structure, processes, tools, infrastructure, legacy systems, architecting for continuous delivery, continuous testing of non-functional requirements, and test execution optimization.","completion":"Software Engineering"}
{"prompt":"Daikon is an implementation of dynamic invariant detection. Daikon runs a program, observes the values that\nthe program computes, and then reports properties that were true over the observed executions, and thus likely true over all executions.","completion":"Software Engineering"}
{"prompt":"Several practices embody courage. One is the commandment to always design and code for today and not for tomorrow. This is an effort to avoid getting bogged down in design and requiring a lot of effort to implement anything else. Courage enables developers to feel comfortable with refactoring their code when necessary. This means reviewing the existing system and modifying it so that future changes can be implemented more easily. Another example of courage is knowing when to throw code away: courage to remove source code that is obsolete, no matter how much effort was used to create that source code. Also, courage means persistence: a programmer might be stuck on a complex problem for an entire day, then solve the problem quickly the next day, but only if they are persistent.","completion":"Software Engineering"}
{"prompt":"Continuous integration is intended to produce benefits such as:\n\nIntegration bugs are detected early and are easy to track down due to small changesets. This saves both time and money over the lifespan of a project.\nAvoids last-minute chaos at release dates, when everyone tries to check in their slightly incompatible versions\nWhen unit tests fail or a bug emerges, if developers need to revert the codebase to a bug-free state without debugging, only a small number of changes are lost (because integration happens frequently)\nConstant availability of a \"current\" build for testing, demo, or release purposes\nFrequent code check-in pushes developers to create modular, less complex codeWith continuous automated testing, benefits can include:\n\nEnforces discipline of frequent automated testing\nImmediate feedback on the system-wide impact of local changes\nSoftware metrics generated from automated testing and CI (such as metrics for code coverage, code complexity, and feature completeness) focus developers on developing functional, quality code, and help develop momentum in a teamSome downsides of continuous integration can include:\n\nConstructing an automated test suite requires a considerable amount of work, including ongoing effort to cover new features and follow intentional code modifications.\nTesting is considered a best practice for software development in its own right, regardless of whether or not continuous integration is employed, and automation is an integral part of project methodologies like test-driven development.\nContinuous integration can be performed without any test suite, but the cost of quality assurance to produce a releasable product can be high if it must be done manually and frequently.\nThere is some work involved to set up a build system, and it can become complex, making it difficult to modify flexibly.However, there are a number of continuous integration software projects, both proprietary and open-source, which can be used.\nContinuous integration is not necessarily valuable if the scope of the project is small or contains untestable legacy code.\nValue added depends on the quality of tests and how testable the code really is.\nLarger teams mean that new code is constantly added to the integration queue, so tracking deliveries (while preserving quality) is difficult and builds queueing up can slow down everyone.\nWith multiple commits and merges a day, partial code for a feature could easily be pushed and therefore integration tests will fail until the feature is complete.\nSafety and mission-critical development assurance (e.g., DO-178C, ISO 26262) require rigorous documentation and in-process review that are difficult to achieve using continuous integration. This type of life cycle often requires additional steps to be completed prior to product release when regulatory approval of the product is required.","completion":"Software Engineering"}
{"prompt":"This technique effectively adds instructions to the target program to collect the required information. Note that instrumenting a program can cause performance changes, and may in some cases lead to inaccurate results and\/or heisenbugs.  The effect will depend on what information is being collected, on the level of timing details reported, and on whether basic block profiling is used in conjunction with instrumentation.  For example, adding code to count every procedure\/routine call will probably have less effect than counting how many times each statement is obeyed.  A few computers have special hardware to collect information; in this case the impact on the program is minimal.\nInstrumentation is key to determining the level of control and amount of time resolution available to the profilers. \n\nManual: Performed by the programmer, e.g. by adding instructions to explicitly calculate runtimes, simply count events or calls to measurement APIs such as the Application Response Measurement standard.\nAutomatic source level: instrumentation added to the source code by an automatic tool according to an instrumentation policy.\nIntermediate language: instrumentation added to assembly or decompiled bytecodes giving support for multiple higher-level source languages and avoiding (non-symbolic) binary offset re-writing issues.\nCompiler assisted\nBinary translation: The tool adds instrumentation to a compiled executable.\nRuntime instrumentation: Directly before execution the code is instrumented. The program run is fully supervised and controlled by the tool.\nRuntime injection: More lightweight than runtime instrumentation. Code is modified at runtime to have jumps to helper functions.","completion":"Software Engineering"}
{"prompt":"e-CitizenThe e-Citizen qualification allows beginners to get online and start using the Internet. The qualification has been designed to provide a basic understanding of the Internet and to start using the web safely, from reading email to shopping online.\nMoR (Management of Risk)M_o_R Foundation is suitable for any organisation or individual seeing the need for guidance on a controlled approach to identification, assessment and control risk at strategic, programme, project and operational perspectives.","completion":"Software Engineering"}
{"prompt":"It is of great importance to select and adapt a methodology for the daily handoffs e.g. using agile software development or the waterfall model. \nIdentified best practices are the use of agile methods and using technologies to develop FTS activities. Agile supports daily handoffs which is a critical challenge in FTS. Management tools can be used to estimate and plan schedules, manage sprints and track progress. Additionally, technologies like conference video, emails and telephone calls are easy to implement and allow companies to perform synchronous and asynchronous communication between teams and works well in an agile environment.\nUnfortunately, there is no solid best practice that works best since FTS can be applied in numerous ways.","completion":"Software Engineering"}
{"prompt":"A platform combines computer hardware and an operating system. As platforms grow more powerful and less costly, applications and tools grow more widely available.\n\nBREW\nCray supercomputers\nDEC minicomputers\nIBM mainframes\nLinux PCs\nClassic Mac OS and macOS PCs\nMicrosoft .NET\nPalm PDAs\nSun Microsystems Solaris\nWindows PCs (Wintel)\nSymbian OS","completion":"Software Engineering"}
{"prompt":"There are several different examples of BDD software tools in use in projects today, for different platforms and programming languages.\nPossibly the most well-known is JBehave, which was developed by Dan North, Elizabeth Keogh and several others. The following is an example taken from that project:Consider an implementation of the Game of Life. A domain expert (or business analyst) might want to specify what should happen when someone is setting up a starting configuration of the game grid. To do this, he might want to give an example of a number of steps taken by a person who is toggling cells. Skipping over the narrative part, he might do this by writing up the following scenario into a plain text document (which is the type of input document that JBehave reads):\n\nGiven a 5 by 5 game\nWhen I toggle the cell at (3, 2)\nThen the grid should look like\n.....\n.....\n.....\n..X..\n.....\nWhen I toggle the cell at (3, 1)\nThen the grid should look like\n.....\n.....\n.....\n..X..\n..X..\nWhen I toggle the cell at (3, 2)\nThen the grid should look like\n.....\n.....\n.....\n.....\n..X..\n\nThe bold print is not part of the input; it is included here to show which words are recognized as formal language. JBehave recognizes the terms Given (as a precondition which defines the start of a scenario), When (as an event trigger) and Then (as a postcondition which must be verified as the outcome of the action that follows the trigger). Based on this, JBehave is capable of reading the text file containing the scenario and parsing it into clauses (a set-up clause and then three event triggers with verifiable conditions). JBehave then takes these clauses and passes them on to code that is capable of setting a test, responding to the event triggers and verifying the outcome. This code must be written by the developers in the project team (in Java, because that is the platform JBehave is based on). In this case, the code might look like this:\n\nThe code has a method for every type of clause in a scenario. JBehave will identify which method goes with which clause through the use of annotations and will call each method in order while running through the scenario. The text in each clause in the scenario is expected to match the template text given in the code for that clause (for example, a Given in a scenario is expected to be followed by a clause of the form \"a X by Y game\"). JBehave supports the matching of clauses to templates and has built-in support for picking terms out of the template and passing them to methods in the test code as parameters. The test code provides an implementation for each clause type in a scenario which interacts with the code that is being tested and performs a test based on the scenario. In this case:\n\nThe theGameIsRunning method reacts to a Given clause by setting up the initial game grid.\nThe iToggleTheCellAt method reacts to a When clause by firing off the toggle event described in the clause.\nThe theGridShouldLookLike method reacts to a Then clause by comparing the state of the game grid to the expected state from the scenario.The primary function of this code is to be a bridge between a text file with a story and the code being tested. Note that the test code has access to the code being tested (in this case an instance of Game) and is very simple in nature. The test code has to be simple, otherwise a developer would end up having to write tests for his tests.\nFinally, in order to run the tests, JBehave requires some plumbing code that identifies the text files which contain scenarios and which inject dependencies (like instances of Game) into the test code. This plumbing code is not illustrated here, since it is a technical requirement of JBehave and does not relate directly to the principle of BDD-style testing.","completion":"Software Engineering"}
{"prompt":"Cloud computing is a model that allows for the use of computing resources, such as servers or applications, without the need for interaction between the owner of these resources and the end user. It is typically offered as a service, making it an example of Software as a Service, Platforms as a Service, and Infrastructure as a Service, depending on the functionality offered. Key characteristics include on-demand access, broad network access, and the capability of rapid scaling. It allows individual users or small business to benefit from economies of scale.\nOne area of interest in this field is its potential to support energy efficiency. Allowing thousands of instances of computation to occur on one single machine instead of thousands of individual machines could help save energy. It could also ease the transition to renewable energy source, since it would suffice to power one server farm with renewable energy, rather than millions of homes and offices.However, this centralized computing model poses several challenges, especially in security and privacy. Current legislation does not sufficiently protect users from companies mishandling their data on company servers. This suggests potential for further legislative regulations on cloud computing and tech companies.","completion":"Software Engineering"}
{"prompt":"Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.","completion":"Software Engineering"}
{"prompt":"Industrial design\nThe core principles of industrial design overlap with those of interaction design. Industrial designers use their knowledge of physical form, color, aesthetics, human perception and desire, and usability to create a fit of an object with the person using it.Human factors and ergonomics\nCertain basic principles of ergonomics provide grounding for interaction design. These include anthropometry, biomechanics, kinesiology, physiology and psychology as they relate to human behavior in the built environment.Cognitive psychology\nCertain basic principles of cognitive psychology provide grounding for interaction design. These include mental models, mapping, interface metaphors, and affordances. Many of these are laid out in Donald Norman's influential book The Design of Everyday Things.Human\u2013computer interaction\nAcademic research in human\u2013computer interaction (HCI) includes methods for describing and testing the usability of interacting with an interface, such as cognitive dimensions and the cognitive walkthrough.Design research\nInteraction designers are typically informed through iterative cycles of user research. User research is used to identify the needs, motivations and behaviors of end users. They design with an emphasis on user goals and experience, and evaluate designs in terms of usability and affective influence.Architecture\nAs interaction designers increasingly deal with ubiquitous computing, urban informatics and urban computing, the architects' ability to make, place, and create context becomes a point of contact between the disciplines.User interface design\nLike user interface design and experience design, interaction design is often associated with the design of system interfaces in a variety of media but concentrates on the aspects of the interface that define and present its behavior over time, with a focus on developing the system to respond to the user's experience and not the other way around.","completion":"Software Engineering"}
{"prompt":"CI should be used in combination with automated unit tests written through the practices of test-driven development. All unit tests in the developer's local environment should be run and passed before committing to the mainline. This helps prevent one developer's work-in-progress from breaking another developer's copy. Where necessary, incomplete features can be disabled before committing, using feature toggles, for instance.","completion":"Software Engineering"}
{"prompt":"Even though the idea of using computers in medicine emerged as technology advanced in the early 20th century, it was not until the 1950s that informatics began to have an effect in the United States.The earliest use of electronic digital computers for medicine was for dental projects in the 1950s at the United States National Bureau of Standards by Robert Ledley.. During the mid-1950s, the United States Air Force (USAF) carried out several medical projects on its computers while also encouraging civilian agencies such as the National Academy of Sciences \u2013 National Research Council (NAS-NRC) and the National Institutes of Health (NIH) to sponsor such work.. In 1959, Ledley and Lee B. Lusted published \"Reasoning Foundations of Medical Diagnosis,\" a widely read article in Science, which introduced computing (especially operations research) techniques to medical workers.. Ledley and Lusted's article has remained influential for decades, especially within the field of medical decision making.Guided by Ledley's late 1950s survey of computer use in biology and medicine (carried out for the NAS-NRC), and by his and Lusted's articles, the NIH undertook the first major effort to introduce computers to biology and medicine.. This effort, carried out initially by the NIH's Advisory Committee on Computers in Research (ACCR), chaired by Lusted, spent over $40 million between 1960 and 1964 in order to establish dozens of large and small biomedical research centers in the US.One early (1960, non-ACCR) use of computers was to help quantify normal human movement, as a precursor to scientifically measuring deviations from normal, and design of prostheses.. The use of computers (IBM 650, 1620, and 7040) allowed analysis of a large sample size, and of more measurements and subgroups than had been previously practical with mechanical calculators, thus allowing an objective understanding of how human locomotion varies by age and body characteristics.. A study co-author was Dean of the Marquette University College of Engineering; this work led to discrete Biomedical Engineering departments there and elsewhere.. The next steps, in the mid-1960s, were the development (sponsored largely by the NIH) of expert systems such as MYCIN and Internist-I.. In 1965, the National Library of Medicine started to use MEDLINE and MEDLARS.. Around this time, Neil Pappalardo, Curtis Marble, and Robert Greenes developed MUMPS (Massachusetts General Hospital Utility Multi-Programming System) in Octo Barnett's Laboratory of Computer Science at Massachusetts General Hospital in Boston, another center of biomedical computing that received significant support from the NIH.. In the 1970s and 1980s it was the most commonly used programming language for clinical applications.. The MUMPS operating system was used to support MUMPS language specifications..","completion":"Software Engineering"}
{"prompt":"The society provides several awards to recognise outstanding computer scientists, engineers, experienced and young IT professionals.\nThe awards include:\n\nLovelace Medal\nRoger Needham Award\nEarly Career Award\nJohn Perry Prize\nDistinguished Dissertation Award\nUK Industry Award","completion":"Software Engineering"}
{"prompt":"To practice continuous delivery effectively, software applications have to meet a set of architecturally significant requirements (ASRs) such as deployability, modifiability, and testability. These ASRs require a high priority and cannot be traded off lightly.\nMicroservices are often used when architecting for continuous delivery. The use of Microservices can increase a software system's deployability and modifiability. The observed deployability improvements include: deployment independence, shorter deployment time, simpler deployment procedures, and zero downtime deployment. The observed modifiability improvements include: shorter cycle time for small incremental functional changes, easier technology selection changes, incremental quality attribute changes, and easier language and library upgrades.","completion":"Software Engineering"}
{"prompt":"Parnas earned his PhD at Carnegie Mellon University in electrical engineering. Parnas also earned a professional engineering license in Canada and was one of the first to apply traditional engineering principles to software design.\nHe worked there as a professor for many years.  He also taught at the University of North Carolina at Chapel Hill (U.S.), at the Department of Computer Science of the Technische Universit\u00e4t Darmstadt (Germany), the University of Victoria (British Columbia, Canada), Queen's University in Kingston, Ontario, McMaster University in Hamilton, Ontario, and University of Limerick (Republic of Ireland).\nDavid Parnas received a number of awards and honors:\n\nACM \"Best Paper\" Award, 1979\nNorbert Wiener Award for Social and Professional Responsibility, 1987\nTwo \"Most Influential Paper\" awards International Conference on Software Engineering, 1991 and 1995\nDoctor honoris causa of the Computer Science Department, ETH Zurich, Switzerland, 1986\nFellow of the Royal Society of Canada, 1992\nFellow of the Association for Computing Machinery, 1994\nDoctor honoris causa of the Louvain School of Engineering, University of Louvain (UCLouvain), Belgium, 1996\nACM SIGSOFT's \"Outstanding Research\" award, 1998\nIEEE Computer Society's 60th Anniversary Award, 2007\nDoctor honoris causa of the Faculty of Informatics, University of Lugano, Switzerland, 2008\nFellow of the Gesellschaft f\u00fcr Informatik, 2008\nFellow of the Institute of Electrical and Electronics Engineers (IEEE), 2009\nDoctor honoris causa of the Vienna University of Technology (Dr. Tech.H.C.), Vienna Austria, 2011","completion":"Software Engineering"}
{"prompt":"Programmers must listen to what the customers need the system to do, what \"business logic\" is needed. They must understand these needs well enough to give the customer feedback about the technical aspects of how the problem might be solved, or cannot be solved. Communication between the customer and programmer is further addressed in the planning game.","completion":"Software Engineering"}
{"prompt":"Notable definitions of software engineering include:\n\n\"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software\"\u2014The Bureau of Labor Statistics\u2014IEEE Systems and software engineering \u2013 Vocabulary\n\"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software\"\u2014IEEE Standard Glossary of Software Engineering Terminology\n\"an engineering discipline that is concerned with all aspects of software production\"\u2014Ian Sommerville\n\"the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines\"\u2014Fritz Bauer\n\"a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs\"\u2014Merriam-Webster\n\"'software engineering' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as 'programming integrated over time.'\"\u2014Software Engineering at GoogleThe term has also been used less formally:\n\nas the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;\nas the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;\nas the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.","completion":"Software Engineering"}
{"prompt":"Fuzzing is a testing technique that involves executing a program on a wide variety of inputs; often these inputs are randomly generated (at least in part). Gray-box fuzzers use code coverage to guide input generation.","completion":"Software Engineering"}
{"prompt":"The evolution of software engineering is notable in a number of areas:\n\nEmergence as a profession: By the early 1980s software engineering had already emerged as a bona fide profession., to stand beside computer science and traditional engineering.\nRole of women: Before 1970 men filling the more prestigious and better paying hardware engineering roles often delegated the writing of software to women, and legends such as Grace Hopper or Margaret Hamilton filled many computer programming jobs. Today, fewer women work in software engineering than in other professions, a situation whose cause is not clearly identified. Many academic and professional organizations consider this situation unbalanced and are trying hard to solve it.\nProcesses: Processes have become a big part of software engineering. They are hailed for their potential to improve software but sharply criticized for their potential to constrict programmers.\nCost of hardware: The relative cost of software versus hardware has changed substantially over the last 50 years. When mainframes were expensive and required large support staffs, the few organizations buying them also had the resources to fund large, expensive custom software engineering projects. Computers are now much more numerous and much more powerful, which has several effects on software. The larger market can support large projects to create commercial off the shelf software, as done by companies such as Microsoft. The cheap machines allow each programmer to have a terminal capable of fairly rapid compilation. The programs in question can use techniques such as garbage collection, which make them easier and faster for the programmer to write. On the other hand, many fewer organizations are interested in employing programmers for large custom software projects, instead using commercial off the shelf software as much as possible.","completion":"Software Engineering"}
{"prompt":"Type theory is the study of type systems; which are \"a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute\". Many programming languages are distinguished by the characteristics of their type systems.","completion":"Software Engineering"}
{"prompt":"Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and\/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.\nIn the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. NCEES ended the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial.The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge \u2013 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a \"Software Engineering Code of Ethics\".","completion":"Software Engineering"}
{"prompt":"The IDEF0 functional modeling method is designed to model the decisions, actions, and activities of an organization or system. It was derived from the established graphic modeling language structured analysis and design technique (SADT) developed by Douglas T. Ross and SofTech, Inc. In its original form, IDEF0 includes both a definition of a graphical modeling language (syntax and semantics) and a description of a comprehensive methodology for developing models. The US Air Force commissioned the SADT developers to develop a function model method for analyzing and communicating the functional perspective of a system. IDEF0 should assist in organizing system analysis and promote effective communication between the analyst and the customer through simplified graphical devices.","completion":"Software Engineering"}
{"prompt":"A pioneer in the use of artificial intelligence in healthcare was American biomedical informatician Edward H. Shortliffe. This field deals with utilisation of machine-learning algorithms and artificial intelligence, to emulate human cognition in the analysis, interpretation, and comprehension of complicated medical and healthcare data. Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data. AI programs are applied to practices such as diagnosis processes, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. A large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions. Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of \"data assessment, storage, management, and analysis technologies\" which are all crucial parts of the healthcare industry. The following are examples of large companies that have contributed to AI algorithms for use in healthcare:\n\nIBM's Watson Oncology is in development at Memorial Sloan Kettering Cancer Center and Cleveland Clinic. IBM is also working with CVS Health on AI applications in chronic disease treatment and with Johnson & Johnson on analysis of scientific papers to find new connections for drug development. In May 2017, IBM and Rensselaer Polytechnic Institute began a joint project entitled Health Empowerment by Analytics, Learning and Semantics (HEALS), to explore using AI technology to enhance healthcare.\nMicrosoft's Hanover project, in partnership with Oregon Health & Science University's Knight Cancer Institute, analyzes medical research to predict the most effective cancer drug treatment options for patients. Other projects include medical image analysis of tumor progression and the development of programmable cells.\nGoogle's DeepMind platform is being used by the UK National Health Service to detect certain health risks through data collected via a mobile app. A second project with the NHS involves analysis of medical images collected from NHS patients to develop computer vision algorithms to detect cancerous tissues.\nTencent is working on several medical systems and services. These include AI Medical Innovation System (AIMIS), an AI-powered diagnostic medical imaging service; WeChat Intelligent Healthcare; and Tencent Doctorwork\nIntel's venture capital arm Intel Capital recently invested in startup Lumiata which uses AI to identify at-risk patients and develop care options.\nKheiron Medical developed deep learning software to detect breast cancers in mammograms.\nFractal Analytics has incubated Qure.ai which focuses on using deep learning and AI to improve radiology and speed up the analysis of diagnostic x-rays.\n Neuralink has come up with a next generation neuroprosthetic which intricately interfaces with thousands of neural pathways in the brain. Their process allows a chip, roughly the size of a quarter, to be inserted in place of a chunk of skull by a precision surgical robot to avoid accidental injury .Digital consultant apps like Babylon Health's GP at Hand, Ada Health, AliHealth Doctor You, KareXpert and Your.MD use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders). IFlytek launched a service robot \"Xiao Man\", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (\"Cruzr\") and Softbank Robotics (\"Pepper\"). The Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India. With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies. Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well. Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances or in emotional distress. Examples of projects in computational health informatics include the COACH project.","completion":"Software Engineering"}
{"prompt":"Mechanical engineering is an important part of mechatronics engineering. It includes the study of mechanical nature of how an object works. Mechanical elements refer to mechanical structure, mechanism, thermo-fluid, and hydraulic aspects of a mechatronics system. The study of thermodynamics, dynamics, fluid mechanics, pneumatics and hydraulics. Mechatronics engineer who works a mechanical engineer can specialize in hydraulics and pneumatics systems, where they can be found working in automobile industries. A mechatronics engineer can also design a vehicle since they have strong mechanical and electronical background. Knowledge of software applications such as computer-aided design and computer aided manufacturing is essential for designing products. Mechatronics covers a part of mechanical syllabus which is widely applied in automobile industry.\nMechatronic systems represent a large part of the functions of an automobile. The control loop formed by sensor\u2014information processing\u2014actuator\u2014mechanical (physical) change is found in many systems. The system size can be very different. The Anti-lock braking system (ABS) is a mechatronic system. The brake itself is also one. And the control loop formed by driving control (for example cruise control), engine, vehicle driving speed in the real world and speed measurement is a mechatronic system, too. The great importance of mechatronics for automotive engineering is also evident from the fact that vehicle manufacturers often have development departments with \"Mechatronics\" in their names.","completion":"Software Engineering"}
{"prompt":"During this final phase the system is deployed to the production environment. A number of preparatory steps are required. These include:\n\nConfiguring the operating systems, network, servers (application, web, database, load balancer, etc.), and any message queueing software according to the base checklists and the optimizations identified in the performance test environment\nEnsuring all performance monitoring software is deployed and configured\nRunning statistics on the database after the production data load is completedOnce the new system is deployed, ongoing operations pick up performance activities, including:\n\nValidating that weekly and monthly performance reports indicate that critical use cases perform within the specified non functional requirement criteria\nWhere use cases are falling outside of NFR criteria, submit defects\nIdentify projected trends from monthly and quarterly reports, and on a quarterly basis, execute capacity planning management activities","completion":"Software Engineering"}
{"prompt":"A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.","completion":"Software Engineering"}
{"prompt":"Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.","completion":"Software Engineering"}
{"prompt":"Generic data models are generalizations of conventional data models. They define standardized general relation types, together with the kinds of things that may be related by such a relation type. \nThe definition of generic data model is similar to the definition of a natural language. For example, a generic data model may define relation types such as a 'classification relation', being a binary relation between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related.\nGiven an extensible list of classes, this allows the classification of any individual thing and to specify part-whole relations for any individual object. By standardization of an extensible list of relation types, a generic data model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages. Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model.","completion":"Software Engineering"}
{"prompt":"In Bangladesh, the Bachelor of Engineering in Information Technology is awarded following a four-year course of study under the Dhaka University, Jahangirnagar University, Bangladesh University of Professionals, University of Information Technology and Sciences.","completion":"Software Engineering"}
{"prompt":"Goal-oriented design (or Goal-Directed design) \"is concerned with satisfying the needs and desires of the users of a product or service.\":\u200axxviii,\u200a31\u200aAlan Cooper argues in The Inmates Are Running the Asylum that we need a new approach to solving interactive software-based problems.:\u200a1\u200a The problems with designing computer interfaces are fundamentally different from those that do not include software (e.g., hammers).  Cooper introduces the concept of cognitive friction, which is when the interface of a design is complex and difficult to use, and behaves inconsistently and unexpectedly, possessing different modes.:\u200a22\u200aAlternatively, interfaces can be designed to serve the needs of the service\/product provider. User needs may be poorly served by this approach.","completion":"Software Engineering"}
{"prompt":"The earliest known work on continuous integration was the Infuse environment developed by G. E. Kaiser, D. E. Perry, and W. M. Schell.In 1994, Grady Booch used the phrase continuous integration in Object-Oriented Analysis and Design with Applications (2nd edition) to explain how, when developing using micro processes, \"internal releases represent a sort of continuous integration of the system, and exist to force closure of the micro process\".\nIn 1997, Kent Beck and Ron Jeffries invented extreme programming (XP) while on the Chrysler Comprehensive Compensation System project, including continuous integration. Beck published about continuous integration in 1998, emphasising the importance of face-to-face communication over technological support. In 1999, Beck elaborated more in his first full book on Extreme Programming. CruiseControl, one of the first open-source CI tools, was released in 2001.\nIn 2010, Timothy Fitz published an article detailing how IMVU's engineering team had built and been using the first practical CI system. While his post was originally met with scepticism, it quickly caught on and found widespread adoption as part of the Lean software development methodology, also based on IMVU.","completion":"Software Engineering"}
{"prompt":"The logical data structure of a DBMS, whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. That is unless the semantic data model is implemented in the database on purpose, a choice which may slightly impact performance but generally vastly improves productivity.\n\nTherefore, the need to define data from a conceptual view has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure the real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.The purpose of semantic data modeling is to create a structural model of a piece of the real world, called \"universe of discourse\". For this, four fundamental structural relations are considered:\n\nClassification\/instantiation: Objects with some structural similarity are described as instances of classes\nAggregation\/decomposition: Composed objects are obtained joining its parts\nGeneralization\/specialization: Distinct classes with some common properties are reconsidered in a more generic class with the common attributesA semantic data model can be used to serve many purposes, such as:\nPlanning of data resources\nBuilding of shareable databases\nEvaluation of vendor software\nIntegration of existing databasesThe overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the artificial intelligence field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations.","completion":"Software Engineering"}
{"prompt":"This is about treating every problem as if its solution were \"extremely simple\". Traditional system development methods say to plan for the future and to code for reusability. Extreme programming rejects these ideas.\nThe advocates of extreme programming say that making big changes all at once does not work. Extreme programming applies incremental changes: for example, a system might have small releases every three weeks. When many little steps are made, the customer has more control over the development process and the system that is being developed.","completion":"Software Engineering"}
{"prompt":"Input-sensitive profilers add a further dimension to flat or call-graph profilers by relating performance measures to features of the input workloads, such as input size or input values. They generate charts that characterize how an application's performance scales as a function of its input.","completion":"Software Engineering"}
{"prompt":"In modular design, his double dictum of high cohesion within modules and loose coupling between modules is fundamental to modular design in software.  However, in Parnas's seminal 1972 paper On the Criteria to Be Used in Decomposing Systems into Modules, this dictum is expressed in terms of information hiding, and the terms cohesion and coupling are not used. He never used them.","completion":"Software Engineering"}
{"prompt":"Continuous integration\nFrequent merging of several small changes into a main branch.\nContinuous delivery\nWhen teams produce software in short cycles with high speed and frequency so that reliable software can be released at any time, and with a simple and repeatable deployment process when deciding to deploy.\nContinuous deployment\nWhen new software functionality is rolled out completely automatically.","completion":"Software Engineering"}
{"prompt":"Extreme programming encourages starting with the simplest solution. Extra functionality can then be added later. The difference between this approach and more conventional system development methods is the focus on designing and coding for the needs of today instead of those of tomorrow, next week, or next month. This is sometimes summed up as the \"You aren't gonna need it\" (YAGNI) approach. Proponents of XP acknowledge the disadvantage that this can sometimes entail more effort tomorrow to change the system; their claim is that this is more than compensated for by the advantage of not investing in possible future requirements that might change before they become relevant. Coding and designing for uncertain future requirements implies the risk of spending resources on something that might not be needed, while perhaps delaying crucial features. Related to the \"communication\" value, simplicity in design and coding should improve the quality of communication. A simple design with very simple code could be easily understood by most programmers in the team.","completion":"Software Engineering"}
{"prompt":"An entity may be defined as a thing that is capable of an independent existence that can be uniquely identified, and is capable of storing data. An entity is an abstraction from the complexities of a domain. When we speak of an entity, we normally speak of some aspect of the real world that can be distinguished from other aspects of the real world.An entity is a thing that exists either physically or logically. An entity may be a physical object such as a house or a car (they exist physically), an event such as a house sale or a car service, or a concept such as a customer transaction or order (they exist logically\u2014as a concept). Although the term entity is the one most commonly used, following Chen we should really distinguish between an entity and an entity-type. An entity-type is a category. An entity, strictly speaking, is an instance of a given entity-type. There are usually many instances of an entity-type. Because the term entity-type is somewhat cumbersome, most people tend to use the term entity as a synonym for this term\nEntities can be thought of as nouns. Examples: a computer, an employee, a song, a mathematical theorem, etc.\nA relationship captures how entities are related to one another. Relationships can be thought of as verbs, linking two or more nouns. Examples: an owns relationship between a company and a computer, a supervises relationship between an employee and a department, a performs relationship between an artist and a song, a proves relationship between a mathematician and a conjecture, etc.\nThe model's linguistic aspect described above is utilized in the declarative database query language ERROL, which mimics natural language constructs. ERROL's semantics and implementation are based on reshaped relational algebra (RRA), a relational algebra that is adapted to the entity\u2013relationship model and captures its linguistic aspect.\nEntities and relationships can both have attributes. Examples: an employee entity might have a Social Security Number (SSN) attribute, while a proved relationship may have a date attribute.\nAll entities except weak entities must have a minimal set of uniquely identifying attributes which may be used as a unique\/primary key.\nEntity-relationship diagrams (ERDs) don't show single entities or single instances of relations. Rather, they show entity sets (all entities of the same entity type) and relationship sets (all relationships of the same relationship type). Examples: a particular song is an entity; the collection of all songs in a database is an entity set; the eaten relationship between a child and his lunch is a single relationship; the set of all such child-lunch relationships in a database is a relationship set.\nIn other words, a relationship set corresponds to a relation in mathematics, while a relationship corresponds to a member of the relation.\nCertain cardinality constraints on relationship sets may be indicated as well.","completion":"Software Engineering"}
{"prompt":"In 1976, an entity-relationship (ER) graphic notation was introduced by Peter Chen. He stressed that it was a \"semantic\" modelling technique and independent of any database modelling techniques such as Hierarchical, CODASYL, Relational etc. Since then, languages for information models have continued to evolve. Some examples are the Integrated Definition Language 1 Extended (IDEF1X), the EXPRESS language and the Unified Modeling Language (UML).Research by contemporaries of Peter Chen such as J.R.Abrial (1974) and G.M Nijssen (1976) led to today's Fact Oriented Modeling (FOM) languages which are based on linguistic propositions rather than on \"entities\". FOM tools can be used to generate an ER model which means that the modeler can avoid the time-consuming and error prone practice of manual normalization. Object-Role Modeling language (ORM) and Fully Communication Oriented Information Modeling (FCO-IM) are both research results developed in the early 1990s, based upon earlier research.\nIn the 1980s there were several approaches to extend Chen\u2019s Entity Relationship Model. Also important in this decade is REMORA by Colette Rolland.The ICAM Definition (IDEF) Language was developed from the U.S. Air Force ICAM Program during the 1976 to 1982 timeframe. The objective of the ICAM Program, according to Lee (1999), was to increase manufacturing productivity through the systematic application of computer technology. IDEF includes three different modeling methods: IDEF0, IDEF1, and IDEF2 for producing a functional model, an information model, and a dynamic model respectively. IDEF1X is an extended version of IDEF1. The language is in the public domain. It is a graphical representation and is designed using the ER approach and the relational theory. It is used to represent the \u201creal world\u201d in terms of entities, attributes, and relationships between entities. Normalization is enforced by KEY Structures and KEY Migration. The language identifies property groupings (Aggregation) to form complete entity definitions.EXPRESS was created as ISO 10303-11 for formally specifying information requirements of product data model. It is part of a suite of standards informally known as the STandard for the Exchange of Product model data (STEP). It was first introduced in the early 1990s. The language, according to Lee (1999), is a textual representation. In addition, a graphical subset of EXPRESS called EXPRESS-G is available. EXPRESS is based on programming languages and the O-O paradigm. A number of languages have contributed to EXPRESS. In particular, Ada, Algol, C, C++, Euler, Modula-2, Pascal, PL\/1, and SQL. EXPRESS consists of language elements that allow an unambiguous object definition and specification of constraints on the objects defined. It uses SCHEMA declaration to provide partitioning and it supports specification of data properties, constraints, and operations.UML is a modeling language for specifying, visualizing, constructing, and documenting the artifacts, rather than processes, of software systems. It was conceived originally by Grady Booch, James Rumbaugh, and Ivar Jacobson. UML was approved by the Object Management Group (OMG) as a standard in 1997. The language, according to Lee (1999), is non-proprietary and is available to the public. It is a graphical representation. The language is based on the objected-oriented paradigm. UML contains notations and rules and is designed to represent data requirements in terms of O-O diagrams. UML organizes a model in a number of views that present different aspects of a system. The contents of a view are described in diagrams that are graphs with model elements. A diagram contains model elements that represent common O-O concepts such as classes, objects, messages, and relationships among these concepts.IDEF1X, EXPRESS, and UML all can be used to create a conceptual model and, according to Lee (1999), each has its own characteristics. Although some may lead to a natural usage (e.g., implementation), one is not necessarily better than another. In practice, it may require more than one language to develop all information models when an application is complex. In fact, the modeling practice is often more important than the language chosen.Information models can also be expressed in formalized natural languages, such as Gellish. Gellish, which has natural language variants Gellish Formal English, Gellish Formal Dutch (Gellish Formeel Nederlands), etc. is an information representation language or modeling language that is defined in the Gellish smart Dictionary-Taxonomy, which has the form of a Taxonomy\/Ontology. A Gellish Database is not only suitable to store information models, but also knowledge models, requirements models and dictionaries, taxonomies and ontologies. Information models in Gellish English use Gellish Formal English expressions. For example, a geographic information model might consist of a number of Gellish Formal English expressions, such as: \n- the Eiffel tower <is located in> Paris\n- Paris <is classified as a> city\n\nwhereas information requirements and knowledge can be expressed for example as follows:\n\n- tower <shall be located in a> geographical area\n- city <is a kind of> geographical area\n\nSuch Gellish expressions use names of concepts (such as 'city') and relation types (such as \u27e8is located in\u27e9 and \u27e8is classified as a\u27e9) that should be selected from the Gellish Formal English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models, because the dictionary contains definitions of more than 40000 concepts, including more than 600 standard relation types. Thus, an information model in Gellish consists of a collection of Gellish expressions that use those phrases and dictionary concepts to express facts or make statements, queries and answers.","completion":"Software Engineering"}
{"prompt":"This section lists best practices suggested by various authors on how to achieve continuous integration, and how to automate this practice. Build automation is a best practice itself.Continuous integration\u2014the practice of frequently integrating one's new or changed code with the existing code repository \u2014should occur frequently enough that no intervening window remains between commit and build, and such that no errors can arise without developers noticing them and correcting them immediately. Normal practice is to trigger these builds by every commit to a repository, rather than a periodically scheduled build. The practicalities of doing this in a multi-developer environment of rapid commits are such that it is usual to trigger a short time after each commit, then to start a build when either this timer expires or after a rather longer interval since the last build. Note that since each new commit resets the timer used for the short time trigger, this is the same technique used in many button debouncing algorithms. In this way, the commit events are \"debounced\" to prevent unnecessary builds between a series of rapid-fire commits. Many automated tools offer this scheduling automatically.\nAnother factor is the need for a version control system that supports atomic commits; i.e., all of a developer's changes may be seen as a single commit operation. There is no point in trying to build from only half of the changed files.\nTo achieve these objectives, continuous integration relies on the following principles.","completion":"Software Engineering"}
{"prompt":"Imaging informatics and medical image computing develops computational and mathematical methods for solving problems pertaining to medical images and their use for biomedical research and clinical care. Those fields aims to extract clinically relevant information or knowledge from medical images and computational analysis of the images. The methods can be grouped into several broad categories: image segmentation, image registration, image-based physiological modeling, and others.","completion":"Software Engineering"}
{"prompt":"The first computer engineering degree program in the United States was established in 1971 at Case Western Reserve University in Cleveland, Ohio. As of 2015, there were 250 ABET-accredited computer engineering programs in the U.S. In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor's degree generally called computer engineering. Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.","completion":"Software Engineering"}
{"prompt":"The profession is trying to define its boundary and content. The Software Engineering Body of Knowledge SWEBOK has been tabled as an ISO standard during 2006 (ISO\/IEC TR 19759).In 2006, Money Magazine and Salary.com rated software engineering as the best job in America in terms of growth, pay, stress levels, flexibility in hours and working environment, creativity, and how easy it is to enter and advance in the field.","completion":"Software Engineering"}
{"prompt":"Continuous delivery takes automation from source control all the way through production. There are various tools that help accomplish all or part of this process. These tools are part of the deployment pipeline which includes continuous delivery. The types of tools that execute various parts of the process include: continuous integration, application release automation, build automation, application lifecycle management.","completion":"Software Engineering"}
{"prompt":"Kent Beck developed extreme programming during his work on the Chrysler Comprehensive Compensation System (C3) payroll project. Beck became the C3 project leader in March 1996. He began to refine the development methodology used in the project and wrote a book on the methodology (Extreme Programming Explained, published in October 1999). Chrysler cancelled the C3 project in February 2000, after seven years, when Daimler-Benz acquired the company. Ward Cunningham was another major influence on XP.\nMany extreme-programming practices have been around for some time; the methodology takes \"best practices\" to extreme levels. For example, the \"practice of test-first development, planning and writing tests before each micro-increment\" was used as early as NASA's Project Mercury, in the early 1960s. To shorten the total development time, some formal test documents (such as for acceptance testing) have been developed in parallel with (or shortly before) the software being ready for testing. A NASA independent test group can write the test procedures, based on formal requirements and logical limits, before programmers write the software and integrate it with the hardware. XP takes this concept to the extreme level, writing automated tests (sometimes inside software modules) which validate the operation of even small sections of software coding, rather than only testing the larger features.","completion":"Software Engineering"}
{"prompt":"Functional decomposition refers broadly to the process of resolving a functional relationship into its constituent parts in such a way that the original function can be reconstructed from those parts by function composition. In general, this process of decomposition is undertaken either for the purpose of gaining insight into the identity of the constituent components, or for the purpose of obtaining a compressed representation of the global function, a task which is feasible only when the constituent processes possess a certain level of modularity.\nFunctional decomposition has a prominent role in computer programming, where a major goal is to modularize processes to the greatest extent possible. For example, a library management system may be broken up into an inventory module, a patron information module, and a fee assessment module. In the early decades of computer programming, this was manifested as the \"art of subroutining,\" as it was called by some prominent practitioners.\nFunctional decomposition of engineering systems is a method for analyzing engineered systems. The basic idea is to try to divide a system in such a way that each block of the block diagram can be described without an \"and\" or \"or\" in the description.\nThis exercise forces each part of the system to have a pure function. When a system is composed of pure functions, they can be reused, or replaced. A usual side effect is that the interfaces between blocks become simple and generic. Since the interfaces usually become simple, it is easier to replace a pure function with a related, similar function.","completion":"Software Engineering"}
{"prompt":"The rise of the Internet led to very rapid growth in the demand for international information display\/e-mail systems on the World Wide Web.  Programmers were required to handle illustrations, maps, photographs, and other images, plus simple animation, at a rate never before seen, with few well-known methods to optimize image display\/storage (such as the use of thumbnail images).The growth of browser usage, running on the HyperText Markup Language (HTML), changed the way in which information-display and retrieval was organized.  The widespread network connections led to the growth and prevention of international computer viruses on MS Windows computers, and the vast proliferation of spam e-mail became a major design issue in e-mail systems, flooding communication channels and requiring semi-automated pre-screening. Keyword-search systems evolved into web-based search engines, and many software systems had to be re-designed, for international searching, depending on search engine optimization (SEO) .  Human natural-language translation systems were needed to attempt to translate the information flow in multiple foreign languages, with many software systems being designed for multi-language usage, based on design concepts from human translators.  Typical computer-user bases went from hundreds, or thousands of users, to, often, many-millions of international users.","completion":"Software Engineering"}
{"prompt":"To ensure that there is proper feedback validating that the system meets the NFR specified performance metrics, any major system needs a monitoring subsystem. The planning, design, installation, configuration, and control of the monitoring subsystem are specified by an appropriately defined monitoring process.\nThe benefits are as follows:\n\nIt is possible to establish service level agreements at the use case level.\nIt is possible to turn on and turn off monitoring at periodic points or to support problem resolution.\nIt enables the generation of regular reports.\nIt enables the ability to track trends over time, such as the impact of increasing user loads and growing data sets on use case level performance.The trend analysis component of this cannot be undervalued. This functionality, properly implemented, will enable predicting when a given application undergoing gradually increasing user loads and growing data sets will exceed the specified non functional performance requirements for a given use case. This permits proper management budgeting, acquisition of, and deployment of the required resources to keep the system running within the parameters of the non functional performance requirements.","completion":"Software Engineering"}
{"prompt":"A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep learning to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.\nA 2011 McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.","completion":"Software Engineering"}
{"prompt":"XP generated significant interest among software communities in the late 1990s and early 2000s, seeing adoption in a number of environments radically different from its origins.\nThe high discipline required by the original practices often went by the wayside, causing some of these practices, such as those thought too rigid, to be deprecated or reduced, or even left unfinished, on individual sites. For example, the practice of end-of-day integration tests for a particular project could be changed to an end-of-week schedule, or simply reduced to testing on mutually agreed dates. Such a more relaxed schedule could avoid people feeling rushed to generate artificial stubs just to pass the end-of-day testing. A less-rigid schedule allows, instead, the development of complex features over a period of several days.\nMeanwhile, other agile-development practices have not stood still, and as of 2019 XP continues to evolve, assimilating more lessons from experiences in the field, to use other practices. In the second edition of Extreme Programming Explained (November 2004), five years after the first edition, Beck added more  values and practices and differentiated between primary and corollary practices.","completion":"Software Engineering"}
{"prompt":"Although first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM\u2014turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), inform\u00e1tica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (\u03c0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"A folkloric quotation, often attributed to\u2014but almost certainly not first formulated by\u2014Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt G\u00f6del, Alan Turing, John von Neumann, R\u00f3zsa P\u00e9ter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.","completion":"Software Engineering"}
{"prompt":"Mills served on the faculties of Iowa State University, Princeton, New York and Johns Hopkins Universities, the Universities of Maryland and Florida,\nand Florida Institute of Technology (FIT). At Johns Hopkins and Maryland, he initiated one of the first American university courses in structured programming. At Maryland, he developed a new two-semester freshman introduction to computer science and textbook \"Principles of Computer Programming: A Mathematical Approach\" with co-authors Basili, Gannon, and Hamlet. At FIT, he developed a new freshman and sophomore curriculum for software engineering using Ada as the underlying language with colleagues Engle and Newman.","completion":"Software Engineering"}
{"prompt":"For capacity management, performance engineering focuses on ensuring that the systems will remain within performance compliance. This means executing trend analysis on historical monitoring generated data, such that the future time of non compliance is predictable. For example, if a system is showing a trend of slowing transaction processing (which might be due to growing data set sizes, or increasing numbers of concurrent users, or other factors) then at some point the system will no longer meet the criteria specified within the service level agreements. Capacity management is charged with ensuring that additional capacity is added in advance of that point (additional CPUs, more memory, new database indexing, et cetera) so that the trend lines are reset and the system will remain within the specified performance range.","completion":"Software Engineering"}
{"prompt":"Hamilton, M.; Zeldin, S. (March 1976). \"Higher Order Software\u2014A Methodology for Defining Software\". IEEE Transactions on Software Engineering. SE-2 (1): 9\u201332. doi:10.1109\/TSE.1976.233798. S2CID 7799553.\nHamilton, M.; Zeldin, S. (January 1, 1979). \"The relationship between design and verification\". Journal of Systems and Software. 1: 29\u201356. doi:10.1016\/0164-1212(79)90004-9.\nHamilton, M. (April 1994). \"Inside Development Before the Fact\". (Cover story). Special Editorial Supplement. 8ES-24ES. Electronic Design.\nHamilton, M. (June 1994). \"001: A Full Life Cycle Systems Engineering and Software Development Environment\". (Cover story). Special Editorial Supplement. 22ES-30ES. Electronic Design.\nHamilton, M.; Hackler, W. R. (2004). \"Deeply Integrated Guidance Navigation Unit (DI-GNU) Common Software Architecture Principles\". (Revised December 29, 2004). DAAAE30-02-D-1020 and DAAB07-98-D-H502\/0180, Picatinny Arsenal, NJ, 2003\u20132004.\nHamilton, M.; Hackler, W. R. (2007). \"Universal Systems Language for Preventative Systems Engineering\", Proc. 5th Ann. Conf. Systems Eng. Res. (CSER), Stevens Institute of Technology, Mar. 2007, paper #36.\nHamilton, Margaret H.; Hackler, William R. (2007). \"8.3.2 A Formal Universal Systems Semantics for SysML\". INCOSE International Symposium. Wiley. 17 (1): 1333\u20131357. doi:10.1002\/j.2334-5837.2007.tb02952.x. ISSN 2334-5837. S2CID 57214708.\nHamilton, Margaret H.; Hackler, William R. (2008). \"Universal Systems Language: Lessons Learned from Apollo\". Computer. Institute of Electrical and Electronics Engineers (IEEE). 41 (12): 34\u201343. doi:10.1109\/mc.2008.541. ISSN 0018-9162.\nHamilton, M. H. (September 2018). \"What the Errors Tell Us\". IEEE Software. 35 (5): 32\u201337. doi:10.1109\/MS.2018.290110447. S2CID 52896962.","completion":"Software Engineering"}
{"prompt":"Software construction, the main activity of software development, is the combination of programming, unit testing, integration testing, and debugging so as to implement the design. Testing during this phase is generally performed by the programmer while the software is under construction, to verify what was just written and decide when the code is ready to be sent to the next step.","completion":"Software Engineering"}
{"prompt":"Margaret Hamilton promoted the term \"software engineering\" during her work on the Apollo program. The term \"engineering\" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new \"term\" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.","completion":"Software Engineering"}
{"prompt":"Early symbolic AI inspired Lisp and Prolog, which dominated early AI programming. Modern AI development often uses mainstream languages such as Python or C++, or niche languages such as Wolfram Language.","completion":"Software Engineering"}
{"prompt":"Parnas has joined the group of scientists which openly criticize the number-of-publications-based approach towards ranking academic production. On his November 2007 paper Stop the Numbers Game, he elaborates on several reasons on why the current number-based academic evaluation system used in many fields by universities all over the world (be it either oriented to the amount of publications or the amount of quotations each of those get) is flawed and, instead of contributing to scientific progress, it leads to knowledge stagnation.","completion":"Software Engineering"}
{"prompt":"The history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems. The earliest known tool for use in computation is the abacus, and it is thought to have been invented in Babylon circa between 2700\u20132300 BC. Abaci, of a more modern design, are still used as calculation tools today.\nThe first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, the Manchester Baby. However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. The metal\u2013oxide\u2013silicon field-effect transistor (MOSFET, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.  The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer revolution.","completion":"Software Engineering"}
{"prompt":"Worldwide use of computer technology in medicine began in the early 1950s with the rise of the computers. In 1949, Gustav Wagner established the first professional organization for informatics in Germany. The prehistory, history, and future of medical information and health information technology are discussed in reference. Specialized university departments and Informatics training programs began during the 1960s in France, Germany, Belgium and The Netherlands. Medical informatics research units began to appear during the 1970s in Poland and in the U.S. Since then the development of high-quality health informatics research, education and infrastructure has been a goal of the U.S. and the European Union.\nEarly names for health informatics included medical computing, biomedical computing, medical computer science, computer medicine, medical electronic data processing, medical automatic data processing, medical information processing, medical information science, medical software engineering, and medical computer technology.\nThe health informatics community is still growing, it is by no means a mature profession, but work in the UK by the voluntary registration body, the UK Council of Health Informatics Professions has suggested eight key constituencies within the domain\u2013information management, knowledge management, portfolio\/program\/project management, ICT, education and research, clinical informatics, health records(service and business-related), health informatics service management. These constituencies accommodate professionals in and for the NHS, in academia and commercial service and solution providers.\nSince the 1970s the most prominent international coordinating body has been the International Medical Informatics Association (IMIA).","completion":"Software Engineering"}
{"prompt":"In using a modeled database, users can encounter two well known issues where the returned results mean something other than the results assumed by the query author.\nThe first is the 'fan trap'. It occurs with a (master) table that links to multiple tables in a one-to-many relationship. The issue derives its name from the way the model looks when it's drawn in an entity\u2013relationship diagram: the linked tables 'fan out' from the master table. This type of model looks similar to a star schema, a type of model used in data warehouses. When trying to calculate sums over aggregates using standard SQL over the master table, unexpected (and incorrect) results may occur. The solution is to either adjust the model or the SQL. This issue occurs mostly in databases for decision support systems, and software that queries such systems sometimes includes specific methods for handling this issue.\nThe second issue is a 'chasm trap'. A chasm trap occurs when a model suggests the existence of a relationship between entity types, but the pathway does not exist between certain entity occurrences. For example, a Building has one-or-more Rooms, that hold zero-or-more Computers. One would expect to be able to query the model to see all the Computers in the Building. However, Computers not currently assigned to a Room (because they are under repair or somewhere else) are not shown on the list. Another relation between Building and Computers is needed to capture all the computers in the building. This last modelling issue is the result of a failure to capture all the relationships that exist in the real world in the model. See Entity-Relationship Modelling 2 for details.","completion":"Software Engineering"}
{"prompt":"The N2 Chart is a diagram in the shape of a matrix, representing functional or physical interfaces between system elements. It is used to systematically identify, define, tabulate, design, and analyze functional and physical interfaces. It applies to system interfaces and hardware and\/or software interfaces.The N2 diagram has been used extensively to develop data interfaces, primarily in the software areas. However, it can also be used to develop hardware interfaces. The basic N2 chart is shown in Figure 2. The system functions are placed on the diagonal; the remainder of the squares in the N \u00d7 N matrix represent the interface inputs and outputs.","completion":"Software Engineering"}
{"prompt":"Most CI systems allow the running of scripts after a build finishes. In most situations, it is possible to write a script to deploy the application to a live test server that everyone can look at. A further advance in this way of thinking is continuous deployment, which calls for the software to be deployed directly into production, often with additional automation to prevent defects or regressions.","completion":"Software Engineering"}
{"prompt":"A computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information. When at least one process in one device is able to send or receive data to or from at least one process residing in a remote device, the two devices are said to be in a network. Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.\nCommunications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. One well-known communications protocol is Ethernet, a hardware and link layer standard that is ubiquitous in local area networks. Another common protocol is the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, host-to-host data transfer, and application-specific data transmission formats.Computer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.","completion":"Software Engineering"}
{"prompt":"The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.","completion":"Software Engineering"}
{"prompt":"In principle a BDD support tool is a testing framework for software, much like the tools that support TDD. However, where TDD tools tend to be quite free-format in what is allowed for specifying tests, BDD tools are linked to the definition of the ubiquitous language discussed earlier.\nAs discussed, the ubiquitous language allows business analysts to write down behavioral requirements in a way that will also be understood by developers. The principle of BDD support tooling is to make these same requirements documents directly executable as a collection of tests. If this cannot be achieved because of reasons related to the technical tool that enables the execution of the specifications, then either the style of writing the behavioral requirements must be altered or the tool must be changed. The exact implementation of behavioral requirements varies per tool, but agile practice has come up with the following general process:\n\nThe tooling reads a specification document.\nThe tooling directly understands completely formal parts of the ubiquitous language (such as the Given keyword in the example above). Based on this, the tool breaks each scenario up into meaningful clauses.\nEach individual clause in a scenario is transformed into some sort of parameter for a test for the user story. This part requires project-specific work by the software developers.\nThe framework then executes the test for each scenario, with the parameters from that scenario.Dan North has developed a number of frameworks that support BDD (including JBehave and RBehave), whose operation is based on the template that he suggested for recording user stories. These tools use a textual description for use cases and several other tools (such as CBehave) have followed suit. However, this format is not required and so there are other tools that use other formats as well. For example, Fitnesse (which is built around decision tables), has also been used to roll out BDD.","completion":"Software Engineering"}
{"prompt":"This phase involves the determination of costs, benefits, and schedule impact. It has four components:\n\nSort by Value: Business sorts the user stories by Business Value.\nSort by Risk: Development sorts the stories by risk.\nSet Velocity: Development determines at what speed they can perform the project.\nChoose scope: The user stories that will be finished in the next release will be picked. Based on the user stories the release date is determined.","completion":"Software Engineering"}
{"prompt":"FTS is based on the following four principles:\n\nThe main objective is the reduction of development duration \/ time to market.\nProduction sites are many time zones apart.\nThere is always one and only one site that owns and works on the project.\nHandoffs are conducted daily at the end of each shift. The next production site is several time zones west.","completion":"Software Engineering"}
{"prompt":"Programmable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices. In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" \u2013 a series of pasteboard cards with holes punched in them.\nCode-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. However, Charles Babbage had already written his first program for the Analytical Engine in 1837.\nIn the 1880s, Herman Hollerith invented the concept of storing data in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.","completion":"Software Engineering"}
{"prompt":"Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.","completion":"Software Engineering"}
{"prompt":"Most widely read of Sommerville's publications is probably his student text book \"Software Engineering\", currently in its 10th edition along with other textbooks Sommerville has also authored or co-authored numerous peer reviewed articles, papers.","completion":"Software Engineering"}
{"prompt":"Interpreter debug options can enable the collection of performance metrics as the interpreter encounters each target statement. A bytecode, control table or JIT interpreters are three examples that usually have complete control over execution of the target code, thus enabling extremely comprehensive data collection opportunities.","completion":"Software Engineering"}
{"prompt":"IDEF originally stood for ICAM Definition, initiated in the 1970s at the US Air Force Materials Laboratory, Wright-Patterson Air Force Base in Ohio by Dennis E. Wisnosky, Dan L. Shunk, and others. and completed in the 1980s. IDEF was a product of the ICAM initiative of the United States Air Force. The IEEE recast the IDEF abbreviation as Integration Definition.\"The specific projects that produced IDEF were ICAM project priorities 111 and 112 (later renumbered 1102). The subsequent Integrated Information Support System (IISS) project priorities 6201, 6202, and 6203 attempted to create an information processing environment that could be run in heterogeneous physical computing environments. Further development of IDEF occurred under those projects as a result of the experience gained from applications of the new modeling techniques. The intent of the IISS efforts was to create 'generic subsystems' that could be used by a large number of collaborating enterprises, such as U.S. defense contractors and the armed forces of friendly nations.\nAt the time of the ICAM 1102 effort there were numerous, mostly incompatible, data model methods for storing computer data \u2014 sequential (VSAM), hierarchical (IMS), network (Cincom's TOTAL and CODASYL, and Cullinet's IDMS). The relational data model was just emerging as a promising way of thinking about structuring data for easy, efficient, and accurate access. Relational database management systems had not yet emerged as a general standard for data management.\nThe ICAM program office deemed it valuable to create a \"neutral\" way of describing the data content of large-scale systems. The emerging academic literature suggested that methods were needed to process data independently of the way it was physically stored. Thus the IDEF1 language was created to allow a neutral description of data structures that could be applied regardless of the storage method or file access method.\nIDEF1 was developed under ICAM program priority 1102 by Robert R. Brown of the Hughes Aircraft Company, under contract to SofTech, Inc. Brown had previously been responsible for the development of IMS while working at Rockwell International. Rockwell chose not to pursue IMS as a marketable product but IBM, which had served as a support contractor during development, subsequently took over the product and was successful in further developing it for market. Brown credits his Hughes colleague Timothy Ramey as the inventor of IDEF1 as a viable formalism for modeling information structures. The two Hughes researchers built on ideas from and interactions with many luminaries in the field at the time. In particular, IDEF1 draws on the following techniques:\n\nthe evolving natural language information model (ENALIM) technique of G. M. Nijssen (Control Data Corporation) \u2014 this technique is now more widely known as NIAM or the object\u2013role model ORM;\nthe network data structures technique, popularly called the CODASYL approach, of Charles Bachman (Honeywell Information Systems);\nthe hierarchical data management technique, implemented in IBM's IMS data management system, developed by R. R. Brown (Rockwell International);\nthe relational approach to data of E. F. Codd (IBM);\nThe entity\u2013relationship approach (E\u2013R) of Peter Chen (UCLA).The effort to develop IDEF1 resulted in both a new method for information modeling and an example of its use in the form of a \"reference information model of manufacturing.\" This latter artifact was developed by D. S. Coleman of the D. Appleton Company (DACOM) acting as a sub-contractor to Hughes and under the direction of Ramey. Personnel at DACOM became expert at IDEF1 modeling and subsequently produced a training course and accompanying materials for the IDEF1 modeling technique.\nExperience with IDEF1 revealed that the translation of information requirements into database designs was more difficult than had originally been anticipated. The most beneficial value of the IDEF1 information modeling technique was its ability to represent data independent of how those data were to be stored and used. It provided data modelers and data analysts with a way to represent data requirements during the requirements-gathering process. This allowed designers to decide which DBMS to use after the nature of the data requirements was understood and thus reduced the \"misfit\" between data requirements and the capabilities and limitations of the DBMS. The translation of IDEF1 models to database designs, however, proved to be difficult.","completion":"Software Engineering"}
{"prompt":"In 1976, Hamilton co-founded with Saydean Zeldin a company called Higher Order Software (HOS) to further develop ideas about error prevention and fault tolerance emerging from their experience at MIT working on the Apollo program. They created a product called USE.IT, based on the HOS methodology they developed at MIT. It was successfully used in numerous government programs including a project to formalize and implement C-IDEF, an automated version of IDEF, a modeling language developed by the U.S. Air Force in the Integrated Computer-Aided Manufacturing (ICAM) project. In 1980, British-Israeli computer scientist David Harel published a proposal for a structured programming language derived from HOS from the viewpoint of and\/or subgoals. Others have used HOS to formalize the semantics of linguistic quantifiers, and to formalize the design of reliable real-time embedded systems.Hamilton was the CEO of HOS through 1984 and left the company in 1985. In March 1986, she founded Hamilton Technologies, Inc. in Cambridge, Massachusetts. The company was developed around the Universal Systems Language (USL) and its associated automated environment, the 001 Tool Suite, based on her paradigm of development before the fact for systems design and software development.","completion":"Software Engineering"}
{"prompt":"Jan van Bemmel has described medical informatics as the theoretical and practical aspects of information processing and communication based on knowledge and experience derived from processes in medicine and health care.The Faculty of Clinical Informatics has identified six high level domains of core competency for clinical informaticians:\nHealth and Wellbeing in Practice\nInformation Technologies and Systems\nWorking with Data and Analytical Methods\nEnabling Human and Organizational Change\nDecision Making\nLeading Informatics Teams and projects.","completion":"Software Engineering"}
{"prompt":"The principle of embracing change is about not working against changes but embracing them. For instance, if at one of the iterative meetings it appears that the customer's requirements have changed dramatically, programmers are to embrace this and plan the new requirements for the next iteration.","completion":"Software Engineering"}
{"prompt":"Kroll et al. (2013) have researched papers published between 1990 and 2012 and found 36 best practices and 17 challenges for FTS. The challenges were grouped in three categories: coordination, communication and culture. These challenges should be overcome to implement FTS successfully.","completion":"Software Engineering"}
{"prompt":"In the United States, clinical informatics is a subspecialty within several medical specialties. For example, in pathology, the American Board of Pathology offers clinical informatics certification for pathologists who have completed 24 months of related training, and the American Board of Preventive Medicine offers clinical informatics certification within preventive medicine.In October 2011 American Board of Medical Specialties (ABMS), the organization overseeing the certification of specialist MDs in the United States, announced the creation of MD-only physician certification in clinical informatics. The first examination for board certification in the subspecialty of clinical informatics was offered in October 2013 by American Board of Preventive Medicine (ABPM) with 432 passing to become the 2014 inaugural class of Diplomates in clinical informatics. Fellowship programs exist for physicians who wish to become board-certified in clinical informatics. Physicians must have graduated from a medical school in the United States or Canada, or a school located elsewhere that is approved by the ABPM. In addition, they must complete a primary residency program such as Internal Medicine (or any of the 24 subspecialties recognized by the ABMS) and be eligible to become licensed to practice medicine in the state where their fellowship program is located. The fellowship program is 24 months in length, with fellows dividing their time between Informatics rotations, didactic method, research, and clinical work in their primary specialty.","completion":"Software Engineering"}
{"prompt":"The main planning process within extreme programming is called the Planning Game. The game is a meeting that occurs once per iteration, typically once a week. The planning process is divided into two parts:\n\nRelease Planning: This is focused on determining what requirements are included in which near-term releases, and when they should be delivered. The customers and developers are both part of this. Release Planning consists of three phases:\nExploration Phase: In this phase the customer will provide a shortlist of high-value requirements for the system. These will be written down on user story cards.\nCommitment Phase: Within the commitment phase business and developers will commit themselves to the functionality that will be included and the date of the next release.\nSteering Phase: In the steering phase the plan can be adjusted, new requirements can be added and\/or existing requirements can be changed or removed.\nIteration Planning: This plans the activities and tasks of the developers. In this process the customer is not involved. Iteration Planning also consists of three phases:\nExploration Phase: Within this phase the requirement will be translated to different tasks. The tasks are recorded on task cards.\nCommitment Phase: The tasks will be assigned to the programmers and the time it takes to complete will be estimated.\nSteering Phase: The tasks are performed and the end result is matched with the original user story.The purpose of the Planning Game is to guide the product into delivery. Instead of predicting the exact dates of when deliverables will be needed and produced, which is difficult to do, it aims to \"steer the project\" into delivery using a straightforward approach. The Planning Game approach has also been adopted by non-software projects and teams in the context of business agility.","completion":"Software Engineering"}
{"prompt":"For decades, solving the software crisis was paramount to researchers and companies producing software tools.\nThe cost of owning and maintaining software in the 1980s was twice as expensive as developing the software.\nDuring the 1990s, the cost of ownership and maintenance increased by 30% over the 1980s.\nIn 1995, statistics showed that half of surveyed development projects were operational, but were not considered successful.\nThe average software project overshoots its schedule by half.\nThree-quarters of all large software products delivered to the customer are failures that are either not used at all, or do not meet the customer's requirements.","completion":"Software Engineering"}
{"prompt":"By committing regularly, every committer can reduce the number of conflicting changes. Checking in a week's worth of work runs the risk of conflicting with other features and can be very difficult to resolve. Early, small conflicts in an area of the system cause team members to communicate about the change they are making. Committing all changes at least once a day (once per feature built) is generally considered part of the definition of Continuous Integration. In addition, performing a nightly build is generally recommended. These are lower bounds; the typical frequency is expected to be much higher.","completion":"Software Engineering"}
{"prompt":"The Internet is a global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP\/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web and the infrastructure to support email.","completion":"Software Engineering"}
{"prompt":"Following this fundamental choice, a second choice made by BDD relates to how the desired behavior should be specified. In this area BDD chooses to use a semi-formal format for behavioral specification which is borrowed from user story specifications from the field of object-oriented analysis and design. The scenario aspect of this format may be regarded as an application of Hoare logic to behavioral specification of software units using the domain-specific language of the situation.\nBDD specifies that business analysts and developers should collaborate in this area and should specify behavior in terms of user stories, which are each explicitly written down in a dedicated document. Each user story should, in some way, follow the following structure:\nTitle\nAn explicit title.Narrative\nA short introductory section with the following structure:\nAs a: the person or role who will benefit from the feature;\nI want: the feature;\nso that: the benefit or value of the feature.Acceptance criteria\nA description of each specific scenario of the narrative with the following structure:\nGiven: the initial context at the beginning of the scenario, in one or more clauses;\nWhen: the event that triggers the scenario;\nThen: the expected outcome, in one or more clauses.BDD does not have any formal requirements for exactly how these user stories must be written down, but it does insist that each team using BDD come up with a simple, standardized format for writing down the user stories which includes the elements listed above. However, in 2007 Dan North suggested a template for a textual format which has found wide following in different BDD software tools. A very brief example of this format might look like this:\n\nTitle: Returns and exchanges go to inventory.\n\nAs a store owner,\nI want to add items back to inventory when they are returned or exchanged,\nso that I can sell them again.\n\nScenario 1: Items returned for refund should be added to inventory.\nGiven that a customer previously bought a black sweater from me\nand I have three black sweaters in inventory,\nwhen they return the black sweater for a refund,\nthen I should have four black sweaters in inventory.\n\nScenario 2: Exchanged items should be returned to inventory.\nGiven that a customer previously bought a blue garment from me\nand I have two blue garments in inventory\nand three black garments in inventory,\nwhen they exchange the blue garment for a black garment,\nthen I should have three blue garments in inventory\nand two black garments in inventory.\n\nThe scenarios are ideally phrased declaratively rather than imperatively \u2014 in the business language, with no reference to elements of the UI through which the interactions take place.This format is referred to as the Gherkin language, which has a syntax similar to the above example. The term Gherkin, however, is specific to the Cucumber, JBehave, Lettuce, behave and Behat  software tools.","completion":"Software Engineering"}
{"prompt":"A mechatronics engineer unites the principles of mechanics, electrical, electronics, and computing to generate a simpler, more economical and reliable system.Engineering cybernetics deals with the question of control engineering of mechatronic systems. It is used to control or regulate such a system (see control theory). Through collaboration, the mechatronic modules perform the production goals and inherit flexible and agile manufacturing properties in the production scheme. Modern production equipment consists of mechatronic modules that are integrated according to a control architecture. The most known architectures involve hierarchy, polyarchy, heterarchy, and hybrid. The methods for achieving a technical effect are described by control algorithms, which might or might not utilize formal methods in their design. Hybrid systems important to mechatronics include production systems, synergy drives,\nexploration rovers, automotive subsystems such as anti-lock braking systems and spin-assist, and everyday equipment such as autofocus cameras, video, hard disks, CD players and phones.","completion":"Software Engineering"}
{"prompt":"Dr Parnas took a public stand against the US Strategic Defense Initiative (also known as \"Star Wars\") in the mid 1980s, arguing that it would be impossible to write an application of sufficient quality that it could be trusted to prevent a nuclear attack.  He has also been in the forefront of those urging the professionalization of \"software engineering\" (a term that he characterizes as \"an unconsummated marriage\"). Dr. Parnas is also a heavy promoter of ethics in the field of software engineering.","completion":"Software Engineering"}
{"prompt":"Mills had an abiding interest in fostering sound software engineering practices through federal programs. During the formative period of the DoD DARPA STARS Program in the 1980s, he provided fundamental concepts for development of high quality software at high productivity. In 1986, he served as Chairman of the Computer Science Panel for the U.S. Air Force Scientific Advisory Board. During 1974-77, he was Chairman of the NSF Computer Science Research Panel on Software Methodology.","completion":"Software Engineering"}
{"prompt":"Peter Chen, the father of ER modeling said in his seminal paper:\n\n\"The entity-relationship model adopts the more natural view that the real world consists of entities and relationships. It incorporates some of the important semantic information about the real world.\" In his original 1976 article Chen explicitly contrasts entity\u2013relationship diagrams with record modelling techniques:\n\n\"The data structure diagram is a representation of the organization of records and is not an exact representation of entities and relationships.\"Several other authors also support Chen's program:","completion":"Software Engineering"}
{"prompt":"Born in 1951, Bruegge received a bachelor's degree in computer science at the University of Hamburg in 1978, a master's degree in computer science from Carnegie Mellon University in 1982 and a PhD degree in computer science from Carnegie Mellon University in 1985.\nBruegge has been a professor at the Technische Universit\u00e4t M\u00fcnchen in Munich since 1997. From 2000 to 2003, he was a member of the Deutsche Telekom research committee. He has been a member of the research committee of Munich district (German: M\u00fcnchner Kreis), a nonprofit association, since 2003 and member of the CIO Colloquium scientific advisory board since 2009.   Bruegge is also the liaison professor for the German National Academic Foundation (German: Studienstiftung des Deutschen Volkes).","completion":"Software Engineering"}
{"prompt":"Software engineering was spurred by the so-called software crisis of the 1960s, 1970s, and 1980s, which identified many of the problems of software development. Many projects ran over budget and schedule. Some projects caused property damage. A few projects caused loss of life. The software crisis was originally defined in terms of productivity, but evolved to emphasize quality. Some used the term software crisis to refer to their inability to hire enough qualified programmers.\nCost and Budget Overruns: The OS\/360 operating system was a classic example. This decade-long project from the 1960s eventually produced one of the most complex software systems at the time. OS\/360 was one of the first large (1000 programmers) software projects. Fred Brooks claims in The Mythical Man-Month that he made a multimillion-dollar mistake of not developing a coherent architecture before starting development.\nProperty Damage: Software defects can cause property damage. Poor software security allows hackers to steal identities, costing time, money, and reputations.\nLife and Death: Software defects can kill. Embedded systems used in radiotherapy machines prove the ability to fail so catastrophically that they administered lethal doses of radiation to patients. The most famous of these failures are the Therac-25 incidents.Peter G. Neumann has kept a contemporary list of software problems and disasters. The software crisis has been fading from view, because it is psychologically extremely difficult to remain in crisis mode for a protracted period (more than 20 years). Nevertheless, software \u2013 especially real-time embedded software \u2013 remains risky and is pervasive, and it is crucial not to give in to complacency. Over the last 10\u201315 years Michael A. Jackson has written extensively about the nature of software engineering, has identified the main source of its difficulties as lack of specialization, and has suggested that his problem frames provide the basis for a \"normal practice\" of software engineering, a prerequisite if software engineering is to become an engineering science.","completion":"Software Engineering"}
{"prompt":"Mechanical modeling calls for modeling and simulating physical complex phenomena in the scope of a multi-scale and multi-physical approach. This implies to implement and to manage modeling and optimization methods and tools, which are integrated in a systemic approach.\nThe specialty is aimed for students in mechanics who want to open their mind to systems engineering, and able to integrate different physics or technologies, as well as students in mechatronics who want to increase their knowledge in optimization and multidisciplinary simulation techniques.\nThe specialty educates students in robust and\/or optimized conception methods for structures or many technological systems, and to the main modeling and simulation tools used in R&D. Special courses are also proposed for original applications (multi-materials composites, innovating transducers and actuators, integrated systems, \u2026) to prepare the students to the coming breakthrough in the domains covering the materials and the systems.\nFor some mechatronic systems, the main issue is no longer how to implement a control system, but how to implement actuators. Within the mechatronic field, mainly two technologies are used to produce movement\/motion.","completion":"Software Engineering"}
{"prompt":"The original CD book written by Jez Humble and David Farley (2010) popularized the term; however, since its creation the definition has continued to advance and now has a more developed meaning. Companies today are implementing these continuous delivery principles and best practices. The difference in domains, e.g. medical vs. web, is still significant and affects the implementation and usage. Well-known companies that have this approach include Yahoo!, Amazon, Facebook, Google, Paddy Power and Wells Fargo.","completion":"Software Engineering"}
{"prompt":"Ian Sommerville was a lecturer in Computer Science at Heriot-Watt University in Edinburgh, Scotland from 1975 to 1978 and at Strathclyde University, Glasgow from 1978\u201386.\nFrom 1986 to 2006, he was Professor of Software Engineering in the Computing Department at the University of Lancaster, and in April 2006 he joined the School of Computer Science at St Andrews University, where he taught courses in advanced software engineering and critical systems engineering. He retired in January 2014 and since continues to do software-related things that he finds interesting.Ian Sommerville's research work, partly funded by the EPSRC has included systems requirements engineering and system evolution. He defined the process of Construction by configuration (CbC). A major focus has been system dependability, including the use of social analysis techniques such as ethnography to better understand how people and computers deliver dependability. He was a partner in the DIRC (Interdisciplinary Research Collaboration in Dependability) consortium, which focused on dependable systems design and is now (2006) working on the related INDEED (Interdisciplinary Design and Evaluation of Dependability) project. He has also been a member of the board of advisors to the IEEE SWEBOK project. He has worked on a number of European projects involving collaboration between academia and commercial enterprises, such as the ESPRIT project REAIMS (Requirements Engineering adaptation and improvement for safety and dependability).","completion":"Software Engineering"}
{"prompt":"A computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the CPU type.The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.","completion":"Software Engineering"}
{"prompt":"Automation and programmed logical controls\nPower and driving electronics\nModern and digital control systems\nRobotics systems\nMechatronics systems\nAutomated control\nComputer applications for mechatronics\nCalculus\nElectrical circuits\nSystems Dynamics\nDynamics and vibrations\nArtificial Intelligence\nEngineering Mathematics\nNumerical engineering methods\nThermostatology\nGeneral Physics\nPractical General Physics\nSignal measurements and processing\nGeneral Chemistry\nPractical General Chemistry\/Laboratory\nPrinciples of Statistics\nSensors and power transformers\nAutomated Control Laboratory\nElectrical Circuit Laboratory\nDynamics and Vibration Laboratory\nCapability and Driving Electronics Laboratory\nElectronics Lab for Mechatronics\nProcessing laboratory and microcontrollers\nDigital Logic Lab\nHydraulic and Antenna Systems Laboratory\nFluid Capability Engineering Laboratory\nProcessing and microcontrollers\nDigital Logic\nEngineering materials and manufacturing technology\nMachinery mechanics\nFluid capability engineering\nMathematics","completion":"Software Engineering"}
{"prompt":"Application software, also known as an application or an app, is computer software designed to help the user perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user. \nApplication software applies the power of a particular computing platform or system software to a particular purpose. Some apps, such as Microsoft Office, are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a geography application for Windows or an Android application for education or Linux gaming. Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as killer applications.","completion":"Software Engineering"}
{"prompt":"It has also become prevalent to name roles with phrases such as is the owner of and is owned by. Correct nouns in this case are owner and possession. Thus person plays the role of owner and car plays the role of possession rather than person plays the role of, is the owner of, etc.\nThe use of nouns has direct benefit when generating physical implementations from semantic models. When a person has two relationships with car then it is possible to generate names such as owner_person and driver_person, which are immediately meaningful.","completion":"Software Engineering"}
{"prompt":"Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\nReliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\nRobustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services, and network connections, user error, and unexpected power outages.\nUsability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical, and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.\nPortability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled\/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform-specific compilers (and sometimes libraries) for the language of the source code.\nMaintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or to customize, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.\nEfficiency\/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks. This is often discussed under the shadow of a chosen programming language. Although the language certainly affects performance, even slower languages, such as Python, can execute programs instantly from a human perspective. Speed, resource usage, and performance are important for programs that bottleneck the system, but efficient use of programmer time is also important and is related to cost: more hardware may be cheaper.","completion":"Software Engineering"}
{"prompt":"The forerunner of BCS was the \"London Computer Group\" (LCG), founded in 1956. BCS was formed a year later from the merger of the LCG and an unincorporated association of scientists into an unincorporated club. In October 1957, BCS was incorporated, by Articles of Association, as \"The British Computer Society Ltd\": the first President of BCS was Sir Maurice Wilkes (1913\u20132010), FRS.\nIn 1966, the BCS was granted charitable status and in 1970, the BCS was given Armorial Bearings including the shield and crest.\nThe major ethical responsibilities of BCS are emphasised by the leopard's face, surmounting the whole crest and depicting eternal vigilance over the integrity of the Society and its members.\nThe BCS patron is The Duke of Kent, KG. He became patron in December 1976 and has been actively involved in BCS activities, particularly having been President in the Silver Jubilee Year in 1982\u20131983.\nOn 21 September 2009, the British Computer Society went through a transformation and re-branded itself as \"BCS \u2013 The Chartered Institute for IT\". In 2010, an Extraordinary General Meeting was called to discuss the direction of the BCS. The debate has been covered by the computing press.","completion":"Software Engineering"}
{"prompt":"The basic principles of the cleanroom process are\n\nSoftware development based on formal methods\nSoftware tool support based on some mathematical formalism includes model checking, process algebras, and Petri nets. The Box Structure Method might be one such means of specifying and designing a software product. Verification that the design correctly implements the specification is performed through team review, often with software tool support.Incremental implementation under statistical quality control\nCleanroom development uses an iterative approach, in which the product is developed in increments that gradually increase the implemented functionality. The quality of each increment is measured against pre-established standards to verify that the development process is proceeding acceptably. A failure to meet quality standards results in the cessation of testing for the current increment, and a return to the design phase.Statistically sound testing\nSoftware testing in the cleanroom process is carried out as a statistical experiment. Based on the formal specification, a representative subset of software input\/output trajectories is selected and tested. This sample is then statistically analyzed to produce an estimate of the reliability of the software, and a level of confidence in that estimate.","completion":"Software Engineering"}
{"prompt":"Performance-analysis tools existed on IBM\/360 and IBM\/370 platforms from the early 1970s, usually based on timer interrupts which recorded the program status word (PSW) at set timer-intervals to detect \"hot spots\" in executing code. This was an early example of sampling (see below). In early 1974 instruction-set simulators permitted full trace and other performance-monitoring features.Profiler-driven program analysis on Unix dates back to 1973, when Unix systems included a basic tool, prof, which listed each function and how much of program execution time it used. In 1982 gprof extended the concept to a complete call graph analysis.In 1994, Amitabh Srivastava and Alan Eustace of Digital Equipment Corporation published a paper describing ATOM (Analysis Tools with OM). The ATOM platform converts a program into its own profiler: at compile time, it inserts code into the program to be analyzed. That inserted code outputs analysis data. This technique - modifying a program to analyze itself - is known as \"instrumentation\".\nIn 2004 both the gprof and ATOM papers appeared on the list of the 50 most influential PLDI papers for the 20-year period ending in 1999.","completion":"Software Engineering"}
{"prompt":"Axiomatic design is a top down hierarchical functional decomposition process used as a solution synthesis framework for the analysis, development, re-engineering, and integration of products, information systems, business processes or software engineering solutions. Its structure is suited mathematically to analyze coupling between functions in order to optimize the architectural robustness of potential functional solution models.","completion":"Software Engineering"}
{"prompt":"In Thailand, the Bachelor of Science in Information Technology (BS IT) is a four-year undergraduate degree program which is a subject of accreditation by the Office of the Higher Education Commission (OHEC) and the Office for National Education Standards and Quality Assessment (ONESQA) of the Ministry of Higher Education, Science, Research and Innovation (MHESI).\nThe first international BS IT program, using English as a medium of instruction (EMI), has been established in 1990 at the Faculty of Science and Technology (renamed in 2013 to Vincent Mary School of Science and Technology (VMS)), Assumption University of Thailand (AU). The 2019 BS IT curriculum has been updated by VMS to respond to the discovery of the students' potential and also blended with marketing communications needs.","completion":"Software Engineering"}
{"prompt":"Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates.","completion":"Software Engineering"}
{"prompt":"The development of IDEF4 came from the recognition that the modularity, maintainability and code reusability that results from the object-oriented programming paradigm can be realized in traditional data processing applications. The proven ability of the object-oriented programming paradigm to support data level integration in large complex distributed systems is also a major factor in the widespread interest in this technology from the traditional data processing community.IDEF4 was developed as a design tool for software designers who use object-oriented languages such as the Common Lisp Object System, Flavors, Smalltalk, Objective-C, C++, and others. Since effective usage of the object-oriented paradigm requires a different thought process than used with conventional procedural or database languages, standard methodologies such as structure charts, data flow diagrams, and traditional data design models (hierarchical, relational, and network) are not sufficient. IDEF4 seeks to provide the necessary facilities to support the object-oriented design decision making process.","completion":"Software Engineering"}
{"prompt":"Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.","completion":"Software Engineering"}
{"prompt":"Generally, software and information technology companies look for people who have strong programming skills, system analysis, and software testing skills.Many colleges teach practical skills that are crucial to becoming a software developer. As logical reasoning and critical thinking are important in becoming a software professional, this degree encompasses the complete process of software development from software design and development to final testing.Students who complete their undergraduate education in software engineering at a satisfactory level often pursue graduate studies such as a Master of Science in Information Technology (M.Sc IT) and sometimes continuing onto a doctoral program and earning a doctorate such as a Doctor of Information Technology (DIT).","completion":"Software Engineering"}
{"prompt":"AddressSanitizer: Memory error detection for Linux, macOS, Windows, and more. Part of LLVM.\nBoundsChecker: Memory error detection for Windows based applications.  Part of Micro Focus DevPartner.\nDmalloc: Library for checking memory allocation and leaks. Software must be recompiled, and all files must include the special C header file dmalloc.h.\nIntel Inspector: Dynamic memory error debugger for C, C++, and Fortran applications that run on Windows and Linux.\nPurify: Mainly memory corruption detection and memory leak detection.\nValgrind: Runs programs on a virtual processor and can detect memory errors (e.g., misuse of malloc and free) and race conditions in multithread programs.","completion":"Software Engineering"}
{"prompt":"Because XP doctrine advocates programming only what is needed today, and implementing it as simply as possible, at times this may result in a system that is stuck. One of the symptoms of this is the need for dual (or multiple) maintenance: functional changes start requiring changes to multiple copies of the same (or similar) code. Another symptom is that changes in one part of the code affect many other parts. XP doctrine says that when this occurs, the system is telling you to refactor your code by changing the architecture, making it simpler and more generic.","completion":"Software Engineering"}
{"prompt":"Making builds readily available to stakeholders and testers can reduce the amount of rework necessary when rebuilding a feature that doesn't meet requirements. Additionally, early testing reduces the chances that defects survive until deployment. Finding errors earlier can reduce the amount of work necessary to resolve them.\nAll programmers should start the day by updating the project from the repository. That way, they will all stay up to date.","completion":"Software Engineering"}
{"prompt":"Structured Analysis and Design Technique (SADT) is a software engineering methodology for describing systems as a hierarchy of functions, a diagrammatic notation for constructing a sketch for a software application. It offers building blocks to represent entities and activities, and a variety of arrows to relate boxes. These boxes and arrows have an associated informal semantics. SADT can be used as a functional analysis tool of a given process, using successive levels of details. The SADT method allows to define user needs for IT developments, which is used in industrial Information Systems, but also to explain and to present an activity's manufacturing processes, procedures.The SADT supplies a specific functional view of any enterprise by describing the functions and their relationships in a company. These functions fulfill the objectives of a company, such as sales, order planning, product design, part manufacturing, and human resource management. The SADT can depict simple functional relationships and can reflect data and control flow relationships between different functions. The IDEF0 formalism is based on SADT, developed by Douglas T. Ross in 1985.","completion":"Software Engineering"}
{"prompt":"Designers must be aware of elements that influence user emotional responses. For instance, products must convey positive emotions while avoiding negative ones. Other important aspects include motivational, learning, creative, social and persuasive influences. One method that can help convey such aspects is for example, the use of dynamic icons, animations and sound to help communicate, creating a sense of interactivity. Interface aspects such as fonts, color palettes and graphical layouts can influence acceptance. Studies showed that affective aspects can affect perceptions of usability.Emotion and pleasure theories exist to explain interface responses. These include Don Norman's emotional design model, Patrick Jordan's pleasure model and McCarthy and Wright's Technology as Experience framework.","completion":"Software Engineering"}
{"prompt":"The first version of rules for XP was published in 1999 by Don Wells at the XP website. 29 rules are given in the categories of planning, managing, designing, coding, and testing. Planning, managing and designing are called out explicitly to counter claims that XP doesn't support those activities.\nAnother version of XP rules was proposed by Ken Auer in XP\/Agile Universe 2003. He felt XP was defined by its rules, not its practices (which are subject to more variation and ambiguity). He defined two categories: \"Rules of Engagement\" which dictate the environment in which software development can take place effectively, and \"Rules of Play\" which define the minute-by-minute activities and rules within the framework of the Rules of Engagement.\nHere are some of the rules (incomplete):\nCoding\n\nThe customer is always available\nCode the unit test first\nOnly one pair integrates code at a time\nLeave optimization until last\nNo overtimeTesting\n\nAll code must have unit tests\nAll code must pass all unit tests before it can be released.\nWhen a bug is found, tests are created before the bug is addressed (a bug is not an error in logic; it is a test that was not written)\nAcceptance tests are run often and the results are published","completion":"Software Engineering"}
{"prompt":"Software product lines, aka product family engineering, is a systematic way to produce families of software systems, instead of creating a succession of completely individual products. This method emphasizes extensive, systematic, formal code reuse, to try to industrialize the software development process.\nThe Future of Software Engineering conference (FOSE), held at ICSE 2000, documented the state of the art of SE in 2000 and listed many problems to be solved over the next decade. The FOSE tracks at the  ICSE 2000  and the ICSE 2007 conferences also help identify the state of the art in software engineering.","completion":"Software Engineering"}
{"prompt":"The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.\nBroader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.","completion":"Software Engineering"}
{"prompt":"The PSP aims to provide software engineers with disciplined methods for improving personal software development processes. The PSP helps software engineers to:\n\nImprove their estimating and planning skills.\nMake commitments they can keep.\nManage the quality of their projects.\nReduce the number of defects in their work.","completion":"Software Engineering"}
{"prompt":"Building software systems requires communicating system requirements to the developers of the system. In formal software development methodologies, this task is accomplished through documentation. Extreme programming techniques can be viewed as methods for rapidly building and disseminating institutional knowledge among members of a development team. The goal is to give all developers a shared view of the system which matches the view held by the users of the system. To this end, extreme programming favors simple designs, common metaphors, collaboration of users and programmers, frequent verbal communication, and feedback.","completion":"Software Engineering"}
{"prompt":"IDEF8, or integrated definition for human-system interaction design, is a method for producing high-quality designs of interactions between users and the systems they operate. Systems are characterized as a collection of objects that perform functions to accomplish a particular goal. The system with which the user interacts can be any system, not necessarily a computer program. Human-system interactions are designed at three levels of specification within the IDEF8 method. The first level defines the philosophy of system operation and produces a set of models and textual descriptions of overall system processes. The second level of design specifies role-centered scenarios of system use. The third level of IDEF8 design is for human-system design detailing. At this level of design, IDEF8 provides a library of metaphors to help users and designers specify the desired behavior in terms of other objects whose behavior is more familiar. Metaphors provide a model of abstract concepts in terms of familiar, concrete objects and experiences.","completion":"Software Engineering"}
{"prompt":"A functional block diagram is a block diagram, that describes the functions and interrelationships of a system. The functional block diagram can picture:\nFunctions of a system pictured by blocks\nInput of a block pictured with lines, and\nRelationships between 9 functions\nFunctional sequences and paths for matter and or signalsThe block diagram can use additional schematic symbols to show particular properties.\nSpecific function block diagram are the classic functional flow block diagram, and the Function Block Diagram (FBD) used in the design of programmable logic controllers.","completion":"Software Engineering"}
{"prompt":"Having a test environment can lead to failures in tested systems when they deploy in the production environment because the production environment may differ from the test environment in a significant way. However, building a replica of a production environment is cost-prohibitive. Instead, the test environment or a separate pre-production environment (\"staging\") should be built to be a scalable version of the production environment to alleviate costs while maintaining technology stack composition and nuances. Within these test environments, service virtualisation is commonly used to obtain on-demand access to dependencies (e.g., APIs, third-party applications, services, mainframes, etc.) that are beyond the team's control, still evolving, or too complex to configure in a virtual test lab.","completion":"Software Engineering"}
{"prompt":"The broad history of health informatics has been captured in the book UK Health Computing: Recollections and reflections, Hayes G, Barnett D (Eds.), BCS (May 2008)  by those active in the field, predominantly members of BCS Health and its constituent groups. The book describes the path taken as 'early development of health informatics was unorganized and idiosyncratic'. In the early 1950s, it was prompted by those involved in NHS finance and only in the early 1960s did solutions including those in pathology (1960), radiotherapy (1962), immunization (1963), and primary care (1968)  emerge.  Many of these solutions, even in the early 1970s were developed in-house by pioneers in the field to meet their own requirements. In part, this was due to some areas of health services (for example the immunization and vaccination of children) still being provided by Local Authorities. The coalition government has proposed broadly to return to the 2010 strategy Equity and Excellence: Liberating the NHS (July 2010); stating: \"We will put patients at the heart of the NHS, through an information revolution and greater choice and control' with shared decision-making becoming the norm: 'no decision about me without me' and patients having access to the information they want, to make choices about their care. They will have increased control over their own care records.\"There are different models of health informatics delivery in each of the home countries (England, Scotland, Northern Ireland and Wales) but some bodies like UKCHIP (see below) operate for those 'in and for' all the home countries and beyond.\nNHS informatics in England was contracted out to several vendors for national health informatics solutions under the National Programme for Information Technology (NPfIT) label in the early to mid-2000s, under the auspices of NHS Connecting for Health (part of the Health and Social Care Information Centre as of 1 April 2013). NPfIT originally divided the country into five regions, with strategic 'systems integration' contracts awarded to one of several Local Service Providers (LSP).  The various specific technical solutions were required to connect securely with the NHS 'Spine', a system designed to broker data between different systems and care settings. NPfIT fell significantly behind schedule and its scope and design were being revised in real time, exacerbated by media and political lambasting of the Programme's spend (past and projected) against the proposed budget. In 2010 a consultation was launched as part of the new Conservative\/Liberal Democrat Coalition Government's White Paper 'Liberating the NHS'. This initiative provided little in the way of innovative thinking, primarily re-stating existing strategies within the proposed new context of the Coalition's vision for the NHS.\nThe degree of computerization in NHS secondary care was quite high before NPfIT, and the programme stagnated further development of the install base \u2013 the original NPfIT regional approach provided neither a single, nationwide solution nor local health community agility or autonomy to purchase systems, but instead tried to deal with a hinterland in the middle.\nAlmost all general practices in England and Wales are computerized under the GP Systems of Choice programme, and patients have relatively extensive computerized primary care clinical records. System choice is the responsibility of individual general practices and while there is no single, standardized GP system, it sets relatively rigid minimum standards of performance and functionality for vendors to adhere to. Interoperation between primary and secondary care systems is rather primitive. It is hoped that a focus on interworking (for interfacing and integration) standards will stimulate synergy between primary and secondary care in sharing necessary information to support the care of individuals.  Notable successes to date are in the electronic requesting and viewing of test results, and in some areas, GPs have access to digital x-ray images from secondary care systems.\nIn 2019 the GP Systems of Choice framework was replaced by the GP IT Futures framework, which is to be the main vehicle used by clinical commissioning groups to buy services for GPs.  This is intended to increase competition in an area that is dominated by  EMIS and TPP.  69 technology companies offering more than 300 solutions have been accepted on to the new framework.Wales has a dedicated Health Informatics function that supports NHS Wales in leading on the new integrated digital information services and promoting Health Informatics as a career.\nThe British Computer Society (BCS)  provides 4 different professional registration levels for Health and Care Informatics Professionals: Practitioner, Senior Practitioner, Advanced Practitioner, and Leading Practitioner. The Faculty of Clinical Informatics (FCI)  is the professional membership society for health and social care professionals in clinical informatics offering Fellowship, Membership and Associateship. BCS and FCI are member organisations of the Federation for Informatics Professionals in Health and Social Care (FedIP), a collaboration between the leading professional bodies in health and care informatics supporting the development of the informatics professions.\nThe Faculty of Clinical Informatics has produced a Core Competency Framework that describes the wide range of skills needed by practitioners.","completion":"Software Engineering"}
{"prompt":"Timeline of DOS operating systems\nClassic Mac OS\nHistory of macOS\nHistory of Microsoft Windows\nTimeline of the Apple II series\nTimeline of Apple products\nTimeline of file sharing\nTimeline of OpenBSD","completion":"Software Engineering"}
{"prompt":"Agile\nAgile software development\nExtreme programming\nLean software development\nRapid application development (RAD)\nRational Unified Process\nScrum\nHeavyweight\nCleanroom\nISO\/IEC 12207 \u2014 software life cycle processes\nISO 9000 and ISO 9001\nProcess Models\nCMM and CMMI\/SCAMPI\nISO 15504 (SPICE)\nMetamodels\nISO\/IEC 24744\nSPEM","completion":"Software Engineering"}
{"prompt":"The term information model in general is used for models of individual things, such as facilities, buildings, process plants, etc. In those cases, the concept is specialised to facility information model, building information model, plant information model, etc. Such an information model is an integration of a model of the facility with the data and documents about the facility.\nWithin the field of software engineering and data modeling, an information model is usually an abstract, formal representation of entity types that may include their properties, relationships and the operations that can be performed on them. The entity types in the model may be kinds of real-world objects, such as devices in a network, or occurrences, or they may themselves be abstract, such as for the entities used in a billing system. Typically, they are used to model a constrained domain that can be described by a closed set of entity types, properties, relationships and operations.\nAn information model provides formalism to the description of a problem domain without constraining how that description is mapped to an actual implementation in software. There may be many mappings of the information model. Such mappings are called data models, irrespective of whether they are object models (e.g. using UML), entity relationship models or XML schemas.","completion":"Software Engineering"}
{"prompt":"In Chen's original paper he gives an example of a relationship and its roles. He describes a relationship \"marriage\" and its two roles \"husband\" and \"wife\".\nA person plays the role of husband in a marriage (relationship) and another person plays the role of wife in the (same) marriage. These words are nouns. That is no surprise; naming things requires a noun.\nChen's terminology has also been applied to earlier ideas. The lines, arrows and crow's-feet of some diagrams owes more to the earlier Bachman diagrams than to Chen's relationship diagrams.\nAnother common extension to Chen's model is to \"name\" relationships and roles as verbs or phrases.","completion":"Software Engineering"}
{"prompt":"High-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed\u2014in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\nThese compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.","completion":"Software Engineering"}
{"prompt":"Continuous delivery is enabled through the deployment pipeline. The purpose of the deployment pipeline has three components: visibility, feedback, and continually deploy.\nVisibility \u2013 All aspects of the delivery system including building, deploying, testing, and releasing are visible to every member of the team to promote collaboration.\nFeedback \u2013 Team members learn of problems as soon as possible when they occur so that they are able to fix them as quickly as possible.\nContinually deploy \u2013 Through a fully automated process, you can deploy and release any version of the software to any environment.","completion":"Software Engineering"}
{"prompt":"The system metaphor is a story that everyone - customers, programmers, and managers - can tell about how the system works. It's a naming concept for classes and methods that should make it easy for a team member to guess the functionality of a particular class\/method, from its name only. For example a library system may create loan_records(class) for borrowers(class), and if the item were to become overdue it may perform a make_overdue operation on a catalogue(class).  For each class or operation the functionality is obvious to the entire team.","completion":"Software Engineering"}
{"prompt":"The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance \/ time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.\nWhile global outsourcing has several advantages, global \u2013 and generally distributed \u2013 development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.","completion":"Software Engineering"}
{"prompt":"High-quality software is the goal of the PSP, and quality is measured in terms of defects. For the PSP, a quality process should produce low-defect software that meets the user needs.\nThe PSP phase structure enables PSP developers to catch defects early. By catching defects early, the PSP can reduce the amount of time spent in later phases, such as Test.\nThe PSP theory is that it is more economical and effective to remove defects as close as possible to where and when they were injected, so software engineers are encouraged to conduct personal reviews for each phase of development. Therefore, the PSP phase structure includes two review phases:\n\nDesign Review\nCode ReviewTo do an effective review, you need to follow a structured review process. The PSP recommends using checklists to help developers to consistently follow an orderly procedure.\nThe PSP follows the premise that when people make mistakes, their errors are usually predictable, so PSP developers can personalize their checklists to target their own common errors. Software engineers are also expected to complete process improvement proposals, to identify areas of weakness in their current performance that they should target for improvement. Historical project data, which exposes where time is spent and defects introduced, help developers to identify areas to improve.\nPSP developers are also expected to conduct personal reviews before their work undergoes a peer or team review.","completion":"Software Engineering"}
{"prompt":"Within XP, the \"customer\" is not the one who pays the bill, but the one who really uses the system. XP says that the customer should be on hand at all times and available for questions. For instance, the team developing a financial administration system should include a financial administrator.","completion":"Software Engineering"}
{"prompt":"In 1986, Hamilton received the Augusta Ada Lovelace Award by the Association for Women in Computing.\nIn 2003, she was given the NASA Exceptional Space Act Award for scientific and technical contributions. The award included $37,200, the largest amount awarded to any individual in NASA's history.\nIn 2009, she received the Outstanding Alumni Award by Earlham College.\nIn 2016, she received the Presidential Medal of Freedom from Barack Obama, the highest civilian honor in the United States.\nOn April 28, 2017, she received the Computer History Museum Fellow Award, which honors exceptional men and women whose computing ideas have changed the world.\nIn 2017, a \"Women of NASA\" LEGO set went on sale featuring minifigures of Hamilton, Mae Jemison, Sally Ride, and Nancy Grace Roman.\nIn 2018, she was awarded an honorary doctorate degree by the Polytechnic University of Catalonia.\nIn 2019, she was awarded The Washington Award.\nIn 2019, she was awarded an honorary doctorate degree by Bard College.\nIn 2019, she was awarded the Intrepid Lifetime Achievement Award.\nIn 2022, she was inducted into the National Aviation Hall of Fame in Dayton, Ohio.","completion":"Software Engineering"}
{"prompt":"An extensional model is one that maps to the elements of a particular methodology or technology, and is thus a \"platform specific model\". The UML specification explicitly states that associations in class models are extensional and this is in fact self-evident by considering the extensive array of additional \"adornments\" provided by the specification over and above those provided by any of the prior candidate \"semantic modelling languages\".\"UML as a Data Modeling Notation, Part 2\"","completion":"Software Engineering"}
{"prompt":"The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM.The cleanroom process first saw use in the mid to late 1980s. Demonstration projects within the military began in the early 1990s. Recent work on the cleanroom process has examined fusing cleanroom with the automated verification capabilities provided by specifications expressed in CSP.","completion":"Software Engineering"}
{"prompt":"Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.","completion":"Software Engineering"}
{"prompt":"In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\n\nDifferent indent styles (whitespace)\nComments\nDecomposition\nNaming conventions for objects (such as variables, classes, functions, procedures, etc.)The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.","completion":"Software Engineering"}
{"prompt":"Software tools come in many forms:\n\nBinary compatibility analysis tools\nBug databases: Comparison of issue tracking systems \u2013 Including bug tracking systems\nBuild tools: Build automation, List of build automation software\nCall graph\nCode coverage: Code coverage#Software code coverage tools.\nCode review: List of tools for code review\nCode sharing sites: Freshmeat, Krugle, SourceForge, GitHub. See also Code search engines.\nCompilation and linking tools: GNU toolchain, gcc, Microsoft Visual Studio, CodeWarrior, Xcode, ICC\nDebuggers: Debugger#List of debuggers. See also Debugging.\nDisassemblers: Generally reverse-engineering tools.\nDocumentation generators: Comparison of documentation generators, help2man, Plain Old Documentation, asciidoc\nFormal methods: Mathematical techniques for specification, development and verification\nGUI interface generators\nLibrary interface generators: SWIG\nIntegration Tools\nMemory debuggers are frequently used in programming languages (such as C and C++) that allow manual memory management and thus the possibility of memory leaks and other problems.  They are also useful to optimize efficiency of memory usage.  Examples: dmalloc, Electric Fence, Insure++, Valgrind\nParser generators: Parsing#Parser development software\nPerformance analysis or profiling: List of performance analysis tools\nRevision control: List of revision control software, Comparison of revision control software\nScripting languages: PHP, Awk, Perl, Python, REXX, Ruby, Shell, Tcl\nSearch: grep, find\nSource code Clones\/Duplications Finding: Duplicate code#Tools\nSource code editor\nText editors: List of text editors, Comparison of text editors\nSource code formatting: indent, pretty-printers, beautifiers, minifiers\nSource code generation tools: Automatic programming#Implementations\nStatic code analysis: lint, List of tools for static code analysis\nUnit testing: List of unit testing frameworks","completion":"Software Engineering"}
{"prompt":"Information technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.","completion":"Software Engineering"}
{"prompt":"Computer programmers are those who write computer software. Their jobs usually involve:\n\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.","completion":"Software Engineering"}
{"prompt":"Graduate prospects are projected to be excellent with the amount of software engineers in the industry estimated to rise by roughly 38% from 2006 to 2016, with total real wage in the industry increasing by an estimated 38.2%. The U.S. Bureau of Labor Statistics Occupational Outlook Job Outlook for Software engineers and Developers 2019-29 predicts only a 22% growth. After the crash of the dot-com bubble (1999\u20132001) and the Great Recession (2008), many U.S. software professions were left without work or with lower wages. In addition, enrollment in computer-related degrees and other STEM degrees (STEM attrition) in the US has been dropping for years, especially for women, which, according to Beaubouef and Mason  could be attributed to a lack of general interest in science and mathematics and also out of an apparent fear that software will be subject to the same pressures as manufacturing and agriculture careers. The U.S. Bureau of Labor Statistics Occupational Outlook 2014-24 predicts a decline for Computer Programmers of -8 percent, then for 2016-26 predicts a decline of -7 percent, then a decline of -9 percent from 2019 to 2029, and finally predicts a decline of -10 percent from 2021 to 2031.","completion":"Software Engineering"}
{"prompt":"Development life cycle phase\nRequirements gathering \/ analysis\nSoftware architecture\nComputer programming\nTesting, detects bugs\nBlack box testing\nWhite box testing\nQuality assurance, ensures compliance with process.\nProduct Life cycle phase and Project lifecycle\nInception\nFirst development\nMajor release\nMinor release\nBug fix release\nMaintenance\nObsolescence\nRelease development stage, near the end of a release cycle\nAlpha\nBeta\nGold master\n1.0; 2.0\nSoftware development lifecycle\nWaterfall model \u2014 Structured programming and Stepwise refinement\nSSADM\nSpiral model \u2014 Iterative development\nV-model\nAgile software development\nDSDM\nChaos model \u2014 Chaos strategy","completion":"Software Engineering"}
{"prompt":"The programming languages listed here have event-based profilers:\n\nJava: the JVMTI (JVM Tools Interface) API, formerly JVMPI (JVM Profiling Interface), provides hooks to profilers, for trapping events like calls, class-load, unload, thread enter leave.\n.NET: Can attach a profiling agent as a COM server to the CLR using Profiling API. Like Java, the runtime then provides various callbacks into the agent, for trapping events like method JIT \/ enter \/ leave, object creation, etc. Particularly powerful in that the profiling agent can rewrite the target application's bytecode in arbitrary ways.\nPython: Python profiling includes the profile module, hotshot (which is call-graph based), and using the 'sys.setprofile' function to trap events like c_{call,return,exception}, python_{call,return,exception}.\nRuby: Ruby also uses a similar interface to Python for profiling. Flat-profiler in profile.rb, module, and ruby-prof a C-extension are present.","completion":"Software Engineering"}
{"prompt":"The focus of the cleanroom process is on defect prevention, rather than defect removal. The name \"cleanroom\" was chosen to evoke the cleanrooms used in the electronics industry to prevent the introduction of defects during the fabrication of semiconductors.","completion":"Software Engineering"}
{"prompt":"Bachman notation\nBarker's notation\nEXPRESS\nIDEF1X\n\u00a7 Crow's foot notation (also Martin notation)\n(min, max)-notation of Jean-Raymond Abrial in 1974\nUML class diagrams\nMerise\nObject-role modeling","completion":"Software Engineering"}
{"prompt":"Modifications to the original specification can be beneficial. Chen described look-across cardinalities. As an aside, the Barker\u2013Ellis notation, used in Oracle Designer, uses same-side for minimum cardinality (analogous to optionality) and role, but look-across for maximum cardinality (the crows foot).Research by Merise, Elmasri & Navathe and others has shown there is a preference for same-side for roles and both minimum and maximum cardinalities, and researchers (Feinerer, Dullea et al.) have shown that this is more coherent when applied to n-ary relationships of order greater than 2.In Dullea et al. one reads \"A 'look across' notation such as used in the UML does not effectively represent the semantics of participation constraints imposed on relationships where the degree is higher than binary.\"\nIn Feinerer it says \"Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail.\" (Although the \"reduction\" mentioned is spurious as the two diagrams 3.4 and 3.5 are in fact the same) and also \"As we will see on the next few pages, the look-across interpretation introduces several difficulties that prevent the extension of simple mechanisms from binary to n-ary associations.\"\n\n Chen's notation for entity\u2013relationship modeling uses rectangles to represent entity sets, and diamonds to represent relationships appropriate for first-class objects: they can have attributes and relationships of their own. If an entity set participates in a relationship set, they are connected with a line.\nAttributes are drawn as ovals and are connected with a line to exactly one entity or relationship set.\nCardinality constraints are expressed as follows:\n\na double line indicates a participation constraint,  totality or surjectivity: all entities in the entity set must participate in at least one relationship in the relationship set;\nan arrow from entity set to relationship set indicates a key constraint, i.e. injectivity: each entity of the entity set can participate in at most one relationship in the relationship set;\na thick line indicates both, i.e. bijectivity: each entity in the entity set is involved in exactly one relationship.\nan underlined name of an attribute indicates that it is a key: two different entities or relationships with this attribute always have different values for this attribute.Attributes are often omitted as they can clutter up a diagram; other diagram techniques often list entity attributes within the rectangles drawn for entity sets.","completion":"Software Engineering"}
{"prompt":"Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused  and should be considered harmful, particularly in the United States.","completion":"Software Engineering"}
{"prompt":"Aspects help software engineers deal with quality attributes by providing tools to add or remove boilerplate code from many areas in the source code. Aspects describe how all objects or functions should behave in particular circumstances. For example, aspects can add debugging, logging, or locking control into all objects of particular types. Researchers are currently working to understand how to use aspects to design general-purpose code. Related concepts include generative programming and templates.","completion":"Software Engineering"}
{"prompt":"Individuals working in this area design technology for enhancing the speed, reliability, and performance of systems. Embedded systems are found in many devices from a small FM radio to the space shuttle. According to the Sloan Cornerstone Career Center, ongoing developments in embedded systems include \"automated vehicles and equipment to conduct search and rescue, automated transportation systems, and human-robot coordination to repair equipment in space.\" As of 2018, computer embedded systems specializations include system-on-chip design, architecture of edge computing and the Internet of things.","completion":"Software Engineering"}
{"prompt":"Despite the word \"science\" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.","completion":"Software Engineering"}
{"prompt":"There are several notations for data modeling. The actual model is frequently called \"entity\u2013relationship model\", because it depicts data in terms of the entities and relationships described in the data. An entity\u2013relationship model (ERM) is an abstract conceptual representation of structured data. Entity\u2013relationship modeling is a relational schema database modeling method, used in software engineering to produce a type of conceptual data model (or semantic data model) of a system, often a relational database, and its requirements in a top-down fashion.\nThese models are being used in the first stage of information system design during the requirements analysis to describe information needs or the type of information that is to be stored in a database. The data modeling technique can be used to describe any ontology (i.e. an overview and classifications of used terms and their relationships) for a certain universe of discourse i.e. area of interest.\nSeveral techniques have been developed for the design of data models. While these methodologies guide data modelers in their work, two different people using the same methodology will often come up with very different results. Most notable are:\n\nBachman diagrams\nBarker's notation\nChen's notation\nData Vault Modeling\nExtended Backus\u2013Naur form\nIDEF1X\nObject-relational mapping\nObject-Role Modeling and Fully Communication Oriented Information Modeling\nRelational Model\nRelational Model\/Tasmania","completion":"Software Engineering"}
{"prompt":"Ian Sommerville was born in Glasgow, Scotland in 1951.\nHe studied Physics at Strathclyde University and Computer Science at the University of St Andrews. He is married and has two daughters. As an amateur gourmet, he has written a number of restaurant reviews.","completion":"Software Engineering"}
{"prompt":"There are several fields of study that either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.","completion":"Software Engineering"}
{"prompt":"An important application of information engineering in medicine is medical signal processing. It refers to the generation, analysis, and use of signals, which could take many forms such as image, sound, electrical, or biological.","completion":"Software Engineering"}
{"prompt":"Clinical research informatics (CRI) is a sub-field of health informatics that tries to improve the efficiency of clinical research by using informatics methods. Some of the problems tackled by CRI are: creation of data warehouses of health care data that can be used for research, support of data collection in clinical trials by the use of electronic data capture systems, streamlining ethical approvals and renewals (in US the responsible entity is the local institutional review board), maintenance of repositories of past clinical trial data (de-identified). CRI is a fairly new branch of informatics and has met growing pains as any up and coming field does.  Some issue CRI faces is the ability for the statisticians and the computer system architects to work with the clinical research staff in designing a system and lack of funding to support the development of a new system.  Researchers and the informatics team have a difficult time coordinating plans and ideas in order to design a system that is easy to use for the research team yet fits in the system requirements of the computer team. The lack of funding can be a hindrance to the development of the CRI.  Many organizations who are performing research are struggling to get financial support to conduct the research, much less invest that money in an informatics system that will not provide them any more income or improve the outcome of the research (Embi, 2009). Ability to integrate data from multiple clinical trials is an important part of clinical research informatics. Initiatives, such as PhenX and Patient-Reported Outcomes Measurement Information System triggered a general effort to improve secondary use of data collected in past human clinical trials. CDE initiatives, for example, try to allow clinical trial designers to adopt standardized research instruments (electronic case report forms). A parallel effort to standardizing how data is collected are initiatives that offer de-identified patient level clinical study data to be downloaded by researchers who wish to re-use this data. Examples of such platforms are Project Data Sphere, dbGaP, ImmPort or Clinical Study Data Request. Informatics issues in data formats for sharing results (plain CSV files, FDA endorsed formats, such as CDISC Study Data Tabulation Model) are important challenges within the field of clinical research informatics. There are a number of activities within clinical research that CRI supports, including:\n\nmore efficient and effective data collection and acquisition\nimproved recruitment into clinical trials\noptimal protocol design and efficient management\npatient recruitment and management\nadverse event reporting\nregulatory compliance\ndata storage, transfer, processing and analysis\nrepositories of data from completed clinical trials (for secondary analyses)One of the fundamental elements of biomedical and translation research is the use of integrated data repositories. A survey conducted in 2010 defined \"integrated data repository\" (IDR) as a data warehouse incorporating various sources of clinical data to support queries for a range of research-like functions. Integrated data repositories are complex systems developed to solve a variety of problems ranging from identity management, protection of confidentiality, semantic and syntactic comparability of data from different sources, and most importantly convenient and flexible query. Development of the field of clinical informatics led to the creation of large data sets with electronic health record data integrated with other data (such as genomic data). Types of data repositories include operational data stores (ODSs), clinical data warehouses (CDWs), clinical data marts, and clinical registries. Operational data stores established for extracting, transferring and loading before creating warehouse or data marts. Clinical registries repositories have long been in existence, but their contents are disease specific and sometimes considered archaic. Clinical data stores and clinical data warehouses are considered fast and reliable. Though these large integrated repositories have impacted clinical research significantly, it still faces challenges and barriers. One big problem is the requirement for ethical approval by the institutional review board (IRB) for each research analysis meant for publication. Some research resources do not require IRB approval. For example, CDWs with data of deceased patients have been de-identified and IRB approval is not required for their usage. Another challenge is data quality. Methods that adjust for bias (such as using propensity score matching methods) assume that a complete health record is captured. Tools that examine data quality (e.g., point to missing data) help in discovering data quality problems.","completion":"Software Engineering"}
{"prompt":"Hamilton then joined the MIT Instrumentation Laboratory, which developed the Apollo Guidance Computer for the Apollo lunar exploration program. Hamilton was the first programmer hired for the Apollo project and in 1965 became Director of the Software Engineering Division. She was responsible for the team writing and testing all on board in flight software for the Apollo spacecraft's Command and Lunar Module and for the subsequent Skylab space station. Another part of her team designed and developed the systems software. This included error detection and recovery software such as restarts and the Display Interface Routines (also known as the Priority Displays), which Hamilton designed and developed. She worked to gain hands-on experience during a time when computer science courses were uncommon and software engineering courses did not exist.Her areas of expertise include: systems design and software development, enterprise and process modeling, development paradigm, formal systems modeling languages, system-oriented objects for systems modeling and development, automated life-cycle environments, methods for maximizing software reliability and reuse, domain analysis, correctness by built-in language properties, open-architecture techniques for robust systems, full life-cycle automation, quality assurance, seamless integration, error detection and recovery techniques, human-machine interface systems, operating systems, end-to-end testing techniques, and life-cycle management techniques. These techniques are intended to make code more reliable because they help programmers identify and fix errors sooner in the development process.","completion":"Software Engineering"}
{"prompt":"Conferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS).\nNotable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.","completion":"Software Engineering"}
{"prompt":"The concept of dimensions of interaction design were introduced in Moggridge's book Designing Interactions. Crampton Smith wrote that interaction design draws on four existing design languages, 1D, 2D, 3D, 4D. Kevin Silver later proposed a fifth dimension, behaviour.","completion":"Software Engineering"}
{"prompt":"Software design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called software architecture. Software design is divided into three different levels of design. The three levels are interface design, architectural design, and detailed design. Interface design is the interaction between a system and its environment. This happens at a high level of abstraction along with the inner workings of the system. Architectural design has to do with the major components of a system and their responsibilities, properties, interfaces, and their relationships and interactions that occur between them. Detailed design is the internal elements of all the major system components, their properties, relationships, processing, and usually their algorithms and the data structures.","completion":"Software Engineering"}
{"prompt":"CI is often intertwined with continuous delivery or continuous deployment in what is called a CI\/CD pipeline. \"Continuous delivery\" ensures the software checked in on the mainline is always in a state that can be deployed to users, while \"continuous deployment\" fully automates the deployment process.","completion":"Software Engineering"}
{"prompt":"Clinical informaticians use their knowledge of patient care combined with their understanding of informatics concepts, methods, and health informatics tools to:\n\nassess information and knowledge needs of health care professionals, patients and their families.\ncharacterize, evaluate, and refine clinical processes,\ndevelop, implement, and refine clinical decision support systems, and\nlead or participate in the procurement, customization, development, implementation, management, evaluation, and continuous improvement of clinical information systems.Clinicians collaborate with other health care and information technology professionals to develop health informatics tools which promote patient care that is safe, efficient, effective, timely, patient-centered, and equitable. Many clinical informaticists are also computer scientists.\nThe frustration experiences by many practitioners is described in \"Why Doctors Hate their Computers\"","completion":"Software Engineering"}
{"prompt":"The advocates of XP argue that the only truly important product of the system development process is code \u2013 software instructions that a computer can interpret. Without code, there is no working product.\nCoding can be used to figure out the most suitable solution. Coding can also help to communicate thoughts about programming problems. A programmer dealing with a complex programming problem, or finding it hard to explain the solution to fellow programmers, might code it in a simplified manner and use the code to demonstrate what they mean. Code, say the proponents of this position, is always clear and concise and cannot be interpreted in more than one way. Other programmers can give feedback on this code by also coding their thoughts.","completion":"Software Engineering"}
{"prompt":"Dynamic symbolic execution (also known as DSE or concolic execution) involves executing a test program on a concrete input, collecting the path constraints associated with the execution, and using a constraint solver (generally, an SMT solver) to generate new inputs that would cause the program to take a different control-flow path, thus increasing code coverage of the test suite. DSE can considered a type of fuzzing (\"white-box\" fuzzing).","completion":"Software Engineering"}
{"prompt":"Candidates had to undergo a peer review of their education and professional qualifications in order to receive authorization to take the CSDP examination. Candidates therefore had to submit an application to the IEEE Computer Society that provided verifiable information regarding their educational background and professional experience.\nThe Certified Software Development Associate (CSDA) certification was available to graduating students and early-career software professionals who did not meet the eligibility requirements for the CSDP.","completion":"Software Engineering"}
{"prompt":"Because this discipline is applied within multiple methodologies, the following activities will occur within differently specified phases. However, if the phases of the rational unified process (RUP) are used as a framework, then the activities will occur as follows:\nDuring the first, Conceptual phase of a program or project, critical business processes are identified. Typically they are classified as critical based upon revenue value, cost savings, or other assigned business value. This classification is done by the business unit, not the IT organization. High level risks that may impact system performance are identified and described at this time. An example might be known performance risks for a particular vendor system. Finally, performance activities, roles and deliverables are identified for the Elaboration phase. Activities and resource loading are incorporated into the Elaboration phase project plans.","completion":"Software Engineering"}
{"prompt":"The CSDP examination content was based on the Guide To The Software Engineering Body of Knowledge.  The examination covered content from all primary knowledge areas in the SWEBOK Guide Version 3. Below is a list of the topics tested in terms of their proportion of the total examination.\nSoftware requirements 11%\nSoftware design 11%\nSoftware construction 9%\nSoftware testing 11%\nSoftware maintenance 5%\nSoftware configuration management 5%\nSoftware engineering management 8%\nSoftware engineering process 5%\nSoftware engineering methods 4%\nSoftware quality 7%\nSoftware engineering professional practice 5%\nSoftware engineering economics  5%\nComputing foundations 5%\nMathematical foundations 3%\nEngineering foundations 4%","completion":"Software Engineering"}
{"prompt":"The development team should always be working on the latest version of the software.  Since different team members may have versions  saved locally with various changes and improvements, they should try to upload their current version to the code repository every few hours, or when a significant break presents itself.  Continuous integration will avoid delays later on in the project cycle, caused by integration problems.","completion":"Software Engineering"}
{"prompt":"BCS offers qualifications that cover all areas of IT, including understanding Spreadsheets and Presentation Software, Animation, Video Editing and Social Networking safety.The current IT user qualifications are:\n\nEuropean Computer Driving Licence (ECDL) \u2013 BCS is the only organisation licensed to offer ECDL qualifications in the UK.\nAdvanced ECDL \u2013 the advanced course of ECDL (\"Advanced ECDL\") has four sections, each a qualification in its own right. Upon achieving all four advanced qualifications, the individual will receive a qualification as an \"ECDL Expert\" \u2013 in the UK, this confers upon the person Associate Membership of The British Computer Society, should that person wish to sign up to a code of conduct and join BCS.","completion":"Software Engineering"}
{"prompt":"Translational Bioinformatics (TBI) is a relatively new field that surfaced in the year of 2000 when human genome sequence was released. The commonly used definition of TBI is lengthy and could be found on the AMIA website. In simpler terms, TBI could be defined as a collection of colossal amounts of health related data (biomedical and genomic) and translation of the data into individually tailored clinical entities.\nToday, TBI field is categorized into four major themes that are briefly described below:\n\nClinical big data is a collection of electronic health records that are used for innovations. The evidence-based approach that is currently practiced in medicine is suggested to be merged with the practice-based medicine to achieve better outcomes for patients. As CEO of California-based cognitive computing firm Apixio, Darren Schutle, explains that the care can be better fitted to the patient if the data could be collected from various medical records, merged, and analyzed. Further, the combination of similar profiles can serve as a basis for personalized medicine pointing to what works and what does not for certain condition (Marr, 2016).\nGenomics in clinical careGenomic data are used to identify the genes involvement in unknown or rare conditions\/syndromes. Currently, the most vigorous area of using genomics is oncology. The identification of genomic sequencing of cancer may define reasons of drug(s) sensitivity and resistance during oncological treatment processes.\nOmics for drugs discovery and repurposingRepurposing of the drug is an appealing idea that allows the pharmaceutical companies to sell an already approved drug to treat a different condition\/disease that the drug was not initially approved for by the FDA. The observation of \"molecular signatures in disease and compare those to signatures observed in cells\" points to the possibility of a drug ability to cure and\/or relieve symptoms of a disease.\nPersonalized genomic testingIn the US, several companies offer direct-to-consumer (DTC) genetic testing. The company that performs the majority of testing is called 23andMe. Utilizing genetic testing in health care raises many ethical, legal and social concerns; one of the main questions is whether the health care providers are ready to include patient-supplied genomic information while providing care that is unbiased (despite the intimate genomic knowledge) and a high quality. The documented examples of incorporating such information into a health care delivery showed both positive and negative impacts on the overall health care related outcomes.","completion":"Software Engineering"}
{"prompt":"The developers sort the user stories by risk. They also categorize into three piles: low, medium and high risk user stories. The following is an example of an approach to this:\n\nDetermine Risk Index: Give each user story an index from 0 to 2 on each of the following factors:\nCompleteness (do we know all of the story details?)\nComplete (0)\nIncomplete (1)\nUnknown (2)\nVolatility (is it likely to change?)\nlow (0)\nmedium (1)\nhigh (2)\nComplexity (how hard is it to build?)\nsimple (0)\nstandard (1)\ncomplex (2)All indexes for a user story are added, assigning the user stories a risk index of low (0\u20131), medium (2\u20134), or high (5\u20136).","completion":"Software Engineering"}
{"prompt":"CI\/CD bridges the gaps between development and operation activities and teams by enforcing automation in building, testing and deployment of applications. CI\/CD services compile the incremental code changes made by developers, then link and package them into software deliverables. Automated tests verify the software  functionality, and automated deployment services deliver them to end users. The aim is to increase early defect discovery, increase productivity, and provide faster release cycles. The process contrasts with traditional methods where a collection of software updates were integrated into one large batch before deploying the newer version. Modern-day DevOps practices involve:\n\ncontinuous development,\ncontinuous testing,\ncontinuous integration,\ncontinuous deployment, and\ncontinuous monitoringof software applications throughout its development life cycle. The CI\/CD practice, or CI\/CD pipeline, forms the backbone of modern day DevOps operations.","completion":"Software Engineering"}
{"prompt":"Test-driven development is a software-development methodology which essentially states that for each unit of software, a software developer must\n\ndefine a test set for the unit first;\nmake the tests fail;\nthen implement the unit;\nfinally verify that the implementation of the unit makes the tests succeed.This definition is rather non-specific in that it allows tests in terms of high-level software requirements, low-level technical details or anything in between. One way of looking at BDD therefore, is that it is a continued development of TDD which makes more specific choices than TDD.\nBehavior-driven development specifies that tests of any unit of software should be specified in terms of the desired behavior of the unit. Borrowing from agile software development the \"desired behavior\" in this case consists of the requirements set by the business \u2014 that is, the desired behavior that has business value for whatever entity commissioned the software unit under construction. Within BDD practice, this is referred to as BDD being an \"outside-in\" activity.","completion":"Software Engineering"}
{"prompt":"The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.","completion":"Software Engineering"}
{"prompt":"The third IDEF (IDEF2) was originally intended as a user interface modeling method. However, since the Integrated Computer-Aided Manufacturing (ICAM) program needed a simulation modeling tool, the resulting IDEF2 was a method for representing the time varying behavior of resources in a manufacturing system, providing a framework for specification of math model based simulations. It was the intent of the methodology program within ICAM to rectify this situation but limitation of funding did not allow this to happen. As a result, the lack of a method which would support the structuring of descriptions of the user view of a system has been a major shortcoming of the IDEF system. The basic problem from a methodology point of view is the need to distinguish between a description of what a system (existing or proposed) is supposed to do and a representative simulation model that predicts what a system will do. The latter was the focus of IDEF2, the former is the focus of IDEF3.","completion":"Software Engineering"}
{"prompt":"The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. The BLS estimates from 2014 to 2024 that computer software engineering would increase by 17% . This is down from the 2012 to 2022 BLS estimate of 22% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031. and then a decline of -11 percent from 2022 to 2032. Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower. Furthermore, women in many software fields has also been declining over the years as compared to other engineering fields. Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers. However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades.","completion":"Software Engineering"}
{"prompt":"Within the problem management domain, the performance engineering practices are focused on resolving the root cause of performance related problems. These typically involve system tuning, changing operating system or device parameters, or even refactoring the application software to resolve poor performance due to poor design or bad coding practices.","completion":"Software Engineering"}
{"prompt":"Quantum computing is an area of research that brings together the disciplines of computer science, information theory, and quantum physics. While the idea of information as part of physics is relatively new, there appears to be a strong tie between information theory and quantum mechanics. Whereas traditional computing operates on a binary system of ones and zeros, quantum computing uses qubits. Qubits are capable of being in a superposition, i.e. in both states of one and zero, simultaneously. Thus, the value of the qubit is not between 1 and 0, but changes depending on when it is measured. This trait of qubits is known as quantum entanglement, and is the core idea of quantum computing that allows quantum computers to do large scale computations. Quantum computing is often used for scientific research in cases where traditional computers do not have the computing power to do the necessary calculations, such in molecular modeling. Large molecules and their reactions are far too complex for traditional computers to calculate, but the computational power of quantum computers could provide a tool to perform such calculations.","completion":"Software Engineering"}
{"prompt":"Software engineering (SE) is the application of a systematic, disciplined and quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches. That is, the application of engineering to software. It is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference, and was intended to provoke thought regarding the perceived software crisis at the time. Software development, a widely used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard in ISO\/IEC TR 19759:2015.","completion":"Software Engineering"}
{"prompt":"Computer programming is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language that is often more restrictive than natural languages, but easily translated by the computer. Programming is used to invoke some desired behavior (customization) from the machine.Writing high-quality source code requires knowledge of both the computer science domain and the domain in which the application will be used. The highest-quality software is thus often developed by a team of domain experts, each a specialist in some area of development. However, the term programmer may apply to a range of program quality, from hacker to open source contributor to professional. It is also possible for a single programmer to do most or all of the computer programming needed to generate the proof of concept to launch a new killer application.","completion":"Software Engineering"}
{"prompt":"Data models provide a framework for data to be used within information systems by providing specific definition and format. If a data model is used consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data seamlessly. The results of this are indicated in the diagram. However, systems and interfaces are often expensive to build, operate, and maintain. They may also constrain the business rather than support it. This may occur when the quality of the data models implemented in systems and interfaces is poor.Some common problems found in data models are:\n\nBusiness rules, specific to how things are done in a particular place, are often fixed in the structure of a data model. This means that small changes in the way business is conducted lead to large changes in computer systems and interfaces. So, business rules need to be implemented in a flexible way that does not result in complicated dependencies, rather the data model should be flexible enough so that changes in the business can be implemented within the data model in a relatively quick and efficient way.\nEntity types are often not identified, or are identified incorrectly.  This can lead to replication of data, data structure and functionality, together with the attendant costs of that duplication in development and maintenance. Therefore, data definitions should be made as explicit and easy to understand as possible to minimize misinterpretation and duplication.\nData models for different systems are arbitrarily different. The result of this is that complex interfaces are required between systems that share data. These interfaces can account for between 25 and 70% of the cost of current systems. Required interfaces should be considered inherently while designing a data model, as a data model on its own would not be usable without interfaces within different systems.\nData cannot be shared electronically with customers and suppliers, because the structure and meaning of data has not been standardised. To obtain optimal value from an implemented data model, it is very important to define standards that will ensure that data models will both meet business needs and be consistent.","completion":"Software Engineering"}
{"prompt":"Beginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle. It was difficult to keep up with the hardware which caused many problems for software engineers. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established.The origins of the term software engineering have been attributed to various sources. The term  appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\" and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) \"letter to the ACM membership\" by the ACM President Anthony A. Oettinger. It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer. Margaret Hamilton described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy.  At the time there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton.In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.\nModern, generally accepted best-practices for software engineering have been collected by the ISO\/IEC JTC 1\/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of the major computing disciplines.","completion":"Software Engineering"}
{"prompt":"Charles Bachman (1924\u20132017) is particularly known for his work in the area of databases.\nL\u00e1szl\u00f3 B\u00e9l\u00e1dy (1928\u20132021) the editor-in-chief of the IEEE Transactions on Software Engineering in the 1980s.\nFred Brooks (born 1931) best known for managing the development of OS\/360.\nPeter Chen (born 1947) known for the development of entity\u2013relationship modeling.\nEdsger W. Dijkstra (1930\u20132002) developed the framework for a form of structured programming.\nDavid Parnas (born 1941) developed the concept of information hiding in modular programming.\nMichael A. Jackson (born 1936) software engineering methodologist responsible for JSP method of program design; JSD method of system development (with John Cameron); and Problem Frames approach for analysing and structuring software development problems.\nRichard Stallman, created the GNU system utilities and championed free software.","completion":"Software Engineering"}
{"prompt":"The business side sorts the user stories by business value. They will arrange them into three piles:\n\nCritical: stories without which the system cannot function or has no meaning.\nSignificant Business Value: Non-critical user stories that have significant business value.\nNice to have: User stories that do not have significant business value.","completion":"Software Engineering"}
{"prompt":"During this defining phase, the critical business processes are decomposed to critical use cases. Probe cases will be decomposed further, as needed, to single page (screen) transitions. These are the use cases that will be subjected to script driven performance testing.\nThe type of requirements that relate to performance engineering are the non-functional requirements, or NFR. While a functional requirement relates to which business operations are to be performed, a performance related non-functional requirement will relate to how fast that business operation performs under defined circumstances.","completion":"Software Engineering"}
{"prompt":"BCS has different grades of membership:\n\nHonorary gradesDistinguished Fellow (Only 24 awards since 1971)\nHonorary Fellowship (Hon FBCS) (Only 104 awarded to date)Professional gradesFellow (FBCS)\nMember (MBCS)Ordinary gradesAssociate Member (AMBCS)\nStudent MemberGroup, corporate and other membership categoriesAffiliate: for those with an interest in IT but not yet employed in an IT role.\nGroup membership: nearly 200 organisations now encourage their IT professionals to join the Society through its Group Membership Scheme.\nEducation affiliates: education intuitions can also be accredited by BCS.Other Chartered designationsThe Engineering Council UK has licensed the BCS to award Chartered Engineer status (CEng) and Incorporated Engineer status (IEng).\nThe Science Council formerly licensed the BCS to award Chartered Scientist status (CSci). However, the BCS no longer offers Chartered Scientist status (CSci)\nMembers may also apply through BCS to the European Federation of National Engineering Associations (FEANI) for European Engineer (EUR ING) status.Designatory (post-nominal) lettersMembers are encouraged to display the designatory letters to which they are entitled whenever appropriate. The order of designatory (post-nominal) letters is complex and open to a certain amount of interpretation. The accepted authority on this subject is Debrett's Correct Form. Normally these should appear after decorations, degrees and chartered letters. Members holding CEng should also display the designatory letters of the institution through which they are registered immediately after the CEng. Conventionally, members holding Chartered status (CITP) display this immediately after their membership letters (e.g., FBCS CITP or MBCS CITP). However, as CITP may now be awarded by other organisations it may also be displayed separately, following that of the awarding institution.\nSome examples of BCS-related post-nominals:\n\nMr Frank James MBE, FBCS.\nMr Frank James MBE, MSc, CEng, MBCS, MIET.\nMr Frank James MBE, BSc (Hons), MBCS, CITP.\nMr Frank James MBE, MSc, CSci, MIET, CITP.\nMr Frank James MBE, MSc, MCGI, CEng, MBCS, FEDIPAdvPra.","completion":"Software Engineering"}
{"prompt":"Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier \u2013 whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.","completion":"Software Engineering"}
{"prompt":"Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.","completion":"Software Engineering"}
{"prompt":"Computer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world's first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took five years to complete.\nWhile the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947, planar process by Jean Hoerni, the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959, the metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.","completion":"Software Engineering"}
{"prompt":"Continuous deployment is a software engineering approach which uses automated software deployments.\nIn it, software is produced in short cycles but through automated software deployments even to production rather than requiring a \"click of a button\" for that last step.:\u200a52\u200a Therefore, continuous deployment can be considered a more sophisticated form of automation.\nAcademic literature differentiates between continuous delivery and continuous deployment according to deployment method; manual vs. automated.","completion":"Software Engineering"}
{"prompt":"Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality. As a separate phase in software development, it is typically performed by quality assurance staff or a developer other than the one who wrote the code.","completion":"Software Engineering"}
{"prompt":"Business Process Model and Notation (BPMN) is a graphical representation for specifying business processes in a workflow. BPMN was developed by Business Process Management Initiative (BPMI), and is currently maintained by the Object Management Group since the two organizations merged in 2005. The current version of BPMN is 2.0.The Business Process Model and Notation (BPMN) specification provides a graphical notation for specifying business processes in a Business Process Diagram (BPD).  The objective of BPMN is to support business process management for both technical users and business users by providing a notation that is intuitive to business users yet able to represent complex process semantics. The BPMN specification also provides a mapping between the graphics of the notation to the underlying constructs of execution languages, particularly BPEL4WS.","completion":"Software Engineering"}
{"prompt":"Bernd Bruegge is the author of the following books:\n\nBernd Bruegge, Allen Dutoit:  Object-Oriented Software Engineering: Using UML, Patterns and Java (Third Edition). Prentice Hall, 2009. ISBN 978-0136061250.\nEva-Maria Kern, Heinz-Gerd Hegering, Bernd Br\u00fcgge: Managing Development and Application of Digital Technologies. Sringer. 2006. ISBN 3-540-34128-5.He is also the author of many academic papers, for example:\n\nStephan Krusche, Dora Dzvonyar, Han Xu and Bernd Bruegge. Software Theater \u2014 Teaching Demo Oriented Prototyping. Transactions on Computing Education. ACM Journal. 2018\nStephan Krusche, Bernd Bruegge, Irina Camilleri, Kirill Krinkin, Andreas Seitz and Cecil W\u00f6bker. Chaordic Learning: A Case Study. 39th International Conference on Software Engineering (ICSE'17), Software Engineering Education and Training, pages 87\u201396. ACM. Buenos Aires - Argentina, May 2017\nStephan Krusche, Andreas Seitz, J\u00fcrgen B\u00f6rstler and Bernd Bruegge. Interactive Learning \u2013 Increasing Student Participation through Shorter Exercise Cycles. 19th Australasian Computing Education Conference (ACE'17), pages 17\u201326. ACM. Geelong - Australia, January 2017\nBernd Bruegge, Stephan Krusche and Lukas Alperowitz. Software Engineering Project Courses with Industrial Clients. Transactions on Computing Education 15(4), pages 17:1-17:31. ACM Journal. 2015\nBernd Bruegge, Allen Dutoit, Timo Wolf. Sysiphus: Enabling informal collaboration in global software development. In the proceedings of the International Conference on Global Software Engineering (ICGSE) 2006.\nMartin Bauer, Bernd Bruegge, et al. Design of a component-based augmented reality framework. In the Proceedings of IEEE and ACM International Symposium on Augmented Reality. 2001.\nBernd Bruegge, Allen Dutoit, et al. Transatlantic project courses in a university environment. In the Proceedings of the 7th Asia-Pacific Software Engineering Conference (APSEC), 2000.\nAllen Dutoit, Bernd Bruegge. Communication metrics for software development. IEEE Transactions on Software Engineering, 24(8), pp. 615\u2013628. 1998.\nBernd Bruegge, Allen Dutoit. Communication metrics for software development. In the proceedings of the 19th ACM International Conference on Software Engineering (ICSE), 1997.\nBernd Bruegge, Ben Bennington. Applications of mobile computing and communication. IEEE Personal Communications, 3(1), pp. 64\u201371. 1996.\nBernd Bruegge, Robert Coyne. Teaching iterative and collaborative design: lessons and directions. Software Engineering Education, Springer, pp. 411\u2013427, 1994.\nBernd Bruegge, Tim Gottschalk, Bin Luo. A framework for dynamic program analyzers. In ACM SIGPLAN Notices, 28(10), 1993.\nBernd Bruegge, Jim Blythe, et al. Object-oriented system modeling with OMT. In ACM SIGPLAN Notices, 27(10), pp. 359\u2013376. 1992.\nBernd Bruegge, Peter Hibbard. Generalized path expressions: A high-level debugging mechanism. Journal of Systems and Software, 3(4), pp. 265\u2013276. 1983.\nBernd Bruegge. Teaching an industry-oriented software engineering course. Software Engineering Education, Springer, pp. 63\u201387, 1992.","completion":"Software Engineering"}
{"prompt":"According to the BLS, Job Outlook employment for computer hardware engineers, the expected ten-year growth from 2019 to 2029 for computer hardware engineering was an estimated 2% and a total of 71,100 jobs. (\"Slower than average\" in their own words when compared to other occupations)\". This is a decrease from the 2014 to 2024 BLS computer hardware engineering estimate of 3% and  a total of 77,700 jobs. \" and is down from 7% for the 2012 to 2022 BLS estimate and is further down from 9% in the BLS 2010 to 2020 estimate.\" Today, computer hardware is somehow equal to electronic and computer engineering (ECE) and has been divided into many subcategories; the most significant is embedded system design.","completion":"Software Engineering"}
{"prompt":"In practice, PSP skills are used in a TSP team environment. TSP teams consist of PSP-trained developers who volunteer for areas of project responsibility, so the project is managed by the team itself. Using personal data gathered using their PSP skills; the team makes the plans, the estimates, and controls the quality.\nUsing PSP process methods can help TSP teams to meet their schedule commitments and produce high quality software. For example, according to research by Watts Humphrey, a third of all software projects fail, but an SEI study on 20 TSP projects in 13 different organizations found that TSP teams missed their target schedules by an average of only six percent.Successfully meeting schedule commitments can be attributed to using historical data to make more accurate estimates, so projects are based on realistic plans \u2013 and by using PSP quality methods, they produce low-defect software, which reduces time spent on removing defects in later phases, such as integration and acceptance testing.","completion":"Software Engineering"}
{"prompt":"IDEF refers to a family of modeling language, which cover a wide range of uses, from functional modeling to data, simulation, object-oriented analysis\/design and knowledge acquisition. Eventually the IDEF methods have been defined up to IDEF14:  \n\nIDEF0: Function modeling\nIDEF1: Information modeling\nIDEF1X: Data modeling\nIDEF2: Simulation model design\nIDEF3: Process description capture\nIDEF4: Object-oriented design\nIDEF5: Ontology description capture\nIDEF6: Design rationale capture\nIDEF7: Information system auditing\nIDEF8: User interface modeling\nIDEF9: Business constraint discovery\nIDEF10: Implementation architecture modeling\nIDEF11: Information artifact modeling\nIDEF12: Organization modeling\nIDEF13: Three-schema mapping design\nIDEF14: Network designIn 1995 only the IDEF0, IDEF1X, IDEF2, IDEF3 and IDEF4 had been developed in full. Some of the other IDEF concepts had some preliminary design. Some of the last efforts were new IDEF developments in 1995 toward establishing reliable methods for business constraint discovery IDEF9, design rationale capture IDEF6, human system, interaction design IDEF8, and network design IDEF14.The methods IDEF7, IDEF10, IDEF11, IDEF 12 and IDEF13 haven't been developed any further than their initial definition.","completion":"Software Engineering"}
{"prompt":"Requirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software requirements can be of three different types. There are functional requirements, non-functional requirements, and domain requirements. The operation of the software should be performed and the proper output should be expected for the user to use. Non-functional requirements deal with issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.","completion":"Software Engineering"}
{"prompt":"Computer engineering is referred to as computer science and engineering at some universities. Most entry-level computer engineering jobs require at least a bachelor's degree in computer engineering (or computer science and engineering). Typically one must learn an array of mathematics such as calculus, algebra and trigonometry and some computer science classes. Degrees in electronic or electric engineering also suffice due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a strong background in computer programming is necessary. According to BLS, \"a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum\". Some large firms or specialized jobs require a master's degree.\nIt is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers. This can be helpful, especially when it comes to learning new skills or improving existing ones. For example, as the relative cost of fixing a bug increases the further along it is in the software development cycle, there can be greater cost savings attributed to developing and testing for quality code as soon as possible in the process,  particularly before release.","completion":"Software Engineering"}
{"prompt":"It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).","completion":"Software Engineering"}
{"prompt":"With a worldwide membership of 57,625 members as of 2021, BCS is a registered charity and was incorporated by Royal Charter in 1984. Its objectives are to promote the study and application of communications technology and computing technology and to advance knowledge of education in ICT for the benefit of professional practitioners and the general public. \nBCS is a member institution of Engineering Council, through which it is licensed to award the designation of Incorporated Engineer and Chartered Engineer and therefore is responsible for the regulation of ICT and computer science fields within the UK. The BCS is also a member of the Council of European Professional Informatics Societies, the Seoul Accord for international tertiary degree recognition, and the European Quality Assurance Network for Informatics Education EQANIE. BCS was previously a member organisation of the Science Council through which it was licensed to award the designation of Chartered Scientist.\nBCS has offices in the City of London. The main administrative offices are in Swindon, Wiltshire, west of London. It also has two overseas offices in Sri Lanka and Mauritius.\nMembers are sent the quarterly IT professional magazine ITNOW (formerly The Computer Bulletin).\nBCS is a member organisation of the Federation of Enterprise Architecture Professional Organizations (FEAPO), a worldwide association of professional organisations which have come together to provide a forum to standardise, professionalise, and otherwise advance the discipline of Enterprise Architecture.","completion":"Software Engineering"}
{"prompt":"Metaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.","completion":"Software Engineering"}
{"prompt":"Computer software, or just software, is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of programs, procedures, algorithms, as well as its documentation concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible.Software is also sometimes used in a more narrow sense, meaning application software only.","completion":"Software Engineering"}
{"prompt":"Software analysis is the process of analyzing the behavior of computer programs regarding a property such as performance, robustness, and security It can be performed without executing the program (static program analysis), during runtime (dynamic program analysis) or in a combination of both.","completion":"Software Engineering"}
{"prompt":"The computer industry is made up of businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, manufacturing computer components and providing information technology services, including system administration and maintenance.The software industry includes businesses engaged in development, maintenance and publication of software. The industry also includes software services, such as training, documentation, and consulting.","completion":"Software Engineering"}
{"prompt":"With the expanding demand for software in many smaller organizations, the need for inexpensive software solutions led to the growth of simpler, faster methodologies that developed running software, from requirements to deployment, quicker & easier.  The use of rapid-prototyping evolved to entire lightweight methodologies, such as Extreme Programming (XP), which attempted to simplify many areas of software engineering, including requirements gathering and reliability testing for the growing, vast number of small software systems.  Very large software systems still used heavily documented methodologies, with many volumes in the documentation set; however, smaller systems had a simpler, faster alternative approach to managing the development and maintenance of software calculations and algorithms, information storage\/retrieval and display.","completion":"Software Engineering"}
{"prompt":"In common with many professional institutions, BCS has a number of regional branches and specialist groups.  Currently, there are 45 regional branches in the UK, 16 international sections and over 50 specialist groups.","completion":"Software Engineering"}
{"prompt":"A separate subcategory of behavior-driven development is formed by tools that use specifications as an input language rather than user stories. An example of this style is the RSpec tool that was also originally developed by Dan North. Specification tools don't use user stories as an input format for test scenarios but rather use functional specifications for units that are being tested. These specifications often have a more technical nature than user stories and are usually less convenient for communication with business personnel than are user stories. An example of a specification for a stack might look like this:\n\nSpecification: Stack\n\nWhen a new stack is created\nThen it is empty\n\nWhen an element is added to the stack\nThen that element is at the top of the stack\n\nWhen a stack has N elements \nAnd element E is on top of the stack\nThen a pop operation returns E\nAnd the new size of the stack is N-1\n\nSuch a specification may exactly specify the behavior of the component being tested, but is less meaningful to a business user. As a result, specification-based testing is seen in BDD practice as a complement to story-based testing and operates at a lower level. Specification testing is often seen as a replacement for free-format unit testing.Specification testing tools like RSpec and JDave are somewhat different in nature from tools like JBehave. Since they are seen as alternatives to basic unit testing tools like JUnit, these tools tend to favor forgoing the separation of story and testing code and prefer embedding the specification directly in the test code instead. For example, an RSpec test for a hashtable might look like this:\n\nThis example shows a specification in readable language embedded in executable code. In this case a choice of the tool is to formalize the specification language into the language of the test code by adding methods named it and should. Also there is the concept of a specification precondition \u2013 the before section establishes the preconditions that the specification is based on.\nThe result of test will be:\n\n Hash\n   should eq {}\n   includes key\n   hashes the correct information in a key","completion":"Software Engineering"}
{"prompt":"Computational science and engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, \"computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more\".","completion":"Software Engineering"}
{"prompt":"About Face: The Essentials of User Interface Design by Alan Cooper, about user interface design. ISBN 0-7645-2641-3\nThe Capability Maturity Model by Watts Humphrey. Written for the Software Engineering Institute, emphasizing management and process. (See Managing the Software Process ISBN 0-201-18095-2)\nThe Cathedral and the Bazaar by Eric Raymond about open source development.\nThe Decline and Fall of the American Programmer by Ed Yourdon predicts the end of software development in the U.S. ISBN 0-13-191958-X\nDesign Patterns by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. ISBN 0-201-63361-2\nExtreme Programming Explained by Kent Beck ISBN 0-321-27865-8\n\"Go To Statement Considered Harmful\" by Edsger Dijkstra.\n\"Internet, Innovation and Open Source:Actors in the Network\" \u2014 First Monday article by Ilkka Tuomi (2000) source\nThe Mythical Man-Month by Fred Brooks, about project management. ISBN 0-201-83595-9\nObject-oriented Analysis and Design by Grady Booch. ISBN 0-8053-5340-2\nPeopleware by Tom DeMarco and Tim Lister.  ISBN 0-932633-43-9\nThe pragmatic engineer versus the scientific designer by E. W. Dijkstra [1]\nPrinciples of Software Engineering Management by Tom Gilb about evolutionary processes. ISBN 0-201-19246-2\nThe Psychology of Computer Programming by Gerald Weinberg. Written as an independent consultant, partly about his years at IBM. ISBN 0-932633-42-0\nRefactoring: Improving the Design of Existing Code by Martin Fowler, Kent Beck, John Brant, William Opdyke, and Don Roberts. ISBN 0-201-48567-2\nThe Pragmatic Programmer: from journeyman to master by Andrew Hunt, and David Thomas. ISBN 0-201-61622-X\nSoftware Engineering Body of Knowledge (SWEBOK) ISO\/IEC TR 19759See also:\n\nImportant publications in software engineering in CS.","completion":"Software Engineering"}
{"prompt":"Chen proposed the following guiding rules for mapping natural language descriptions into ER diagrams: \"English, Chinese and ER diagrams\" by Peter Chen.\n\nPhysical view show how data is actually stored.","completion":"Software Engineering"}
{"prompt":"The term interaction design was coined by Bill Moggridge and Bill Verplank in the mid-1980s, but it took 10 years before the concept started to take hold.:\u200a31\u200a To Verplank, it was an adaptation of the computer science term user interface design for the industrial design profession. To Moggridge, it was an improvement over soft-face, which he had coined in 1984 to refer to the application of industrial design to products containing software.The earliest programs in design for interactive technologies were the Visible Language Workshop, started by Muriel Cooper at MIT in 1975, and the Interactive Telecommunications Program founded at NYU in 1979 by Martin Elton and later headed by Red Burns.The first academic program officially named \"Interaction Design\" was established at Carnegie Mellon University in 1994, as a Master of Design in Interaction Design. At the outset, the program focused mainly on screen interfaces, before shifting to a greater emphasis on the \"big picture\" aspects of interaction\u2014people, organizations, culture, service and system.\nIn 1990, Gillian Crampton Smith founded the Computer-Related Design MA at the Royal College of Art (RCA) in London, which in 2005 was renamed Design Interactions, headed by Anthony Dunne. In 2001, Crampton Smith helped found the Interaction Design Institute Ivrea (IDII), a specialized institute in Olivetti's hometown in Northern Italy, dedicated solely to interaction design. In 2007, after IDII closed due to a lack of funding, some of the people originally involved with IDII set up the Copenhagen Institute of Interaction Design (CIID), in Denmark. After Ivrea, Crampton Smith and Philip Tabor added the Interaction Design (IxD) track in the Visual and Multimedia Communication at the University of Venice, Italy.\nIn 1998, the Swedish Foundation for Strategic Research founded The Interactive Institute\u2014a Swedish research institute in the field of interaction design.","completion":"Software Engineering"}
{"prompt":"A certification covering PSP is offered by the SEI at Carnegie Mellon University.  The steps to becoming an SEI-Certified PSP Developer are: learn the PSP; take the certification exam; maintain credentials.   \nThe PSP Developer examination is based on concepts found in the PSP Body of Knowledge. The SEI maintains an FAQ on certification.","completion":"Software Engineering"}
{"prompt":"The Operator Function Model (OFM) is proposed as an alternative to traditional task analysis techniques used by human factors engineers. An operator function model attempts to represent in mathematical form how an operator might decompose a complex system into simpler parts and coordinate control actions and system configurations so that acceptable overall system performance is achieved. The model represents basic issues of knowledge representation, information flow, and decision making in complex systems. Miller (1985) suggests that the network structure can be thought of as a possible representation of an operator's internal model of the system plus a control structure which specifies how the model is used to solve the decision problems that comprise operator control functions.","completion":"Software Engineering"}
{"prompt":"System software, or systems software, is computer software designed to operate and control computer hardware, and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently used development tools such as compilers, linkers, and debuggers are classified as system software. System software and middleware manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user, unlike application software.","completion":"Software Engineering"}
{"prompt":"The principles that form the basis of XP are based on the values just described and are intended to foster decisions in a system development project. The principles are intended to be more concrete than the values and more easily translated to guidance in a practical situation.","completion":"Software Engineering"}
{"prompt":"Operations\nUsers\nAdministrators\nManagers\nBuyers\nDevelopment\nAnalysts\nProgrammers\nTesters\nManagers\nBusiness\nConsulting \u2014 customization and installation of applications\nSales\nMarketing\nLegal \u2014 contracts, intellectual property rights\nPrivacy and Privacy engineering\nSupport \u2014 helping customers use applications\nPersonnel \u2014 hiring and training qualified personnel\nFinance \u2014 funding new development\nAcademe\nEducators\nResearchers","completion":"Software Engineering"}
{"prompt":"HIPO for hierarchical input process output is a popular 1970s systems analysis design aid and documentation technique for representing the modules of a system as a hierarchy and for documenting each module.It was used to develop requirements, construct the design, and support implementation of an expert system to demonstrate automated rendezvous. Verification was then conducted systematically because of the method of design and implementation.The overall design of the system is documented using HIPO charts or structure charts. The structure chart is similar in appearance to an organizational chart, but has been modified to show additional detail. Structure charts can be used to display several types of information, but are used most commonly to diagram either data structures or code structures.","completion":"Software Engineering"}
{"prompt":"The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\nGottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent \"anything\".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on\/off\", \"magnetized\/de-magnetized\", \"high-voltage\/low-voltage\", etc.).\nAlan Turing's insight: there are only five actions that a computer has to perform in order to do \"anything\".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;\nmove right one location;\nread symbol at current location;\nprint 0 at current location;\nprint 1 at current location.\nCorrado B\u00f6hm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".Only three rules are needed to combine any set of basic instructions into more complex ones:\nsequence: first do this, then do that;\n selection: IF such-and-such is the case, THEN do this, ELSE do that;\nrepetition: WHILE such-and-such is the case, DO this.\nThe three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).","completion":"Software Engineering"}
{"prompt":"Hamilton has been credited with naming the discipline of \"software engineering\". Hamilton details how she came to make up the term \"software engineering\":\n\nWhen I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new 'term' per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.\nWhen Hamilton started using the term \"software engineering\" during the early Apollo missions, software development was not taken seriously compared to other engineering, nor was it regarded as a science. Hamilton was concerned with legitimizing software development as an engineering discipline. Over time the term \"software engineering\" gained the same respect as any other technical discipline. The IEEE Software September\/October 2018 issue celebrates the 50th anniversary of software engineering. Hamilton talks about \"Errors\" and how they influenced her work related to software engineering and how her language, USL, could be used to prevent the majority of \"Errors\" in a system. Writing in Wired, Robert McMillan noted: \"At MIT she assisted in the creation of the core principles in computer programming as she worked with her colleagues in writing code for the world's first portable computer\". Hamilton's innovations go beyond the feats of playing an important role in getting humans to the Moon. According to Wired's Karen Tegan Padir: \"She, along with that other early programming pioneer, COBOL inventor Grace Hopper, also deserve tremendous credit for helping to open the door for more women to enter and succeed in STEM fields like software.\"In 2019, to celebrate 50 years to the Apollo landing, Google decided to make a tribute to Hamilton. The mirrors at the Ivanpah Solar Power Facility were configured to create a picture of Hamilton and the Apollo 11 by moonlight.","completion":"Software Engineering"}
{"prompt":"A Business Function Model (BFM) is a general description or category of operations performed routinely to carry out an organization's mission. They \"provide a conceptual structure for the identification of general business functions\". It can show the critical business processes in the context of the business area functions. The processes in the business function model must be consistent with the processes in the value chain models. Processes are a group of related business activities performed to produce an end product or to provide a service. Unlike business functions that are performed on a continual basis, processes are characterized by the fact that they have a specific beginning and an end point marked by the delivery of a desired output. The figure on the right depicts the relationship between the business processes, business functions, and the business area's business reference model.","completion":"Software Engineering"}
{"prompt":"One of the core aspects of the PSP is using historical data to analyze and improve process performance. PSP data collection is supported by four main elements:\n\nScripts\nMeasures\nStandards\nFormsThe PSP scripts provide expert-level guidance to following the process steps and they provide a framework for applying the PSP measures. The PSP has four core measures:\n\nSize \u2013 the size measure for a product part, such as lines of code (LOC).\nEffort \u2013 the time required to complete a task, usually recorded in minutes.\nQuality \u2013 the number of defects in the product.\nSchedule \u2013 a measure of project progression, tracked against planned and actual completion dates.Applying standards to the process can ensure the data is precise and consistent.\nData is logged in forms, normally using a PSP software tool. The SEI has developed a PSP tool and there are also open source options available, such as Process Dashboard.\nThe key data collected in the PSP tool are time, defect, and size data \u2013 the time spent in each phase; when and where defects were injected, found, and fixed; and the size of the product parts.  Software developers use many other measures that are derived from these three basic measures to understand and improve their performance. Derived measures include:\n\nestimation accuracy (size\/time)\nprediction intervals (size\/time)\ntime in phase distribution\ndefect injection distribution\ndefect removal distribution\nproductivity\nreuse percentage\ncost performance index\nplanned value\nearned value\npredicted earned value\ndefect density\ndefect density by phase\ndefect removal rate by phase\ndefect removal leverage\nreview rates\nprocess yield\nphase yield\nfailure cost of quality (COQ)\nappraisal COQ\nappraisal\/failure COQ ratio","completion":"Software Engineering"}
{"prompt":"The field of cybersecurity pertains to the protection of computer systems and networks. This includes information and data privacy, preventing disruption of IT services and prevention of theft of and damage to hardware, software and data.","completion":"Software Engineering"}
{"prompt":"This practice advocates the use of a revision control system for the project's source code. All artifacts required to build the project should be placed in the repository. In this practice and the revision control community, the convention is that the system should be buildable from a fresh checkout and not require additional dependencies. Extreme Programming advocate Martin Fowler also mentions that where branching is supported by tools, its use should be minimised. Instead, it is preferred for changes to be integrated rather than for multiple versions of the software to be maintained simultaneously. The mainline (or trunk) should be the place for the working version of the software.","completion":"Software Engineering"}
{"prompt":"For software engineering, several types of models (and their corresponding modeling activities) can be distinguished:\n\nMetadata modeling (MetaData model)\nMeta-process modeling (MetaProcess model)\nExecutable meta-modeling (combining both of the above and much more, as in the general purpose tool Kermeta)\nModel transformation language (see below)\nPolynomial metamodels\nNeural network metamodels\nKriging metamodels\nPiecewise polynomial (spline) metamodels\nGradient-enhanced kriging (GEK)","completion":"Software Engineering"}
{"prompt":"PSP training follows an evolutionary improvement approach: an engineer learning to integrate the PSP into his or her process begins at the first level \u2013 PSP0 \u2013 and progresses in process maturity to the final level \u2013 PSP2.1.  Each Level has detailed scripts, checklists and templates to guide the engineer through required steps and helps the engineer improve their own personal software process. Humphrey encourages proficient engineers to customize these scripts and templates as they gain an understanding of their own strengths and weaknesses.\n\nProcessThe input to PSP is the requirements; requirements document is completed and delivered to the engineer.\n\nPSP0, PSP0.1 (Introduces process discipline and measurement)PSP0 has 3 phases: planning, development (design, code, compile, test) and a post mortem. \nA baseline is established of current process measuring: time spent on programming, faults injected\/removed, size of a program.\nIn a post mortem, the engineer ensures all data for the projects has been properly recorded and analysed.  \nPSP0.1 advances the process by adding a coding standard, a size measurement and the development of a personal process improvement plan (PIP). In the PIP, the engineer records ideas for improving his own process.\n\nPSP1, PSP1.1 (Introduces estimating and planning)Based upon the baseline data collected in PSP0 and PSP0.1, the engineer estimates how large a new program will be and prepares a test report (PSP1).\nAccumulated data from previous projects is used to estimate the total time.\nEach new project will record the actual time spent.\nThis information is used for task and schedule planning and estimation (PSP1.1).\n\nPSP2, PSP2.1 (Introduces quality management and design)PSP2 adds two new phases: design review and code review.  Defect prevention and removal of them are the focus at the PSP2. Engineers learn to evaluate and improve their process by measuring how long tasks take and the number of defects they inject and remove in each phase of development.\nEngineers construct and use checklists for design and code reviews.\nPSP2.1 introduces design specification and analysis techniques\n(PSP3 is a legacy level that has been superseded by TSP.)","completion":"Software Engineering"}
{"prompt":"Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error\/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.\nDebugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.","completion":"Software Engineering"}
{"prompt":"The practices in XP have been heavily debated. Proponents of extreme programming claim that by having the on-site customer request changes informally, the process becomes flexible, and saves the cost of formal overhead. Critics of XP claim this can lead to costly rework and project scope creep beyond what was previously agreed or funded.Change-control boards are a sign that there are potential conflicts in project objectives and constraints between multiple users. XP's expedited methods are somewhat dependent on programmers being able to assume a unified client viewpoint so the programmer can concentrate on coding, rather than documentation of compromise objectives and constraints. This also applies when multiple programming organizations are involved, particularly organizations which compete for shares of projects.Other potentially controversial aspects of extreme programming include:\n\nRequirements are expressed as automated acceptance tests rather than specification documents.\nRequirements are defined incrementally, rather than trying to get them all in advance.\nSoftware developers are usually required to work in pairs.\nThere is no Big Design Up Front. Most of the design activity takes place on the fly and incrementally, starting with \"the simplest thing that could possibly work\" and adding complexity only when it's required by failing tests. Critics characterize this as \"debugging a system into appearance\" and fear this will result in more re-design effort than only re-designing when requirements change.\nA customer representative is attached to the project. This role can become a single-point-of-failure for the project, and some people have found it to be a source of stress. Also, there is the danger of micro-management by a non-technical representative trying to dictate the use of technical software features and architecture.Critics have noted several potential drawbacks, including problems with unstable requirements, no documented compromises of user conflicts, and a lack of an overall design specification or document.","completion":"Software Engineering"}
{"prompt":"Modern computers are very complex and in order to productively program them, various abstractions are needed. For example, rather than writing down a program's binary representation a programmer will write a program in a programming language like C, Java or Python. Programming tools like assemblers, compilers and linkers translate a program from a human write-able and readable source language into the bits and bytes that can be executed by a computer. Interpreters interpret the program on the fly to produce the desired behavior.\nThese programs perform many well defined and repetitive tasks that would nonetheless be time-consuming and error-prone when performed by a human, like laying out parts of a program in memory and fixing up the references between parts of a program as a linker does. Optimizing compilers on the other hand can perform complex transformations on the source code in order to improve the execution speed or other characteristics of a program. This allows a programmer to focus more on higher level, conceptual aspects of a program without worrying about the details of the machine it is running on.","completion":"Software Engineering"}
{"prompt":"In one of the critical moments of the Apollo 11 mission, the Apollo Guidance Computer, together with the on-board flight software, averted an abort of the landing on the Moon. Three minutes before the lunar lander reached the Moon's surface, several computer alarms were triggered. According to software engineer Robert Wills, Buzz Aldrin entered the codes to request that the computer display altitude and other data on the computer\u2019s screen. The system was designed to support seven simultaneous programs running, but Aldrin\u2019s request was the eighth. This action was something he requested many times whilst working in the simulator. The result was a series of unexpected error codes during the live descent. The on-board flight software captured these alarms with the \"never supposed to happen displays\" interrupting the astronauts with priority alarm displays.\nHamilton had prepared for just this situation years before:\n\nThere was one other failsafe that Hamilton likes to remember. Her \"priority display\" innovation had created a knock-on risk that astronaut and computer would slip out of synch just when it mattered most. As the alarms went off and priority displays replaced normal ones, the actual switchover to new programmes behind the screens was happening \"a step slower\" than it would today.\nHamilton had thought long and hard about this. It meant that if Aldrin, say, hit a button on the priority display too quickly, he might still get a \"normal\" response. Her solution: when you see a priority display, first count to\nfive.\n\nBy some accounts, the astronauts had inadvertently left the rendezvous radar switch on, causing these alarms to be triggered (the claim that the radar was left on inadvertently by the astronauts is disputed by Robert Wills with the National Museum of Computing). The computer was overloaded with interrupts caused by incorrectly phased power supplied to the lander's rendezvous radar. The program alarms indicated \"executive overflows\", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them. The asynchronous executive designed by J. Halcombe Laning was used by Hamilton's team to develop asynchronous flight software:\n\nBecause of the flight software's system-software's error detection and recovery techniques that included its system-wide \"kill and recompute\" from a \"safe place\" restart approach to its snapshot and rollback techniques, the Display Interface Routines (AKA the priority displays) together with its man-in-the-loop capabilities were able to be created in order to have the capability to interrupt the astronauts' normal mission displays with priority displays of critical alarms in case of an emergency. This depended on our assigning a unique priority to every process in the software in order to ensure that all of its events would take place in the correct order and at the right time relative to everything else that was going on.\nHamilton's priority alarm displays interrupted the astronauts' normal displays to warn them that there was an emergency \"giving the astronauts a go\/no go decision (to land or not to land)\". Jack Garman, a NASA computer engineer in mission control, recognized the meaning of the errors that were presented to the astronauts by the priority displays and shouted, \"Go, go!\" and they continued. Paul Curto, senior technologist who nominated Hamilton for a NASA Space Act Award, called Hamilton's work \"the foundation for ultra-reliable software design\".Hamilton later wrote of the incident:\n\nThe computer (or rather the software in it) was smart enough to recognize that it was being asked to perform more tasks than it should be performing. It then sent out an alarm, which meant to the astronaut, 'I'm overloaded with more tasks than I should be doing at this time and I'm going to keep only the more important tasks'; i.e., the ones needed for landing ... Actually, the computer was programmed to do more than recognize error conditions. A complete set of recovery programs was incorporated into the software. The software's action, in this case, was to eliminate lower priority tasks and re-establish the more important ones ... If the computer hadn't recognized this problem and taken recovery action, I doubt if Apollo 11 would have been the successful moon landing it was.","completion":"Software Engineering"}
{"prompt":"In the Netherlands, health informatics is currently a priority for research and implementation. The Netherlands Federation of University medical centers (NFU) has created the Citrienfonds, which includes the programs eHealth and Registration at the Source. The Netherlands also has the national organizations Society for Healthcare Informatics (VMBI) and Nictiz, the national center for standardization and eHealth.","completion":"Software Engineering"}
{"prompt":"ISEBBCS also offers professional qualifications via its Professional Certifications board, formerly known as ISEB (Information Systems Examination Board).\nProfessional Certifications (ISEB) provides a wide range of qualifications for IT professionals covering major areas including Management, Development, Service Delivery and Quality.\nInformatics ProfessionalBCS via FEDIP\nprovides 4 different professional registration levels for health and care informatics professionals:\nPractitioner, Senior Practitioner, Advanced Practitioner, Leading Practitioner.\nFEDIPAdvPra \u2013 post-nominals for Advanced Practitioner.\nFEDIP is the Federation for Informatics Professionals in Health and Social Care, a collaboration between the leading professional bodies in health and care informatics supporting the development of the informatics profession.","completion":"Software Engineering"}
{"prompt":"IDEF14, or integrated definition for network design method, is a method that targets the modeling and design of computer and communication networks. It can be used to model existing (\"as is\") or envisioned (\"to be\") networks. It helps the network designer to investigate potential network designs and to document design rationale. The fundamental goals of the IDEF14 research project developed from a perceived need for good network designs that can be implemented quickly and accurately.","completion":"Software Engineering"}
{"prompt":"Margaret Elaine Heafield was born August 17, 1936, in Paoli, Indiana, to Kenneth Heafield and Ruth Esther Heafield (n\u00e9e Partington). The family later moved to Michigan, where Margaret graduated from Hancock High School in 1954. She studied mathematics at the University of Michigan in 1955 before transferring to Earlham College, where her mother had been a student; she earned a BA in mathematics with a minor in philosophy in 1958. She cites Florence Long, the head of the math department at Earlham, as helping with her desire to pursue abstract mathematics and become a mathematics professor. She says her poet father and headmaster grandfather inspired her to include a minor in philosophy in her studies.","completion":"Software Engineering"}
{"prompt":"This specialty focuses on compilers and operating systems design and development. Engineers in this field develop new operating system architecture, program analysis techniques, and new techniques to assure quality. Examples of work in this field include post-link-time code transformation algorithm development and new operating system development.","completion":"Software Engineering"}
{"prompt":"In  Pakistan and Nepal, Bachelor of Engineering in Software Engineering (BE Software) is an 8-semester course of study. This degree is provided by University of Engineering and Technology, Taxila, \nVirtual University of Pakistan,Superior university and many others and  Pokhara University Nepal.\nIn Bangladesh, this degree is named Bachelor of Science in Software Engineering (BS SE) which is also an 8-semester course of study.  University of Dhaka is the pioneer of Software Engineering education in Bangladesh offering Bachelor of Science in Software Engineering (BSSE) degree since 2009 with 6 months industry internship program.The Bachelor of Software Engineering degree is awarded to those who successfully complete an eight-semester program.","completion":"Software Engineering"}
{"prompt":"Employers generally seek applicants with strong programming, systems analysis and business skills.\n\"A large difference exists between the software engineering skills taught at a typical\nuniversity or college and the skills that are desired of a software engineer by a typical\nsoftware development organization. At the heart of this difference seems to be the way\nsoftware engineering is typically introduced to students: general theory is presented in a\nseries of lectures and put into (limited) practice in an associated class project.\"","completion":"Software Engineering"}
{"prompt":"Meta-models are closely related to ontologies. Both are often used to describe and analyze the relations between concepts\nOntologies: express something meaningful within a specified universe or domain of discourse by utilizing a grammar for using vocabulary. The grammar specifies what it means to be a well-formed statement, assertion, query, etc. (formal constraints) on how terms in the ontology\u2019s controlled vocabulary can be used together.\nMeta-modeling: can be considered as an explicit description (constructs and rules) of how a domain-specific model is built. In particular, this comprises a formalized specification of the domain-specific notations. Typically, metamodels are \u2013 and always should follow - a strict rule set. \"A valid metamodel is an ontology, but not all ontologies are modeled explicitly as metamodels\".","completion":"Software Engineering"}
{"prompt":"A semantic model is a model of concepts, it is sometimes called a \"platform independent model\". It is an intensional model. At least since Carnap, it is well known that:\n\"...the full meaning of a concept is constituted by two aspects, its intension and its extension. The first part comprises the embedding of a concept in the world of concepts as a whole, i.e. the totality of all relations to other concepts. The second part establishes the referential meaning of the concept, i.e. its counterpart in the real or in a possible world\".","completion":"Software Engineering"}
{"prompt":"Information systems (IS) is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. The ACM's Computing Careers describes IS as: \n\n\"A majority of IS [degree] programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required.\"\n\nThe study of IS bridges business and computer science, using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline.  The field of Computer Information Systems (CIS) studies computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.","completion":"Software Engineering"}
{"prompt":"Unit tests are automated tests that test the functionality of pieces of the code (e.g. classes, methods). Within XP, unit tests are written before the eventual code is coded. This approach is intended to stimulate the programmer to think about conditions in which his or her code could fail. XP says that the programmer is finished with a certain piece of code when he or she cannot come up with any further conditions under which the code may fail.\nTest driven development proceeds by quickly cycling through the following steps, with each step taking minutes at most, preferably much less. Since each user story will usually require one to two days of work, a very large number of such cycles will be necessary per story.\n\nWrite unit test: The programmers write a minimal test that should fail because the functionality hasn't been fully implemented in the production code.\nWatch the new test fail: The programmers verify the test does indeed fail. While it may seem like a waste of time, this step is critical because it verifies that your belief about the state of the production code is correct. If the test does not fail, the programmers should determine whether there is a bug in the test code, or that the production code does support the functionality described by the new test.\nWrite code: The programmers write just enough production code so the new test will pass.\nRun test: The unit tests are executed to verify that the new production code passes the new test, and that no other tests are failing.\nRefactor: Remove any code smells from both the production and test code.For a more intense version of the above process, see Uncle Bob's Three Rules of TDD.","completion":"Software Engineering"}
{"prompt":"The Fellow of the BCS (FBCS) title is conferred to individuals to recognise their outstanding achievements and contributions to Information Technology (engineering, product management, business leadership, etc). Fellows are expected to give something back to the profession, by promoting and evangelising the profession to the\npublic and society, and contributing to debates in conferences, panels, meetings, etc. \nFellows are nominated to the society each year and have to be supported by one or more existing fellows. Criteria for election to fellow include:\n\nDemonstrate leadership in the profession\nWide acknowledgement of specific IT expertise\nContribution to the advancement of knowledge\nEminent individual\nAuthority and seniority, including leading major projects and managing teamsCurrent fellows\ninclude distinguished individuals from industries and universities. Some of the prominent fellows include:\n\nDame Wendy Hall, FBCS \u2013 ex-President of BCS\nAndy Harter, FBCS \u2013CEO of RealVNC\nTony Hey, FBCS \u2013 ex-VP of Microsoft Research\nHermann Hauser, Distinguished FBCS \u2013 founder of ARM Ltd.\nFrank Zhigang Wang, FBCS \u2013inventor of spin-tunneling random access memoryThe society also awards Honorary Fellowships. Examples include:\n\nDorothy Monekosso, who received the honour for her work on Smart Homes for people living with dementia and for her campaigning work to promote diversity in the tech sector.Since July 2021, Fellows are eligible to be appointed to the Fellows Technical Advisory Group (F-TAG). F-TAG provides technical thought leadership governance for BCS, informing policy positions  and content.","completion":"Software Engineering"}
{"prompt":"The concept is that programmers or software developers should not work more than 40 hour weeks, and if there is overtime one week, that the next week should not include more overtime. Since the development cycles are short cycles of continuous integration, and full development (release) cycles are more frequent, the projects in XP do not follow the typical crunch time that other projects require (requiring overtime).\nAlso, included in this concept is that people perform best and most creatively if they are well rested.\nA key enabler to achieve sustainable pace is frequent code-merge and always executable & test covered high quality code. The constant refactoring way of working enforces team members with fresh and alert minds.  The intense collaborative way of working within the team drives\na need to recharge over weekends.\nWell-tested, continuously integrated, frequently deployed code and environments also minimize the frequency of unexpected production problems and outages, and the associated after-hours nights and weekends work that is required.","completion":"Software Engineering"}
{"prompt":"Metadata modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable and useful to some predefined class of problems. (see also: data modeling).","completion":"Software Engineering"}
{"prompt":"A programmer, computer programmer, or coder is a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with Web. The term programmer can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming.","completion":"Software Engineering"}
{"prompt":"The Russian health care system is based on the principles of the Soviet health care system, which was oriented on mass prophylaxis, prevention of infection and epidemic diseases, vaccination and immunisation of the population on a socially protected basis. The current government health care system consists of several directions:\n\nPreventive health care\nPrimary health care\nSpecialised medical care\nObstetrical and gynecologic medical care\nPediatric medical care\nSurgery\nRehabilitation\/ Health resort treatmentOne of the main issues of the post-Soviet medical health care system was the absence of the united system providing optimisation of work for medical institutes with one, single database and structured appointment schedule and hence hours-long lines. Efficiency of medical workers might have been also doubtful because of the paperwork administrating or lost book records.\nAlong with the development of the information systems IT and health care departments in Moscow agreed on design of a system that would improve public services of health care institutes. Tackling the issues appearing in the existing system, the Moscow Government ordered that the design of a system would provide simplified electronic booking to public clinics and automate the work of medical workers on the first level.\nThe system designed for that purposes was called EMIAS (United Medical Information and Analysis System) and presents an electronic health record (EHR) with the majority of other services set in the system that manages the flow of patients, contains outpatient card integrated in the system, and provides an opportunity to manage consolidated managerial accounting and personalised list of medical help. Besides that, the system contains information about availability of the medical institutions and various doctors.\nThe implementation of the system started in 2013 with the organisation of one computerised database for all patients in the city, including a front-end for the users. EMIAS was implemented in Moscow and the region and it is planned that the project should extend to most parts of the country.","completion":"Software Engineering"}
{"prompt":"Data modeling is a process  used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.  The data requirements are initially recorded as a conceptual data model which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into a logical data model, which documents structures of the data that can be implemented in databases. Implementation of one conceptual data model may require multiple logical data models. The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details. Data modeling defines not just data elements, but also their structures and the relationships between them.Data modeling techniques and methodologies are used to model data in a standard, consistent, predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using data modeling:\n\nto assist business analysts, programmers, testers, manual writers, IT package selectors, engineers, managers, related organizations and clients to understand and use an agreed upon semi-formal model that encompasses the concepts of the organization and how they relate to one another\nto manage data as a resource\nto integrate information systems\nto design databases\/data warehouses (aka data repositories)Data modeling may be performed during various types of projects and in multiple phases of projects. Data models are progressive; there is no such thing as the final data model for a business or application. Instead a data model should be considered a living document that will change in response to a changing business. The data models should ideally be stored in a repository so that they can be retrieved, expanded, and edited over time. Whitten et al. (2004) determined two types of data modeling:\nStrategic data modeling: This is part of the creation of an information systems strategy, which defines an overall vision and architecture for information systems. Information technology engineering is a methodology that embraces this approach.\nData modeling during systems analysis: In systems analysis logical data models are created as part of the development of new databases.Data modeling is also used as a technique for detailing business requirements for specific databases. It is sometimes called database modeling because a data model is eventually implemented in a database.","completion":"Software Engineering"}
{"prompt":"Time line of computing 2400 BC - 1949 - 1950-1979 - 1980-1989 - 1990-1999 - 2000-2009\nHistory of computing hardware up to third generation (1960s)\nHistory of computing hardware from 1960s to current\nHistory of computer hardware in Eastern Bloc countries\nHistory of personal computers\nHistory of laptops\nHistory of software engineering\nHistory of compiler writing\nHistory of the Internet\nHistory of the World Wide Web\nHistory of video games\nHistory of the graphical user interface\nTimeline of computing\nTimeline of operating systems\nTimeline of programming languages\nTimeline of artificial intelligence\nTimeline of cryptography\nTimeline of algorithms\nTimeline of quantum computing","completion":"Software Engineering"}
{"prompt":"Behavior-driven development borrows the concept of the ubiquitous language from domain driven design. A ubiquitous language is a (semi-)formal language that is shared by all members of a software development team \u2014 both software developers and non-technical personnel. The language in question is both used and developed by all team members as a common means of discussing the domain of the software in question. In this way BDD becomes a vehicle for communication between all the different roles in a software project.A common risk with software development includes communication breakdowns between Developers and Business Stakeholders. BDD uses the specification of desired behavior as a ubiquitous language for the project Team members. This is the reason that BDD insists on a semi-formal language for behavioral specification: some formality is a requirement for being a ubiquitous language. In addition, having such a ubiquitous language creates a domain model of specifications, so that specifications may be reasoned about formally. This model is also the basis for the different BDD-supporting software tools that are available.\nThe example given above establishes a user story for a software system under development. This user story identifies a stakeholder, a business effect and a business value. It also describes several scenarios, each with a precondition, trigger and expected outcome. Each of these parts is exactly identified by the more formal part of the language (the term Given might be considered a keyword, for example) and may therefore be processed in some way by a tool that understands the formal parts of the ubiquitous language.\nMost BDD applications use text-based DSLs and specification approaches. However, graphical modeling of integration scenarios has also been applied successfully in practice, e.g., for testing purposes.","completion":"Software Engineering"}
{"prompt":"Programs were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.","completion":"Software Engineering"}
{"prompt":"The BCS is the only professional body in the United Kingdom with the ability to grant chartered status to IT professionals under its Royal Charter, granted to them by the Privy Council. Thus having the ability to grant Chartered (Professional) status to both its Fellows and Professional members. Known as Chartered IT Professional, they are entitled to use the suffix CITP. The BCS keeps a register of current Chartered Members and Fellows.Other Professional membership bodies apply to the BCS for a licence that enables them to award CITP to their eligible members.","completion":"Software Engineering"}
{"prompt":"Extreme programming's initial buzz and controversial tenets, such as pair programming and continuous design, have attracted particular criticisms, such as the ones coming from McBreen and Boehm and Turner, Matt Stephens and Doug Rosenberg. Many of the criticisms, however, are believed by Agile practitioners to be misunderstandings of agile development.In particular, extreme programming has been reviewed and critiqued by Matt Stephens's and Doug Rosenberg's Extreme Programming Refactored.","completion":"Software Engineering"}
{"prompt":"There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers.\nMost software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.","completion":"Software Engineering"}
{"prompt":"IDEF6, or integrated definition for design rationale capture, is a method to facilitate the acquisition, representation, and manipulation of the design rationale used in the development of enterprise systems. Rationale is the reason, justification, underlying motivation, or excuse that moved the designer to select a particular strategy or design feature. More simply, rationale is interpreted as the answer to the question, \u201cWhy is this design being done in this manner?\u201d Most design methods focus on what the design is (i.e. on the final product, rather than why the design is the way it is).IDEF6 is a method that possesses the conceptual resources and linguistic capabilities needed\n\nto represent the nature and structure of the information that constitutes design rationale within a given system, and\nto associate that rationale with design specifications, models, and documentation for the system.IDEF6 is applicable to all phases of the information system development process, from initial conceptualization through both preliminary and detailed design activities. To the extent that detailed design decisions for software systems are relegated to the coding phase, the IDEF6 technique should be usable during the software construction process as well.","completion":"Software Engineering"}
{"prompt":"Dynamic analysis can be used to detect security problems.\n\nIBM Rational AppScan is a suite of application security solutions targeted for different stages of the development lifecycle. The suite includes two main dynamic analysis products: IBM Rational AppScan Standard Edition, and IBM Rational AppScan Enterprise Edition. In addition, the suite includes IBM Rational AppScan Source Edition\u2014a static analysis tool.","completion":"Software Engineering"}
{"prompt":"Robotics is one of the newest emerging subfield of mechatronics. It is the study of robots that how they are manufactured and operated. Since 2000, this branch of mechatronics is attracting a number of aspirants. Robotics is interrelated with automation because here also not much human intervention is required. A large number of factories especially in automobile factories, robots are founds in assembly lines where they perform the job of drilling, installation and fitting. Programming skills are necessary for specialization in robotics. Knowledge of programming language \u2014ROBOTC is important for functioning robots. An industrial robot is a prime example of a mechatronics system; it includes aspects of electronics, mechanics, and computing to do its day-to-day jobs.","completion":"Software Engineering"}
{"prompt":"Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.\nSoftware engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.\nOne of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\"\nEdsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what\nhe called the \"radical novelty\" of computer science:\n\nA number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\"","completion":"Software Engineering"}
{"prompt":"Health informatics law deals with evolving and sometimes complex legal principles as they apply to information technology in health-related fields. It addresses the privacy, ethical and operational issues that invariably arise when electronic tools, information and media are used in health care delivery. Health Informatics Law also applies to all matters that involve information technology, health care and the interaction of information. It deals with the circumstances under which data and records are shared with other fields or areas that support and enhance patient care.\nAs many health care systems are making an effort to have patient records more readily available to them via the internet, it is important that providers implement security standards in order to ensure that the patients' information is safe. They have to be able to assure confidentiality, integrity, and security of the people, process, and technology. Since there is also the possibility of payments being made through this system, it is vital that this aspect of their private information will also be protected through cryptography.\nThe use of technology in health care settings has become popular and this trend is expected to continue.  Various health care facilities had instigated different kinds of health information technology systems in the provision of patient care, such as electronic health records (EHRs), computerized charting, etc. The growing popularity of health information technology systems and the escalation in the amount of health information that can be exchanged and transferred electronically increased the risk of potential infringement in patients' privacy and confidentiality. This concern triggered the establishment of strict measures by both policymakers and individual facility to ensure patient privacy and confidentiality.\nOne of the federal laws enacted to safeguard patient's health information (medical record, billing information, treatment plan, etc.) and to guarantee patient's privacy is the Health Insurance Portability and Accountability Act of 1996 or HIPAA. HIPAA gives patients the autonomy and control over their own health records. Furthermore, according to the U.S. Department of Health & Human Services (n.d.), this law enables patients to:\nview their own health records\nrequest a copy of their own medical records\nrequest correction to any incorrect health information\nknow who has access to their health record\nrequest who can and cannot view\/access their health information","completion":"Software Engineering"}
{"prompt":"Usability answers the question \"can someone use this interface?\". Jakob Nielsen describes usability as the quality attribute that describes how usable the interface is. Shneiderman proposes principles for designing more usable interfaces called \"Eight Golden Rules of Interface Design\"\u2014which are well-known heuristics for creating usable systems.","completion":"Software Engineering"}
{"prompt":"In India an engineering degree in Information Technology is 4 year academic program equivalent to Computer Science&Engineering because in the first year basic engineering subjects and Calculus are taught and in the succeeding years core computer science topics are taught in both B.Tech-IT and B.Tech-CSE.","completion":"Software Engineering"}
{"prompt":"When embarking on a change, a developer takes a copy of the current code base on which to work. As other developers submit changed code to the source code repository, this copy gradually ceases to reflect the repository code. Not only can the existing code base change, but new code can be added as well as new libraries, and other resources that create dependencies, and potential conflicts.\nThe longer development continues on a branch without merging back to the mainline, the greater the risk of multiple integration conflicts and failures when the developer branch is eventually merged back. When developers submit code to the repository they must first update their code to reflect the changes in the repository since they took their copy. The more changes the repository contains, the more work developers must do before submitting their own changes.\nEventually, the repository may become so different from the developers' baselines that they enter what is sometimes referred to as \"merge hell\", or \"integration hell\", where the time it takes to integrate exceeds the time it took to make their original changes.","completion":"Software Engineering"}
{"prompt":"This area integrates the quantum behaviour of small particles such as superposition, interference and entanglement, with classical computers to solve complex problems and formulate algorithms much efficiently. Individuals focus on fields like Quantum cryptography, physical simulations and quantum algorithms.","completion":"Software Engineering"}
{"prompt":"Early in this phase a number of performance tool related activities are required. These include:\n\nIdentify key development team members as subject matter experts for the selected tools.\nSpecify a profiling tool for the development\/component unit test environment.\nSpecify an automated unit (component) performance test tool for the development\/component unit test environment; this is used when no GUI yet exists to drive the components under development.\nSpecify an automated tool for driving server-side unit (components) for the development\/component unit test environment.\nSpecify an automated multi-user capable script-driven end-to-end tool for the development\/component unit test environment; this is used to execute screen-driven use cases.\nIdentify a database test data load tool for the development\/component unit test environment; this is required to ensure that the database optimizer chooses correct execution paths and to enable reinitializing and reloading the database as needed.\nDeploy the performance tools for the development team.\nPresentations and training must be given to development team members on the selected tools.The performance test team normally does not execute performance tests in the development environment, but rather in a specialized pre-deployment environment that is configured to be as close as possible to the planned production environment. This team will execute performance testing against test cases, validating that the critical use cases conform to the specified non-functional requirements. The team will execute load testing against a normally expected (median) load as well as a peak load. They will often run stress tests that will identify the system bottlenecks. The data gathered, and the analysis, will be fed back to the group that does performance tuning. Where necessary, the system will be tuned to bring nonconforming tests into conformance with the non-functional requirements.\nIf performance engineering has been properly applied at each iteration and phase of the project to this point, hopefully this will be sufficient to enable the system to receive performance certification. However, if for some reason (perhaps proper performance engineering working practices were not applied) there are tests that cannot be tuned into compliance, then it will be necessary to return portions of the system to development for refactoring. In some cases the problem can be resolved with additional hardware, but adding more hardware leads quickly to diminishing returns.","completion":"Software Engineering"}
{"prompt":"The delivery of the software is done via frequent releases of live functionality creating concrete value. The small releases help the customer to gain confidence in the progress of the project. This helps maintain the concept of the whole team as the customer can now come up with his suggestions on the project based on real experience.","completion":"Software Engineering"}
{"prompt":"In the United States, a B.S. in Information Technology is awarded after a four-year course of study. Some degree programs are accredited by the Computing Accreditation Commission of the Accreditation Board for Engineering and Technology (ABET).","completion":"Software Engineering"}
{"prompt":"Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11\u201316-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science. According to a 2021 report, only 51% of high schools in the US offer computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.","completion":"Software Engineering"}
{"prompt":"The PSP is a personal process that can be adapted to suit the needs of the individual developer. It is not specific to any programming or design methodology; therefore it can be used with different methodologies, including Agile software development.\nSoftware engineering methods can be considered to vary from predictive through adaptive. The PSP is a predictive methodology, and Agile is considered adaptive, but despite their differences, the TSP\/PSP and Agile share several concepts and approaches \u2013 particularly in regard to team organization. They both enable the team to:\n\nDefine their goals and standards.\nEstimate and schedule the work.\nDetermine realistic and attainable schedules.\nMake plans and process improvements.Both Agile and the TSP\/PSP share the idea of team members taking responsibility for their own work and working together to agree on a realistic plan, creating an environment of trust and accountability. However, the TSP\/PSP differs from Agile in its emphasis on documenting the process and its use of data for predicting and defining project schedules.","completion":"Software Engineering"}
{"prompt":"From the point of view of simplicity, of course one could say that system development doesn't need more than coding, testing and listening. If those activities are performed well, the result should always be a system that works. In practice, this will not work. One can come a long way without designing but at a given time one will get stuck. The system becomes too complex and the dependencies within the system cease to be clear. One can avoid this by creating a design structure that organizes the logic in the system. Good design will avoid many dependencies within a system; this means that changing one part of the system will not affect other parts of the system.","completion":"Software Engineering"}
{"prompt":"Hamilton has a sister Kathryn.She met her first husband, James Cox Hamilton, in the mid-1950s while attending college. They were married on June 15, 1958, the summer after she graduated from Earlham. She briefly taught high school mathematics and French at a public school in Boston, Indiana. The couple then moved to Boston, Massachusetts, where they had a daughter, Lauren, born on November 10, 1959. They divorced in 1967 and Margaret married Dan Lickly two years later.","completion":"Software Engineering"}
{"prompt":"Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.","completion":"Software Engineering"}
{"prompt":"In software engineering, the use of models is an alternative to more common code-based development techniques. A model always conforms to a unique metamodel. One of the currently most active branch of Model Driven Engineering is the approach named model-driven architecture proposed by OMG. This approach is embodied in the Meta Object Facility (MOF) specification.Typical metamodelling specifications proposed by OMG are UML, SysML, SPEM or CWM. ISO has also published the standard metamodel ISO\/IEC 24744. All the languages presented below could be defined as MOF metamodels.","completion":"Software Engineering"}
